_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_not_match_regex_list:basic_negative_test] _

test_case = {'expectation_type': 'expect_column_values_to_not_match_regex_list', 'pk_column': True, 'skip': False, 'test': {'exact...['[12]+', '[45]+']}, 'include_in_gallery': True, 'input': {'column': 'w', 'regex_list': ['[12]+', '[45]+']}, ...}, ...}

    @pytest.mark.order(index=0)
    @pytest.mark.integration
    @pytest.mark.slow  # 12.68s
    def test_case_runner_v3_api(test_case):
        if test_case["skip"]:
            pytest.skip()

        # Note: this should never be done in practice, but we are wiping expectations to reuse batches during testing.
        # test_case["batch"]._initialize_expectations()
        if "parse_strings_as_datetimes" in test_case["test"]["in"]:
            with pytest.deprecated_call():
                evaluate_json_test_v3_api(
                    validator=test_case["validator_with_data"],
                    expectation_type=test_case["expectation_type"],
                    test=test_case["test"],
                    pk_column=test_case["pk_column"],
                )
        else:
>           evaluate_json_test_v3_api(
                validator=test_case["validator_with_data"],
                expectation_type=test_case["expectation_type"],
                test=test_case["test"],
                pk_column=test_case["pk_column"],
            )

tests/test_definitions/test_expectations_v3_api.py:418:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
great_expectations/self_check/util.py:2901: in evaluate_json_test_v3_api
    check_json_test_result(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

test = {'exact_match_out': False, 'in': {'column': 'w', 'regex_list': ['[12]+', '[45]+']}, 'include_in_gallery': True, 'input': {'column': 'w', 'regex_list': ['[12]+', '[45]+']}, ...}
result = {'exception_info': {'exception_message': None, 'exception_traceback': None, 'raised_exception': False}, 'expectation_c...ue': '222'}, {'count': 1, 'value': '321'}, {'count': 1, 'value': '444'}, {'count': 1, 'value': '456'}, ...], ...}, ...}
data_asset = <great_expectations.execution_engine.pandas_batch_data.PandasBatchData object at 0x7f95671cf940>
pk_column = True

    def check_json_test_result(  # noqa: C901 - 52
        test, result, data_asset=None, pk_column=False
    ) -> None:

        # check for id_pk results in cases where pk_column is true and unexpected_index_list already exists
        # this will work for testing since result_format is COMPLETE
        if pk_column:
            if not result["success"]:
                if "unexpected_index_list" in result["result"]:
                    assert "unexpected_index_query" in result["result"]

        if "unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )
            elif "unexpected_list" in test["output"]:
                (
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )

        if "partial_unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "partial_unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )
            elif "partial_unexpected_list" in test["output"]:
                (
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )

        # Determine if np.allclose(..) might be needed for float comparison
        try_allclose = False
        if "observed_value" in test["output"]:
            if RX_FLOAT.match(repr(test["output"]["observed_value"])):
                try_allclose = True

        # Check results
        if test["exact_match_out"] is True:
            if "result" in result and "observed_value" in result["result"]:
                if isinstance(result["result"]["observed_value"], (np.floating, float)):
                    assert np.allclose(
                        result["result"]["observed_value"],
                        expectationValidationResultSchema.load(test["output"])["result"][
                            "observed_value"
                        ],
                        rtol=RTOL,
                        atol=ATOL,
                    ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {expectationValidationResultSchema.load(test['output'])['result']['observed_value']}"
                else:
                    assert result == expectationValidationResultSchema.load(
                        test["output"]
                    ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
            else:
                assert result == expectationValidationResultSchema.load(
                    test["output"]
                ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
        else:
            # Convert result to json since our tests are reading from json so cannot easily contain richer types (e.g. NaN)
            # NOTE - 20191031 - JPC - we may eventually want to change these tests as we update our view on how
            # representations, serializations, and objects should interact and how much of that is shown to the user.
            result = result.to_json_dict()
            for key, value in test["output"].items():
                # Apply our great expectations-specific test logic

                if key == "success":
                    if isinstance(value, (np.floating, float)):
                        try:
                            assert np.allclose(
                                result["success"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['success']} not np.allclose to {value}"
                        except TypeError:
                            assert (
                                result["success"] == value
                            ), f"{result['success']} != {value}"
                    else:
                        assert result["success"] == value, f"{result['success']} != {value}"

                elif key == "observed_value":
                    if "tolerance" in test:
                        if isinstance(value, dict):
                            assert set(result["result"]["observed_value"].keys()) == set(
                                value.keys()
                            ), f"{set(result['result']['observed_value'].keys())} != {set(value.keys())}"
                            for k, v in value.items():
                                assert np.allclose(
                                    result["result"]["observed_value"][k],
                                    v,
                                    rtol=test["tolerance"],
                                )
                        else:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=test["tolerance"],
                            )
                    else:
                        if isinstance(value, dict) and "values" in value:
                            try:
                                assert np.allclose(
                                    result["result"]["observed_value"]["values"],
                                    value["values"],
                                    rtol=RTOL,
                                    atol=ATOL,
                                ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']['values']} not np.allclose to {value['values']}"
                            except TypeError as e:
                                print(e)
                                assert (
                                    result["result"]["observed_value"] == value
                                ), f"{result['result']['observed_value']} != {value}"
                        elif try_allclose:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {value}"
                        else:
                            assert (
                                result["result"]["observed_value"] == value
                            ), f"{result['result']['observed_value']} != {value}"

                # NOTE: This is a key used ONLY for testing cases where an expectation is legitimately allowed to return
                # any of multiple possible observed_values. expect_column_values_to_be_of_type is one such expectation.
                elif key == "observed_value_list":
                    assert result["result"]["observed_value"] in value

                elif key == "unexpected_index_list":
                    if isinstance(data_asset, (SqlAlchemyDataset, SparkDFDataset)):
                        pass
                    elif isinstance(data_asset, (SqlAlchemyBatchData, SparkDFBatchData)):
                        pass
                    else:
>                       assert (
                            result["result"]["unexpected_index_list"] == value
                        ), f"{result['result']['unexpected_index_list']} != {value}"
E                       AssertionError: [{'w': '111', 'pk_index': 0}, {'w': '222', 'pk_index': 1}, {'w': '123', 'pk_index': 3}, {'w': '321', 'pk_index': 4}, {'w': '444', 'pk_index': 5}, {'w': '456', 'pk_index': 6}, {'w': '654', 'pk_index': 7}, {'w': '555', 'pk_index': 8}] != [0, 1, 3, 4, 5, 6, 7, 8]

great_expectations/self_check/util.py:3075: AssertionError
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 2658.83it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 337.54it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 477.62it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 262.58it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 340.18it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 264.30it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 422.66it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 394.06it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 392.25it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 390.36it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_not_match_regex_list:negative_test_with_more_string-ish_strings] _

test_case = {'expectation_type': 'expect_column_values_to_not_match_regex_list', 'pk_column': True, 'skip': False, 'test': {'exact...mus', 'ovat', 'h.*t']}, 'out': {'success': False, 'unexpected_index_list': [4], 'unexpected_list': ['hat']}, ...}, ...}

    @pytest.mark.order(index=0)
    @pytest.mark.integration
    @pytest.mark.slow  # 12.68s
    def test_case_runner_v3_api(test_case):
        if test_case["skip"]:
            pytest.skip()

        # Note: this should never be done in practice, but we are wiping expectations to reuse batches during testing.
        # test_case["batch"]._initialize_expectations()
        if "parse_strings_as_datetimes" in test_case["test"]["in"]:
            with pytest.deprecated_call():
                evaluate_json_test_v3_api(
                    validator=test_case["validator_with_data"],
                    expectation_type=test_case["expectation_type"],
                    test=test_case["test"],
                    pk_column=test_case["pk_column"],
                )
        else:
>           evaluate_json_test_v3_api(
                validator=test_case["validator_with_data"],
                expectation_type=test_case["expectation_type"],
                test=test_case["test"],
                pk_column=test_case["pk_column"],
            )

tests/test_definitions/test_expectations_v3_api.py:418:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
great_expectations/self_check/util.py:2901: in evaluate_json_test_v3_api
    check_json_test_result(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

test = {'exact_match_out': False, 'in': {'column': 'x', 'regex_list': ['opatomus', 'ovat', 'h.*t']}, 'input': {'column': 'x',...'opatomus', 'ovat', 'h.*t']}, 'out': {'success': False, 'unexpected_index_list': [4], 'unexpected_list': ['hat']}, ...}
result = {'exception_info': {'exception_message': None, 'exception_traceback': None, 'raised_exception': False}, 'expectation_c...10, 'missing_count': 0, 'missing_percent': 0.0, 'partial_unexpected_counts': [{'count': 1, 'value': 'hat'}], ...}, ...}
data_asset = <great_expectations.execution_engine.pandas_batch_data.PandasBatchData object at 0x7f95671cf940>
pk_column = True

    def check_json_test_result(  # noqa: C901 - 52
        test, result, data_asset=None, pk_column=False
    ) -> None:

        # check for id_pk results in cases where pk_column is true and unexpected_index_list already exists
        # this will work for testing since result_format is COMPLETE
        if pk_column:
            if not result["success"]:
                if "unexpected_index_list" in result["result"]:
                    assert "unexpected_index_query" in result["result"]

        if "unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )
            elif "unexpected_list" in test["output"]:
                (
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )

        if "partial_unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "partial_unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )
            elif "partial_unexpected_list" in test["output"]:
                (
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )

        # Determine if np.allclose(..) might be needed for float comparison
        try_allclose = False
        if "observed_value" in test["output"]:
            if RX_FLOAT.match(repr(test["output"]["observed_value"])):
                try_allclose = True

        # Check results
        if test["exact_match_out"] is True:
            if "result" in result and "observed_value" in result["result"]:
                if isinstance(result["result"]["observed_value"], (np.floating, float)):
                    assert np.allclose(
                        result["result"]["observed_value"],
                        expectationValidationResultSchema.load(test["output"])["result"][
                            "observed_value"
                        ],
                        rtol=RTOL,
                        atol=ATOL,
                    ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {expectationValidationResultSchema.load(test['output'])['result']['observed_value']}"
                else:
                    assert result == expectationValidationResultSchema.load(
                        test["output"]
                    ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
            else:
                assert result == expectationValidationResultSchema.load(
                    test["output"]
                ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
        else:
            # Convert result to json since our tests are reading from json so cannot easily contain richer types (e.g. NaN)
            # NOTE - 20191031 - JPC - we may eventually want to change these tests as we update our view on how
            # representations, serializations, and objects should interact and how much of that is shown to the user.
            result = result.to_json_dict()
            for key, value in test["output"].items():
                # Apply our great expectations-specific test logic

                if key == "success":
                    if isinstance(value, (np.floating, float)):
                        try:
                            assert np.allclose(
                                result["success"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['success']} not np.allclose to {value}"
                        except TypeError:
                            assert (
                                result["success"] == value
                            ), f"{result['success']} != {value}"
                    else:
                        assert result["success"] == value, f"{result['success']} != {value}"

                elif key == "observed_value":
                    if "tolerance" in test:
                        if isinstance(value, dict):
                            assert set(result["result"]["observed_value"].keys()) == set(
                                value.keys()
                            ), f"{set(result['result']['observed_value'].keys())} != {set(value.keys())}"
                            for k, v in value.items():
                                assert np.allclose(
                                    result["result"]["observed_value"][k],
                                    v,
                                    rtol=test["tolerance"],
                                )
                        else:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=test["tolerance"],
                            )
                    else:
                        if isinstance(value, dict) and "values" in value:
                            try:
                                assert np.allclose(
                                    result["result"]["observed_value"]["values"],
                                    value["values"],
                                    rtol=RTOL,
                                    atol=ATOL,
                                ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']['values']} not np.allclose to {value['values']}"
                            except TypeError as e:
                                print(e)
                                assert (
                                    result["result"]["observed_value"] == value
                                ), f"{result['result']['observed_value']} != {value}"
                        elif try_allclose:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {value}"
                        else:
                            assert (
                                result["result"]["observed_value"] == value
                            ), f"{result['result']['observed_value']} != {value}"

                # NOTE: This is a key used ONLY for testing cases where an expectation is legitimately allowed to return
                # any of multiple possible observed_values. expect_column_values_to_be_of_type is one such expectation.
                elif key == "observed_value_list":
                    assert result["result"]["observed_value"] in value

                elif key == "unexpected_index_list":
                    if isinstance(data_asset, (SqlAlchemyDataset, SparkDFDataset)):
                        pass
                    elif isinstance(data_asset, (SqlAlchemyBatchData, SparkDFBatchData)):
                        pass
                    else:
>                       assert (
                            result["result"]["unexpected_index_list"] == value
                        ), f"{result['result']['unexpected_index_list']} != {value}"
E                       AssertionError: [{'x': 'hat', 'pk_index': 4}] != [4]

great_expectations/self_check/util.py:3075: AssertionError
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 2777.68it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 330.53it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 468.81it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 263.41it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 330.34it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 260.82it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 413.84it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 374.70it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 372.65it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 370.89it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_increasing:basic_negative_test] _

test_case = {'expectation_type': 'expect_column_values_to_be_increasing', 'pk_column': True, 'skip': False, 'test': {'exact_match_out': False, 'in': {'column': 'w'}, 'include_in_gallery': True, 'input': {'column': 'w'}, ...}, ...}

    @pytest.mark.order(index=0)
    @pytest.mark.integration
    @pytest.mark.slow  # 12.68s
    def test_case_runner_v3_api(test_case):
        if test_case["skip"]:
            pytest.skip()

        # Note: this should never be done in practice, but we are wiping expectations to reuse batches during testing.
        # test_case["batch"]._initialize_expectations()
        if "parse_strings_as_datetimes" in test_case["test"]["in"]:
            with pytest.deprecated_call():
                evaluate_json_test_v3_api(
                    validator=test_case["validator_with_data"],
                    expectation_type=test_case["expectation_type"],
                    test=test_case["test"],
                    pk_column=test_case["pk_column"],
                )
        else:
>           evaluate_json_test_v3_api(
                validator=test_case["validator_with_data"],
                expectation_type=test_case["expectation_type"],
                test=test_case["test"],
                pk_column=test_case["pk_column"],
            )

tests/test_definitions/test_expectations_v3_api.py:418:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
great_expectations/self_check/util.py:2901: in evaluate_json_test_v3_api
    check_json_test_result(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

test = {'exact_match_out': False, 'in': {'column': 'w'}, 'include_in_gallery': True, 'input': {'column': 'w'}, ...}
result = {'exception_info': {'exception_message': None, 'exception_traceback': None, 'raised_exception': False}, 'expectation_c...': [{'count': 1, 'value': 1}, {'count': 1, 'value': 2}, {'count': 1, 'value': 3}, {'count': 1, 'value': 4}], ...}, ...}
data_asset = <great_expectations.execution_engine.pandas_batch_data.PandasBatchData object at 0x7f95672dfa30>
pk_column = True

    def check_json_test_result(  # noqa: C901 - 52
        test, result, data_asset=None, pk_column=False
    ) -> None:

        # check for id_pk results in cases where pk_column is true and unexpected_index_list already exists
        # this will work for testing since result_format is COMPLETE
        if pk_column:
            if not result["success"]:
                if "unexpected_index_list" in result["result"]:
                    assert "unexpected_index_query" in result["result"]

        if "unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )
            elif "unexpected_list" in test["output"]:
                (
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )

        if "partial_unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "partial_unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )
            elif "partial_unexpected_list" in test["output"]:
                (
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )

        # Determine if np.allclose(..) might be needed for float comparison
        try_allclose = False
        if "observed_value" in test["output"]:
            if RX_FLOAT.match(repr(test["output"]["observed_value"])):
                try_allclose = True

        # Check results
        if test["exact_match_out"] is True:
            if "result" in result and "observed_value" in result["result"]:
                if isinstance(result["result"]["observed_value"], (np.floating, float)):
                    assert np.allclose(
                        result["result"]["observed_value"],
                        expectationValidationResultSchema.load(test["output"])["result"][
                            "observed_value"
                        ],
                        rtol=RTOL,
                        atol=ATOL,
                    ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {expectationValidationResultSchema.load(test['output'])['result']['observed_value']}"
                else:
                    assert result == expectationValidationResultSchema.load(
                        test["output"]
                    ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
            else:
                assert result == expectationValidationResultSchema.load(
                    test["output"]
                ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
        else:
            # Convert result to json since our tests are reading from json so cannot easily contain richer types (e.g. NaN)
            # NOTE - 20191031 - JPC - we may eventually want to change these tests as we update our view on how
            # representations, serializations, and objects should interact and how much of that is shown to the user.
            result = result.to_json_dict()
            for key, value in test["output"].items():
                # Apply our great expectations-specific test logic

                if key == "success":
                    if isinstance(value, (np.floating, float)):
                        try:
                            assert np.allclose(
                                result["success"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['success']} not np.allclose to {value}"
                        except TypeError:
                            assert (
                                result["success"] == value
                            ), f"{result['success']} != {value}"
                    else:
                        assert result["success"] == value, f"{result['success']} != {value}"

                elif key == "observed_value":
                    if "tolerance" in test:
                        if isinstance(value, dict):
                            assert set(result["result"]["observed_value"].keys()) == set(
                                value.keys()
                            ), f"{set(result['result']['observed_value'].keys())} != {set(value.keys())}"
                            for k, v in value.items():
                                assert np.allclose(
                                    result["result"]["observed_value"][k],
                                    v,
                                    rtol=test["tolerance"],
                                )
                        else:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=test["tolerance"],
                            )
                    else:
                        if isinstance(value, dict) and "values" in value:
                            try:
                                assert np.allclose(
                                    result["result"]["observed_value"]["values"],
                                    value["values"],
                                    rtol=RTOL,
                                    atol=ATOL,
                                ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']['values']} not np.allclose to {value['values']}"
                            except TypeError as e:
                                print(e)
                                assert (
                                    result["result"]["observed_value"] == value
                                ), f"{result['result']['observed_value']} != {value}"
                        elif try_allclose:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {value}"
                        else:
                            assert (
                                result["result"]["observed_value"] == value
                            ), f"{result['result']['observed_value']} != {value}"

                # NOTE: This is a key used ONLY for testing cases where an expectation is legitimately allowed to return
                # any of multiple possible observed_values. expect_column_values_to_be_of_type is one such expectation.
                elif key == "observed_value_list":
                    assert result["result"]["observed_value"] in value

                elif key == "unexpected_index_list":
                    if isinstance(data_asset, (SqlAlchemyDataset, SparkDFDataset)):
                        pass
                    elif isinstance(data_asset, (SqlAlchemyBatchData, SparkDFBatchData)):
                        pass
                    else:
>                       assert (
                            result["result"]["unexpected_index_list"] == value
                        ), f"{result['result']['unexpected_index_list']} != {value}"
E                       AssertionError: [{'w': 4, 'pk_index': 6}, {'w': 3, 'pk_index': 7}, {'w': 2, 'pk_index': 8}, {'w': 1, 'pk_index': 9}] != [6, 7, 8, 9]

great_expectations/self_check/util.py:3075: AssertionError
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 1968.69it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 73.07it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 104.28it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 80.73it/s]alculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 117.12it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 104.74it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 184.04it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 177.82it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 177.43it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 177.02it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_null:negative_mostly] _

test_case = {'expectation_type': 'expect_column_values_to_be_null', 'pk_column': True, 'skip': False, 'test': {'exact_match_out': ...hree_null', 'mostly': 0.8}, 'out': {'success': False, 'unexpected_index_list': [3], 'unexpected_list': [5]}, ...}, ...}

    @pytest.mark.order(index=0)
    @pytest.mark.integration
    @pytest.mark.slow  # 12.68s
    def test_case_runner_v3_api(test_case):
        if test_case["skip"]:
            pytest.skip()

        # Note: this should never be done in practice, but we are wiping expectations to reuse batches during testing.
        # test_case["batch"]._initialize_expectations()
        if "parse_strings_as_datetimes" in test_case["test"]["in"]:
            with pytest.deprecated_call():
                evaluate_json_test_v3_api(
                    validator=test_case["validator_with_data"],
                    expectation_type=test_case["expectation_type"],
                    test=test_case["test"],
                    pk_column=test_case["pk_column"],
                )
        else:
>           evaluate_json_test_v3_api(
                validator=test_case["validator_with_data"],
                expectation_type=test_case["expectation_type"],
                test=test_case["test"],
                pk_column=test_case["pk_column"],
            )

tests/test_definitions/test_expectations_v3_api.py:418:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
great_expectations/self_check/util.py:2901: in evaluate_json_test_v3_api
    check_json_test_result(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

test = {'exact_match_out': False, 'in': {'column': 'three_null', 'mostly': 0.8}, 'input': {'column': 'three_null', 'mostly': 0.8}, 'out': {'success': False, 'unexpected_index_list': [3], 'unexpected_list': [5]}, ...}
result = {'exception_info': {'exception_message': None, 'exception_traceback': None, 'raised_exception': False}, 'expectation_c...}], 'partial_unexpected_index_list': [{'pk_index': 3, 'three_null': 5.0}], 'partial_unexpected_list': [5.0], ...}, ...}
data_asset = <great_expectations.execution_engine.pandas_batch_data.PandasBatchData object at 0x7f95671abb20>
pk_column = True

    def check_json_test_result(  # noqa: C901 - 52
        test, result, data_asset=None, pk_column=False
    ) -> None:

        # check for id_pk results in cases where pk_column is true and unexpected_index_list already exists
        # this will work for testing since result_format is COMPLETE
        if pk_column:
            if not result["success"]:
                if "unexpected_index_list" in result["result"]:
                    assert "unexpected_index_query" in result["result"]

        if "unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )
            elif "unexpected_list" in test["output"]:
                (
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )

        if "partial_unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "partial_unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )
            elif "partial_unexpected_list" in test["output"]:
                (
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )

        # Determine if np.allclose(..) might be needed for float comparison
        try_allclose = False
        if "observed_value" in test["output"]:
            if RX_FLOAT.match(repr(test["output"]["observed_value"])):
                try_allclose = True

        # Check results
        if test["exact_match_out"] is True:
            if "result" in result and "observed_value" in result["result"]:
                if isinstance(result["result"]["observed_value"], (np.floating, float)):
                    assert np.allclose(
                        result["result"]["observed_value"],
                        expectationValidationResultSchema.load(test["output"])["result"][
                            "observed_value"
                        ],
                        rtol=RTOL,
                        atol=ATOL,
                    ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {expectationValidationResultSchema.load(test['output'])['result']['observed_value']}"
                else:
                    assert result == expectationValidationResultSchema.load(
                        test["output"]
                    ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
            else:
                assert result == expectationValidationResultSchema.load(
                    test["output"]
                ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
        else:
            # Convert result to json since our tests are reading from json so cannot easily contain richer types (e.g. NaN)
            # NOTE - 20191031 - JPC - we may eventually want to change these tests as we update our view on how
            # representations, serializations, and objects should interact and how much of that is shown to the user.
            result = result.to_json_dict()
            for key, value in test["output"].items():
                # Apply our great expectations-specific test logic

                if key == "success":
                    if isinstance(value, (np.floating, float)):
                        try:
                            assert np.allclose(
                                result["success"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['success']} not np.allclose to {value}"
                        except TypeError:
                            assert (
                                result["success"] == value
                            ), f"{result['success']} != {value}"
                    else:
                        assert result["success"] == value, f"{result['success']} != {value}"

                elif key == "observed_value":
                    if "tolerance" in test:
                        if isinstance(value, dict):
                            assert set(result["result"]["observed_value"].keys()) == set(
                                value.keys()
                            ), f"{set(result['result']['observed_value'].keys())} != {set(value.keys())}"
                            for k, v in value.items():
                                assert np.allclose(
                                    result["result"]["observed_value"][k],
                                    v,
                                    rtol=test["tolerance"],
                                )
                        else:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=test["tolerance"],
                            )
                    else:
                        if isinstance(value, dict) and "values" in value:
                            try:
                                assert np.allclose(
                                    result["result"]["observed_value"]["values"],
                                    value["values"],
                                    rtol=RTOL,
                                    atol=ATOL,
                                ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']['values']} not np.allclose to {value['values']}"
                            except TypeError as e:
                                print(e)
                                assert (
                                    result["result"]["observed_value"] == value
                                ), f"{result['result']['observed_value']} != {value}"
                        elif try_allclose:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {value}"
                        else:
                            assert (
                                result["result"]["observed_value"] == value
                            ), f"{result['result']['observed_value']} != {value}"

                # NOTE: This is a key used ONLY for testing cases where an expectation is legitimately allowed to return
                # any of multiple possible observed_values. expect_column_values_to_be_of_type is one such expectation.
                elif key == "observed_value_list":
                    assert result["result"]["observed_value"] in value

                elif key == "unexpected_index_list":
                    if isinstance(data_asset, (SqlAlchemyDataset, SparkDFDataset)):
                        pass
                    elif isinstance(data_asset, (SqlAlchemyBatchData, SparkDFBatchData)):
                        pass
                    else:
>                       assert (
                            result["result"]["unexpected_index_list"] == value
                        ), f"{result['result']['unexpected_index_list']} != {value}"
E                       AssertionError: [{'three_null': 5.0, 'pk_index': 3}] != [3]

great_expectations/self_check/util.py:3075: AssertionError
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/8 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/8 [00:00<?, ?it/s]Calculating Metrics:  25%|██▌       | 2/8 [00:00<00:00, 3885.41it/s]Calculating Metrics:  25%|██▌       | 2/8 [00:00<00:00, 615.00it/s]Calculating Metrics:  38%|███▊      | 3/8 [00:00<00:00, 851.58it/s]Calculating Metrics:  38%|███▊      | 3/8 [00:00<00:00, 489.32it/s]Calculating Metrics:  50%|█████     | 4/8 [00:00<00:00, 592.27it/s]Calculating Metrics:  50%|█████     | 4/8 [00:00<00:00, 439.32it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 640.11it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 583.82it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 580.00it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 573.39it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_null:positive_case_with_mostly_and_no_unexpected_values] _

test_case = {'expectation_type': 'expect_column_values_to_be_null', 'pk_column': True, 'skip': False, 'test': {'exact_match_out': ...t': {'success': True, 'unexpected_index_list': [{'pk_index': 3, 'three_null': 5.0}], 'unexpected_list': []}, ...}, ...}

    @pytest.mark.order(index=0)
    @pytest.mark.integration
    @pytest.mark.slow  # 12.68s
    def test_case_runner_v3_api(test_case):
        if test_case["skip"]:
            pytest.skip()

        # Note: this should never be done in practice, but we are wiping expectations to reuse batches during testing.
        # test_case["batch"]._initialize_expectations()
        if "parse_strings_as_datetimes" in test_case["test"]["in"]:
            with pytest.deprecated_call():
                evaluate_json_test_v3_api(
                    validator=test_case["validator_with_data"],
                    expectation_type=test_case["expectation_type"],
                    test=test_case["test"],
                    pk_column=test_case["pk_column"],
                )
        else:
>           evaluate_json_test_v3_api(
                validator=test_case["validator_with_data"],
                expectation_type=test_case["expectation_type"],
                test=test_case["test"],
                pk_column=test_case["pk_column"],
            )

tests/test_definitions/test_expectations_v3_api.py:418:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
great_expectations/self_check/util.py:2901: in evaluate_json_test_v3_api
    check_json_test_result(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

test = {'exact_match_out': False, 'in': {'column': 'all_null', 'mostly': 0.9}, 'input': {'column': 'all_null', 'mostly': 0.9}, 'out': {'success': True, 'unexpected_index_list': [{'pk_index': 3, 'three_null': 5.0}], 'unexpected_list': []}, ...}
result = {'exception_info': {'exception_message': None, 'exception_traceback': None, 'raised_exception': False}, 'expectation_c...nt': 4, 'partial_unexpected_counts': [], 'partial_unexpected_index_list': [], 'partial_unexpected_list': [], ...}, ...}
data_asset = <great_expectations.execution_engine.pandas_batch_data.PandasBatchData object at 0x7f95671abb20>
pk_column = True

    def check_json_test_result(  # noqa: C901 - 52
        test, result, data_asset=None, pk_column=False
    ) -> None:

        # check for id_pk results in cases where pk_column is true and unexpected_index_list already exists
        # this will work for testing since result_format is COMPLETE
        if pk_column:
            if not result["success"]:
                if "unexpected_index_list" in result["result"]:
                    assert "unexpected_index_query" in result["result"]

        if "unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )
            elif "unexpected_list" in test["output"]:
                (
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )

        if "partial_unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "partial_unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )
            elif "partial_unexpected_list" in test["output"]:
                (
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )

        # Determine if np.allclose(..) might be needed for float comparison
        try_allclose = False
        if "observed_value" in test["output"]:
            if RX_FLOAT.match(repr(test["output"]["observed_value"])):
                try_allclose = True

        # Check results
        if test["exact_match_out"] is True:
            if "result" in result and "observed_value" in result["result"]:
                if isinstance(result["result"]["observed_value"], (np.floating, float)):
                    assert np.allclose(
                        result["result"]["observed_value"],
                        expectationValidationResultSchema.load(test["output"])["result"][
                            "observed_value"
                        ],
                        rtol=RTOL,
                        atol=ATOL,
                    ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {expectationValidationResultSchema.load(test['output'])['result']['observed_value']}"
                else:
                    assert result == expectationValidationResultSchema.load(
                        test["output"]
                    ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
            else:
                assert result == expectationValidationResultSchema.load(
                    test["output"]
                ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
        else:
            # Convert result to json since our tests are reading from json so cannot easily contain richer types (e.g. NaN)
            # NOTE - 20191031 - JPC - we may eventually want to change these tests as we update our view on how
            # representations, serializations, and objects should interact and how much of that is shown to the user.
            result = result.to_json_dict()
            for key, value in test["output"].items():
                # Apply our great expectations-specific test logic

                if key == "success":
                    if isinstance(value, (np.floating, float)):
                        try:
                            assert np.allclose(
                                result["success"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['success']} not np.allclose to {value}"
                        except TypeError:
                            assert (
                                result["success"] == value
                            ), f"{result['success']} != {value}"
                    else:
                        assert result["success"] == value, f"{result['success']} != {value}"

                elif key == "observed_value":
                    if "tolerance" in test:
                        if isinstance(value, dict):
                            assert set(result["result"]["observed_value"].keys()) == set(
                                value.keys()
                            ), f"{set(result['result']['observed_value'].keys())} != {set(value.keys())}"
                            for k, v in value.items():
                                assert np.allclose(
                                    result["result"]["observed_value"][k],
                                    v,
                                    rtol=test["tolerance"],
                                )
                        else:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=test["tolerance"],
                            )
                    else:
                        if isinstance(value, dict) and "values" in value:
                            try:
                                assert np.allclose(
                                    result["result"]["observed_value"]["values"],
                                    value["values"],
                                    rtol=RTOL,
                                    atol=ATOL,
                                ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']['values']} not np.allclose to {value['values']}"
                            except TypeError as e:
                                print(e)
                                assert (
                                    result["result"]["observed_value"] == value
                                ), f"{result['result']['observed_value']} != {value}"
                        elif try_allclose:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {value}"
                        else:
                            assert (
                                result["result"]["observed_value"] == value
                            ), f"{result['result']['observed_value']} != {value}"

                # NOTE: This is a key used ONLY for testing cases where an expectation is legitimately allowed to return
                # any of multiple possible observed_values. expect_column_values_to_be_of_type is one such expectation.
                elif key == "observed_value_list":
                    assert result["result"]["observed_value"] in value

                elif key == "unexpected_index_list":
                    if isinstance(data_asset, (SqlAlchemyDataset, SparkDFDataset)):
                        pass
                    elif isinstance(data_asset, (SqlAlchemyBatchData, SparkDFBatchData)):
                        pass
                    else:
>                       assert (
                            result["result"]["unexpected_index_list"] == value
                        ), f"{result['result']['unexpected_index_list']} != {value}"
E                       AssertionError: [] != [{'three_null': 5.0, 'pk_index': 3}]

FAILED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_not_match_regex_list:basic_negative_test]
FAILED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_not_match_regex_list:negative_test_with_more_string-ish_strings]
FAILED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_increasing:basic_negative_test]
FAILED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_null:negative_mostly]
FAILED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_null:positive_case_with_mostly_and_no_unexpected_values]
=========== 5 failed, 914 passed, 190 skipped, 45 warnings in 52.54s ===========
