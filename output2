=================================== FAILURES ===================================
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_between:3rd_positive_test_with_mostly0] _

test_case = {'expectation_type': 'expect_column_values_to_be_between', 'pk_column': True, 'skip': False, 'test': {'exact_match_out..._value': 1, 'mostly': 0.9}, 'out': {'success': True, 'unexpected_index_list': [9], 'unexpected_list': [10]}, ...}, ...}

    @pytest.mark.order(index=0)
    @pytest.mark.integration
    @pytest.mark.slow  # 12.68s
    def test_case_runner_v3_api(test_case):
        if test_case["skip"]:
            pytest.skip()

        # Note: this should never be done in practice, but we are wiping expectations to reuse batches during testing.
        # test_case["batch"]._initialize_expectations()
        if "parse_strings_as_datetimes" in test_case["test"]["in"]:
            with pytest.deprecated_call():
                evaluate_json_test_v3_api(
                    validator=test_case["validator_with_data"],
                    expectation_type=test_case["expectation_type"],
                    test=test_case["test"],
                    pk_column=test_case["pk_column"],
                )
        else:
>           evaluate_json_test_v3_api(
                validator=test_case["validator_with_data"],
                expectation_type=test_case["expectation_type"],
                test=test_case["test"],
                pk_column=test_case["pk_column"],
            )

tests/test_definitions/test_expectations_v3_api.py:418:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
great_expectations/self_check/util.py:2901: in evaluate_json_test_v3_api
    check_json_test_result(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

test = {'exact_match_out': False, 'in': {'column': 'x', 'max_value': 9, 'min_value': 1, 'mostly': 0.9}, 'input': {'column': '..., 'min_value': 1, 'mostly': 0.9}, 'out': {'success': True, 'unexpected_index_list': [9], 'unexpected_list': [10]}, ...}
result = {'exception_info': {'exception_message': None, 'exception_traceback': None, 'raised_exception': False}, 'expectation_c...': 10, 'missing_count': 0, 'missing_percent': 0.0, 'partial_unexpected_counts': [{'count': 1, 'value': 10}], ...}, ...}
data_asset = <great_expectations.execution_engine.pandas_batch_data.PandasBatchData object at 0x7f78056ce850>
pk_column = True

    def check_json_test_result(  # noqa: C901 - 52
        test, result, data_asset=None, pk_column=False
    ) -> None:

        # check for id_pk results in cases where pk_column is true and unexpected_index_list already exists
        # this will work for testing since result_format is COMPLETE
        if pk_column:
            if not result["success"]:
                if "unexpected_index_list" in result["result"]:
                    assert "unexpected_index_query" in result["result"]

        if "unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )
            elif "unexpected_list" in test["output"]:
                (
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )

        if "partial_unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "partial_unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )
            elif "partial_unexpected_list" in test["output"]:
                (
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )

        # Determine if np.allclose(..) might be needed for float comparison
        try_allclose = False
        if "observed_value" in test["output"]:
            if RX_FLOAT.match(repr(test["output"]["observed_value"])):
                try_allclose = True

        # Check results
        if test["exact_match_out"] is True:
            if "result" in result and "observed_value" in result["result"]:
                if isinstance(result["result"]["observed_value"], (np.floating, float)):
                    assert np.allclose(
                        result["result"]["observed_value"],
                        expectationValidationResultSchema.load(test["output"])["result"][
                            "observed_value"
                        ],
                        rtol=RTOL,
                        atol=ATOL,
                    ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {expectationValidationResultSchema.load(test['output'])['result']['observed_value']}"
                else:
                    assert result == expectationValidationResultSchema.load(
                        test["output"]
                    ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
            else:
                assert result == expectationValidationResultSchema.load(
                    test["output"]
                ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
        else:
            # Convert result to json since our tests are reading from json so cannot easily contain richer types (e.g. NaN)
            # NOTE - 20191031 - JPC - we may eventually want to change these tests as we update our view on how
            # representations, serializations, and objects should interact and how much of that is shown to the user.
            result = result.to_json_dict()
            for key, value in test["output"].items():
                # Apply our great expectations-specific test logic

                if key == "success":
                    if isinstance(value, (np.floating, float)):
                        try:
                            assert np.allclose(
                                result["success"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['success']} not np.allclose to {value}"
                        except TypeError:
                            assert (
                                result["success"] == value
                            ), f"{result['success']} != {value}"
                    else:
                        assert result["success"] == value, f"{result['success']} != {value}"

                elif key == "observed_value":
                    if "tolerance" in test:
                        if isinstance(value, dict):
                            assert set(result["result"]["observed_value"].keys()) == set(
                                value.keys()
                            ), f"{set(result['result']['observed_value'].keys())} != {set(value.keys())}"
                            for k, v in value.items():
                                assert np.allclose(
                                    result["result"]["observed_value"][k],
                                    v,
                                    rtol=test["tolerance"],
                                )
                        else:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=test["tolerance"],
                            )
                    else:
                        if isinstance(value, dict) and "values" in value:
                            try:
                                assert np.allclose(
                                    result["result"]["observed_value"]["values"],
                                    value["values"],
                                    rtol=RTOL,
                                    atol=ATOL,
                                ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']['values']} not np.allclose to {value['values']}"
                            except TypeError as e:
                                print(e)
                                assert (
                                    result["result"]["observed_value"] == value
                                ), f"{result['result']['observed_value']} != {value}"
                        elif try_allclose:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {value}"
                        else:
                            assert (
                                result["result"]["observed_value"] == value
                            ), f"{result['result']['observed_value']} != {value}"

                # NOTE: This is a key used ONLY for testing cases where an expectation is legitimately allowed to return
                # any of multiple possible observed_values. expect_column_values_to_be_of_type is one such expectation.
                elif key == "observed_value_list":
                    assert result["result"]["observed_value"] in value

                elif key == "unexpected_index_list":
                    if isinstance(data_asset, (SqlAlchemyDataset, SparkDFDataset)):
                        pass
                    elif isinstance(data_asset, (SqlAlchemyBatchData, SparkDFBatchData)):
                        pass
                    else:
>                       assert (
                            result["result"]["unexpected_index_list"] == value
                        ), f"{result['result']['unexpected_index_list']} != {value}"
E                       AssertionError: [{'x': 10, 'pk_index': 9}] != [9]

great_expectations/self_check/util.py:3075: AssertionError
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 3816.47it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 368.58it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 526.13it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 286.26it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 405.45it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 311.35it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 482.18it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 445.87it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 443.77it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 441.71it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_between:negative_test_with_mostly0] _

test_case = {'expectation_type': 'expect_column_values_to_be_between', 'pk_column': True, 'skip': False, 'test': {'exact_match_out...': 3, 'mostly': 0.9}, 'out': {'success': False, 'unexpected_index_list': [0, 1], 'unexpected_list': [1, 2]}, ...}, ...}

    @pytest.mark.order(index=0)
    @pytest.mark.integration
    @pytest.mark.slow  # 12.68s
    def test_case_runner_v3_api(test_case):
        if test_case["skip"]:
            pytest.skip()

        # Note: this should never be done in practice, but we are wiping expectations to reuse batches during testing.
        # test_case["batch"]._initialize_expectations()
        if "parse_strings_as_datetimes" in test_case["test"]["in"]:
            with pytest.deprecated_call():
                evaluate_json_test_v3_api(
                    validator=test_case["validator_with_data"],
                    expectation_type=test_case["expectation_type"],
                    test=test_case["test"],
                    pk_column=test_case["pk_column"],
                )
        else:
>           evaluate_json_test_v3_api(
                validator=test_case["validator_with_data"],
                expectation_type=test_case["expectation_type"],
                test=test_case["test"],
                pk_column=test_case["pk_column"],
            )

tests/test_definitions/test_expectations_v3_api.py:418:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
great_expectations/self_check/util.py:2901: in evaluate_json_test_v3_api
    check_json_test_result(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

test = {'exact_match_out': False, 'in': {'column': 'x', 'max_value': 10, 'min_value': 3, 'mostly': 0.9}, 'input': {'column': ..._value': 3, 'mostly': 0.9}, 'out': {'success': False, 'unexpected_index_list': [0, 1], 'unexpected_list': [1, 2]}, ...}
result = {'exception_info': {'exception_message': None, 'exception_traceback': None, 'raised_exception': False}, 'expectation_c..., 'missing_percent': 0.0, 'partial_unexpected_counts': [{'count': 1, 'value': 1}, {'count': 1, 'value': 2}], ...}, ...}
data_asset = <great_expectations.execution_engine.pandas_batch_data.PandasBatchData object at 0x7f78056ce850>
pk_column = True

    def check_json_test_result(  # noqa: C901 - 52
        test, result, data_asset=None, pk_column=False
    ) -> None:

        # check for id_pk results in cases where pk_column is true and unexpected_index_list already exists
        # this will work for testing since result_format is COMPLETE
        if pk_column:
            if not result["success"]:
                if "unexpected_index_list" in result["result"]:
                    assert "unexpected_index_query" in result["result"]

        if "unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )
            elif "unexpected_list" in test["output"]:
                (
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )

        if "partial_unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "partial_unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )
            elif "partial_unexpected_list" in test["output"]:
                (
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )

        # Determine if np.allclose(..) might be needed for float comparison
        try_allclose = False
        if "observed_value" in test["output"]:
            if RX_FLOAT.match(repr(test["output"]["observed_value"])):
                try_allclose = True

        # Check results
        if test["exact_match_out"] is True:
            if "result" in result and "observed_value" in result["result"]:
                if isinstance(result["result"]["observed_value"], (np.floating, float)):
                    assert np.allclose(
                        result["result"]["observed_value"],
                        expectationValidationResultSchema.load(test["output"])["result"][
                            "observed_value"
                        ],
                        rtol=RTOL,
                        atol=ATOL,
                    ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {expectationValidationResultSchema.load(test['output'])['result']['observed_value']}"
                else:
                    assert result == expectationValidationResultSchema.load(
                        test["output"]
                    ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
            else:
                assert result == expectationValidationResultSchema.load(
                    test["output"]
                ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
        else:
            # Convert result to json since our tests are reading from json so cannot easily contain richer types (e.g. NaN)
            # NOTE - 20191031 - JPC - we may eventually want to change these tests as we update our view on how
            # representations, serializations, and objects should interact and how much of that is shown to the user.
            result = result.to_json_dict()
            for key, value in test["output"].items():
                # Apply our great expectations-specific test logic

                if key == "success":
                    if isinstance(value, (np.floating, float)):
                        try:
                            assert np.allclose(
                                result["success"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['success']} not np.allclose to {value}"
                        except TypeError:
                            assert (
                                result["success"] == value
                            ), f"{result['success']} != {value}"
                    else:
                        assert result["success"] == value, f"{result['success']} != {value}"

                elif key == "observed_value":
                    if "tolerance" in test:
                        if isinstance(value, dict):
                            assert set(result["result"]["observed_value"].keys()) == set(
                                value.keys()
                            ), f"{set(result['result']['observed_value'].keys())} != {set(value.keys())}"
                            for k, v in value.items():
                                assert np.allclose(
                                    result["result"]["observed_value"][k],
                                    v,
                                    rtol=test["tolerance"],
                                )
                        else:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=test["tolerance"],
                            )
                    else:
                        if isinstance(value, dict) and "values" in value:
                            try:
                                assert np.allclose(
                                    result["result"]["observed_value"]["values"],
                                    value["values"],
                                    rtol=RTOL,
                                    atol=ATOL,
                                ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']['values']} not np.allclose to {value['values']}"
                            except TypeError as e:
                                print(e)
                                assert (
                                    result["result"]["observed_value"] == value
                                ), f"{result['result']['observed_value']} != {value}"
                        elif try_allclose:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {value}"
                        else:
                            assert (
                                result["result"]["observed_value"] == value
                            ), f"{result['result']['observed_value']} != {value}"

                # NOTE: This is a key used ONLY for testing cases where an expectation is legitimately allowed to return
                # any of multiple possible observed_values. expect_column_values_to_be_of_type is one such expectation.
                elif key == "observed_value_list":
                    assert result["result"]["observed_value"] in value

                elif key == "unexpected_index_list":
                    if isinstance(data_asset, (SqlAlchemyDataset, SparkDFDataset)):
                        pass
                    elif isinstance(data_asset, (SqlAlchemyBatchData, SparkDFBatchData)):
                        pass
                    else:
>                       assert (
                            result["result"]["unexpected_index_list"] == value
                        ), f"{result['result']['unexpected_index_list']} != {value}"
E                       AssertionError: [{'x': 1, 'pk_index': 0}, {'x': 2, 'pk_index': 1}] != [0, 1]

great_expectations/self_check/util.py:3075: AssertionError
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 3504.01it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 347.34it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 488.13it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 267.07it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 377.48it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 293.22it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 461.51it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 427.01it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 425.10it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 423.26it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_between:negative_test_to_verify_that_the_denominator_for_mostly_works_with_missing_values0] _

test_case = {'expectation_type': 'expect_column_values_to_be_between', 'pk_column': True, 'skip': False, 'test': {'exact_match_out..._value': 1, 'mostly': 0.9}, 'out': {'success': False, 'unexpected_index_list': [4], 'unexpected_list': [5]}, ...}, ...}

    @pytest.mark.order(index=0)
    @pytest.mark.integration
    @pytest.mark.slow  # 12.68s
    def test_case_runner_v3_api(test_case):
        if test_case["skip"]:
            pytest.skip()

        # Note: this should never be done in practice, but we are wiping expectations to reuse batches during testing.
        # test_case["batch"]._initialize_expectations()
        if "parse_strings_as_datetimes" in test_case["test"]["in"]:
            with pytest.deprecated_call():
                evaluate_json_test_v3_api(
                    validator=test_case["validator_with_data"],
                    expectation_type=test_case["expectation_type"],
                    test=test_case["test"],
                    pk_column=test_case["pk_column"],
                )
        else:
>           evaluate_json_test_v3_api(
                validator=test_case["validator_with_data"],
                expectation_type=test_case["expectation_type"],
                test=test_case["test"],
                pk_column=test_case["pk_column"],
            )

tests/test_definitions/test_expectations_v3_api.py:418:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
great_expectations/self_check/util.py:2901: in evaluate_json_test_v3_api
    check_json_test_result(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

test = {'exact_match_out': False, 'in': {'column': 'z', 'max_value': 4, 'min_value': 1, 'mostly': 0.9}, 'input': {'column': '..., 'min_value': 1, 'mostly': 0.9}, 'out': {'success': False, 'unexpected_index_list': [4], 'unexpected_list': [5]}, ...}
result = {'exception_info': {'exception_message': None, 'exception_traceback': None, 'raised_exception': False}, 'expectation_c...': 10, 'missing_count': 5, 'missing_percent': 50.0, 'partial_unexpected_counts': [{'count': 1, 'value': 5}], ...}, ...}
data_asset = <great_expectations.execution_engine.pandas_batch_data.PandasBatchData object at 0x7f78056ce850>
pk_column = True

    def check_json_test_result(  # noqa: C901 - 52
        test, result, data_asset=None, pk_column=False
    ) -> None:

        # check for id_pk results in cases where pk_column is true and unexpected_index_list already exists
        # this will work for testing since result_format is COMPLETE
        if pk_column:
            if not result["success"]:
                if "unexpected_index_list" in result["result"]:
                    assert "unexpected_index_query" in result["result"]

        if "unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )
            elif "unexpected_list" in test["output"]:
                (
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )

        if "partial_unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "partial_unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )
            elif "partial_unexpected_list" in test["output"]:
                (
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )

        # Determine if np.allclose(..) might be needed for float comparison
        try_allclose = False
        if "observed_value" in test["output"]:
            if RX_FLOAT.match(repr(test["output"]["observed_value"])):
                try_allclose = True

        # Check results
        if test["exact_match_out"] is True:
            if "result" in result and "observed_value" in result["result"]:
                if isinstance(result["result"]["observed_value"], (np.floating, float)):
                    assert np.allclose(
                        result["result"]["observed_value"],
                        expectationValidationResultSchema.load(test["output"])["result"][
                            "observed_value"
                        ],
                        rtol=RTOL,
                        atol=ATOL,
                    ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {expectationValidationResultSchema.load(test['output'])['result']['observed_value']}"
                else:
                    assert result == expectationValidationResultSchema.load(
                        test["output"]
                    ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
            else:
                assert result == expectationValidationResultSchema.load(
                    test["output"]
                ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
        else:
            # Convert result to json since our tests are reading from json so cannot easily contain richer types (e.g. NaN)
            # NOTE - 20191031 - JPC - we may eventually want to change these tests as we update our view on how
            # representations, serializations, and objects should interact and how much of that is shown to the user.
            result = result.to_json_dict()
            for key, value in test["output"].items():
                # Apply our great expectations-specific test logic

                if key == "success":
                    if isinstance(value, (np.floating, float)):
                        try:
                            assert np.allclose(
                                result["success"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['success']} not np.allclose to {value}"
                        except TypeError:
                            assert (
                                result["success"] == value
                            ), f"{result['success']} != {value}"
                    else:
                        assert result["success"] == value, f"{result['success']} != {value}"

                elif key == "observed_value":
                    if "tolerance" in test:
                        if isinstance(value, dict):
                            assert set(result["result"]["observed_value"].keys()) == set(
                                value.keys()
                            ), f"{set(result['result']['observed_value'].keys())} != {set(value.keys())}"
                            for k, v in value.items():
                                assert np.allclose(
                                    result["result"]["observed_value"][k],
                                    v,
                                    rtol=test["tolerance"],
                                )
                        else:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=test["tolerance"],
                            )
                    else:
                        if isinstance(value, dict) and "values" in value:
                            try:
                                assert np.allclose(
                                    result["result"]["observed_value"]["values"],
                                    value["values"],
                                    rtol=RTOL,
                                    atol=ATOL,
                                ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']['values']} not np.allclose to {value['values']}"
                            except TypeError as e:
                                print(e)
                                assert (
                                    result["result"]["observed_value"] == value
                                ), f"{result['result']['observed_value']} != {value}"
                        elif try_allclose:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {value}"
                        else:
                            assert (
                                result["result"]["observed_value"] == value
                            ), f"{result['result']['observed_value']} != {value}"

                # NOTE: This is a key used ONLY for testing cases where an expectation is legitimately allowed to return
                # any of multiple possible observed_values. expect_column_values_to_be_of_type is one such expectation.
                elif key == "observed_value_list":
                    assert result["result"]["observed_value"] in value

                elif key == "unexpected_index_list":
                    if isinstance(data_asset, (SqlAlchemyDataset, SparkDFDataset)):
                        pass
                    elif isinstance(data_asset, (SqlAlchemyBatchData, SparkDFBatchData)):
                        pass
                    else:
>                       assert (
                            result["result"]["unexpected_index_list"] == value
                        ), f"{result['result']['unexpected_index_list']} != {value}"
E                       AssertionError: [{'z': 5, 'pk_index': 4}] != [4]

great_expectations/self_check/util.py:3075: AssertionError
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 4013.69it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 374.17it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 531.71it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 288.68it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 407.20it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 309.75it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 457.47it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 425.02it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 423.38it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 421.60it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_between:positive_test_to_verify_that_the_denominator_for_mostly_works_with_missing_values0] _

test_case = {'expectation_type': 'expect_column_values_to_be_between', 'pk_column': True, 'skip': False, 'test': {'exact_match_out...n_value': 1, 'mostly': 0.8}, 'out': {'success': True, 'unexpected_index_list': [4], 'unexpected_list': [5]}, ...}, ...}

    @pytest.mark.order(index=0)
    @pytest.mark.integration
    @pytest.mark.slow  # 12.68s
    def test_case_runner_v3_api(test_case):
        if test_case["skip"]:
            pytest.skip()

        # Note: this should never be done in practice, but we are wiping expectations to reuse batches during testing.
        # test_case["batch"]._initialize_expectations()
        if "parse_strings_as_datetimes" in test_case["test"]["in"]:
            with pytest.deprecated_call():
                evaluate_json_test_v3_api(
                    validator=test_case["validator_with_data"],
                    expectation_type=test_case["expectation_type"],
                    test=test_case["test"],
                    pk_column=test_case["pk_column"],
                )
        else:
>           evaluate_json_test_v3_api(
                validator=test_case["validator_with_data"],
                expectation_type=test_case["expectation_type"],
                test=test_case["test"],
                pk_column=test_case["pk_column"],
            )

tests/test_definitions/test_expectations_v3_api.py:418:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
great_expectations/self_check/util.py:2901: in evaluate_json_test_v3_api
    check_json_test_result(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

test = {'exact_match_out': False, 'in': {'column': 'z', 'max_value': 4, 'min_value': 1, 'mostly': 0.8}, 'input': {'column': '...4, 'min_value': 1, 'mostly': 0.8}, 'out': {'success': True, 'unexpected_index_list': [4], 'unexpected_list': [5]}, ...}
result = {'exception_info': {'exception_message': None, 'exception_traceback': None, 'raised_exception': False}, 'expectation_c...': 10, 'missing_count': 5, 'missing_percent': 50.0, 'partial_unexpected_counts': [{'count': 1, 'value': 5}], ...}, ...}
data_asset = <great_expectations.execution_engine.pandas_batch_data.PandasBatchData object at 0x7f78056ce850>
pk_column = True

    def check_json_test_result(  # noqa: C901 - 52
        test, result, data_asset=None, pk_column=False
    ) -> None:

        # check for id_pk results in cases where pk_column is true and unexpected_index_list already exists
        # this will work for testing since result_format is COMPLETE
        if pk_column:
            if not result["success"]:
                if "unexpected_index_list" in result["result"]:
                    assert "unexpected_index_query" in result["result"]

        if "unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )
            elif "unexpected_list" in test["output"]:
                (
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )

        if "partial_unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "partial_unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )
            elif "partial_unexpected_list" in test["output"]:
                (
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )

        # Determine if np.allclose(..) might be needed for float comparison
        try_allclose = False
        if "observed_value" in test["output"]:
            if RX_FLOAT.match(repr(test["output"]["observed_value"])):
                try_allclose = True

        # Check results
        if test["exact_match_out"] is True:
            if "result" in result and "observed_value" in result["result"]:
                if isinstance(result["result"]["observed_value"], (np.floating, float)):
                    assert np.allclose(
                        result["result"]["observed_value"],
                        expectationValidationResultSchema.load(test["output"])["result"][
                            "observed_value"
                        ],
                        rtol=RTOL,
                        atol=ATOL,
                    ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {expectationValidationResultSchema.load(test['output'])['result']['observed_value']}"
                else:
                    assert result == expectationValidationResultSchema.load(
                        test["output"]
                    ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
            else:
                assert result == expectationValidationResultSchema.load(
                    test["output"]
                ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
        else:
            # Convert result to json since our tests are reading from json so cannot easily contain richer types (e.g. NaN)
            # NOTE - 20191031 - JPC - we may eventually want to change these tests as we update our view on how
            # representations, serializations, and objects should interact and how much of that is shown to the user.
            result = result.to_json_dict()
            for key, value in test["output"].items():
                # Apply our great expectations-specific test logic

                if key == "success":
                    if isinstance(value, (np.floating, float)):
                        try:
                            assert np.allclose(
                                result["success"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['success']} not np.allclose to {value}"
                        except TypeError:
                            assert (
                                result["success"] == value
                            ), f"{result['success']} != {value}"
                    else:
                        assert result["success"] == value, f"{result['success']} != {value}"

                elif key == "observed_value":
                    if "tolerance" in test:
                        if isinstance(value, dict):
                            assert set(result["result"]["observed_value"].keys()) == set(
                                value.keys()
                            ), f"{set(result['result']['observed_value'].keys())} != {set(value.keys())}"
                            for k, v in value.items():
                                assert np.allclose(
                                    result["result"]["observed_value"][k],
                                    v,
                                    rtol=test["tolerance"],
                                )
                        else:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=test["tolerance"],
                            )
                    else:
                        if isinstance(value, dict) and "values" in value:
                            try:
                                assert np.allclose(
                                    result["result"]["observed_value"]["values"],
                                    value["values"],
                                    rtol=RTOL,
                                    atol=ATOL,
                                ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']['values']} not np.allclose to {value['values']}"
                            except TypeError as e:
                                print(e)
                                assert (
                                    result["result"]["observed_value"] == value
                                ), f"{result['result']['observed_value']} != {value}"
                        elif try_allclose:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {value}"
                        else:
                            assert (
                                result["result"]["observed_value"] == value
                            ), f"{result['result']['observed_value']} != {value}"

                # NOTE: This is a key used ONLY for testing cases where an expectation is legitimately allowed to return
                # any of multiple possible observed_values. expect_column_values_to_be_of_type is one such expectation.
                elif key == "observed_value_list":
                    assert result["result"]["observed_value"] in value

                elif key == "unexpected_index_list":
                    if isinstance(data_asset, (SqlAlchemyDataset, SparkDFDataset)):
                        pass
                    elif isinstance(data_asset, (SqlAlchemyBatchData, SparkDFBatchData)):
                        pass
                    else:
>                       assert (
                            result["result"]["unexpected_index_list"] == value
                        ), f"{result['result']['unexpected_index_list']} != {value}"
E                       AssertionError: [{'z': 5, 'pk_index': 4}] != [4]

great_expectations/self_check/util.py:3075: AssertionError
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 3401.71it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 349.34it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 499.50it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 282.72it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 405.68it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 309.31it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 477.23it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 442.85it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 441.13it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 439.21it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_between:test_allow_cross_type_comparisons0] _

test_case = {'expectation_type': 'expect_column_values_to_be_between', 'pk_column': True, 'skip': False, 'test': {'exact_match_out...': 10, 'min_value': 0}, 'out': {'success': False, 'unexpected_index_list': [9], 'unexpected_list': ['abc']}, ...}, ...}

    @pytest.mark.order(index=0)
    @pytest.mark.integration
    @pytest.mark.slow  # 12.68s
    def test_case_runner_v3_api(test_case):
        if test_case["skip"]:
            pytest.skip()

        # Note: this should never be done in practice, but we are wiping expectations to reuse batches during testing.
        # test_case["batch"]._initialize_expectations()
        if "parse_strings_as_datetimes" in test_case["test"]["in"]:
            with pytest.deprecated_call():
                evaluate_json_test_v3_api(
                    validator=test_case["validator_with_data"],
                    expectation_type=test_case["expectation_type"],
                    test=test_case["test"],
                    pk_column=test_case["pk_column"],
                )
        else:
>           evaluate_json_test_v3_api(
                validator=test_case["validator_with_data"],
                expectation_type=test_case["expectation_type"],
                test=test_case["test"],
                pk_column=test_case["pk_column"],
            )

tests/test_definitions/test_expectations_v3_api.py:418:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
great_expectations/self_check/util.py:2901: in evaluate_json_test_v3_api
    check_json_test_result(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

test = {'exact_match_out': False, 'in': {'allow_cross_type_comparisons': True, 'column': 'y', 'max_value': 10, 'min_value': 0..._value': 10, 'min_value': 0}, 'out': {'success': False, 'unexpected_index_list': [9], 'unexpected_list': ['abc']}, ...}
result = {'exception_info': {'exception_message': None, 'exception_traceback': None, 'raised_exception': False}, 'expectation_c...10, 'missing_count': 0, 'missing_percent': 0.0, 'partial_unexpected_counts': [{'count': 1, 'value': 'abc'}], ...}, ...}
data_asset = <great_expectations.execution_engine.pandas_batch_data.PandasBatchData object at 0x7f78056ce850>
pk_column = True

    def check_json_test_result(  # noqa: C901 - 52
        test, result, data_asset=None, pk_column=False
    ) -> None:

        # check for id_pk results in cases where pk_column is true and unexpected_index_list already exists
        # this will work for testing since result_format is COMPLETE
        if pk_column:
            if not result["success"]:
                if "unexpected_index_list" in result["result"]:
                    assert "unexpected_index_query" in result["result"]

        if "unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )
            elif "unexpected_list" in test["output"]:
                (
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )

        if "partial_unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "partial_unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )
            elif "partial_unexpected_list" in test["output"]:
                (
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )

        # Determine if np.allclose(..) might be needed for float comparison
        try_allclose = False
        if "observed_value" in test["output"]:
            if RX_FLOAT.match(repr(test["output"]["observed_value"])):
                try_allclose = True

        # Check results
        if test["exact_match_out"] is True:
            if "result" in result and "observed_value" in result["result"]:
                if isinstance(result["result"]["observed_value"], (np.floating, float)):
                    assert np.allclose(
                        result["result"]["observed_value"],
                        expectationValidationResultSchema.load(test["output"])["result"][
                            "observed_value"
                        ],
                        rtol=RTOL,
                        atol=ATOL,
                    ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {expectationValidationResultSchema.load(test['output'])['result']['observed_value']}"
                else:
                    assert result == expectationValidationResultSchema.load(
                        test["output"]
                    ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
            else:
                assert result == expectationValidationResultSchema.load(
                    test["output"]
                ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
        else:
            # Convert result to json since our tests are reading from json so cannot easily contain richer types (e.g. NaN)
            # NOTE - 20191031 - JPC - we may eventually want to change these tests as we update our view on how
            # representations, serializations, and objects should interact and how much of that is shown to the user.
            result = result.to_json_dict()
            for key, value in test["output"].items():
                # Apply our great expectations-specific test logic

                if key == "success":
                    if isinstance(value, (np.floating, float)):
                        try:
                            assert np.allclose(
                                result["success"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['success']} not np.allclose to {value}"
                        except TypeError:
                            assert (
                                result["success"] == value
                            ), f"{result['success']} != {value}"
                    else:
                        assert result["success"] == value, f"{result['success']} != {value}"

                elif key == "observed_value":
                    if "tolerance" in test:
                        if isinstance(value, dict):
                            assert set(result["result"]["observed_value"].keys()) == set(
                                value.keys()
                            ), f"{set(result['result']['observed_value'].keys())} != {set(value.keys())}"
                            for k, v in value.items():
                                assert np.allclose(
                                    result["result"]["observed_value"][k],
                                    v,
                                    rtol=test["tolerance"],
                                )
                        else:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=test["tolerance"],
                            )
                    else:
                        if isinstance(value, dict) and "values" in value:
                            try:
                                assert np.allclose(
                                    result["result"]["observed_value"]["values"],
                                    value["values"],
                                    rtol=RTOL,
                                    atol=ATOL,
                                ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']['values']} not np.allclose to {value['values']}"
                            except TypeError as e:
                                print(e)
                                assert (
                                    result["result"]["observed_value"] == value
                                ), f"{result['result']['observed_value']} != {value}"
                        elif try_allclose:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {value}"
                        else:
                            assert (
                                result["result"]["observed_value"] == value
                            ), f"{result['result']['observed_value']} != {value}"

                # NOTE: This is a key used ONLY for testing cases where an expectation is legitimately allowed to return
                # any of multiple possible observed_values. expect_column_values_to_be_of_type is one such expectation.
                elif key == "observed_value_list":
                    assert result["result"]["observed_value"] in value

                elif key == "unexpected_index_list":
                    if isinstance(data_asset, (SqlAlchemyDataset, SparkDFDataset)):
                        pass
                    elif isinstance(data_asset, (SqlAlchemyBatchData, SparkDFBatchData)):
                        pass
                    else:
>                       assert (
                            result["result"]["unexpected_index_list"] == value
                        ), f"{result['result']['unexpected_index_list']} != {value}"
E                       AssertionError: [{'y': 'abc', 'pk_index': 9}] != [9]

great_expectations/self_check/util.py:3075: AssertionError
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 3929.09it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 394.55it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 560.01it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 308.04it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 431.07it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 323.04it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 487.68it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 454.38it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 452.65it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 450.65it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_between:test_allow_cross_type_comparisons_again0] _

test_case = {'expectation_type': 'expect_column_values_to_be_between', 'pk_column': True, 'skip': False, 'test': {'exact_match_out... 'unexpected_index_list': [0, 1, 2, 3, 4, 5, ...], 'unexpected_list': ['1', '10', '2', '3', '4', '5', ...]}, ...}, ...}

    @pytest.mark.order(index=0)
    @pytest.mark.integration
    @pytest.mark.slow  # 12.68s
    def test_case_runner_v3_api(test_case):
        if test_case["skip"]:
            pytest.skip()

        # Note: this should never be done in practice, but we are wiping expectations to reuse batches during testing.
        # test_case["batch"]._initialize_expectations()
        if "parse_strings_as_datetimes" in test_case["test"]["in"]:
            with pytest.deprecated_call():
                evaluate_json_test_v3_api(
                    validator=test_case["validator_with_data"],
                    expectation_type=test_case["expectation_type"],
                    test=test_case["test"],
                    pk_column=test_case["pk_column"],
                )
        else:
>           evaluate_json_test_v3_api(
                validator=test_case["validator_with_data"],
                expectation_type=test_case["expectation_type"],
                test=test_case["test"],
                pk_column=test_case["pk_column"],
            )

tests/test_definitions/test_expectations_v3_api.py:418:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
great_expectations/self_check/util.py:2901: in evaluate_json_test_v3_api
    check_json_test_result(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

test = {'exact_match_out': False, 'in': {'allow_cross_type_comparisons': True, 'column': 'numeric', 'max_value': 10, 'min_val...False, 'unexpected_index_list': [0, 1, 2, 3, 4, 5, ...], 'unexpected_list': ['1', '10', '2', '3', '4', '5', ...]}, ...}
result = {'exception_info': {'exception_message': None, 'exception_traceback': None, 'raised_exception': False}, 'expectation_c... 1, 'value': '2'}, {'count': 1, 'value': '3'}, {'count': 1, 'value': '4'}, {'count': 1, 'value': '5'}, ...], ...}, ...}
data_asset = <great_expectations.execution_engine.pandas_batch_data.PandasBatchData object at 0x7f78056ce850>
pk_column = True

    def check_json_test_result(  # noqa: C901 - 52
        test, result, data_asset=None, pk_column=False
    ) -> None:

        # check for id_pk results in cases where pk_column is true and unexpected_index_list already exists
        # this will work for testing since result_format is COMPLETE
        if pk_column:
            if not result["success"]:
                if "unexpected_index_list" in result["result"]:
                    assert "unexpected_index_query" in result["result"]

        if "unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )
            elif "unexpected_list" in test["output"]:
                (
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )

        if "partial_unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "partial_unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )
            elif "partial_unexpected_list" in test["output"]:
                (
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )

        # Determine if np.allclose(..) might be needed for float comparison
        try_allclose = False
        if "observed_value" in test["output"]:
            if RX_FLOAT.match(repr(test["output"]["observed_value"])):
                try_allclose = True

        # Check results
        if test["exact_match_out"] is True:
            if "result" in result and "observed_value" in result["result"]:
                if isinstance(result["result"]["observed_value"], (np.floating, float)):
                    assert np.allclose(
                        result["result"]["observed_value"],
                        expectationValidationResultSchema.load(test["output"])["result"][
                            "observed_value"
                        ],
                        rtol=RTOL,
                        atol=ATOL,
                    ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {expectationValidationResultSchema.load(test['output'])['result']['observed_value']}"
                else:
                    assert result == expectationValidationResultSchema.load(
                        test["output"]
                    ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
            else:
                assert result == expectationValidationResultSchema.load(
                    test["output"]
                ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
        else:
            # Convert result to json since our tests are reading from json so cannot easily contain richer types (e.g. NaN)
            # NOTE - 20191031 - JPC - we may eventually want to change these tests as we update our view on how
            # representations, serializations, and objects should interact and how much of that is shown to the user.
            result = result.to_json_dict()
            for key, value in test["output"].items():
                # Apply our great expectations-specific test logic

                if key == "success":
                    if isinstance(value, (np.floating, float)):
                        try:
                            assert np.allclose(
                                result["success"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['success']} not np.allclose to {value}"
                        except TypeError:
                            assert (
                                result["success"] == value
                            ), f"{result['success']} != {value}"
                    else:
                        assert result["success"] == value, f"{result['success']} != {value}"

                elif key == "observed_value":
                    if "tolerance" in test:
                        if isinstance(value, dict):
                            assert set(result["result"]["observed_value"].keys()) == set(
                                value.keys()
                            ), f"{set(result['result']['observed_value'].keys())} != {set(value.keys())}"
                            for k, v in value.items():
                                assert np.allclose(
                                    result["result"]["observed_value"][k],
                                    v,
                                    rtol=test["tolerance"],
                                )
                        else:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=test["tolerance"],
                            )
                    else:
                        if isinstance(value, dict) and "values" in value:
                            try:
                                assert np.allclose(
                                    result["result"]["observed_value"]["values"],
                                    value["values"],
                                    rtol=RTOL,
                                    atol=ATOL,
                                ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']['values']} not np.allclose to {value['values']}"
                            except TypeError as e:
                                print(e)
                                assert (
                                    result["result"]["observed_value"] == value
                                ), f"{result['result']['observed_value']} != {value}"
                        elif try_allclose:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {value}"
                        else:
                            assert (
                                result["result"]["observed_value"] == value
                            ), f"{result['result']['observed_value']} != {value}"

                # NOTE: This is a key used ONLY for testing cases where an expectation is legitimately allowed to return
                # any of multiple possible observed_values. expect_column_values_to_be_of_type is one such expectation.
                elif key == "observed_value_list":
                    assert result["result"]["observed_value"] in value

                elif key == "unexpected_index_list":
                    if isinstance(data_asset, (SqlAlchemyDataset, SparkDFDataset)):
                        pass
                    elif isinstance(data_asset, (SqlAlchemyBatchData, SparkDFBatchData)):
                        pass
                    else:
>                       assert (
                            result["result"]["unexpected_index_list"] == value
                        ), f"{result['result']['unexpected_index_list']} != {value}"
E                       AssertionError: [{'numeric': '1', 'pk_index': 0}, {'numeric': '2', 'pk_index': 1}, {'numeric': '3', 'pk_index': 2}, {'numeric': '4', 'pk_index': 3}, {'numeric': '5', 'pk_index': 4}, {'numeric': '6', 'pk_index': 5}, {'numeric': '7', 'pk_index': 6}, {'numeric': '8', 'pk_index': 7}, {'numeric': '9', 'pk_index': 8}, {'numeric': '10', 'pk_index': 9}] != [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]

great_expectations/self_check/util.py:3075: AssertionError
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 3729.93it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 387.57it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 542.48it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 301.92it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 427.68it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 328.92it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 495.02it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 459.32it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 457.32it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 455.26it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_between:test_strict_min_failure0] _

test_case = {'expectation_type': 'expect_column_values_to_be_between', 'pk_column': True, 'skip': False, 'test': {'exact_match_out...e': 1, 'strict_min': True}, 'out': {'success': False, 'unexpected_index_list': [0], 'unexpected_list': [1]}, ...}, ...}

    @pytest.mark.order(index=0)
    @pytest.mark.integration
    @pytest.mark.slow  # 12.68s
    def test_case_runner_v3_api(test_case):
        if test_case["skip"]:
            pytest.skip()

        # Note: this should never be done in practice, but we are wiping expectations to reuse batches during testing.
        # test_case["batch"]._initialize_expectations()
        if "parse_strings_as_datetimes" in test_case["test"]["in"]:
            with pytest.deprecated_call():
                evaluate_json_test_v3_api(
                    validator=test_case["validator_with_data"],
                    expectation_type=test_case["expectation_type"],
                    test=test_case["test"],
                    pk_column=test_case["pk_column"],
                )
        else:
>           evaluate_json_test_v3_api(
                validator=test_case["validator_with_data"],
                expectation_type=test_case["expectation_type"],
                test=test_case["test"],
                pk_column=test_case["pk_column"],
            )

tests/test_definitions/test_expectations_v3_api.py:418:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
great_expectations/self_check/util.py:2901: in evaluate_json_test_v3_api
    check_json_test_result(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

test = {'exact_match_out': False, 'in': {'column': 'x', 'max_value': 10, 'min_value': 1, 'strict_min': True}, 'input': {'colu...n_value': 1, 'strict_min': True}, 'out': {'success': False, 'unexpected_index_list': [0], 'unexpected_list': [1]}, ...}
result = {'exception_info': {'exception_message': None, 'exception_traceback': None, 'raised_exception': False}, 'expectation_c...t': 10, 'missing_count': 0, 'missing_percent': 0.0, 'partial_unexpected_counts': [{'count': 1, 'value': 1}], ...}, ...}
data_asset = <great_expectations.execution_engine.pandas_batch_data.PandasBatchData object at 0x7f78056ce850>
pk_column = True

    def check_json_test_result(  # noqa: C901 - 52
        test, result, data_asset=None, pk_column=False
    ) -> None:

        # check for id_pk results in cases where pk_column is true and unexpected_index_list already exists
        # this will work for testing since result_format is COMPLETE
        if pk_column:
            if not result["success"]:
                if "unexpected_index_list" in result["result"]:
                    assert "unexpected_index_query" in result["result"]

        if "unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )
            elif "unexpected_list" in test["output"]:
                (
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )

        if "partial_unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "partial_unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )
            elif "partial_unexpected_list" in test["output"]:
                (
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )

        # Determine if np.allclose(..) might be needed for float comparison
        try_allclose = False
        if "observed_value" in test["output"]:
            if RX_FLOAT.match(repr(test["output"]["observed_value"])):
                try_allclose = True

        # Check results
        if test["exact_match_out"] is True:
            if "result" in result and "observed_value" in result["result"]:
                if isinstance(result["result"]["observed_value"], (np.floating, float)):
                    assert np.allclose(
                        result["result"]["observed_value"],
                        expectationValidationResultSchema.load(test["output"])["result"][
                            "observed_value"
                        ],
                        rtol=RTOL,
                        atol=ATOL,
                    ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {expectationValidationResultSchema.load(test['output'])['result']['observed_value']}"
                else:
                    assert result == expectationValidationResultSchema.load(
                        test["output"]
                    ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
            else:
                assert result == expectationValidationResultSchema.load(
                    test["output"]
                ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
        else:
            # Convert result to json since our tests are reading from json so cannot easily contain richer types (e.g. NaN)
            # NOTE - 20191031 - JPC - we may eventually want to change these tests as we update our view on how
            # representations, serializations, and objects should interact and how much of that is shown to the user.
            result = result.to_json_dict()
            for key, value in test["output"].items():
                # Apply our great expectations-specific test logic

                if key == "success":
                    if isinstance(value, (np.floating, float)):
                        try:
                            assert np.allclose(
                                result["success"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['success']} not np.allclose to {value}"
                        except TypeError:
                            assert (
                                result["success"] == value
                            ), f"{result['success']} != {value}"
                    else:
                        assert result["success"] == value, f"{result['success']} != {value}"

                elif key == "observed_value":
                    if "tolerance" in test:
                        if isinstance(value, dict):
                            assert set(result["result"]["observed_value"].keys()) == set(
                                value.keys()
                            ), f"{set(result['result']['observed_value'].keys())} != {set(value.keys())}"
                            for k, v in value.items():
                                assert np.allclose(
                                    result["result"]["observed_value"][k],
                                    v,
                                    rtol=test["tolerance"],
                                )
                        else:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=test["tolerance"],
                            )
                    else:
                        if isinstance(value, dict) and "values" in value:
                            try:
                                assert np.allclose(
                                    result["result"]["observed_value"]["values"],
                                    value["values"],
                                    rtol=RTOL,
                                    atol=ATOL,
                                ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']['values']} not np.allclose to {value['values']}"
                            except TypeError as e:
                                print(e)
                                assert (
                                    result["result"]["observed_value"] == value
                                ), f"{result['result']['observed_value']} != {value}"
                        elif try_allclose:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {value}"
                        else:
                            assert (
                                result["result"]["observed_value"] == value
                            ), f"{result['result']['observed_value']} != {value}"

                # NOTE: This is a key used ONLY for testing cases where an expectation is legitimately allowed to return
                # any of multiple possible observed_values. expect_column_values_to_be_of_type is one such expectation.
                elif key == "observed_value_list":
                    assert result["result"]["observed_value"] in value

                elif key == "unexpected_index_list":
                    if isinstance(data_asset, (SqlAlchemyDataset, SparkDFDataset)):
                        pass
                    elif isinstance(data_asset, (SqlAlchemyBatchData, SparkDFBatchData)):
                        pass
                    else:
>                       assert (
                            result["result"]["unexpected_index_list"] == value
                        ), f"{result['result']['unexpected_index_list']} != {value}"
E                       AssertionError: [{'x': 1, 'pk_index': 0}] != [0]

great_expectations/self_check/util.py:3075: AssertionError
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 4017.53it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 378.56it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 540.74it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 289.91it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 407.67it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 312.46it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 485.11it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 443.83it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 441.38it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 438.98it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_between:test_strict_max_failure0] _

test_case = {'expectation_type': 'expect_column_values_to_be_between', 'pk_column': True, 'skip': False, 'test': {'exact_match_out...': 0, 'strict_max': True}, 'out': {'success': False, 'unexpected_index_list': [9], 'unexpected_list': [10]}, ...}, ...}

    @pytest.mark.order(index=0)
    @pytest.mark.integration
    @pytest.mark.slow  # 12.68s
    def test_case_runner_v3_api(test_case):
        if test_case["skip"]:
            pytest.skip()

        # Note: this should never be done in practice, but we are wiping expectations to reuse batches during testing.
        # test_case["batch"]._initialize_expectations()
        if "parse_strings_as_datetimes" in test_case["test"]["in"]:
            with pytest.deprecated_call():
                evaluate_json_test_v3_api(
                    validator=test_case["validator_with_data"],
                    expectation_type=test_case["expectation_type"],
                    test=test_case["test"],
                    pk_column=test_case["pk_column"],
                )
        else:
>           evaluate_json_test_v3_api(
                validator=test_case["validator_with_data"],
                expectation_type=test_case["expectation_type"],
                test=test_case["test"],
                pk_column=test_case["pk_column"],
            )

tests/test_definitions/test_expectations_v3_api.py:418:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
great_expectations/self_check/util.py:2901: in evaluate_json_test_v3_api
    check_json_test_result(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

test = {'exact_match_out': False, 'in': {'column': 'x', 'max_value': 10, 'min_value': 0, 'strict_max': True}, 'input': {'colu..._value': 0, 'strict_max': True}, 'out': {'success': False, 'unexpected_index_list': [9], 'unexpected_list': [10]}, ...}
result = {'exception_info': {'exception_message': None, 'exception_traceback': None, 'raised_exception': False}, 'expectation_c...': 10, 'missing_count': 0, 'missing_percent': 0.0, 'partial_unexpected_counts': [{'count': 1, 'value': 10}], ...}, ...}
data_asset = <great_expectations.execution_engine.pandas_batch_data.PandasBatchData object at 0x7f78056ce850>
pk_column = True

    def check_json_test_result(  # noqa: C901 - 52
        test, result, data_asset=None, pk_column=False
    ) -> None:

        # check for id_pk results in cases where pk_column is true and unexpected_index_list already exists
        # this will work for testing since result_format is COMPLETE
        if pk_column:
            if not result["success"]:
                if "unexpected_index_list" in result["result"]:
                    assert "unexpected_index_query" in result["result"]

        if "unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )
            elif "unexpected_list" in test["output"]:
                (
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )

        if "partial_unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "partial_unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )
            elif "partial_unexpected_list" in test["output"]:
                (
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )

        # Determine if np.allclose(..) might be needed for float comparison
        try_allclose = False
        if "observed_value" in test["output"]:
            if RX_FLOAT.match(repr(test["output"]["observed_value"])):
                try_allclose = True

        # Check results
        if test["exact_match_out"] is True:
            if "result" in result and "observed_value" in result["result"]:
                if isinstance(result["result"]["observed_value"], (np.floating, float)):
                    assert np.allclose(
                        result["result"]["observed_value"],
                        expectationValidationResultSchema.load(test["output"])["result"][
                            "observed_value"
                        ],
                        rtol=RTOL,
                        atol=ATOL,
                    ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {expectationValidationResultSchema.load(test['output'])['result']['observed_value']}"
                else:
                    assert result == expectationValidationResultSchema.load(
                        test["output"]
                    ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
            else:
                assert result == expectationValidationResultSchema.load(
                    test["output"]
                ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
        else:
            # Convert result to json since our tests are reading from json so cannot easily contain richer types (e.g. NaN)
            # NOTE - 20191031 - JPC - we may eventually want to change these tests as we update our view on how
            # representations, serializations, and objects should interact and how much of that is shown to the user.
            result = result.to_json_dict()
            for key, value in test["output"].items():
                # Apply our great expectations-specific test logic

                if key == "success":
                    if isinstance(value, (np.floating, float)):
                        try:
                            assert np.allclose(
                                result["success"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['success']} not np.allclose to {value}"
                        except TypeError:
                            assert (
                                result["success"] == value
                            ), f"{result['success']} != {value}"
                    else:
                        assert result["success"] == value, f"{result['success']} != {value}"

                elif key == "observed_value":
                    if "tolerance" in test:
                        if isinstance(value, dict):
                            assert set(result["result"]["observed_value"].keys()) == set(
                                value.keys()
                            ), f"{set(result['result']['observed_value'].keys())} != {set(value.keys())}"
                            for k, v in value.items():
                                assert np.allclose(
                                    result["result"]["observed_value"][k],
                                    v,
                                    rtol=test["tolerance"],
                                )
                        else:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=test["tolerance"],
                            )
                    else:
                        if isinstance(value, dict) and "values" in value:
                            try:
                                assert np.allclose(
                                    result["result"]["observed_value"]["values"],
                                    value["values"],
                                    rtol=RTOL,
                                    atol=ATOL,
                                ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']['values']} not np.allclose to {value['values']}"
                            except TypeError as e:
                                print(e)
                                assert (
                                    result["result"]["observed_value"] == value
                                ), f"{result['result']['observed_value']} != {value}"
                        elif try_allclose:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {value}"
                        else:
                            assert (
                                result["result"]["observed_value"] == value
                            ), f"{result['result']['observed_value']} != {value}"

                # NOTE: This is a key used ONLY for testing cases where an expectation is legitimately allowed to return
                # any of multiple possible observed_values. expect_column_values_to_be_of_type is one such expectation.
                elif key == "observed_value_list":
                    assert result["result"]["observed_value"] in value

                elif key == "unexpected_index_list":
                    if isinstance(data_asset, (SqlAlchemyDataset, SparkDFDataset)):
                        pass
                    elif isinstance(data_asset, (SqlAlchemyBatchData, SparkDFBatchData)):
                        pass
                    else:
>                       assert (
                            result["result"]["unexpected_index_list"] == value
                        ), f"{result['result']['unexpected_index_list']} != {value}"
E                       AssertionError: [{'x': 10, 'pk_index': 9}] != [9]

great_expectations/self_check/util.py:3075: AssertionError
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 3622.02it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 350.45it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 499.16it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 281.58it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 398.62it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 301.15it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 470.14it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 437.04it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 435.43it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 433.58it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_between:test_conditional_expectation_fails0] _

test_case = {'expectation_type': 'expect_column_values_to_be_between', 'pk_column': True, 'skip': False, 'test': {'exact_match_out...'column': 'x', 'condition_parser': 'pandas', 'max_value': 10, 'min_value': 10, ...}, 'only_for': ['pandas'], ...}, ...}

    @pytest.mark.order(index=0)
    @pytest.mark.integration
    @pytest.mark.slow  # 12.68s
    def test_case_runner_v3_api(test_case):
        if test_case["skip"]:
            pytest.skip()

        # Note: this should never be done in practice, but we are wiping expectations to reuse batches during testing.
        # test_case["batch"]._initialize_expectations()
        if "parse_strings_as_datetimes" in test_case["test"]["in"]:
            with pytest.deprecated_call():
                evaluate_json_test_v3_api(
                    validator=test_case["validator_with_data"],
                    expectation_type=test_case["expectation_type"],
                    test=test_case["test"],
                    pk_column=test_case["pk_column"],
                )
        else:
>           evaluate_json_test_v3_api(
                validator=test_case["validator_with_data"],
                expectation_type=test_case["expectation_type"],
                test=test_case["test"],
                pk_column=test_case["pk_column"],
            )

tests/test_definitions/test_expectations_v3_api.py:418:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
great_expectations/self_check/util.py:2901: in evaluate_json_test_v3_api
    check_json_test_result(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

test = {'exact_match_out': False, 'in': {'column': 'x', 'condition_parser': 'pandas', 'max_value': 10, 'min_value': 10, ...},...ut': {'column': 'x', 'condition_parser': 'pandas', 'max_value': 10, 'min_value': 10, ...}, 'only_for': ['pandas'], ...}
result = {'exception_info': {'exception_message': None, 'exception_traceback': None, 'raised_exception': False}, 'expectation_c...nt': 1, 'missing_count': 0, 'missing_percent': 0.0, 'partial_unexpected_counts': [{'count': 1, 'value': 9}], ...}, ...}
data_asset = <great_expectations.execution_engine.pandas_batch_data.PandasBatchData object at 0x7f78056ce850>
pk_column = True

    def check_json_test_result(  # noqa: C901 - 52
        test, result, data_asset=None, pk_column=False
    ) -> None:

        # check for id_pk results in cases where pk_column is true and unexpected_index_list already exists
        # this will work for testing since result_format is COMPLETE
        if pk_column:
            if not result["success"]:
                if "unexpected_index_list" in result["result"]:
                    assert "unexpected_index_query" in result["result"]

        if "unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )
            elif "unexpected_list" in test["output"]:
                (
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )

        if "partial_unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "partial_unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )
            elif "partial_unexpected_list" in test["output"]:
                (
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )

        # Determine if np.allclose(..) might be needed for float comparison
        try_allclose = False
        if "observed_value" in test["output"]:
            if RX_FLOAT.match(repr(test["output"]["observed_value"])):
                try_allclose = True

        # Check results
        if test["exact_match_out"] is True:
            if "result" in result and "observed_value" in result["result"]:
                if isinstance(result["result"]["observed_value"], (np.floating, float)):
                    assert np.allclose(
                        result["result"]["observed_value"],
                        expectationValidationResultSchema.load(test["output"])["result"][
                            "observed_value"
                        ],
                        rtol=RTOL,
                        atol=ATOL,
                    ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {expectationValidationResultSchema.load(test['output'])['result']['observed_value']}"
                else:
                    assert result == expectationValidationResultSchema.load(
                        test["output"]
                    ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
            else:
                assert result == expectationValidationResultSchema.load(
                    test["output"]
                ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
        else:
            # Convert result to json since our tests are reading from json so cannot easily contain richer types (e.g. NaN)
            # NOTE - 20191031 - JPC - we may eventually want to change these tests as we update our view on how
            # representations, serializations, and objects should interact and how much of that is shown to the user.
            result = result.to_json_dict()
            for key, value in test["output"].items():
                # Apply our great expectations-specific test logic

                if key == "success":
                    if isinstance(value, (np.floating, float)):
                        try:
                            assert np.allclose(
                                result["success"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['success']} not np.allclose to {value}"
                        except TypeError:
                            assert (
                                result["success"] == value
                            ), f"{result['success']} != {value}"
                    else:
                        assert result["success"] == value, f"{result['success']} != {value}"

                elif key == "observed_value":
                    if "tolerance" in test:
                        if isinstance(value, dict):
                            assert set(result["result"]["observed_value"].keys()) == set(
                                value.keys()
                            ), f"{set(result['result']['observed_value'].keys())} != {set(value.keys())}"
                            for k, v in value.items():
                                assert np.allclose(
                                    result["result"]["observed_value"][k],
                                    v,
                                    rtol=test["tolerance"],
                                )
                        else:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=test["tolerance"],
                            )
                    else:
                        if isinstance(value, dict) and "values" in value:
                            try:
                                assert np.allclose(
                                    result["result"]["observed_value"]["values"],
                                    value["values"],
                                    rtol=RTOL,
                                    atol=ATOL,
                                ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']['values']} not np.allclose to {value['values']}"
                            except TypeError as e:
                                print(e)
                                assert (
                                    result["result"]["observed_value"] == value
                                ), f"{result['result']['observed_value']} != {value}"
                        elif try_allclose:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {value}"
                        else:
                            assert (
                                result["result"]["observed_value"] == value
                            ), f"{result['result']['observed_value']} != {value}"

                # NOTE: This is a key used ONLY for testing cases where an expectation is legitimately allowed to return
                # any of multiple possible observed_values. expect_column_values_to_be_of_type is one such expectation.
                elif key == "observed_value_list":
                    assert result["result"]["observed_value"] in value

                elif key == "unexpected_index_list":
                    if isinstance(data_asset, (SqlAlchemyDataset, SparkDFDataset)):
                        pass
                    elif isinstance(data_asset, (SqlAlchemyBatchData, SparkDFBatchData)):
                        pass
                    else:
>                       assert (
                            result["result"]["unexpected_index_list"] == value
                        ), f"{result['result']['unexpected_index_list']} != {value}"
E                       AssertionError: [{'x': 9, 'pk_index': 8}] != [8]

great_expectations/self_check/util.py:3075: AssertionError
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 484.75it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 254.39it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 370.24it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 257.29it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 320.37it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 269.44it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 387.93it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 370.14it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 369.02it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 367.70it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_match_regex:negative_test_insufficient_mostly_and_one_non_matching_value] _

test_case = {'expectation_type': 'expect_column_values_to_match_regex', 'pk_column': True, 'skip': False, 'test': {'exact_match_ou...y': 0.9, 'regex': '^a'}, 'include_in_gallery': True, 'input': {'column': 'a', 'mostly': 0.9, 'regex': '^a'}, ...}, ...}

    @pytest.mark.order(index=0)
    @pytest.mark.integration
    @pytest.mark.slow  # 12.68s
    def test_case_runner_v3_api(test_case):
        if test_case["skip"]:
            pytest.skip()

        # Note: this should never be done in practice, but we are wiping expectations to reuse batches during testing.
        # test_case["batch"]._initialize_expectations()
        if "parse_strings_as_datetimes" in test_case["test"]["in"]:
            with pytest.deprecated_call():
                evaluate_json_test_v3_api(
                    validator=test_case["validator_with_data"],
                    expectation_type=test_case["expectation_type"],
                    test=test_case["test"],
                    pk_column=test_case["pk_column"],
                )
        else:
>           evaluate_json_test_v3_api(
                validator=test_case["validator_with_data"],
                expectation_type=test_case["expectation_type"],
                test=test_case["test"],
                pk_column=test_case["pk_column"],
            )

tests/test_definitions/test_expectations_v3_api.py:418:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
great_expectations/self_check/util.py:2901: in evaluate_json_test_v3_api
    check_json_test_result(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

test = {'exact_match_out': False, 'in': {'column': 'a', 'mostly': 0.9, 'regex': '^a'}, 'include_in_gallery': True, 'input': {'column': 'a', 'mostly': 0.9, 'regex': '^a'}, ...}
result = {'exception_info': {'exception_message': None, 'exception_traceback': None, 'raised_exception': False}, 'expectation_c... 5, 'missing_count': 0, 'missing_percent': 0.0, 'partial_unexpected_counts': [{'count': 1, 'value': 'bee'}], ...}, ...}
data_asset = <great_expectations.execution_engine.pandas_batch_data.PandasBatchData object at 0x7f78062f0b50>
pk_column = True

    def check_json_test_result(  # noqa: C901 - 52
        test, result, data_asset=None, pk_column=False
    ) -> None:

        # check for id_pk results in cases where pk_column is true and unexpected_index_list already exists
        # this will work for testing since result_format is COMPLETE
        if pk_column:
            if not result["success"]:
                if "unexpected_index_list" in result["result"]:
                    assert "unexpected_index_query" in result["result"]

        if "unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )
            elif "unexpected_list" in test["output"]:
                (
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )

        if "partial_unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "partial_unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )
            elif "partial_unexpected_list" in test["output"]:
                (
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )

        # Determine if np.allclose(..) might be needed for float comparison
        try_allclose = False
        if "observed_value" in test["output"]:
            if RX_FLOAT.match(repr(test["output"]["observed_value"])):
                try_allclose = True

        # Check results
        if test["exact_match_out"] is True:
            if "result" in result and "observed_value" in result["result"]:
                if isinstance(result["result"]["observed_value"], (np.floating, float)):
                    assert np.allclose(
                        result["result"]["observed_value"],
                        expectationValidationResultSchema.load(test["output"])["result"][
                            "observed_value"
                        ],
                        rtol=RTOL,
                        atol=ATOL,
                    ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {expectationValidationResultSchema.load(test['output'])['result']['observed_value']}"
                else:
                    assert result == expectationValidationResultSchema.load(
                        test["output"]
                    ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
            else:
                assert result == expectationValidationResultSchema.load(
                    test["output"]
                ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
        else:
            # Convert result to json since our tests are reading from json so cannot easily contain richer types (e.g. NaN)
            # NOTE - 20191031 - JPC - we may eventually want to change these tests as we update our view on how
            # representations, serializations, and objects should interact and how much of that is shown to the user.
            result = result.to_json_dict()
            for key, value in test["output"].items():
                # Apply our great expectations-specific test logic

                if key == "success":
                    if isinstance(value, (np.floating, float)):
                        try:
                            assert np.allclose(
                                result["success"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['success']} not np.allclose to {value}"
                        except TypeError:
                            assert (
                                result["success"] == value
                            ), f"{result['success']} != {value}"
                    else:
                        assert result["success"] == value, f"{result['success']} != {value}"

                elif key == "observed_value":
                    if "tolerance" in test:
                        if isinstance(value, dict):
                            assert set(result["result"]["observed_value"].keys()) == set(
                                value.keys()
                            ), f"{set(result['result']['observed_value'].keys())} != {set(value.keys())}"
                            for k, v in value.items():
                                assert np.allclose(
                                    result["result"]["observed_value"][k],
                                    v,
                                    rtol=test["tolerance"],
                                )
                        else:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=test["tolerance"],
                            )
                    else:
                        if isinstance(value, dict) and "values" in value:
                            try:
                                assert np.allclose(
                                    result["result"]["observed_value"]["values"],
                                    value["values"],
                                    rtol=RTOL,
                                    atol=ATOL,
                                ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']['values']} not np.allclose to {value['values']}"
                            except TypeError as e:
                                print(e)
                                assert (
                                    result["result"]["observed_value"] == value
                                ), f"{result['result']['observed_value']} != {value}"
                        elif try_allclose:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {value}"
                        else:
                            assert (
                                result["result"]["observed_value"] == value
                            ), f"{result['result']['observed_value']} != {value}"

                # NOTE: This is a key used ONLY for testing cases where an expectation is legitimately allowed to return
                # any of multiple possible observed_values. expect_column_values_to_be_of_type is one such expectation.
                elif key == "observed_value_list":
                    assert result["result"]["observed_value"] in value

                elif key == "unexpected_index_list":
                    if isinstance(data_asset, (SqlAlchemyDataset, SparkDFDataset)):
                        pass
                    elif isinstance(data_asset, (SqlAlchemyBatchData, SparkDFBatchData)):
                        pass
                    else:
>                       assert (
                            result["result"]["unexpected_index_list"] == value
                        ), f"{result['result']['unexpected_index_list']} != {value}"
E                       AssertionError: [{'a': 'bee', 'pk_index': 4}] != [4]

great_expectations/self_check/util.py:3075: AssertionError
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 3855.06it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 444.95it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 631.61it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 348.07it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 476.87it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 362.53it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 562.22it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 523.29it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 520.97it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 518.30it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_match_regex:negative_test_insufficient_mostly_and_one_non_matching_value_summary_output] _

test_case = {'expectation_type': 'expect_column_values_to_match_regex', 'pk_column': True, 'skip': False, 'test': {'exact_match_ou...y': 0.9, 'regex': '^a'}, 'include_in_gallery': True, 'input': {'column': 'a', 'mostly': 0.9, 'regex': '^a'}, ...}, ...}

    @pytest.mark.order(index=0)
    @pytest.mark.integration
    @pytest.mark.slow  # 12.68s
    def test_case_runner_v3_api(test_case):
        if test_case["skip"]:
            pytest.skip()

        # Note: this should never be done in practice, but we are wiping expectations to reuse batches during testing.
        # test_case["batch"]._initialize_expectations()
        if "parse_strings_as_datetimes" in test_case["test"]["in"]:
            with pytest.deprecated_call():
                evaluate_json_test_v3_api(
                    validator=test_case["validator_with_data"],
                    expectation_type=test_case["expectation_type"],
                    test=test_case["test"],
                    pk_column=test_case["pk_column"],
                )
        else:
>           evaluate_json_test_v3_api(
                validator=test_case["validator_with_data"],
                expectation_type=test_case["expectation_type"],
                test=test_case["test"],
                pk_column=test_case["pk_column"],
            )

tests/test_definitions/test_expectations_v3_api.py:418:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
great_expectations/self_check/util.py:2901: in evaluate_json_test_v3_api
    check_json_test_result(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

test = {'exact_match_out': False, 'in': {'column': 'a', 'mostly': 0.9, 'regex': '^a'}, 'include_in_gallery': True, 'input': {'column': 'a', 'mostly': 0.9, 'regex': '^a'}, ...}
result = {'exception_info': {'exception_message': None, 'exception_traceback': None, 'raised_exception': False}, 'expectation_c... 5, 'missing_count': 0, 'missing_percent': 0.0, 'partial_unexpected_counts': [{'count': 1, 'value': 'bee'}], ...}, ...}
data_asset = <great_expectations.execution_engine.pandas_batch_data.PandasBatchData object at 0x7f78062f0b50>
pk_column = True

    def check_json_test_result(  # noqa: C901 - 52
        test, result, data_asset=None, pk_column=False
    ) -> None:

        # check for id_pk results in cases where pk_column is true and unexpected_index_list already exists
        # this will work for testing since result_format is COMPLETE
        if pk_column:
            if not result["success"]:
                if "unexpected_index_list" in result["result"]:
                    assert "unexpected_index_query" in result["result"]

        if "unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )
            elif "unexpected_list" in test["output"]:
                (
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )

        if "partial_unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "partial_unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )
            elif "partial_unexpected_list" in test["output"]:
                (
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )

        # Determine if np.allclose(..) might be needed for float comparison
        try_allclose = False
        if "observed_value" in test["output"]:
            if RX_FLOAT.match(repr(test["output"]["observed_value"])):
                try_allclose = True

        # Check results
        if test["exact_match_out"] is True:
            if "result" in result and "observed_value" in result["result"]:
                if isinstance(result["result"]["observed_value"], (np.floating, float)):
                    assert np.allclose(
                        result["result"]["observed_value"],
                        expectationValidationResultSchema.load(test["output"])["result"][
                            "observed_value"
                        ],
                        rtol=RTOL,
                        atol=ATOL,
                    ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {expectationValidationResultSchema.load(test['output'])['result']['observed_value']}"
                else:
                    assert result == expectationValidationResultSchema.load(
                        test["output"]
                    ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
            else:
                assert result == expectationValidationResultSchema.load(
                    test["output"]
                ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
        else:
            # Convert result to json since our tests are reading from json so cannot easily contain richer types (e.g. NaN)
            # NOTE - 20191031 - JPC - we may eventually want to change these tests as we update our view on how
            # representations, serializations, and objects should interact and how much of that is shown to the user.
            result = result.to_json_dict()
            for key, value in test["output"].items():
                # Apply our great expectations-specific test logic

                if key == "success":
                    if isinstance(value, (np.floating, float)):
                        try:
                            assert np.allclose(
                                result["success"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['success']} not np.allclose to {value}"
                        except TypeError:
                            assert (
                                result["success"] == value
                            ), f"{result['success']} != {value}"
                    else:
                        assert result["success"] == value, f"{result['success']} != {value}"

                elif key == "observed_value":
                    if "tolerance" in test:
                        if isinstance(value, dict):
                            assert set(result["result"]["observed_value"].keys()) == set(
                                value.keys()
                            ), f"{set(result['result']['observed_value'].keys())} != {set(value.keys())}"
                            for k, v in value.items():
                                assert np.allclose(
                                    result["result"]["observed_value"][k],
                                    v,
                                    rtol=test["tolerance"],
                                )
                        else:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=test["tolerance"],
                            )
                    else:
                        if isinstance(value, dict) and "values" in value:
                            try:
                                assert np.allclose(
                                    result["result"]["observed_value"]["values"],
                                    value["values"],
                                    rtol=RTOL,
                                    atol=ATOL,
                                ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']['values']} not np.allclose to {value['values']}"
                            except TypeError as e:
                                print(e)
                                assert (
                                    result["result"]["observed_value"] == value
                                ), f"{result['result']['observed_value']} != {value}"
                        elif try_allclose:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {value}"
                        else:
                            assert (
                                result["result"]["observed_value"] == value
                            ), f"{result['result']['observed_value']} != {value}"

                # NOTE: This is a key used ONLY for testing cases where an expectation is legitimately allowed to return
                # any of multiple possible observed_values. expect_column_values_to_be_of_type is one such expectation.
                elif key == "observed_value_list":
                    assert result["result"]["observed_value"] in value

                elif key == "unexpected_index_list":
                    if isinstance(data_asset, (SqlAlchemyDataset, SparkDFDataset)):
                        pass
                    elif isinstance(data_asset, (SqlAlchemyBatchData, SparkDFBatchData)):
                        pass
                    else:
>                       assert (
                            result["result"]["unexpected_index_list"] == value
                        ), f"{result['result']['unexpected_index_list']} != {value}"
E                       AssertionError: [{'a': 'bee', 'pk_index': 4}] != [4]

great_expectations/self_check/util.py:3075: AssertionError
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 4122.17it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 411.61it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 579.46it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 328.36it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 460.32it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 359.19it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 566.25it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 526.00it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 523.39it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 520.48it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_match_regex:positive_test_exact_mostly_w_one_non_matching_value] _

test_case = {'expectation_type': 'expect_column_values_to_match_regex', 'pk_column': True, 'skip': False, 'test': {'exact_match_ou...y': 0.8, 'regex': '^a'}, 'include_in_gallery': True, 'input': {'column': 'a', 'mostly': 0.8, 'regex': '^a'}, ...}, ...}

    @pytest.mark.order(index=0)
    @pytest.mark.integration
    @pytest.mark.slow  # 12.68s
    def test_case_runner_v3_api(test_case):
        if test_case["skip"]:
            pytest.skip()

        # Note: this should never be done in practice, but we are wiping expectations to reuse batches during testing.
        # test_case["batch"]._initialize_expectations()
        if "parse_strings_as_datetimes" in test_case["test"]["in"]:
            with pytest.deprecated_call():
                evaluate_json_test_v3_api(
                    validator=test_case["validator_with_data"],
                    expectation_type=test_case["expectation_type"],
                    test=test_case["test"],
                    pk_column=test_case["pk_column"],
                )
        else:
>           evaluate_json_test_v3_api(
                validator=test_case["validator_with_data"],
                expectation_type=test_case["expectation_type"],
                test=test_case["test"],
                pk_column=test_case["pk_column"],
            )

tests/test_definitions/test_expectations_v3_api.py:418:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
great_expectations/self_check/util.py:2901: in evaluate_json_test_v3_api
    check_json_test_result(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

test = {'exact_match_out': False, 'in': {'column': 'a', 'mostly': 0.8, 'regex': '^a'}, 'include_in_gallery': True, 'input': {'column': 'a', 'mostly': 0.8, 'regex': '^a'}, ...}
result = {'exception_info': {'exception_message': None, 'exception_traceback': None, 'raised_exception': False}, 'expectation_c... 5, 'missing_count': 0, 'missing_percent': 0.0, 'partial_unexpected_counts': [{'count': 1, 'value': 'bee'}], ...}, ...}
data_asset = <great_expectations.execution_engine.pandas_batch_data.PandasBatchData object at 0x7f78062f0b50>
pk_column = True

    def check_json_test_result(  # noqa: C901 - 52
        test, result, data_asset=None, pk_column=False
    ) -> None:

        # check for id_pk results in cases where pk_column is true and unexpected_index_list already exists
        # this will work for testing since result_format is COMPLETE
        if pk_column:
            if not result["success"]:
                if "unexpected_index_list" in result["result"]:
                    assert "unexpected_index_query" in result["result"]

        if "unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )
            elif "unexpected_list" in test["output"]:
                (
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )

        if "partial_unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "partial_unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )
            elif "partial_unexpected_list" in test["output"]:
                (
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )

        # Determine if np.allclose(..) might be needed for float comparison
        try_allclose = False
        if "observed_value" in test["output"]:
            if RX_FLOAT.match(repr(test["output"]["observed_value"])):
                try_allclose = True

        # Check results
        if test["exact_match_out"] is True:
            if "result" in result and "observed_value" in result["result"]:
                if isinstance(result["result"]["observed_value"], (np.floating, float)):
                    assert np.allclose(
                        result["result"]["observed_value"],
                        expectationValidationResultSchema.load(test["output"])["result"][
                            "observed_value"
                        ],
                        rtol=RTOL,
                        atol=ATOL,
                    ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {expectationValidationResultSchema.load(test['output'])['result']['observed_value']}"
                else:
                    assert result == expectationValidationResultSchema.load(
                        test["output"]
                    ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
            else:
                assert result == expectationValidationResultSchema.load(
                    test["output"]
                ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
        else:
            # Convert result to json since our tests are reading from json so cannot easily contain richer types (e.g. NaN)
            # NOTE - 20191031 - JPC - we may eventually want to change these tests as we update our view on how
            # representations, serializations, and objects should interact and how much of that is shown to the user.
            result = result.to_json_dict()
            for key, value in test["output"].items():
                # Apply our great expectations-specific test logic

                if key == "success":
                    if isinstance(value, (np.floating, float)):
                        try:
                            assert np.allclose(
                                result["success"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['success']} not np.allclose to {value}"
                        except TypeError:
                            assert (
                                result["success"] == value
                            ), f"{result['success']} != {value}"
                    else:
                        assert result["success"] == value, f"{result['success']} != {value}"

                elif key == "observed_value":
                    if "tolerance" in test:
                        if isinstance(value, dict):
                            assert set(result["result"]["observed_value"].keys()) == set(
                                value.keys()
                            ), f"{set(result['result']['observed_value'].keys())} != {set(value.keys())}"
                            for k, v in value.items():
                                assert np.allclose(
                                    result["result"]["observed_value"][k],
                                    v,
                                    rtol=test["tolerance"],
                                )
                        else:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=test["tolerance"],
                            )
                    else:
                        if isinstance(value, dict) and "values" in value:
                            try:
                                assert np.allclose(
                                    result["result"]["observed_value"]["values"],
                                    value["values"],
                                    rtol=RTOL,
                                    atol=ATOL,
                                ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']['values']} not np.allclose to {value['values']}"
                            except TypeError as e:
                                print(e)
                                assert (
                                    result["result"]["observed_value"] == value
                                ), f"{result['result']['observed_value']} != {value}"
                        elif try_allclose:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {value}"
                        else:
                            assert (
                                result["result"]["observed_value"] == value
                            ), f"{result['result']['observed_value']} != {value}"

                # NOTE: This is a key used ONLY for testing cases where an expectation is legitimately allowed to return
                # any of multiple possible observed_values. expect_column_values_to_be_of_type is one such expectation.
                elif key == "observed_value_list":
                    assert result["result"]["observed_value"] in value

                elif key == "unexpected_index_list":
                    if isinstance(data_asset, (SqlAlchemyDataset, SparkDFDataset)):
                        pass
                    elif isinstance(data_asset, (SqlAlchemyBatchData, SparkDFBatchData)):
                        pass
                    else:
>                       assert (
                            result["result"]["unexpected_index_list"] == value
                        ), f"{result['result']['unexpected_index_list']} != {value}"
E                       AssertionError: [{'a': 'bee', 'pk_index': 4}] != [4]

great_expectations/self_check/util.py:3075: AssertionError
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 3983.19it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 447.03it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 630.00it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 330.43it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 439.25it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 340.23it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 533.33it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 496.28it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 493.83it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 491.45it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_match_regex:positive_test_column_name_has_space] _

test_case = {'expectation_type': 'expect_column_values_to_match_regex', 'pk_column': True, 'skip': False, 'test': {'exact_match_ou...y': 0.8, 'regex': '^a'}, 'out': {'success': True, 'unexpected_index_list': [4], 'unexpected_list': ['bee']}, ...}, ...}

    @pytest.mark.order(index=0)
    @pytest.mark.integration
    @pytest.mark.slow  # 12.68s
    def test_case_runner_v3_api(test_case):
        if test_case["skip"]:
            pytest.skip()

        # Note: this should never be done in practice, but we are wiping expectations to reuse batches during testing.
        # test_case["batch"]._initialize_expectations()
        if "parse_strings_as_datetimes" in test_case["test"]["in"]:
            with pytest.deprecated_call():
                evaluate_json_test_v3_api(
                    validator=test_case["validator_with_data"],
                    expectation_type=test_case["expectation_type"],
                    test=test_case["test"],
                    pk_column=test_case["pk_column"],
                )
        else:
>           evaluate_json_test_v3_api(
                validator=test_case["validator_with_data"],
                expectation_type=test_case["expectation_type"],
                test=test_case["test"],
                pk_column=test_case["pk_column"],
            )

tests/test_definitions/test_expectations_v3_api.py:418:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
great_expectations/self_check/util.py:2901: in evaluate_json_test_v3_api
    check_json_test_result(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

test = {'exact_match_out': False, 'in': {'column': 'column_name with space', 'mostly': 0.8, 'regex': '^a'}, 'input': {'column...'mostly': 0.8, 'regex': '^a'}, 'out': {'success': True, 'unexpected_index_list': [4], 'unexpected_list': ['bee']}, ...}
result = {'exception_info': {'exception_message': None, 'exception_traceback': None, 'raised_exception': False}, 'expectation_c... 5, 'missing_count': 0, 'missing_percent': 0.0, 'partial_unexpected_counts': [{'count': 1, 'value': 'bee'}], ...}, ...}
data_asset = <great_expectations.execution_engine.pandas_batch_data.PandasBatchData object at 0x7f78062f0b50>
pk_column = True

    def check_json_test_result(  # noqa: C901 - 52
        test, result, data_asset=None, pk_column=False
    ) -> None:

        # check for id_pk results in cases where pk_column is true and unexpected_index_list already exists
        # this will work for testing since result_format is COMPLETE
        if pk_column:
            if not result["success"]:
                if "unexpected_index_list" in result["result"]:
                    assert "unexpected_index_query" in result["result"]

        if "unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )
            elif "unexpected_list" in test["output"]:
                (
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )

        if "partial_unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "partial_unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )
            elif "partial_unexpected_list" in test["output"]:
                (
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )

        # Determine if np.allclose(..) might be needed for float comparison
        try_allclose = False
        if "observed_value" in test["output"]:
            if RX_FLOAT.match(repr(test["output"]["observed_value"])):
                try_allclose = True

        # Check results
        if test["exact_match_out"] is True:
            if "result" in result and "observed_value" in result["result"]:
                if isinstance(result["result"]["observed_value"], (np.floating, float)):
                    assert np.allclose(
                        result["result"]["observed_value"],
                        expectationValidationResultSchema.load(test["output"])["result"][
                            "observed_value"
                        ],
                        rtol=RTOL,
                        atol=ATOL,
                    ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {expectationValidationResultSchema.load(test['output'])['result']['observed_value']}"
                else:
                    assert result == expectationValidationResultSchema.load(
                        test["output"]
                    ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
            else:
                assert result == expectationValidationResultSchema.load(
                    test["output"]
                ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
        else:
            # Convert result to json since our tests are reading from json so cannot easily contain richer types (e.g. NaN)
            # NOTE - 20191031 - JPC - we may eventually want to change these tests as we update our view on how
            # representations, serializations, and objects should interact and how much of that is shown to the user.
            result = result.to_json_dict()
            for key, value in test["output"].items():
                # Apply our great expectations-specific test logic

                if key == "success":
                    if isinstance(value, (np.floating, float)):
                        try:
                            assert np.allclose(
                                result["success"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['success']} not np.allclose to {value}"
                        except TypeError:
                            assert (
                                result["success"] == value
                            ), f"{result['success']} != {value}"
                    else:
                        assert result["success"] == value, f"{result['success']} != {value}"

                elif key == "observed_value":
                    if "tolerance" in test:
                        if isinstance(value, dict):
                            assert set(result["result"]["observed_value"].keys()) == set(
                                value.keys()
                            ), f"{set(result['result']['observed_value'].keys())} != {set(value.keys())}"
                            for k, v in value.items():
                                assert np.allclose(
                                    result["result"]["observed_value"][k],
                                    v,
                                    rtol=test["tolerance"],
                                )
                        else:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=test["tolerance"],
                            )
                    else:
                        if isinstance(value, dict) and "values" in value:
                            try:
                                assert np.allclose(
                                    result["result"]["observed_value"]["values"],
                                    value["values"],
                                    rtol=RTOL,
                                    atol=ATOL,
                                ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']['values']} not np.allclose to {value['values']}"
                            except TypeError as e:
                                print(e)
                                assert (
                                    result["result"]["observed_value"] == value
                                ), f"{result['result']['observed_value']} != {value}"
                        elif try_allclose:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {value}"
                        else:
                            assert (
                                result["result"]["observed_value"] == value
                            ), f"{result['result']['observed_value']} != {value}"

                # NOTE: This is a key used ONLY for testing cases where an expectation is legitimately allowed to return
                # any of multiple possible observed_values. expect_column_values_to_be_of_type is one such expectation.
                elif key == "observed_value_list":
                    assert result["result"]["observed_value"] in value

                elif key == "unexpected_index_list":
                    if isinstance(data_asset, (SqlAlchemyDataset, SparkDFDataset)):
                        pass
                    elif isinstance(data_asset, (SqlAlchemyBatchData, SparkDFBatchData)):
                        pass
                    else:
>                       assert (
                            result["result"]["unexpected_index_list"] == value
                        ), f"{result['result']['unexpected_index_list']} != {value}"
E                       AssertionError: [{'column_name with space': 'bee', 'pk_index': 4}] != [4]

great_expectations/self_check/util.py:3075: AssertionError
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 3690.54it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 443.47it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 628.93it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 344.67it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 482.40it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 357.35it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 562.71it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 523.10it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 520.59it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 517.87it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_match_regex:positive_test_sufficient_mostly_w_one_non_matching_value] _

test_case = {'expectation_type': 'expect_column_values_to_match_regex', 'pk_column': True, 'skip': False, 'test': {'exact_match_ou...y': 0.7, 'regex': '^a'}, 'out': {'success': True, 'unexpected_index_list': [4], 'unexpected_list': ['bee']}, ...}, ...}

    @pytest.mark.order(index=0)
    @pytest.mark.integration
    @pytest.mark.slow  # 12.68s
    def test_case_runner_v3_api(test_case):
        if test_case["skip"]:
            pytest.skip()

        # Note: this should never be done in practice, but we are wiping expectations to reuse batches during testing.
        # test_case["batch"]._initialize_expectations()
        if "parse_strings_as_datetimes" in test_case["test"]["in"]:
            with pytest.deprecated_call():
                evaluate_json_test_v3_api(
                    validator=test_case["validator_with_data"],
                    expectation_type=test_case["expectation_type"],
                    test=test_case["test"],
                    pk_column=test_case["pk_column"],
                )
        else:
>           evaluate_json_test_v3_api(
                validator=test_case["validator_with_data"],
                expectation_type=test_case["expectation_type"],
                test=test_case["test"],
                pk_column=test_case["pk_column"],
            )

tests/test_definitions/test_expectations_v3_api.py:418:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
great_expectations/self_check/util.py:2901: in evaluate_json_test_v3_api
    check_json_test_result(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

test = {'exact_match_out': False, 'in': {'column': 'a', 'mostly': 0.7, 'regex': '^a'}, 'input': {'column': 'a', 'mostly': 0.7, 'regex': '^a'}, 'out': {'success': True, 'unexpected_index_list': [4], 'unexpected_list': ['bee']}, ...}
result = {'exception_info': {'exception_message': None, 'exception_traceback': None, 'raised_exception': False}, 'expectation_c... 5, 'missing_count': 0, 'missing_percent': 0.0, 'partial_unexpected_counts': [{'count': 1, 'value': 'bee'}], ...}, ...}
data_asset = <great_expectations.execution_engine.pandas_batch_data.PandasBatchData object at 0x7f78062f0b50>
pk_column = True

    def check_json_test_result(  # noqa: C901 - 52
        test, result, data_asset=None, pk_column=False
    ) -> None:

        # check for id_pk results in cases where pk_column is true and unexpected_index_list already exists
        # this will work for testing since result_format is COMPLETE
        if pk_column:
            if not result["success"]:
                if "unexpected_index_list" in result["result"]:
                    assert "unexpected_index_query" in result["result"]

        if "unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )
            elif "unexpected_list" in test["output"]:
                (
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )

        if "partial_unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "partial_unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )
            elif "partial_unexpected_list" in test["output"]:
                (
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )

        # Determine if np.allclose(..) might be needed for float comparison
        try_allclose = False
        if "observed_value" in test["output"]:
            if RX_FLOAT.match(repr(test["output"]["observed_value"])):
                try_allclose = True

        # Check results
        if test["exact_match_out"] is True:
            if "result" in result and "observed_value" in result["result"]:
                if isinstance(result["result"]["observed_value"], (np.floating, float)):
                    assert np.allclose(
                        result["result"]["observed_value"],
                        expectationValidationResultSchema.load(test["output"])["result"][
                            "observed_value"
                        ],
                        rtol=RTOL,
                        atol=ATOL,
                    ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {expectationValidationResultSchema.load(test['output'])['result']['observed_value']}"
                else:
                    assert result == expectationValidationResultSchema.load(
                        test["output"]
                    ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
            else:
                assert result == expectationValidationResultSchema.load(
                    test["output"]
                ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
        else:
            # Convert result to json since our tests are reading from json so cannot easily contain richer types (e.g. NaN)
            # NOTE - 20191031 - JPC - we may eventually want to change these tests as we update our view on how
            # representations, serializations, and objects should interact and how much of that is shown to the user.
            result = result.to_json_dict()
            for key, value in test["output"].items():
                # Apply our great expectations-specific test logic

                if key == "success":
                    if isinstance(value, (np.floating, float)):
                        try:
                            assert np.allclose(
                                result["success"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['success']} not np.allclose to {value}"
                        except TypeError:
                            assert (
                                result["success"] == value
                            ), f"{result['success']} != {value}"
                    else:
                        assert result["success"] == value, f"{result['success']} != {value}"

                elif key == "observed_value":
                    if "tolerance" in test:
                        if isinstance(value, dict):
                            assert set(result["result"]["observed_value"].keys()) == set(
                                value.keys()
                            ), f"{set(result['result']['observed_value'].keys())} != {set(value.keys())}"
                            for k, v in value.items():
                                assert np.allclose(
                                    result["result"]["observed_value"][k],
                                    v,
                                    rtol=test["tolerance"],
                                )
                        else:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=test["tolerance"],
                            )
                    else:
                        if isinstance(value, dict) and "values" in value:
                            try:
                                assert np.allclose(
                                    result["result"]["observed_value"]["values"],
                                    value["values"],
                                    rtol=RTOL,
                                    atol=ATOL,
                                ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']['values']} not np.allclose to {value['values']}"
                            except TypeError as e:
                                print(e)
                                assert (
                                    result["result"]["observed_value"] == value
                                ), f"{result['result']['observed_value']} != {value}"
                        elif try_allclose:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {value}"
                        else:
                            assert (
                                result["result"]["observed_value"] == value
                            ), f"{result['result']['observed_value']} != {value}"

                # NOTE: This is a key used ONLY for testing cases where an expectation is legitimately allowed to return
                # any of multiple possible observed_values. expect_column_values_to_be_of_type is one such expectation.
                elif key == "observed_value_list":
                    assert result["result"]["observed_value"] in value

                elif key == "unexpected_index_list":
                    if isinstance(data_asset, (SqlAlchemyDataset, SparkDFDataset)):
                        pass
                    elif isinstance(data_asset, (SqlAlchemyBatchData, SparkDFBatchData)):
                        pass
                    else:
>                       assert (
                            result["result"]["unexpected_index_list"] == value
                        ), f"{result['result']['unexpected_index_list']} != {value}"
E                       AssertionError: [{'a': 'bee', 'pk_index': 4}] != [4]

great_expectations/self_check/util.py:3075: AssertionError
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 3921.74it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 441.99it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 624.49it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 349.56it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 479.80it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 369.91it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 543.63it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 496.67it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 493.44it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 489.88it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_match_regex:negative_test_one_missing_value_and_insufficent_mostly] _

test_case = {'expectation_type': 'expect_column_values_to_match_regex', 'pk_column': True, 'skip': False, 'test': {'exact_match_ou...': 0.8, 'regex': '^a'}, 'out': {'success': False, 'unexpected_index_list': [3], 'unexpected_list': ['bdd']}, ...}, ...}

    @pytest.mark.order(index=0)
    @pytest.mark.integration
    @pytest.mark.slow  # 12.68s
    def test_case_runner_v3_api(test_case):
        if test_case["skip"]:
            pytest.skip()

        # Note: this should never be done in practice, but we are wiping expectations to reuse batches during testing.
        # test_case["batch"]._initialize_expectations()
        if "parse_strings_as_datetimes" in test_case["test"]["in"]:
            with pytest.deprecated_call():
                evaluate_json_test_v3_api(
                    validator=test_case["validator_with_data"],
                    expectation_type=test_case["expectation_type"],
                    test=test_case["test"],
                    pk_column=test_case["pk_column"],
                )
        else:
>           evaluate_json_test_v3_api(
                validator=test_case["validator_with_data"],
                expectation_type=test_case["expectation_type"],
                test=test_case["test"],
                pk_column=test_case["pk_column"],
            )

tests/test_definitions/test_expectations_v3_api.py:418:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
great_expectations/self_check/util.py:2901: in evaluate_json_test_v3_api
    check_json_test_result(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

test = {'exact_match_out': False, 'in': {'column': 'b', 'mostly': 0.8, 'regex': '^a'}, 'input': {'column': 'b', 'mostly': 0.8, 'regex': '^a'}, 'out': {'success': False, 'unexpected_index_list': [3], 'unexpected_list': ['bdd']}, ...}
result = {'exception_info': {'exception_message': None, 'exception_traceback': None, 'raised_exception': False}, 'expectation_c...5, 'missing_count': 1, 'missing_percent': 20.0, 'partial_unexpected_counts': [{'count': 1, 'value': 'bdd'}], ...}, ...}
data_asset = <great_expectations.execution_engine.pandas_batch_data.PandasBatchData object at 0x7f78062f0b50>
pk_column = True

    def check_json_test_result(  # noqa: C901 - 52
        test, result, data_asset=None, pk_column=False
    ) -> None:

        # check for id_pk results in cases where pk_column is true and unexpected_index_list already exists
        # this will work for testing since result_format is COMPLETE
        if pk_column:
            if not result["success"]:
                if "unexpected_index_list" in result["result"]:
                    assert "unexpected_index_query" in result["result"]

        if "unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )
            elif "unexpected_list" in test["output"]:
                (
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )

        if "partial_unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "partial_unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )
            elif "partial_unexpected_list" in test["output"]:
                (
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )

        # Determine if np.allclose(..) might be needed for float comparison
        try_allclose = False
        if "observed_value" in test["output"]:
            if RX_FLOAT.match(repr(test["output"]["observed_value"])):
                try_allclose = True

        # Check results
        if test["exact_match_out"] is True:
            if "result" in result and "observed_value" in result["result"]:
                if isinstance(result["result"]["observed_value"], (np.floating, float)):
                    assert np.allclose(
                        result["result"]["observed_value"],
                        expectationValidationResultSchema.load(test["output"])["result"][
                            "observed_value"
                        ],
                        rtol=RTOL,
                        atol=ATOL,
                    ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {expectationValidationResultSchema.load(test['output'])['result']['observed_value']}"
                else:
                    assert result == expectationValidationResultSchema.load(
                        test["output"]
                    ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
            else:
                assert result == expectationValidationResultSchema.load(
                    test["output"]
                ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
        else:
            # Convert result to json since our tests are reading from json so cannot easily contain richer types (e.g. NaN)
            # NOTE - 20191031 - JPC - we may eventually want to change these tests as we update our view on how
            # representations, serializations, and objects should interact and how much of that is shown to the user.
            result = result.to_json_dict()
            for key, value in test["output"].items():
                # Apply our great expectations-specific test logic

                if key == "success":
                    if isinstance(value, (np.floating, float)):
                        try:
                            assert np.allclose(
                                result["success"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['success']} not np.allclose to {value}"
                        except TypeError:
                            assert (
                                result["success"] == value
                            ), f"{result['success']} != {value}"
                    else:
                        assert result["success"] == value, f"{result['success']} != {value}"

                elif key == "observed_value":
                    if "tolerance" in test:
                        if isinstance(value, dict):
                            assert set(result["result"]["observed_value"].keys()) == set(
                                value.keys()
                            ), f"{set(result['result']['observed_value'].keys())} != {set(value.keys())}"
                            for k, v in value.items():
                                assert np.allclose(
                                    result["result"]["observed_value"][k],
                                    v,
                                    rtol=test["tolerance"],
                                )
                        else:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=test["tolerance"],
                            )
                    else:
                        if isinstance(value, dict) and "values" in value:
                            try:
                                assert np.allclose(
                                    result["result"]["observed_value"]["values"],
                                    value["values"],
                                    rtol=RTOL,
                                    atol=ATOL,
                                ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']['values']} not np.allclose to {value['values']}"
                            except TypeError as e:
                                print(e)
                                assert (
                                    result["result"]["observed_value"] == value
                                ), f"{result['result']['observed_value']} != {value}"
                        elif try_allclose:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {value}"
                        else:
                            assert (
                                result["result"]["observed_value"] == value
                            ), f"{result['result']['observed_value']} != {value}"

                # NOTE: This is a key used ONLY for testing cases where an expectation is legitimately allowed to return
                # any of multiple possible observed_values. expect_column_values_to_be_of_type is one such expectation.
                elif key == "observed_value_list":
                    assert result["result"]["observed_value"] in value

                elif key == "unexpected_index_list":
                    if isinstance(data_asset, (SqlAlchemyDataset, SparkDFDataset)):
                        pass
                    elif isinstance(data_asset, (SqlAlchemyBatchData, SparkDFBatchData)):
                        pass
                    else:
>                       assert (
                            result["result"]["unexpected_index_list"] == value
                        ), f"{result['result']['unexpected_index_list']} != {value}"
E                       AssertionError: [{'b': 'bdd', 'pk_index': 3}] != [3]

great_expectations/self_check/util.py:3075: AssertionError
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 2164.80it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 230.42it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 329.99it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 202.88it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 295.86it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 240.77it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 394.06it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 367.23it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 365.39it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 363.65it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_match_regex:positive_test_one_missing_value_and_exact_mostly] _

test_case = {'expectation_type': 'expect_column_values_to_match_regex', 'pk_column': True, 'skip': False, 'test': {'exact_match_ou...': 0.75, 'regex': '^a'}, 'out': {'success': True, 'unexpected_index_list': [3], 'unexpected_list': ['bdd']}, ...}, ...}

    @pytest.mark.order(index=0)
    @pytest.mark.integration
    @pytest.mark.slow  # 12.68s
    def test_case_runner_v3_api(test_case):
        if test_case["skip"]:
            pytest.skip()

        # Note: this should never be done in practice, but we are wiping expectations to reuse batches during testing.
        # test_case["batch"]._initialize_expectations()
        if "parse_strings_as_datetimes" in test_case["test"]["in"]:
            with pytest.deprecated_call():
                evaluate_json_test_v3_api(
                    validator=test_case["validator_with_data"],
                    expectation_type=test_case["expectation_type"],
                    test=test_case["test"],
                    pk_column=test_case["pk_column"],
                )
        else:
>           evaluate_json_test_v3_api(
                validator=test_case["validator_with_data"],
                expectation_type=test_case["expectation_type"],
                test=test_case["test"],
                pk_column=test_case["pk_column"],
            )

tests/test_definitions/test_expectations_v3_api.py:418:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
great_expectations/self_check/util.py:2901: in evaluate_json_test_v3_api
    check_json_test_result(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

test = {'exact_match_out': False, 'in': {'column': 'b', 'mostly': 0.75, 'regex': '^a'}, 'input': {'column': 'b', 'mostly': 0.75, 'regex': '^a'}, 'out': {'success': True, 'unexpected_index_list': [3], 'unexpected_list': ['bdd']}, ...}
result = {'exception_info': {'exception_message': None, 'exception_traceback': None, 'raised_exception': False}, 'expectation_c...5, 'missing_count': 1, 'missing_percent': 20.0, 'partial_unexpected_counts': [{'count': 1, 'value': 'bdd'}], ...}, ...}
data_asset = <great_expectations.execution_engine.pandas_batch_data.PandasBatchData object at 0x7f78062f0b50>
pk_column = True

    def check_json_test_result(  # noqa: C901 - 52
        test, result, data_asset=None, pk_column=False
    ) -> None:

        # check for id_pk results in cases where pk_column is true and unexpected_index_list already exists
        # this will work for testing since result_format is COMPLETE
        if pk_column:
            if not result["success"]:
                if "unexpected_index_list" in result["result"]:
                    assert "unexpected_index_query" in result["result"]

        if "unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )
            elif "unexpected_list" in test["output"]:
                (
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )

        if "partial_unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "partial_unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )
            elif "partial_unexpected_list" in test["output"]:
                (
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )

        # Determine if np.allclose(..) might be needed for float comparison
        try_allclose = False
        if "observed_value" in test["output"]:
            if RX_FLOAT.match(repr(test["output"]["observed_value"])):
                try_allclose = True

        # Check results
        if test["exact_match_out"] is True:
            if "result" in result and "observed_value" in result["result"]:
                if isinstance(result["result"]["observed_value"], (np.floating, float)):
                    assert np.allclose(
                        result["result"]["observed_value"],
                        expectationValidationResultSchema.load(test["output"])["result"][
                            "observed_value"
                        ],
                        rtol=RTOL,
                        atol=ATOL,
                    ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {expectationValidationResultSchema.load(test['output'])['result']['observed_value']}"
                else:
                    assert result == expectationValidationResultSchema.load(
                        test["output"]
                    ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
            else:
                assert result == expectationValidationResultSchema.load(
                    test["output"]
                ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
        else:
            # Convert result to json since our tests are reading from json so cannot easily contain richer types (e.g. NaN)
            # NOTE - 20191031 - JPC - we may eventually want to change these tests as we update our view on how
            # representations, serializations, and objects should interact and how much of that is shown to the user.
            result = result.to_json_dict()
            for key, value in test["output"].items():
                # Apply our great expectations-specific test logic

                if key == "success":
                    if isinstance(value, (np.floating, float)):
                        try:
                            assert np.allclose(
                                result["success"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['success']} not np.allclose to {value}"
                        except TypeError:
                            assert (
                                result["success"] == value
                            ), f"{result['success']} != {value}"
                    else:
                        assert result["success"] == value, f"{result['success']} != {value}"

                elif key == "observed_value":
                    if "tolerance" in test:
                        if isinstance(value, dict):
                            assert set(result["result"]["observed_value"].keys()) == set(
                                value.keys()
                            ), f"{set(result['result']['observed_value'].keys())} != {set(value.keys())}"
                            for k, v in value.items():
                                assert np.allclose(
                                    result["result"]["observed_value"][k],
                                    v,
                                    rtol=test["tolerance"],
                                )
                        else:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=test["tolerance"],
                            )
                    else:
                        if isinstance(value, dict) and "values" in value:
                            try:
                                assert np.allclose(
                                    result["result"]["observed_value"]["values"],
                                    value["values"],
                                    rtol=RTOL,
                                    atol=ATOL,
                                ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']['values']} not np.allclose to {value['values']}"
                            except TypeError as e:
                                print(e)
                                assert (
                                    result["result"]["observed_value"] == value
                                ), f"{result['result']['observed_value']} != {value}"
                        elif try_allclose:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {value}"
                        else:
                            assert (
                                result["result"]["observed_value"] == value
                            ), f"{result['result']['observed_value']} != {value}"

                # NOTE: This is a key used ONLY for testing cases where an expectation is legitimately allowed to return
                # any of multiple possible observed_values. expect_column_values_to_be_of_type is one such expectation.
                elif key == "observed_value_list":
                    assert result["result"]["observed_value"] in value

                elif key == "unexpected_index_list":
                    if isinstance(data_asset, (SqlAlchemyDataset, SparkDFDataset)):
                        pass
                    elif isinstance(data_asset, (SqlAlchemyBatchData, SparkDFBatchData)):
                        pass
                    else:
>                       assert (
                            result["result"]["unexpected_index_list"] == value
                        ), f"{result['result']['unexpected_index_list']} != {value}"
E                       AssertionError: [{'b': 'bdd', 'pk_index': 3}] != [3]

great_expectations/self_check/util.py:3075: AssertionError
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 3766.78it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 414.25it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 576.48it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 323.24it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 442.28it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 331.90it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 528.48it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 493.56it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 491.42it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 489.05it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_match_regex:positive_test_one_missing_value_and_sufficent_mostly] _

test_case = {'expectation_type': 'expect_column_values_to_match_regex', 'pk_column': True, 'skip': False, 'test': {'exact_match_ou...y': 0.7, 'regex': '^a'}, 'out': {'success': True, 'unexpected_index_list': [3], 'unexpected_list': ['bdd']}, ...}, ...}

    @pytest.mark.order(index=0)
    @pytest.mark.integration
    @pytest.mark.slow  # 12.68s
    def test_case_runner_v3_api(test_case):
        if test_case["skip"]:
            pytest.skip()

        # Note: this should never be done in practice, but we are wiping expectations to reuse batches during testing.
        # test_case["batch"]._initialize_expectations()
        if "parse_strings_as_datetimes" in test_case["test"]["in"]:
            with pytest.deprecated_call():
                evaluate_json_test_v3_api(
                    validator=test_case["validator_with_data"],
                    expectation_type=test_case["expectation_type"],
                    test=test_case["test"],
                    pk_column=test_case["pk_column"],
                )
        else:
>           evaluate_json_test_v3_api(
                validator=test_case["validator_with_data"],
                expectation_type=test_case["expectation_type"],
                test=test_case["test"],
                pk_column=test_case["pk_column"],
            )

tests/test_definitions/test_expectations_v3_api.py:418:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
great_expectations/self_check/util.py:2901: in evaluate_json_test_v3_api
    check_json_test_result(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

test = {'exact_match_out': False, 'in': {'column': 'b', 'mostly': 0.7, 'regex': '^a'}, 'input': {'column': 'b', 'mostly': 0.7, 'regex': '^a'}, 'out': {'success': True, 'unexpected_index_list': [3], 'unexpected_list': ['bdd']}, ...}
result = {'exception_info': {'exception_message': None, 'exception_traceback': None, 'raised_exception': False}, 'expectation_c...5, 'missing_count': 1, 'missing_percent': 20.0, 'partial_unexpected_counts': [{'count': 1, 'value': 'bdd'}], ...}, ...}
data_asset = <great_expectations.execution_engine.pandas_batch_data.PandasBatchData object at 0x7f78062f0b50>
pk_column = True

    def check_json_test_result(  # noqa: C901 - 52
        test, result, data_asset=None, pk_column=False
    ) -> None:

        # check for id_pk results in cases where pk_column is true and unexpected_index_list already exists
        # this will work for testing since result_format is COMPLETE
        if pk_column:
            if not result["success"]:
                if "unexpected_index_list" in result["result"]:
                    assert "unexpected_index_query" in result["result"]

        if "unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )
            elif "unexpected_list" in test["output"]:
                (
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )

        if "partial_unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "partial_unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )
            elif "partial_unexpected_list" in test["output"]:
                (
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )

        # Determine if np.allclose(..) might be needed for float comparison
        try_allclose = False
        if "observed_value" in test["output"]:
            if RX_FLOAT.match(repr(test["output"]["observed_value"])):
                try_allclose = True

        # Check results
        if test["exact_match_out"] is True:
            if "result" in result and "observed_value" in result["result"]:
                if isinstance(result["result"]["observed_value"], (np.floating, float)):
                    assert np.allclose(
                        result["result"]["observed_value"],
                        expectationValidationResultSchema.load(test["output"])["result"][
                            "observed_value"
                        ],
                        rtol=RTOL,
                        atol=ATOL,
                    ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {expectationValidationResultSchema.load(test['output'])['result']['observed_value']}"
                else:
                    assert result == expectationValidationResultSchema.load(
                        test["output"]
                    ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
            else:
                assert result == expectationValidationResultSchema.load(
                    test["output"]
                ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
        else:
            # Convert result to json since our tests are reading from json so cannot easily contain richer types (e.g. NaN)
            # NOTE - 20191031 - JPC - we may eventually want to change these tests as we update our view on how
            # representations, serializations, and objects should interact and how much of that is shown to the user.
            result = result.to_json_dict()
            for key, value in test["output"].items():
                # Apply our great expectations-specific test logic

                if key == "success":
                    if isinstance(value, (np.floating, float)):
                        try:
                            assert np.allclose(
                                result["success"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['success']} not np.allclose to {value}"
                        except TypeError:
                            assert (
                                result["success"] == value
                            ), f"{result['success']} != {value}"
                    else:
                        assert result["success"] == value, f"{result['success']} != {value}"

                elif key == "observed_value":
                    if "tolerance" in test:
                        if isinstance(value, dict):
                            assert set(result["result"]["observed_value"].keys()) == set(
                                value.keys()
                            ), f"{set(result['result']['observed_value'].keys())} != {set(value.keys())}"
                            for k, v in value.items():
                                assert np.allclose(
                                    result["result"]["observed_value"][k],
                                    v,
                                    rtol=test["tolerance"],
                                )
                        else:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=test["tolerance"],
                            )
                    else:
                        if isinstance(value, dict) and "values" in value:
                            try:
                                assert np.allclose(
                                    result["result"]["observed_value"]["values"],
                                    value["values"],
                                    rtol=RTOL,
                                    atol=ATOL,
                                ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']['values']} not np.allclose to {value['values']}"
                            except TypeError as e:
                                print(e)
                                assert (
                                    result["result"]["observed_value"] == value
                                ), f"{result['result']['observed_value']} != {value}"
                        elif try_allclose:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {value}"
                        else:
                            assert (
                                result["result"]["observed_value"] == value
                            ), f"{result['result']['observed_value']} != {value}"

                # NOTE: This is a key used ONLY for testing cases where an expectation is legitimately allowed to return
                # any of multiple possible observed_values. expect_column_values_to_be_of_type is one such expectation.
                elif key == "observed_value_list":
                    assert result["result"]["observed_value"] in value

                elif key == "unexpected_index_list":
                    if isinstance(data_asset, (SqlAlchemyDataset, SparkDFDataset)):
                        pass
                    elif isinstance(data_asset, (SqlAlchemyBatchData, SparkDFBatchData)):
                        pass
                    else:
>                       assert (
                            result["result"]["unexpected_index_list"] == value
                        ), f"{result['result']['unexpected_index_list']} != {value}"
E                       AssertionError: [{'b': 'bdd', 'pk_index': 3}] != [3]

great_expectations/self_check/util.py:3075: AssertionError
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 3765.08it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 387.07it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 547.54it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 321.82it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 453.10it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 351.71it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 552.15it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 514.17it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 511.82it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 509.17it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_match_regex:positive_test_match_characters_not_at_the_beginning_of_string] _

test_case = {'expectation_type': 'expect_column_values_to_match_regex', 'pk_column': True, 'skip': False, 'test': {'exact_match_ou...b'}, 'out': {'success': True, 'unexpected_index_list': [0, 2, 3], 'unexpected_list': ['aaa', 'acc', 'add']}, ...}, ...}

    @pytest.mark.order(index=0)
    @pytest.mark.integration
    @pytest.mark.slow  # 12.68s
    def test_case_runner_v3_api(test_case):
        if test_case["skip"]:
            pytest.skip()

        # Note: this should never be done in practice, but we are wiping expectations to reuse batches during testing.
        # test_case["batch"]._initialize_expectations()
        if "parse_strings_as_datetimes" in test_case["test"]["in"]:
            with pytest.deprecated_call():
                evaluate_json_test_v3_api(
                    validator=test_case["validator_with_data"],
                    expectation_type=test_case["expectation_type"],
                    test=test_case["test"],
                    pk_column=test_case["pk_column"],
                )
        else:
>           evaluate_json_test_v3_api(
                validator=test_case["validator_with_data"],
                expectation_type=test_case["expectation_type"],
                test=test_case["test"],
                pk_column=test_case["pk_column"],
            )

tests/test_definitions/test_expectations_v3_api.py:418:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
great_expectations/self_check/util.py:2901: in evaluate_json_test_v3_api
    check_json_test_result(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

test = {'exact_match_out': False, 'in': {'column': 'a', 'mostly': 0.4, 'regex': 'b'}, 'input': {'column': 'a', 'mostly': 0.4,...ex': 'b'}, 'out': {'success': True, 'unexpected_index_list': [0, 2, 3], 'unexpected_list': ['aaa', 'acc', 'add']}, ...}
result = {'exception_info': {'exception_message': None, 'exception_traceback': None, 'raised_exception': False}, 'expectation_c...xpected_counts': [{'count': 1, 'value': 'aaa'}, {'count': 1, 'value': 'acc'}, {'count': 1, 'value': 'add'}], ...}, ...}
data_asset = <great_expectations.execution_engine.pandas_batch_data.PandasBatchData object at 0x7f78062f0b50>
pk_column = True

    def check_json_test_result(  # noqa: C901 - 52
        test, result, data_asset=None, pk_column=False
    ) -> None:

        # check for id_pk results in cases where pk_column is true and unexpected_index_list already exists
        # this will work for testing since result_format is COMPLETE
        if pk_column:
            if not result["success"]:
                if "unexpected_index_list" in result["result"]:
                    assert "unexpected_index_query" in result["result"]

        if "unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )
            elif "unexpected_list" in test["output"]:
                (
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )

        if "partial_unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "partial_unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )
            elif "partial_unexpected_list" in test["output"]:
                (
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )

        # Determine if np.allclose(..) might be needed for float comparison
        try_allclose = False
        if "observed_value" in test["output"]:
            if RX_FLOAT.match(repr(test["output"]["observed_value"])):
                try_allclose = True

        # Check results
        if test["exact_match_out"] is True:
            if "result" in result and "observed_value" in result["result"]:
                if isinstance(result["result"]["observed_value"], (np.floating, float)):
                    assert np.allclose(
                        result["result"]["observed_value"],
                        expectationValidationResultSchema.load(test["output"])["result"][
                            "observed_value"
                        ],
                        rtol=RTOL,
                        atol=ATOL,
                    ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {expectationValidationResultSchema.load(test['output'])['result']['observed_value']}"
                else:
                    assert result == expectationValidationResultSchema.load(
                        test["output"]
                    ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
            else:
                assert result == expectationValidationResultSchema.load(
                    test["output"]
                ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
        else:
            # Convert result to json since our tests are reading from json so cannot easily contain richer types (e.g. NaN)
            # NOTE - 20191031 - JPC - we may eventually want to change these tests as we update our view on how
            # representations, serializations, and objects should interact and how much of that is shown to the user.
            result = result.to_json_dict()
            for key, value in test["output"].items():
                # Apply our great expectations-specific test logic

                if key == "success":
                    if isinstance(value, (np.floating, float)):
                        try:
                            assert np.allclose(
                                result["success"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['success']} not np.allclose to {value}"
                        except TypeError:
                            assert (
                                result["success"] == value
                            ), f"{result['success']} != {value}"
                    else:
                        assert result["success"] == value, f"{result['success']} != {value}"

                elif key == "observed_value":
                    if "tolerance" in test:
                        if isinstance(value, dict):
                            assert set(result["result"]["observed_value"].keys()) == set(
                                value.keys()
                            ), f"{set(result['result']['observed_value'].keys())} != {set(value.keys())}"
                            for k, v in value.items():
                                assert np.allclose(
                                    result["result"]["observed_value"][k],
                                    v,
                                    rtol=test["tolerance"],
                                )
                        else:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=test["tolerance"],
                            )
                    else:
                        if isinstance(value, dict) and "values" in value:
                            try:
                                assert np.allclose(
                                    result["result"]["observed_value"]["values"],
                                    value["values"],
                                    rtol=RTOL,
                                    atol=ATOL,
                                ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']['values']} not np.allclose to {value['values']}"
                            except TypeError as e:
                                print(e)
                                assert (
                                    result["result"]["observed_value"] == value
                                ), f"{result['result']['observed_value']} != {value}"
                        elif try_allclose:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {value}"
                        else:
                            assert (
                                result["result"]["observed_value"] == value
                            ), f"{result['result']['observed_value']} != {value}"

                # NOTE: This is a key used ONLY for testing cases where an expectation is legitimately allowed to return
                # any of multiple possible observed_values. expect_column_values_to_be_of_type is one such expectation.
                elif key == "observed_value_list":
                    assert result["result"]["observed_value"] in value

                elif key == "unexpected_index_list":
                    if isinstance(data_asset, (SqlAlchemyDataset, SparkDFDataset)):
                        pass
                    elif isinstance(data_asset, (SqlAlchemyBatchData, SparkDFBatchData)):
                        pass
                    else:
>                       assert (
                            result["result"]["unexpected_index_list"] == value
                        ), f"{result['result']['unexpected_index_list']} != {value}"
E                       AssertionError: [{'a': 'aaa', 'pk_index': 0}, {'a': 'acc', 'pk_index': 2}, {'a': 'add', 'pk_index': 3}] != [0, 2, 3]

great_expectations/self_check/util.py:3075: AssertionError
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 3951.30it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 453.02it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 641.99it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 356.34it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 492.02it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 371.00it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 577.80it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 536.34it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 533.70it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 530.90it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_between:basic_negative_test1] _

test_case = {'expectation_type': 'expect_column_values_to_be_between', 'pk_column': True, 'skip': False, 'test': {'exact_match_out...': 9, 'min_value': 1}, 'include_in_gallery': True, 'input': {'column': 'x', 'max_value': 9, 'min_value': 1}, ...}, ...}

    @pytest.mark.order(index=0)
    @pytest.mark.integration
    @pytest.mark.slow  # 12.68s
    def test_case_runner_v3_api(test_case):
        if test_case["skip"]:
            pytest.skip()

        # Note: this should never be done in practice, but we are wiping expectations to reuse batches during testing.
        # test_case["batch"]._initialize_expectations()
        if "parse_strings_as_datetimes" in test_case["test"]["in"]:
            with pytest.deprecated_call():
                evaluate_json_test_v3_api(
                    validator=test_case["validator_with_data"],
                    expectation_type=test_case["expectation_type"],
                    test=test_case["test"],
                    pk_column=test_case["pk_column"],
                )
        else:
>           evaluate_json_test_v3_api(
                validator=test_case["validator_with_data"],
                expectation_type=test_case["expectation_type"],
                test=test_case["test"],
                pk_column=test_case["pk_column"],
            )

tests/test_definitions/test_expectations_v3_api.py:418:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
great_expectations/self_check/util.py:2901: in evaluate_json_test_v3_api
    check_json_test_result(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

test = {'exact_match_out': False, 'in': {'column': 'x', 'max_value': 9, 'min_value': 1}, 'include_in_gallery': True, 'input': {'column': 'x', 'max_value': 9, 'min_value': 1}, ...}
result = {'exception_info': {'exception_message': None, 'exception_traceback': None, 'raised_exception': False}, 'expectation_c...': 10, 'missing_count': 0, 'missing_percent': 0.0, 'partial_unexpected_counts': [{'count': 1, 'value': 10}], ...}, ...}
data_asset = <great_expectations.execution_engine.pandas_batch_data.PandasBatchData object at 0x7f7806340190>
pk_column = True

    def check_json_test_result(  # noqa: C901 - 52
        test, result, data_asset=None, pk_column=False
    ) -> None:

        # check for id_pk results in cases where pk_column is true and unexpected_index_list already exists
        # this will work for testing since result_format is COMPLETE
        if pk_column:
            if not result["success"]:
                if "unexpected_index_list" in result["result"]:
                    assert "unexpected_index_query" in result["result"]

        if "unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )
            elif "unexpected_list" in test["output"]:
                (
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )

        if "partial_unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "partial_unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )
            elif "partial_unexpected_list" in test["output"]:
                (
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )

        # Determine if np.allclose(..) might be needed for float comparison
        try_allclose = False
        if "observed_value" in test["output"]:
            if RX_FLOAT.match(repr(test["output"]["observed_value"])):
                try_allclose = True

        # Check results
        if test["exact_match_out"] is True:
            if "result" in result and "observed_value" in result["result"]:
                if isinstance(result["result"]["observed_value"], (np.floating, float)):
                    assert np.allclose(
                        result["result"]["observed_value"],
                        expectationValidationResultSchema.load(test["output"])["result"][
                            "observed_value"
                        ],
                        rtol=RTOL,
                        atol=ATOL,
                    ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {expectationValidationResultSchema.load(test['output'])['result']['observed_value']}"
                else:
                    assert result == expectationValidationResultSchema.load(
                        test["output"]
                    ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
            else:
                assert result == expectationValidationResultSchema.load(
                    test["output"]
                ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
        else:
            # Convert result to json since our tests are reading from json so cannot easily contain richer types (e.g. NaN)
            # NOTE - 20191031 - JPC - we may eventually want to change these tests as we update our view on how
            # representations, serializations, and objects should interact and how much of that is shown to the user.
            result = result.to_json_dict()
            for key, value in test["output"].items():
                # Apply our great expectations-specific test logic

                if key == "success":
                    if isinstance(value, (np.floating, float)):
                        try:
                            assert np.allclose(
                                result["success"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['success']} not np.allclose to {value}"
                        except TypeError:
                            assert (
                                result["success"] == value
                            ), f"{result['success']} != {value}"
                    else:
                        assert result["success"] == value, f"{result['success']} != {value}"

                elif key == "observed_value":
                    if "tolerance" in test:
                        if isinstance(value, dict):
                            assert set(result["result"]["observed_value"].keys()) == set(
                                value.keys()
                            ), f"{set(result['result']['observed_value'].keys())} != {set(value.keys())}"
                            for k, v in value.items():
                                assert np.allclose(
                                    result["result"]["observed_value"][k],
                                    v,
                                    rtol=test["tolerance"],
                                )
                        else:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=test["tolerance"],
                            )
                    else:
                        if isinstance(value, dict) and "values" in value:
                            try:
                                assert np.allclose(
                                    result["result"]["observed_value"]["values"],
                                    value["values"],
                                    rtol=RTOL,
                                    atol=ATOL,
                                ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']['values']} not np.allclose to {value['values']}"
                            except TypeError as e:
                                print(e)
                                assert (
                                    result["result"]["observed_value"] == value
                                ), f"{result['result']['observed_value']} != {value}"
                        elif try_allclose:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {value}"
                        else:
                            assert (
                                result["result"]["observed_value"] == value
                            ), f"{result['result']['observed_value']} != {value}"

                # NOTE: This is a key used ONLY for testing cases where an expectation is legitimately allowed to return
                # any of multiple possible observed_values. expect_column_values_to_be_of_type is one such expectation.
                elif key == "observed_value_list":
                    assert result["result"]["observed_value"] in value

                elif key == "unexpected_index_list":
                    if isinstance(data_asset, (SqlAlchemyDataset, SparkDFDataset)):
                        pass
                    elif isinstance(data_asset, (SqlAlchemyBatchData, SparkDFBatchData)):
                        pass
                    else:
>                       assert (
                            result["result"]["unexpected_index_list"] == value
                        ), f"{result['result']['unexpected_index_list']} != {value}"
E                       AssertionError: [{'x': 10, 'pk_index': 9}] != [9]

great_expectations/self_check/util.py:3075: AssertionError
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 3867.50it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 351.31it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 500.16it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 283.66it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 399.58it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 302.92it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 442.77it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 408.44it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 406.28it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 403.97it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_between:another_negative_test1] _

test_case = {'expectation_type': 'expect_column_values_to_be_between', 'pk_column': True, 'skip': False, 'test': {'exact_match_out... 10, 'min_value': 3}, 'out': {'success': False, 'unexpected_index_list': [0, 1], 'unexpected_list': [1, 2]}, ...}, ...}

    @pytest.mark.order(index=0)
    @pytest.mark.integration
    @pytest.mark.slow  # 12.68s
    def test_case_runner_v3_api(test_case):
        if test_case["skip"]:
            pytest.skip()

        # Note: this should never be done in practice, but we are wiping expectations to reuse batches during testing.
        # test_case["batch"]._initialize_expectations()
        if "parse_strings_as_datetimes" in test_case["test"]["in"]:
            with pytest.deprecated_call():
                evaluate_json_test_v3_api(
                    validator=test_case["validator_with_data"],
                    expectation_type=test_case["expectation_type"],
                    test=test_case["test"],
                    pk_column=test_case["pk_column"],
                )
        else:
>           evaluate_json_test_v3_api(
                validator=test_case["validator_with_data"],
                expectation_type=test_case["expectation_type"],
                test=test_case["test"],
                pk_column=test_case["pk_column"],
            )

tests/test_definitions/test_expectations_v3_api.py:418:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
great_expectations/self_check/util.py:2901: in evaluate_json_test_v3_api
    check_json_test_result(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

test = {'exact_match_out': False, 'in': {'column': 'x', 'max_value': 10, 'min_value': 3}, 'input': {'column': 'x', 'max_value': 10, 'min_value': 3}, 'out': {'success': False, 'unexpected_index_list': [0, 1], 'unexpected_list': [1, 2]}, ...}
result = {'exception_info': {'exception_message': None, 'exception_traceback': None, 'raised_exception': False}, 'expectation_c..., 'missing_percent': 0.0, 'partial_unexpected_counts': [{'count': 1, 'value': 1}, {'count': 1, 'value': 2}], ...}, ...}
data_asset = <great_expectations.execution_engine.pandas_batch_data.PandasBatchData object at 0x7f7806340190>
pk_column = True

    def check_json_test_result(  # noqa: C901 - 52
        test, result, data_asset=None, pk_column=False
    ) -> None:

        # check for id_pk results in cases where pk_column is true and unexpected_index_list already exists
        # this will work for testing since result_format is COMPLETE
        if pk_column:
            if not result["success"]:
                if "unexpected_index_list" in result["result"]:
                    assert "unexpected_index_query" in result["result"]

        if "unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )
            elif "unexpected_list" in test["output"]:
                (
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )

        if "partial_unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "partial_unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )
            elif "partial_unexpected_list" in test["output"]:
                (
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )

        # Determine if np.allclose(..) might be needed for float comparison
        try_allclose = False
        if "observed_value" in test["output"]:
            if RX_FLOAT.match(repr(test["output"]["observed_value"])):
                try_allclose = True

        # Check results
        if test["exact_match_out"] is True:
            if "result" in result and "observed_value" in result["result"]:
                if isinstance(result["result"]["observed_value"], (np.floating, float)):
                    assert np.allclose(
                        result["result"]["observed_value"],
                        expectationValidationResultSchema.load(test["output"])["result"][
                            "observed_value"
                        ],
                        rtol=RTOL,
                        atol=ATOL,
                    ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {expectationValidationResultSchema.load(test['output'])['result']['observed_value']}"
                else:
                    assert result == expectationValidationResultSchema.load(
                        test["output"]
                    ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
            else:
                assert result == expectationValidationResultSchema.load(
                    test["output"]
                ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
        else:
            # Convert result to json since our tests are reading from json so cannot easily contain richer types (e.g. NaN)
            # NOTE - 20191031 - JPC - we may eventually want to change these tests as we update our view on how
            # representations, serializations, and objects should interact and how much of that is shown to the user.
            result = result.to_json_dict()
            for key, value in test["output"].items():
                # Apply our great expectations-specific test logic

                if key == "success":
                    if isinstance(value, (np.floating, float)):
                        try:
                            assert np.allclose(
                                result["success"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['success']} not np.allclose to {value}"
                        except TypeError:
                            assert (
                                result["success"] == value
                            ), f"{result['success']} != {value}"
                    else:
                        assert result["success"] == value, f"{result['success']} != {value}"

                elif key == "observed_value":
                    if "tolerance" in test:
                        if isinstance(value, dict):
                            assert set(result["result"]["observed_value"].keys()) == set(
                                value.keys()
                            ), f"{set(result['result']['observed_value'].keys())} != {set(value.keys())}"
                            for k, v in value.items():
                                assert np.allclose(
                                    result["result"]["observed_value"][k],
                                    v,
                                    rtol=test["tolerance"],
                                )
                        else:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=test["tolerance"],
                            )
                    else:
                        if isinstance(value, dict) and "values" in value:
                            try:
                                assert np.allclose(
                                    result["result"]["observed_value"]["values"],
                                    value["values"],
                                    rtol=RTOL,
                                    atol=ATOL,
                                ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']['values']} not np.allclose to {value['values']}"
                            except TypeError as e:
                                print(e)
                                assert (
                                    result["result"]["observed_value"] == value
                                ), f"{result['result']['observed_value']} != {value}"
                        elif try_allclose:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {value}"
                        else:
                            assert (
                                result["result"]["observed_value"] == value
                            ), f"{result['result']['observed_value']} != {value}"

                # NOTE: This is a key used ONLY for testing cases where an expectation is legitimately allowed to return
                # any of multiple possible observed_values. expect_column_values_to_be_of_type is one such expectation.
                elif key == "observed_value_list":
                    assert result["result"]["observed_value"] in value

                elif key == "unexpected_index_list":
                    if isinstance(data_asset, (SqlAlchemyDataset, SparkDFDataset)):
                        pass
                    elif isinstance(data_asset, (SqlAlchemyBatchData, SparkDFBatchData)):
                        pass
                    else:
>                       assert (
                            result["result"]["unexpected_index_list"] == value
                        ), f"{result['result']['unexpected_index_list']} != {value}"
E                       AssertionError: [{'x': 1, 'pk_index': 0}, {'x': 2, 'pk_index': 1}] != [0, 1]

great_expectations/self_check/util.py:3075: AssertionError
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 2453.53it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 275.86it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 395.88it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 241.45it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 347.49it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 274.54it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 432.19it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 402.83it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 400.98it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 399.31it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_between:basic_negative_test_with_timestamps1] _

test_case = {'expectation_type': 'expect_column_values_to_be_between', 'pk_column': True, 'skip': False, 'test': {'exact_match_out... 12:00:00', 'parse_strings_as_datetimes': True}, 'out': {'success': False, 'unexpected_index_list': [0, 9]}, ...}, ...}

    @pytest.mark.order(index=0)
    @pytest.mark.integration
    @pytest.mark.slow  # 12.68s
    def test_case_runner_v3_api(test_case):
        if test_case["skip"]:
            pytest.skip()

        # Note: this should never be done in practice, but we are wiping expectations to reuse batches during testing.
        # test_case["batch"]._initialize_expectations()
        if "parse_strings_as_datetimes" in test_case["test"]["in"]:
            with pytest.deprecated_call():
>               evaluate_json_test_v3_api(
                    validator=test_case["validator_with_data"],
                    expectation_type=test_case["expectation_type"],
                    test=test_case["test"],
                    pk_column=test_case["pk_column"],
                )

tests/test_definitions/test_expectations_v3_api.py:411:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
great_expectations/self_check/util.py:2901: in evaluate_json_test_v3_api
    check_json_test_result(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

test = {'exact_match_out': False, 'in': {'column': 'ts', 'max_value': 'Jan 01 2001 12:00:00', 'min_value': 'Jan 01 1990 12:00...1 1990 12:00:00', 'parse_strings_as_datetimes': True}, 'out': {'success': False, 'unexpected_index_list': [0, 9]}, ...}
result = {'exception_info': {'exception_message': None, 'exception_traceback': None, 'raised_exception': False}, 'expectation_c...ected_counts': [{'count': 1, 'value': '1970-01-01T12:00:01'}, {'count': 1, 'value': '2001-01-01T12:00:01'}], ...}, ...}
data_asset = <great_expectations.execution_engine.pandas_batch_data.PandasBatchData object at 0x7f7806340190>
pk_column = True

    def check_json_test_result(  # noqa: C901 - 52
        test, result, data_asset=None, pk_column=False
    ) -> None:

        # check for id_pk results in cases where pk_column is true and unexpected_index_list already exists
        # this will work for testing since result_format is COMPLETE
        if pk_column:
            if not result["success"]:
                if "unexpected_index_list" in result["result"]:
                    assert "unexpected_index_query" in result["result"]

        if "unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )
            elif "unexpected_list" in test["output"]:
                (
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )

        if "partial_unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "partial_unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )
            elif "partial_unexpected_list" in test["output"]:
                (
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )

        # Determine if np.allclose(..) might be needed for float comparison
        try_allclose = False
        if "observed_value" in test["output"]:
            if RX_FLOAT.match(repr(test["output"]["observed_value"])):
                try_allclose = True

        # Check results
        if test["exact_match_out"] is True:
            if "result" in result and "observed_value" in result["result"]:
                if isinstance(result["result"]["observed_value"], (np.floating, float)):
                    assert np.allclose(
                        result["result"]["observed_value"],
                        expectationValidationResultSchema.load(test["output"])["result"][
                            "observed_value"
                        ],
                        rtol=RTOL,
                        atol=ATOL,
                    ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {expectationValidationResultSchema.load(test['output'])['result']['observed_value']}"
                else:
                    assert result == expectationValidationResultSchema.load(
                        test["output"]
                    ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
            else:
                assert result == expectationValidationResultSchema.load(
                    test["output"]
                ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
        else:
            # Convert result to json since our tests are reading from json so cannot easily contain richer types (e.g. NaN)
            # NOTE - 20191031 - JPC - we may eventually want to change these tests as we update our view on how
            # representations, serializations, and objects should interact and how much of that is shown to the user.
            result = result.to_json_dict()
            for key, value in test["output"].items():
                # Apply our great expectations-specific test logic

                if key == "success":
                    if isinstance(value, (np.floating, float)):
                        try:
                            assert np.allclose(
                                result["success"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['success']} not np.allclose to {value}"
                        except TypeError:
                            assert (
                                result["success"] == value
                            ), f"{result['success']} != {value}"
                    else:
                        assert result["success"] == value, f"{result['success']} != {value}"

                elif key == "observed_value":
                    if "tolerance" in test:
                        if isinstance(value, dict):
                            assert set(result["result"]["observed_value"].keys()) == set(
                                value.keys()
                            ), f"{set(result['result']['observed_value'].keys())} != {set(value.keys())}"
                            for k, v in value.items():
                                assert np.allclose(
                                    result["result"]["observed_value"][k],
                                    v,
                                    rtol=test["tolerance"],
                                )
                        else:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=test["tolerance"],
                            )
                    else:
                        if isinstance(value, dict) and "values" in value:
                            try:
                                assert np.allclose(
                                    result["result"]["observed_value"]["values"],
                                    value["values"],
                                    rtol=RTOL,
                                    atol=ATOL,
                                ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']['values']} not np.allclose to {value['values']}"
                            except TypeError as e:
                                print(e)
                                assert (
                                    result["result"]["observed_value"] == value
                                ), f"{result['result']['observed_value']} != {value}"
                        elif try_allclose:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {value}"
                        else:
                            assert (
                                result["result"]["observed_value"] == value
                            ), f"{result['result']['observed_value']} != {value}"

                # NOTE: This is a key used ONLY for testing cases where an expectation is legitimately allowed to return
                # any of multiple possible observed_values. expect_column_values_to_be_of_type is one such expectation.
                elif key == "observed_value_list":
                    assert result["result"]["observed_value"] in value

                elif key == "unexpected_index_list":
                    if isinstance(data_asset, (SqlAlchemyDataset, SparkDFDataset)):
                        pass
                    elif isinstance(data_asset, (SqlAlchemyBatchData, SparkDFBatchData)):
                        pass
                    else:
>                       assert (
                            result["result"]["unexpected_index_list"] == value
                        ), f"{result['result']['unexpected_index_list']} != {value}"
E                       AssertionError: [{'ts': '1970-01-01T12:00:01', 'pk_index': 0}, {'ts': '2001-01-01T12:00:01', 'pk_index': 9}] != [0, 9]

great_expectations/self_check/util.py:3075: AssertionError
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 3816.47it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 372.45it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 529.47it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 261.91it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 359.32it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 271.71it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 426.82it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 398.85it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 397.31it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 395.76it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_between:3rd_positive_test_with_mostly1] _

test_case = {'expectation_type': 'expect_column_values_to_be_between', 'pk_column': True, 'skip': False, 'test': {'exact_match_out..._value': 1, 'mostly': 0.9}, 'out': {'success': True, 'unexpected_index_list': [9], 'unexpected_list': [10]}, ...}, ...}

    @pytest.mark.order(index=0)
    @pytest.mark.integration
    @pytest.mark.slow  # 12.68s
    def test_case_runner_v3_api(test_case):
        if test_case["skip"]:
            pytest.skip()

        # Note: this should never be done in practice, but we are wiping expectations to reuse batches during testing.
        # test_case["batch"]._initialize_expectations()
        if "parse_strings_as_datetimes" in test_case["test"]["in"]:
            with pytest.deprecated_call():
                evaluate_json_test_v3_api(
                    validator=test_case["validator_with_data"],
                    expectation_type=test_case["expectation_type"],
                    test=test_case["test"],
                    pk_column=test_case["pk_column"],
                )
        else:
>           evaluate_json_test_v3_api(
                validator=test_case["validator_with_data"],
                expectation_type=test_case["expectation_type"],
                test=test_case["test"],
                pk_column=test_case["pk_column"],
            )

tests/test_definitions/test_expectations_v3_api.py:418:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
great_expectations/self_check/util.py:2901: in evaluate_json_test_v3_api
    check_json_test_result(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

test = {'exact_match_out': False, 'in': {'column': 'x', 'max_value': 9, 'min_value': 1, 'mostly': 0.9}, 'input': {'column': '..., 'min_value': 1, 'mostly': 0.9}, 'out': {'success': True, 'unexpected_index_list': [9], 'unexpected_list': [10]}, ...}
result = {'exception_info': {'exception_message': None, 'exception_traceback': None, 'raised_exception': False}, 'expectation_c...': 10, 'missing_count': 0, 'missing_percent': 0.0, 'partial_unexpected_counts': [{'count': 1, 'value': 10}], ...}, ...}
data_asset = <great_expectations.execution_engine.pandas_batch_data.PandasBatchData object at 0x7f7806340190>
pk_column = True

    def check_json_test_result(  # noqa: C901 - 52
        test, result, data_asset=None, pk_column=False
    ) -> None:

        # check for id_pk results in cases where pk_column is true and unexpected_index_list already exists
        # this will work for testing since result_format is COMPLETE
        if pk_column:
            if not result["success"]:
                if "unexpected_index_list" in result["result"]:
                    assert "unexpected_index_query" in result["result"]

        if "unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )
            elif "unexpected_list" in test["output"]:
                (
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )

        if "partial_unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "partial_unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )
            elif "partial_unexpected_list" in test["output"]:
                (
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )

        # Determine if np.allclose(..) might be needed for float comparison
        try_allclose = False
        if "observed_value" in test["output"]:
            if RX_FLOAT.match(repr(test["output"]["observed_value"])):
                try_allclose = True

        # Check results
        if test["exact_match_out"] is True:
            if "result" in result and "observed_value" in result["result"]:
                if isinstance(result["result"]["observed_value"], (np.floating, float)):
                    assert np.allclose(
                        result["result"]["observed_value"],
                        expectationValidationResultSchema.load(test["output"])["result"][
                            "observed_value"
                        ],
                        rtol=RTOL,
                        atol=ATOL,
                    ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {expectationValidationResultSchema.load(test['output'])['result']['observed_value']}"
                else:
                    assert result == expectationValidationResultSchema.load(
                        test["output"]
                    ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
            else:
                assert result == expectationValidationResultSchema.load(
                    test["output"]
                ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
        else:
            # Convert result to json since our tests are reading from json so cannot easily contain richer types (e.g. NaN)
            # NOTE - 20191031 - JPC - we may eventually want to change these tests as we update our view on how
            # representations, serializations, and objects should interact and how much of that is shown to the user.
            result = result.to_json_dict()
            for key, value in test["output"].items():
                # Apply our great expectations-specific test logic

                if key == "success":
                    if isinstance(value, (np.floating, float)):
                        try:
                            assert np.allclose(
                                result["success"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['success']} not np.allclose to {value}"
                        except TypeError:
                            assert (
                                result["success"] == value
                            ), f"{result['success']} != {value}"
                    else:
                        assert result["success"] == value, f"{result['success']} != {value}"

                elif key == "observed_value":
                    if "tolerance" in test:
                        if isinstance(value, dict):
                            assert set(result["result"]["observed_value"].keys()) == set(
                                value.keys()
                            ), f"{set(result['result']['observed_value'].keys())} != {set(value.keys())}"
                            for k, v in value.items():
                                assert np.allclose(
                                    result["result"]["observed_value"][k],
                                    v,
                                    rtol=test["tolerance"],
                                )
                        else:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=test["tolerance"],
                            )
                    else:
                        if isinstance(value, dict) and "values" in value:
                            try:
                                assert np.allclose(
                                    result["result"]["observed_value"]["values"],
                                    value["values"],
                                    rtol=RTOL,
                                    atol=ATOL,
                                ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']['values']} not np.allclose to {value['values']}"
                            except TypeError as e:
                                print(e)
                                assert (
                                    result["result"]["observed_value"] == value
                                ), f"{result['result']['observed_value']} != {value}"
                        elif try_allclose:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {value}"
                        else:
                            assert (
                                result["result"]["observed_value"] == value
                            ), f"{result['result']['observed_value']} != {value}"

                # NOTE: This is a key used ONLY for testing cases where an expectation is legitimately allowed to return
                # any of multiple possible observed_values. expect_column_values_to_be_of_type is one such expectation.
                elif key == "observed_value_list":
                    assert result["result"]["observed_value"] in value

                elif key == "unexpected_index_list":
                    if isinstance(data_asset, (SqlAlchemyDataset, SparkDFDataset)):
                        pass
                    elif isinstance(data_asset, (SqlAlchemyBatchData, SparkDFBatchData)):
                        pass
                    else:
>                       assert (
                            result["result"]["unexpected_index_list"] == value
                        ), f"{result['result']['unexpected_index_list']} != {value}"
E                       AssertionError: [{'x': 10, 'pk_index': 9}] != [9]

great_expectations/self_check/util.py:3075: AssertionError
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 3951.30it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 374.37it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 534.37it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 277.03it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 385.68it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 292.96it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 441.95it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 408.93it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 407.08it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 405.33it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_between:negative_test_with_mostly1] _

test_case = {'expectation_type': 'expect_column_values_to_be_between', 'pk_column': True, 'skip': False, 'test': {'exact_match_out...': 3, 'mostly': 0.9}, 'out': {'success': False, 'unexpected_index_list': [0, 1], 'unexpected_list': [1, 2]}, ...}, ...}

    @pytest.mark.order(index=0)
    @pytest.mark.integration
    @pytest.mark.slow  # 12.68s
    def test_case_runner_v3_api(test_case):
        if test_case["skip"]:
            pytest.skip()

        # Note: this should never be done in practice, but we are wiping expectations to reuse batches during testing.
        # test_case["batch"]._initialize_expectations()
        if "parse_strings_as_datetimes" in test_case["test"]["in"]:
            with pytest.deprecated_call():
                evaluate_json_test_v3_api(
                    validator=test_case["validator_with_data"],
                    expectation_type=test_case["expectation_type"],
                    test=test_case["test"],
                    pk_column=test_case["pk_column"],
                )
        else:
>           evaluate_json_test_v3_api(
                validator=test_case["validator_with_data"],
                expectation_type=test_case["expectation_type"],
                test=test_case["test"],
                pk_column=test_case["pk_column"],
            )

tests/test_definitions/test_expectations_v3_api.py:418:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
great_expectations/self_check/util.py:2901: in evaluate_json_test_v3_api
    check_json_test_result(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

test = {'exact_match_out': False, 'in': {'column': 'x', 'max_value': 10, 'min_value': 3, 'mostly': 0.9}, 'input': {'column': ..._value': 3, 'mostly': 0.9}, 'out': {'success': False, 'unexpected_index_list': [0, 1], 'unexpected_list': [1, 2]}, ...}
result = {'exception_info': {'exception_message': None, 'exception_traceback': None, 'raised_exception': False}, 'expectation_c..., 'missing_percent': 0.0, 'partial_unexpected_counts': [{'count': 1, 'value': 1}, {'count': 1, 'value': 2}], ...}, ...}
data_asset = <great_expectations.execution_engine.pandas_batch_data.PandasBatchData object at 0x7f7806340190>
pk_column = True

    def check_json_test_result(  # noqa: C901 - 52
        test, result, data_asset=None, pk_column=False
    ) -> None:

        # check for id_pk results in cases where pk_column is true and unexpected_index_list already exists
        # this will work for testing since result_format is COMPLETE
        if pk_column:
            if not result["success"]:
                if "unexpected_index_list" in result["result"]:
                    assert "unexpected_index_query" in result["result"]

        if "unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )
            elif "unexpected_list" in test["output"]:
                (
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )

        if "partial_unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "partial_unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )
            elif "partial_unexpected_list" in test["output"]:
                (
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )

        # Determine if np.allclose(..) might be needed for float comparison
        try_allclose = False
        if "observed_value" in test["output"]:
            if RX_FLOAT.match(repr(test["output"]["observed_value"])):
                try_allclose = True

        # Check results
        if test["exact_match_out"] is True:
            if "result" in result and "observed_value" in result["result"]:
                if isinstance(result["result"]["observed_value"], (np.floating, float)):
                    assert np.allclose(
                        result["result"]["observed_value"],
                        expectationValidationResultSchema.load(test["output"])["result"][
                            "observed_value"
                        ],
                        rtol=RTOL,
                        atol=ATOL,
                    ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {expectationValidationResultSchema.load(test['output'])['result']['observed_value']}"
                else:
                    assert result == expectationValidationResultSchema.load(
                        test["output"]
                    ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
            else:
                assert result == expectationValidationResultSchema.load(
                    test["output"]
                ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
        else:
            # Convert result to json since our tests are reading from json so cannot easily contain richer types (e.g. NaN)
            # NOTE - 20191031 - JPC - we may eventually want to change these tests as we update our view on how
            # representations, serializations, and objects should interact and how much of that is shown to the user.
            result = result.to_json_dict()
            for key, value in test["output"].items():
                # Apply our great expectations-specific test logic

                if key == "success":
                    if isinstance(value, (np.floating, float)):
                        try:
                            assert np.allclose(
                                result["success"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['success']} not np.allclose to {value}"
                        except TypeError:
                            assert (
                                result["success"] == value
                            ), f"{result['success']} != {value}"
                    else:
                        assert result["success"] == value, f"{result['success']} != {value}"

                elif key == "observed_value":
                    if "tolerance" in test:
                        if isinstance(value, dict):
                            assert set(result["result"]["observed_value"].keys()) == set(
                                value.keys()
                            ), f"{set(result['result']['observed_value'].keys())} != {set(value.keys())}"
                            for k, v in value.items():
                                assert np.allclose(
                                    result["result"]["observed_value"][k],
                                    v,
                                    rtol=test["tolerance"],
                                )
                        else:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=test["tolerance"],
                            )
                    else:
                        if isinstance(value, dict) and "values" in value:
                            try:
                                assert np.allclose(
                                    result["result"]["observed_value"]["values"],
                                    value["values"],
                                    rtol=RTOL,
                                    atol=ATOL,
                                ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']['values']} not np.allclose to {value['values']}"
                            except TypeError as e:
                                print(e)
                                assert (
                                    result["result"]["observed_value"] == value
                                ), f"{result['result']['observed_value']} != {value}"
                        elif try_allclose:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {value}"
                        else:
                            assert (
                                result["result"]["observed_value"] == value
                            ), f"{result['result']['observed_value']} != {value}"

                # NOTE: This is a key used ONLY for testing cases where an expectation is legitimately allowed to return
                # any of multiple possible observed_values. expect_column_values_to_be_of_type is one such expectation.
                elif key == "observed_value_list":
                    assert result["result"]["observed_value"] in value

                elif key == "unexpected_index_list":
                    if isinstance(data_asset, (SqlAlchemyDataset, SparkDFDataset)):
                        pass
                    elif isinstance(data_asset, (SqlAlchemyBatchData, SparkDFBatchData)):
                        pass
                    else:
>                       assert (
                            result["result"]["unexpected_index_list"] == value
                        ), f"{result['result']['unexpected_index_list']} != {value}"
E                       AssertionError: [{'x': 1, 'pk_index': 0}, {'x': 2, 'pk_index': 1}] != [0, 1]

great_expectations/self_check/util.py:3075: AssertionError
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 3858.61it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 354.16it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 504.69it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 280.92it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 397.07it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 301.16it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 462.31it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 429.40it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 427.71it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 425.87it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_between:negative_test_to_verify_that_the_denominator_for_mostly_works_with_missing_values1] _

test_case = {'expectation_type': 'expect_column_values_to_be_between', 'pk_column': True, 'skip': False, 'test': {'exact_match_out..._value': 1, 'mostly': 0.9}, 'out': {'success': False, 'unexpected_index_list': [4], 'unexpected_list': [5]}, ...}, ...}

    @pytest.mark.order(index=0)
    @pytest.mark.integration
    @pytest.mark.slow  # 12.68s
    def test_case_runner_v3_api(test_case):
        if test_case["skip"]:
            pytest.skip()

        # Note: this should never be done in practice, but we are wiping expectations to reuse batches during testing.
        # test_case["batch"]._initialize_expectations()
        if "parse_strings_as_datetimes" in test_case["test"]["in"]:
            with pytest.deprecated_call():
                evaluate_json_test_v3_api(
                    validator=test_case["validator_with_data"],
                    expectation_type=test_case["expectation_type"],
                    test=test_case["test"],
                    pk_column=test_case["pk_column"],
                )
        else:
>           evaluate_json_test_v3_api(
                validator=test_case["validator_with_data"],
                expectation_type=test_case["expectation_type"],
                test=test_case["test"],
                pk_column=test_case["pk_column"],
            )

tests/test_definitions/test_expectations_v3_api.py:418:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
great_expectations/self_check/util.py:2901: in evaluate_json_test_v3_api
    check_json_test_result(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

test = {'exact_match_out': False, 'in': {'column': 'z', 'max_value': 4, 'min_value': 1, 'mostly': 0.9}, 'input': {'column': '..., 'min_value': 1, 'mostly': 0.9}, 'out': {'success': False, 'unexpected_index_list': [4], 'unexpected_list': [5]}, ...}
result = {'exception_info': {'exception_message': None, 'exception_traceback': None, 'raised_exception': False}, 'expectation_c...': 10, 'missing_count': 5, 'missing_percent': 50.0, 'partial_unexpected_counts': [{'count': 1, 'value': 5}], ...}, ...}
data_asset = <great_expectations.execution_engine.pandas_batch_data.PandasBatchData object at 0x7f7806340190>
pk_column = True

    def check_json_test_result(  # noqa: C901 - 52
        test, result, data_asset=None, pk_column=False
    ) -> None:

        # check for id_pk results in cases where pk_column is true and unexpected_index_list already exists
        # this will work for testing since result_format is COMPLETE
        if pk_column:
            if not result["success"]:
                if "unexpected_index_list" in result["result"]:
                    assert "unexpected_index_query" in result["result"]

        if "unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )
            elif "unexpected_list" in test["output"]:
                (
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )

        if "partial_unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "partial_unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )
            elif "partial_unexpected_list" in test["output"]:
                (
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )

        # Determine if np.allclose(..) might be needed for float comparison
        try_allclose = False
        if "observed_value" in test["output"]:
            if RX_FLOAT.match(repr(test["output"]["observed_value"])):
                try_allclose = True

        # Check results
        if test["exact_match_out"] is True:
            if "result" in result and "observed_value" in result["result"]:
                if isinstance(result["result"]["observed_value"], (np.floating, float)):
                    assert np.allclose(
                        result["result"]["observed_value"],
                        expectationValidationResultSchema.load(test["output"])["result"][
                            "observed_value"
                        ],
                        rtol=RTOL,
                        atol=ATOL,
                    ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {expectationValidationResultSchema.load(test['output'])['result']['observed_value']}"
                else:
                    assert result == expectationValidationResultSchema.load(
                        test["output"]
                    ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
            else:
                assert result == expectationValidationResultSchema.load(
                    test["output"]
                ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
        else:
            # Convert result to json since our tests are reading from json so cannot easily contain richer types (e.g. NaN)
            # NOTE - 20191031 - JPC - we may eventually want to change these tests as we update our view on how
            # representations, serializations, and objects should interact and how much of that is shown to the user.
            result = result.to_json_dict()
            for key, value in test["output"].items():
                # Apply our great expectations-specific test logic

                if key == "success":
                    if isinstance(value, (np.floating, float)):
                        try:
                            assert np.allclose(
                                result["success"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['success']} not np.allclose to {value}"
                        except TypeError:
                            assert (
                                result["success"] == value
                            ), f"{result['success']} != {value}"
                    else:
                        assert result["success"] == value, f"{result['success']} != {value}"

                elif key == "observed_value":
                    if "tolerance" in test:
                        if isinstance(value, dict):
                            assert set(result["result"]["observed_value"].keys()) == set(
                                value.keys()
                            ), f"{set(result['result']['observed_value'].keys())} != {set(value.keys())}"
                            for k, v in value.items():
                                assert np.allclose(
                                    result["result"]["observed_value"][k],
                                    v,
                                    rtol=test["tolerance"],
                                )
                        else:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=test["tolerance"],
                            )
                    else:
                        if isinstance(value, dict) and "values" in value:
                            try:
                                assert np.allclose(
                                    result["result"]["observed_value"]["values"],
                                    value["values"],
                                    rtol=RTOL,
                                    atol=ATOL,
                                ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']['values']} not np.allclose to {value['values']}"
                            except TypeError as e:
                                print(e)
                                assert (
                                    result["result"]["observed_value"] == value
                                ), f"{result['result']['observed_value']} != {value}"
                        elif try_allclose:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {value}"
                        else:
                            assert (
                                result["result"]["observed_value"] == value
                            ), f"{result['result']['observed_value']} != {value}"

                # NOTE: This is a key used ONLY for testing cases where an expectation is legitimately allowed to return
                # any of multiple possible observed_values. expect_column_values_to_be_of_type is one such expectation.
                elif key == "observed_value_list":
                    assert result["result"]["observed_value"] in value

                elif key == "unexpected_index_list":
                    if isinstance(data_asset, (SqlAlchemyDataset, SparkDFDataset)):
                        pass
                    elif isinstance(data_asset, (SqlAlchemyBatchData, SparkDFBatchData)):
                        pass
                    else:
>                       assert (
                            result["result"]["unexpected_index_list"] == value
                        ), f"{result['result']['unexpected_index_list']} != {value}"
E                       AssertionError: [{'z': 5, 'pk_index': 4}] != [4]

great_expectations/self_check/util.py:3075: AssertionError
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 4000.29it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 370.18it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 513.17it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 285.07it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 402.91it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 303.75it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 455.59it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 423.93it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 422.35it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 420.60it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_between:positive_test_to_verify_that_the_denominator_for_mostly_works_with_missing_values1] _

test_case = {'expectation_type': 'expect_column_values_to_be_between', 'pk_column': True, 'skip': False, 'test': {'exact_match_out...n_value': 1, 'mostly': 0.8}, 'out': {'success': True, 'unexpected_index_list': [4], 'unexpected_list': [5]}, ...}, ...}

    @pytest.mark.order(index=0)
    @pytest.mark.integration
    @pytest.mark.slow  # 12.68s
    def test_case_runner_v3_api(test_case):
        if test_case["skip"]:
            pytest.skip()

        # Note: this should never be done in practice, but we are wiping expectations to reuse batches during testing.
        # test_case["batch"]._initialize_expectations()
        if "parse_strings_as_datetimes" in test_case["test"]["in"]:
            with pytest.deprecated_call():
                evaluate_json_test_v3_api(
                    validator=test_case["validator_with_data"],
                    expectation_type=test_case["expectation_type"],
                    test=test_case["test"],
                    pk_column=test_case["pk_column"],
                )
        else:
>           evaluate_json_test_v3_api(
                validator=test_case["validator_with_data"],
                expectation_type=test_case["expectation_type"],
                test=test_case["test"],
                pk_column=test_case["pk_column"],
            )

tests/test_definitions/test_expectations_v3_api.py:418:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
great_expectations/self_check/util.py:2901: in evaluate_json_test_v3_api
    check_json_test_result(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

test = {'exact_match_out': False, 'in': {'column': 'z', 'max_value': 4, 'min_value': 1, 'mostly': 0.8}, 'input': {'column': '...4, 'min_value': 1, 'mostly': 0.8}, 'out': {'success': True, 'unexpected_index_list': [4], 'unexpected_list': [5]}, ...}
result = {'exception_info': {'exception_message': None, 'exception_traceback': None, 'raised_exception': False}, 'expectation_c...': 10, 'missing_count': 5, 'missing_percent': 50.0, 'partial_unexpected_counts': [{'count': 1, 'value': 5}], ...}, ...}
data_asset = <great_expectations.execution_engine.pandas_batch_data.PandasBatchData object at 0x7f7806340190>
pk_column = True

    def check_json_test_result(  # noqa: C901 - 52
        test, result, data_asset=None, pk_column=False
    ) -> None:

        # check for id_pk results in cases where pk_column is true and unexpected_index_list already exists
        # this will work for testing since result_format is COMPLETE
        if pk_column:
            if not result["success"]:
                if "unexpected_index_list" in result["result"]:
                    assert "unexpected_index_query" in result["result"]

        if "unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )
            elif "unexpected_list" in test["output"]:
                (
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )

        if "partial_unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "partial_unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )
            elif "partial_unexpected_list" in test["output"]:
                (
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )

        # Determine if np.allclose(..) might be needed for float comparison
        try_allclose = False
        if "observed_value" in test["output"]:
            if RX_FLOAT.match(repr(test["output"]["observed_value"])):
                try_allclose = True

        # Check results
        if test["exact_match_out"] is True:
            if "result" in result and "observed_value" in result["result"]:
                if isinstance(result["result"]["observed_value"], (np.floating, float)):
                    assert np.allclose(
                        result["result"]["observed_value"],
                        expectationValidationResultSchema.load(test["output"])["result"][
                            "observed_value"
                        ],
                        rtol=RTOL,
                        atol=ATOL,
                    ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {expectationValidationResultSchema.load(test['output'])['result']['observed_value']}"
                else:
                    assert result == expectationValidationResultSchema.load(
                        test["output"]
                    ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
            else:
                assert result == expectationValidationResultSchema.load(
                    test["output"]
                ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
        else:
            # Convert result to json since our tests are reading from json so cannot easily contain richer types (e.g. NaN)
            # NOTE - 20191031 - JPC - we may eventually want to change these tests as we update our view on how
            # representations, serializations, and objects should interact and how much of that is shown to the user.
            result = result.to_json_dict()
            for key, value in test["output"].items():
                # Apply our great expectations-specific test logic

                if key == "success":
                    if isinstance(value, (np.floating, float)):
                        try:
                            assert np.allclose(
                                result["success"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['success']} not np.allclose to {value}"
                        except TypeError:
                            assert (
                                result["success"] == value
                            ), f"{result['success']} != {value}"
                    else:
                        assert result["success"] == value, f"{result['success']} != {value}"

                elif key == "observed_value":
                    if "tolerance" in test:
                        if isinstance(value, dict):
                            assert set(result["result"]["observed_value"].keys()) == set(
                                value.keys()
                            ), f"{set(result['result']['observed_value'].keys())} != {set(value.keys())}"
                            for k, v in value.items():
                                assert np.allclose(
                                    result["result"]["observed_value"][k],
                                    v,
                                    rtol=test["tolerance"],
                                )
                        else:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=test["tolerance"],
                            )
                    else:
                        if isinstance(value, dict) and "values" in value:
                            try:
                                assert np.allclose(
                                    result["result"]["observed_value"]["values"],
                                    value["values"],
                                    rtol=RTOL,
                                    atol=ATOL,
                                ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']['values']} not np.allclose to {value['values']}"
                            except TypeError as e:
                                print(e)
                                assert (
                                    result["result"]["observed_value"] == value
                                ), f"{result['result']['observed_value']} != {value}"
                        elif try_allclose:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {value}"
                        else:
                            assert (
                                result["result"]["observed_value"] == value
                            ), f"{result['result']['observed_value']} != {value}"

                # NOTE: This is a key used ONLY for testing cases where an expectation is legitimately allowed to return
                # any of multiple possible observed_values. expect_column_values_to_be_of_type is one such expectation.
                elif key == "observed_value_list":
                    assert result["result"]["observed_value"] in value

                elif key == "unexpected_index_list":
                    if isinstance(data_asset, (SqlAlchemyDataset, SparkDFDataset)):
                        pass
                    elif isinstance(data_asset, (SqlAlchemyBatchData, SparkDFBatchData)):
                        pass
                    else:
>                       assert (
                            result["result"]["unexpected_index_list"] == value
                        ), f"{result['result']['unexpected_index_list']} != {value}"
E                       AssertionError: [{'z': 5, 'pk_index': 4}] != [4]

great_expectations/self_check/util.py:3075: AssertionError
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 2448.51it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 289.98it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 413.73it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 231.91it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 331.24it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 252.58it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 403.03it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 377.22it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 375.94it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 374.53it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_between:test_allow_cross_type_comparisons1] _

test_case = {'expectation_type': 'expect_column_values_to_be_between', 'pk_column': True, 'skip': False, 'test': {'exact_match_out...': 10, 'min_value': 0}, 'out': {'success': False, 'unexpected_index_list': [9], 'unexpected_list': ['abc']}, ...}, ...}

    @pytest.mark.order(index=0)
    @pytest.mark.integration
    @pytest.mark.slow  # 12.68s
    def test_case_runner_v3_api(test_case):
        if test_case["skip"]:
            pytest.skip()

        # Note: this should never be done in practice, but we are wiping expectations to reuse batches during testing.
        # test_case["batch"]._initialize_expectations()
        if "parse_strings_as_datetimes" in test_case["test"]["in"]:
            with pytest.deprecated_call():
                evaluate_json_test_v3_api(
                    validator=test_case["validator_with_data"],
                    expectation_type=test_case["expectation_type"],
                    test=test_case["test"],
                    pk_column=test_case["pk_column"],
                )
        else:
>           evaluate_json_test_v3_api(
                validator=test_case["validator_with_data"],
                expectation_type=test_case["expectation_type"],
                test=test_case["test"],
                pk_column=test_case["pk_column"],
            )

tests/test_definitions/test_expectations_v3_api.py:418:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
great_expectations/self_check/util.py:2901: in evaluate_json_test_v3_api
    check_json_test_result(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

test = {'exact_match_out': False, 'in': {'allow_cross_type_comparisons': True, 'column': 'y', 'max_value': 10, 'min_value': 0..._value': 10, 'min_value': 0}, 'out': {'success': False, 'unexpected_index_list': [9], 'unexpected_list': ['abc']}, ...}
result = {'exception_info': {'exception_message': None, 'exception_traceback': None, 'raised_exception': False}, 'expectation_c...10, 'missing_count': 0, 'missing_percent': 0.0, 'partial_unexpected_counts': [{'count': 1, 'value': 'abc'}], ...}, ...}
data_asset = <great_expectations.execution_engine.pandas_batch_data.PandasBatchData object at 0x7f7806340190>
pk_column = True

    def check_json_test_result(  # noqa: C901 - 52
        test, result, data_asset=None, pk_column=False
    ) -> None:

        # check for id_pk results in cases where pk_column is true and unexpected_index_list already exists
        # this will work for testing since result_format is COMPLETE
        if pk_column:
            if not result["success"]:
                if "unexpected_index_list" in result["result"]:
                    assert "unexpected_index_query" in result["result"]

        if "unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )
            elif "unexpected_list" in test["output"]:
                (
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )

        if "partial_unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "partial_unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )
            elif "partial_unexpected_list" in test["output"]:
                (
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )

        # Determine if np.allclose(..) might be needed for float comparison
        try_allclose = False
        if "observed_value" in test["output"]:
            if RX_FLOAT.match(repr(test["output"]["observed_value"])):
                try_allclose = True

        # Check results
        if test["exact_match_out"] is True:
            if "result" in result and "observed_value" in result["result"]:
                if isinstance(result["result"]["observed_value"], (np.floating, float)):
                    assert np.allclose(
                        result["result"]["observed_value"],
                        expectationValidationResultSchema.load(test["output"])["result"][
                            "observed_value"
                        ],
                        rtol=RTOL,
                        atol=ATOL,
                    ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {expectationValidationResultSchema.load(test['output'])['result']['observed_value']}"
                else:
                    assert result == expectationValidationResultSchema.load(
                        test["output"]
                    ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
            else:
                assert result == expectationValidationResultSchema.load(
                    test["output"]
                ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
        else:
            # Convert result to json since our tests are reading from json so cannot easily contain richer types (e.g. NaN)
            # NOTE - 20191031 - JPC - we may eventually want to change these tests as we update our view on how
            # representations, serializations, and objects should interact and how much of that is shown to the user.
            result = result.to_json_dict()
            for key, value in test["output"].items():
                # Apply our great expectations-specific test logic

                if key == "success":
                    if isinstance(value, (np.floating, float)):
                        try:
                            assert np.allclose(
                                result["success"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['success']} not np.allclose to {value}"
                        except TypeError:
                            assert (
                                result["success"] == value
                            ), f"{result['success']} != {value}"
                    else:
                        assert result["success"] == value, f"{result['success']} != {value}"

                elif key == "observed_value":
                    if "tolerance" in test:
                        if isinstance(value, dict):
                            assert set(result["result"]["observed_value"].keys()) == set(
                                value.keys()
                            ), f"{set(result['result']['observed_value'].keys())} != {set(value.keys())}"
                            for k, v in value.items():
                                assert np.allclose(
                                    result["result"]["observed_value"][k],
                                    v,
                                    rtol=test["tolerance"],
                                )
                        else:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=test["tolerance"],
                            )
                    else:
                        if isinstance(value, dict) and "values" in value:
                            try:
                                assert np.allclose(
                                    result["result"]["observed_value"]["values"],
                                    value["values"],
                                    rtol=RTOL,
                                    atol=ATOL,
                                ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']['values']} not np.allclose to {value['values']}"
                            except TypeError as e:
                                print(e)
                                assert (
                                    result["result"]["observed_value"] == value
                                ), f"{result['result']['observed_value']} != {value}"
                        elif try_allclose:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {value}"
                        else:
                            assert (
                                result["result"]["observed_value"] == value
                            ), f"{result['result']['observed_value']} != {value}"

                # NOTE: This is a key used ONLY for testing cases where an expectation is legitimately allowed to return
                # any of multiple possible observed_values. expect_column_values_to_be_of_type is one such expectation.
                elif key == "observed_value_list":
                    assert result["result"]["observed_value"] in value

                elif key == "unexpected_index_list":
                    if isinstance(data_asset, (SqlAlchemyDataset, SparkDFDataset)):
                        pass
                    elif isinstance(data_asset, (SqlAlchemyBatchData, SparkDFBatchData)):
                        pass
                    else:
>                       assert (
                            result["result"]["unexpected_index_list"] == value
                        ), f"{result['result']['unexpected_index_list']} != {value}"
E                       AssertionError: [{'y': 'abc', 'pk_index': 9}] != [9]

great_expectations/self_check/util.py:3075: AssertionError
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 4124.19it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 403.88it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 574.06it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 310.95it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 439.17it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 336.95it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 518.89it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 480.89it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 478.65it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 476.33it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_between:test_allow_cross_type_comparisons_again1] _

test_case = {'expectation_type': 'expect_column_values_to_be_between', 'pk_column': True, 'skip': False, 'test': {'exact_match_out... 'unexpected_index_list': [0, 1, 2, 3, 4, 5, ...], 'unexpected_list': ['1', '10', '2', '3', '4', '5', ...]}, ...}, ...}

    @pytest.mark.order(index=0)
    @pytest.mark.integration
    @pytest.mark.slow  # 12.68s
    def test_case_runner_v3_api(test_case):
        if test_case["skip"]:
            pytest.skip()

        # Note: this should never be done in practice, but we are wiping expectations to reuse batches during testing.
        # test_case["batch"]._initialize_expectations()
        if "parse_strings_as_datetimes" in test_case["test"]["in"]:
            with pytest.deprecated_call():
                evaluate_json_test_v3_api(
                    validator=test_case["validator_with_data"],
                    expectation_type=test_case["expectation_type"],
                    test=test_case["test"],
                    pk_column=test_case["pk_column"],
                )
        else:
>           evaluate_json_test_v3_api(
                validator=test_case["validator_with_data"],
                expectation_type=test_case["expectation_type"],
                test=test_case["test"],
                pk_column=test_case["pk_column"],
            )

tests/test_definitions/test_expectations_v3_api.py:418:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
great_expectations/self_check/util.py:2901: in evaluate_json_test_v3_api
    check_json_test_result(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

test = {'exact_match_out': False, 'in': {'allow_cross_type_comparisons': True, 'column': 'numeric', 'max_value': 10, 'min_val...False, 'unexpected_index_list': [0, 1, 2, 3, 4, 5, ...], 'unexpected_list': ['1', '10', '2', '3', '4', '5', ...]}, ...}
result = {'exception_info': {'exception_message': None, 'exception_traceback': None, 'raised_exception': False}, 'expectation_c... 1, 'value': '2'}, {'count': 1, 'value': '3'}, {'count': 1, 'value': '4'}, {'count': 1, 'value': '5'}, ...], ...}, ...}
data_asset = <great_expectations.execution_engine.pandas_batch_data.PandasBatchData object at 0x7f7806340190>
pk_column = True

    def check_json_test_result(  # noqa: C901 - 52
        test, result, data_asset=None, pk_column=False
    ) -> None:

        # check for id_pk results in cases where pk_column is true and unexpected_index_list already exists
        # this will work for testing since result_format is COMPLETE
        if pk_column:
            if not result["success"]:
                if "unexpected_index_list" in result["result"]:
                    assert "unexpected_index_query" in result["result"]

        if "unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )
            elif "unexpected_list" in test["output"]:
                (
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )

        if "partial_unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "partial_unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )
            elif "partial_unexpected_list" in test["output"]:
                (
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )

        # Determine if np.allclose(..) might be needed for float comparison
        try_allclose = False
        if "observed_value" in test["output"]:
            if RX_FLOAT.match(repr(test["output"]["observed_value"])):
                try_allclose = True

        # Check results
        if test["exact_match_out"] is True:
            if "result" in result and "observed_value" in result["result"]:
                if isinstance(result["result"]["observed_value"], (np.floating, float)):
                    assert np.allclose(
                        result["result"]["observed_value"],
                        expectationValidationResultSchema.load(test["output"])["result"][
                            "observed_value"
                        ],
                        rtol=RTOL,
                        atol=ATOL,
                    ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {expectationValidationResultSchema.load(test['output'])['result']['observed_value']}"
                else:
                    assert result == expectationValidationResultSchema.load(
                        test["output"]
                    ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
            else:
                assert result == expectationValidationResultSchema.load(
                    test["output"]
                ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
        else:
            # Convert result to json since our tests are reading from json so cannot easily contain richer types (e.g. NaN)
            # NOTE - 20191031 - JPC - we may eventually want to change these tests as we update our view on how
            # representations, serializations, and objects should interact and how much of that is shown to the user.
            result = result.to_json_dict()
            for key, value in test["output"].items():
                # Apply our great expectations-specific test logic

                if key == "success":
                    if isinstance(value, (np.floating, float)):
                        try:
                            assert np.allclose(
                                result["success"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['success']} not np.allclose to {value}"
                        except TypeError:
                            assert (
                                result["success"] == value
                            ), f"{result['success']} != {value}"
                    else:
                        assert result["success"] == value, f"{result['success']} != {value}"

                elif key == "observed_value":
                    if "tolerance" in test:
                        if isinstance(value, dict):
                            assert set(result["result"]["observed_value"].keys()) == set(
                                value.keys()
                            ), f"{set(result['result']['observed_value'].keys())} != {set(value.keys())}"
                            for k, v in value.items():
                                assert np.allclose(
                                    result["result"]["observed_value"][k],
                                    v,
                                    rtol=test["tolerance"],
                                )
                        else:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=test["tolerance"],
                            )
                    else:
                        if isinstance(value, dict) and "values" in value:
                            try:
                                assert np.allclose(
                                    result["result"]["observed_value"]["values"],
                                    value["values"],
                                    rtol=RTOL,
                                    atol=ATOL,
                                ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']['values']} not np.allclose to {value['values']}"
                            except TypeError as e:
                                print(e)
                                assert (
                                    result["result"]["observed_value"] == value
                                ), f"{result['result']['observed_value']} != {value}"
                        elif try_allclose:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {value}"
                        else:
                            assert (
                                result["result"]["observed_value"] == value
                            ), f"{result['result']['observed_value']} != {value}"

                # NOTE: This is a key used ONLY for testing cases where an expectation is legitimately allowed to return
                # any of multiple possible observed_values. expect_column_values_to_be_of_type is one such expectation.
                elif key == "observed_value_list":
                    assert result["result"]["observed_value"] in value

                elif key == "unexpected_index_list":
                    if isinstance(data_asset, (SqlAlchemyDataset, SparkDFDataset)):
                        pass
                    elif isinstance(data_asset, (SqlAlchemyBatchData, SparkDFBatchData)):
                        pass
                    else:
>                       assert (
                            result["result"]["unexpected_index_list"] == value
                        ), f"{result['result']['unexpected_index_list']} != {value}"
E                       AssertionError: [{'numeric': '1', 'pk_index': 0}, {'numeric': '2', 'pk_index': 1}, {'numeric': '3', 'pk_index': 2}, {'numeric': '4', 'pk_index': 3}, {'numeric': '5', 'pk_index': 4}, {'numeric': '6', 'pk_index': 5}, {'numeric': '7', 'pk_index': 6}, {'numeric': '8', 'pk_index': 7}, {'numeric': '9', 'pk_index': 8}, {'numeric': '10', 'pk_index': 9}] != [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]

great_expectations/self_check/util.py:3075: AssertionError
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 2617.35it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 335.91it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 469.84it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 271.88it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 387.26it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 296.57it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 444.70it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 411.18it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 409.43it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 407.43it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_between:test_strict_min_failure1] _

test_case = {'expectation_type': 'expect_column_values_to_be_between', 'pk_column': True, 'skip': False, 'test': {'exact_match_out...e': 1, 'strict_min': True}, 'out': {'success': False, 'unexpected_index_list': [0], 'unexpected_list': [1]}, ...}, ...}

    @pytest.mark.order(index=0)
    @pytest.mark.integration
    @pytest.mark.slow  # 12.68s
    def test_case_runner_v3_api(test_case):
        if test_case["skip"]:
            pytest.skip()

        # Note: this should never be done in practice, but we are wiping expectations to reuse batches during testing.
        # test_case["batch"]._initialize_expectations()
        if "parse_strings_as_datetimes" in test_case["test"]["in"]:
            with pytest.deprecated_call():
                evaluate_json_test_v3_api(
                    validator=test_case["validator_with_data"],
                    expectation_type=test_case["expectation_type"],
                    test=test_case["test"],
                    pk_column=test_case["pk_column"],
                )
        else:
>           evaluate_json_test_v3_api(
                validator=test_case["validator_with_data"],
                expectation_type=test_case["expectation_type"],
                test=test_case["test"],
                pk_column=test_case["pk_column"],
            )

tests/test_definitions/test_expectations_v3_api.py:418:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
great_expectations/self_check/util.py:2901: in evaluate_json_test_v3_api
    check_json_test_result(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

test = {'exact_match_out': False, 'in': {'column': 'x', 'max_value': 10, 'min_value': 1, 'strict_min': True}, 'input': {'colu...n_value': 1, 'strict_min': True}, 'out': {'success': False, 'unexpected_index_list': [0], 'unexpected_list': [1]}, ...}
result = {'exception_info': {'exception_message': None, 'exception_traceback': None, 'raised_exception': False}, 'expectation_c...t': 10, 'missing_count': 0, 'missing_percent': 0.0, 'partial_unexpected_counts': [{'count': 1, 'value': 1}], ...}, ...}
data_asset = <great_expectations.execution_engine.pandas_batch_data.PandasBatchData object at 0x7f7806340190>
pk_column = True

    def check_json_test_result(  # noqa: C901 - 52
        test, result, data_asset=None, pk_column=False
    ) -> None:

        # check for id_pk results in cases where pk_column is true and unexpected_index_list already exists
        # this will work for testing since result_format is COMPLETE
        if pk_column:
            if not result["success"]:
                if "unexpected_index_list" in result["result"]:
                    assert "unexpected_index_query" in result["result"]

        if "unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )
            elif "unexpected_list" in test["output"]:
                (
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )

        if "partial_unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "partial_unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )
            elif "partial_unexpected_list" in test["output"]:
                (
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )

        # Determine if np.allclose(..) might be needed for float comparison
        try_allclose = False
        if "observed_value" in test["output"]:
            if RX_FLOAT.match(repr(test["output"]["observed_value"])):
                try_allclose = True

        # Check results
        if test["exact_match_out"] is True:
            if "result" in result and "observed_value" in result["result"]:
                if isinstance(result["result"]["observed_value"], (np.floating, float)):
                    assert np.allclose(
                        result["result"]["observed_value"],
                        expectationValidationResultSchema.load(test["output"])["result"][
                            "observed_value"
                        ],
                        rtol=RTOL,
                        atol=ATOL,
                    ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {expectationValidationResultSchema.load(test['output'])['result']['observed_value']}"
                else:
                    assert result == expectationValidationResultSchema.load(
                        test["output"]
                    ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
            else:
                assert result == expectationValidationResultSchema.load(
                    test["output"]
                ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
        else:
            # Convert result to json since our tests are reading from json so cannot easily contain richer types (e.g. NaN)
            # NOTE - 20191031 - JPC - we may eventually want to change these tests as we update our view on how
            # representations, serializations, and objects should interact and how much of that is shown to the user.
            result = result.to_json_dict()
            for key, value in test["output"].items():
                # Apply our great expectations-specific test logic

                if key == "success":
                    if isinstance(value, (np.floating, float)):
                        try:
                            assert np.allclose(
                                result["success"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['success']} not np.allclose to {value}"
                        except TypeError:
                            assert (
                                result["success"] == value
                            ), f"{result['success']} != {value}"
                    else:
                        assert result["success"] == value, f"{result['success']} != {value}"

                elif key == "observed_value":
                    if "tolerance" in test:
                        if isinstance(value, dict):
                            assert set(result["result"]["observed_value"].keys()) == set(
                                value.keys()
                            ), f"{set(result['result']['observed_value'].keys())} != {set(value.keys())}"
                            for k, v in value.items():
                                assert np.allclose(
                                    result["result"]["observed_value"][k],
                                    v,
                                    rtol=test["tolerance"],
                                )
                        else:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=test["tolerance"],
                            )
                    else:
                        if isinstance(value, dict) and "values" in value:
                            try:
                                assert np.allclose(
                                    result["result"]["observed_value"]["values"],
                                    value["values"],
                                    rtol=RTOL,
                                    atol=ATOL,
                                ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']['values']} not np.allclose to {value['values']}"
                            except TypeError as e:
                                print(e)
                                assert (
                                    result["result"]["observed_value"] == value
                                ), f"{result['result']['observed_value']} != {value}"
                        elif try_allclose:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {value}"
                        else:
                            assert (
                                result["result"]["observed_value"] == value
                            ), f"{result['result']['observed_value']} != {value}"

                # NOTE: This is a key used ONLY for testing cases where an expectation is legitimately allowed to return
                # any of multiple possible observed_values. expect_column_values_to_be_of_type is one such expectation.
                elif key == "observed_value_list":
                    assert result["result"]["observed_value"] in value

                elif key == "unexpected_index_list":
                    if isinstance(data_asset, (SqlAlchemyDataset, SparkDFDataset)):
                        pass
                    elif isinstance(data_asset, (SqlAlchemyBatchData, SparkDFBatchData)):
                        pass
                    else:
>                       assert (
                            result["result"]["unexpected_index_list"] == value
                        ), f"{result['result']['unexpected_index_list']} != {value}"
E                       AssertionError: [{'x': 1, 'pk_index': 0}] != [0]

great_expectations/self_check/util.py:3075: AssertionError
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 4038.81it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 371.95it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 531.98it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 289.12it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 405.71it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 309.44it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 482.55it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 447.09it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 445.31it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 443.32it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_between:test_strict_max_failure1] _

test_case = {'expectation_type': 'expect_column_values_to_be_between', 'pk_column': True, 'skip': False, 'test': {'exact_match_out...': 0, 'strict_max': True}, 'out': {'success': False, 'unexpected_index_list': [9], 'unexpected_list': [10]}, ...}, ...}

    @pytest.mark.order(index=0)
    @pytest.mark.integration
    @pytest.mark.slow  # 12.68s
    def test_case_runner_v3_api(test_case):
        if test_case["skip"]:
            pytest.skip()

        # Note: this should never be done in practice, but we are wiping expectations to reuse batches during testing.
        # test_case["batch"]._initialize_expectations()
        if "parse_strings_as_datetimes" in test_case["test"]["in"]:
            with pytest.deprecated_call():
                evaluate_json_test_v3_api(
                    validator=test_case["validator_with_data"],
                    expectation_type=test_case["expectation_type"],
                    test=test_case["test"],
                    pk_column=test_case["pk_column"],
                )
        else:
>           evaluate_json_test_v3_api(
                validator=test_case["validator_with_data"],
                expectation_type=test_case["expectation_type"],
                test=test_case["test"],
                pk_column=test_case["pk_column"],
            )

tests/test_definitions/test_expectations_v3_api.py:418:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
great_expectations/self_check/util.py:2901: in evaluate_json_test_v3_api
    check_json_test_result(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

test = {'exact_match_out': False, 'in': {'column': 'x', 'max_value': 10, 'min_value': 0, 'strict_max': True}, 'input': {'colu..._value': 0, 'strict_max': True}, 'out': {'success': False, 'unexpected_index_list': [9], 'unexpected_list': [10]}, ...}
result = {'exception_info': {'exception_message': None, 'exception_traceback': None, 'raised_exception': False}, 'expectation_c...': 10, 'missing_count': 0, 'missing_percent': 0.0, 'partial_unexpected_counts': [{'count': 1, 'value': 10}], ...}, ...}
data_asset = <great_expectations.execution_engine.pandas_batch_data.PandasBatchData object at 0x7f7806340190>
pk_column = True

    def check_json_test_result(  # noqa: C901 - 52
        test, result, data_asset=None, pk_column=False
    ) -> None:

        # check for id_pk results in cases where pk_column is true and unexpected_index_list already exists
        # this will work for testing since result_format is COMPLETE
        if pk_column:
            if not result["success"]:
                if "unexpected_index_list" in result["result"]:
                    assert "unexpected_index_query" in result["result"]

        if "unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )
            elif "unexpected_list" in test["output"]:
                (
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )

        if "partial_unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "partial_unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )
            elif "partial_unexpected_list" in test["output"]:
                (
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )

        # Determine if np.allclose(..) might be needed for float comparison
        try_allclose = False
        if "observed_value" in test["output"]:
            if RX_FLOAT.match(repr(test["output"]["observed_value"])):
                try_allclose = True

        # Check results
        if test["exact_match_out"] is True:
            if "result" in result and "observed_value" in result["result"]:
                if isinstance(result["result"]["observed_value"], (np.floating, float)):
                    assert np.allclose(
                        result["result"]["observed_value"],
                        expectationValidationResultSchema.load(test["output"])["result"][
                            "observed_value"
                        ],
                        rtol=RTOL,
                        atol=ATOL,
                    ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {expectationValidationResultSchema.load(test['output'])['result']['observed_value']}"
                else:
                    assert result == expectationValidationResultSchema.load(
                        test["output"]
                    ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
            else:
                assert result == expectationValidationResultSchema.load(
                    test["output"]
                ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
        else:
            # Convert result to json since our tests are reading from json so cannot easily contain richer types (e.g. NaN)
            # NOTE - 20191031 - JPC - we may eventually want to change these tests as we update our view on how
            # representations, serializations, and objects should interact and how much of that is shown to the user.
            result = result.to_json_dict()
            for key, value in test["output"].items():
                # Apply our great expectations-specific test logic

                if key == "success":
                    if isinstance(value, (np.floating, float)):
                        try:
                            assert np.allclose(
                                result["success"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['success']} not np.allclose to {value}"
                        except TypeError:
                            assert (
                                result["success"] == value
                            ), f"{result['success']} != {value}"
                    else:
                        assert result["success"] == value, f"{result['success']} != {value}"

                elif key == "observed_value":
                    if "tolerance" in test:
                        if isinstance(value, dict):
                            assert set(result["result"]["observed_value"].keys()) == set(
                                value.keys()
                            ), f"{set(result['result']['observed_value'].keys())} != {set(value.keys())}"
                            for k, v in value.items():
                                assert np.allclose(
                                    result["result"]["observed_value"][k],
                                    v,
                                    rtol=test["tolerance"],
                                )
                        else:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=test["tolerance"],
                            )
                    else:
                        if isinstance(value, dict) and "values" in value:
                            try:
                                assert np.allclose(
                                    result["result"]["observed_value"]["values"],
                                    value["values"],
                                    rtol=RTOL,
                                    atol=ATOL,
                                ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']['values']} not np.allclose to {value['values']}"
                            except TypeError as e:
                                print(e)
                                assert (
                                    result["result"]["observed_value"] == value
                                ), f"{result['result']['observed_value']} != {value}"
                        elif try_allclose:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {value}"
                        else:
                            assert (
                                result["result"]["observed_value"] == value
                            ), f"{result['result']['observed_value']} != {value}"

                # NOTE: This is a key used ONLY for testing cases where an expectation is legitimately allowed to return
                # any of multiple possible observed_values. expect_column_values_to_be_of_type is one such expectation.
                elif key == "observed_value_list":
                    assert result["result"]["observed_value"] in value

                elif key == "unexpected_index_list":
                    if isinstance(data_asset, (SqlAlchemyDataset, SparkDFDataset)):
                        pass
                    elif isinstance(data_asset, (SqlAlchemyBatchData, SparkDFBatchData)):
                        pass
                    else:
>                       assert (
                            result["result"]["unexpected_index_list"] == value
                        ), f"{result['result']['unexpected_index_list']} != {value}"
E                       AssertionError: [{'x': 10, 'pk_index': 9}] != [9]

great_expectations/self_check/util.py:3075: AssertionError
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 2958.94it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 282.37it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 403.51it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 225.78it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 318.69it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 243.72it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 376.15it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 347.21it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 345.76it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 344.31it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_between:test_conditional_expectation_fails1] _

test_case = {'expectation_type': 'expect_column_values_to_be_between', 'pk_column': True, 'skip': False, 'test': {'exact_match_out...'column': 'x', 'condition_parser': 'pandas', 'max_value': 10, 'min_value': 10, ...}, 'only_for': ['pandas'], ...}, ...}

    @pytest.mark.order(index=0)
    @pytest.mark.integration
    @pytest.mark.slow  # 12.68s
    def test_case_runner_v3_api(test_case):
        if test_case["skip"]:
            pytest.skip()

        # Note: this should never be done in practice, but we are wiping expectations to reuse batches during testing.
        # test_case["batch"]._initialize_expectations()
        if "parse_strings_as_datetimes" in test_case["test"]["in"]:
            with pytest.deprecated_call():
                evaluate_json_test_v3_api(
                    validator=test_case["validator_with_data"],
                    expectation_type=test_case["expectation_type"],
                    test=test_case["test"],
                    pk_column=test_case["pk_column"],
                )
        else:
>           evaluate_json_test_v3_api(
                validator=test_case["validator_with_data"],
                expectation_type=test_case["expectation_type"],
                test=test_case["test"],
                pk_column=test_case["pk_column"],
            )

tests/test_definitions/test_expectations_v3_api.py:418:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
great_expectations/self_check/util.py:2901: in evaluate_json_test_v3_api
    check_json_test_result(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

test = {'exact_match_out': False, 'in': {'column': 'x', 'condition_parser': 'pandas', 'max_value': 10, 'min_value': 10, ...},...ut': {'column': 'x', 'condition_parser': 'pandas', 'max_value': 10, 'min_value': 10, ...}, 'only_for': ['pandas'], ...}
result = {'exception_info': {'exception_message': None, 'exception_traceback': None, 'raised_exception': False}, 'expectation_c...nt': 1, 'missing_count': 0, 'missing_percent': 0.0, 'partial_unexpected_counts': [{'count': 1, 'value': 9}], ...}, ...}
data_asset = <great_expectations.execution_engine.pandas_batch_data.PandasBatchData object at 0x7f7806340190>
pk_column = True

    def check_json_test_result(  # noqa: C901 - 52
        test, result, data_asset=None, pk_column=False
    ) -> None:

        # check for id_pk results in cases where pk_column is true and unexpected_index_list already exists
        # this will work for testing since result_format is COMPLETE
        if pk_column:
            if not result["success"]:
                if "unexpected_index_list" in result["result"]:
                    assert "unexpected_index_query" in result["result"]

        if "unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )
            elif "unexpected_list" in test["output"]:
                (
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )

        if "partial_unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "partial_unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )
            elif "partial_unexpected_list" in test["output"]:
                (
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )

        # Determine if np.allclose(..) might be needed for float comparison
        try_allclose = False
        if "observed_value" in test["output"]:
            if RX_FLOAT.match(repr(test["output"]["observed_value"])):
                try_allclose = True

        # Check results
        if test["exact_match_out"] is True:
            if "result" in result and "observed_value" in result["result"]:
                if isinstance(result["result"]["observed_value"], (np.floating, float)):
                    assert np.allclose(
                        result["result"]["observed_value"],
                        expectationValidationResultSchema.load(test["output"])["result"][
                            "observed_value"
                        ],
                        rtol=RTOL,
                        atol=ATOL,
                    ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {expectationValidationResultSchema.load(test['output'])['result']['observed_value']}"
                else:
                    assert result == expectationValidationResultSchema.load(
                        test["output"]
                    ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
            else:
                assert result == expectationValidationResultSchema.load(
                    test["output"]
                ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
        else:
            # Convert result to json since our tests are reading from json so cannot easily contain richer types (e.g. NaN)
            # NOTE - 20191031 - JPC - we may eventually want to change these tests as we update our view on how
            # representations, serializations, and objects should interact and how much of that is shown to the user.
            result = result.to_json_dict()
            for key, value in test["output"].items():
                # Apply our great expectations-specific test logic

                if key == "success":
                    if isinstance(value, (np.floating, float)):
                        try:
                            assert np.allclose(
                                result["success"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['success']} not np.allclose to {value}"
                        except TypeError:
                            assert (
                                result["success"] == value
                            ), f"{result['success']} != {value}"
                    else:
                        assert result["success"] == value, f"{result['success']} != {value}"

                elif key == "observed_value":
                    if "tolerance" in test:
                        if isinstance(value, dict):
                            assert set(result["result"]["observed_value"].keys()) == set(
                                value.keys()
                            ), f"{set(result['result']['observed_value'].keys())} != {set(value.keys())}"
                            for k, v in value.items():
                                assert np.allclose(
                                    result["result"]["observed_value"][k],
                                    v,
                                    rtol=test["tolerance"],
                                )
                        else:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=test["tolerance"],
                            )
                    else:
                        if isinstance(value, dict) and "values" in value:
                            try:
                                assert np.allclose(
                                    result["result"]["observed_value"]["values"],
                                    value["values"],
                                    rtol=RTOL,
                                    atol=ATOL,
                                ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']['values']} not np.allclose to {value['values']}"
                            except TypeError as e:
                                print(e)
                                assert (
                                    result["result"]["observed_value"] == value
                                ), f"{result['result']['observed_value']} != {value}"
                        elif try_allclose:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {value}"
                        else:
                            assert (
                                result["result"]["observed_value"] == value
                            ), f"{result['result']['observed_value']} != {value}"

                # NOTE: This is a key used ONLY for testing cases where an expectation is legitimately allowed to return
                # any of multiple possible observed_values. expect_column_values_to_be_of_type is one such expectation.
                elif key == "observed_value_list":
                    assert result["result"]["observed_value"] in value

                elif key == "unexpected_index_list":
                    if isinstance(data_asset, (SqlAlchemyDataset, SparkDFDataset)):
                        pass
                    elif isinstance(data_asset, (SqlAlchemyBatchData, SparkDFBatchData)):
                        pass
                    else:
>                       assert (
                            result["result"]["unexpected_index_list"] == value
                        ), f"{result['result']['unexpected_index_list']} != {value}"
E                       AssertionError: [{'x': 9, 'pk_index': 8}] != [8]

great_expectations/self_check/util.py:3075: AssertionError
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 683.56it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 305.44it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 443.26it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 293.66it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 352.69it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 285.89it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 387.01it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 366.20it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 364.68it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 363.08it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_not_be_in_set:negative_test_case_include_one_existing_column_value] _

test_case = {'expectation_type': 'expect_column_values_to_not_be_in_set', 'pk_column': True, 'skip': False, 'test': {'exact_match_...: {'column': 'x', 'value_set': [1]}, 'include_in_gallery': True, 'input': {'column': 'x', 'value_set': [1]}, ...}, ...}

    @pytest.mark.order(index=0)
    @pytest.mark.integration
    @pytest.mark.slow  # 12.68s
    def test_case_runner_v3_api(test_case):
        if test_case["skip"]:
            pytest.skip()

        # Note: this should never be done in practice, but we are wiping expectations to reuse batches during testing.
        # test_case["batch"]._initialize_expectations()
        if "parse_strings_as_datetimes" in test_case["test"]["in"]:
            with pytest.deprecated_call():
                evaluate_json_test_v3_api(
                    validator=test_case["validator_with_data"],
                    expectation_type=test_case["expectation_type"],
                    test=test_case["test"],
                    pk_column=test_case["pk_column"],
                )
        else:
>           evaluate_json_test_v3_api(
                validator=test_case["validator_with_data"],
                expectation_type=test_case["expectation_type"],
                test=test_case["test"],
                pk_column=test_case["pk_column"],
            )

tests/test_definitions/test_expectations_v3_api.py:418:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
great_expectations/self_check/util.py:2901: in evaluate_json_test_v3_api
    check_json_test_result(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

test = {'exact_match_out': False, 'in': {'column': 'x', 'value_set': [1]}, 'include_in_gallery': True, 'input': {'column': 'x', 'value_set': [1]}, ...}
result = {'exception_info': {'exception_message': None, 'exception_traceback': None, 'raised_exception': False}, 'expectation_c...nt': 3, 'missing_count': 0, 'missing_percent': 0.0, 'partial_unexpected_counts': [{'count': 1, 'value': 1}], ...}, ...}
data_asset = <great_expectations.execution_engine.pandas_batch_data.PandasBatchData object at 0x7f78062e4d90>
pk_column = True

    def check_json_test_result(  # noqa: C901 - 52
        test, result, data_asset=None, pk_column=False
    ) -> None:

        # check for id_pk results in cases where pk_column is true and unexpected_index_list already exists
        # this will work for testing since result_format is COMPLETE
        if pk_column:
            if not result["success"]:
                if "unexpected_index_list" in result["result"]:
                    assert "unexpected_index_query" in result["result"]

        if "unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )
            elif "unexpected_list" in test["output"]:
                (
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )

        if "partial_unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "partial_unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )
            elif "partial_unexpected_list" in test["output"]:
                (
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )

        # Determine if np.allclose(..) might be needed for float comparison
        try_allclose = False
        if "observed_value" in test["output"]:
            if RX_FLOAT.match(repr(test["output"]["observed_value"])):
                try_allclose = True

        # Check results
        if test["exact_match_out"] is True:
            if "result" in result and "observed_value" in result["result"]:
                if isinstance(result["result"]["observed_value"], (np.floating, float)):
                    assert np.allclose(
                        result["result"]["observed_value"],
                        expectationValidationResultSchema.load(test["output"])["result"][
                            "observed_value"
                        ],
                        rtol=RTOL,
                        atol=ATOL,
                    ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {expectationValidationResultSchema.load(test['output'])['result']['observed_value']}"
                else:
                    assert result == expectationValidationResultSchema.load(
                        test["output"]
                    ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
            else:
                assert result == expectationValidationResultSchema.load(
                    test["output"]
                ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
        else:
            # Convert result to json since our tests are reading from json so cannot easily contain richer types (e.g. NaN)
            # NOTE - 20191031 - JPC - we may eventually want to change these tests as we update our view on how
            # representations, serializations, and objects should interact and how much of that is shown to the user.
            result = result.to_json_dict()
            for key, value in test["output"].items():
                # Apply our great expectations-specific test logic

                if key == "success":
                    if isinstance(value, (np.floating, float)):
                        try:
                            assert np.allclose(
                                result["success"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['success']} not np.allclose to {value}"
                        except TypeError:
                            assert (
                                result["success"] == value
                            ), f"{result['success']} != {value}"
                    else:
                        assert result["success"] == value, f"{result['success']} != {value}"

                elif key == "observed_value":
                    if "tolerance" in test:
                        if isinstance(value, dict):
                            assert set(result["result"]["observed_value"].keys()) == set(
                                value.keys()
                            ), f"{set(result['result']['observed_value'].keys())} != {set(value.keys())}"
                            for k, v in value.items():
                                assert np.allclose(
                                    result["result"]["observed_value"][k],
                                    v,
                                    rtol=test["tolerance"],
                                )
                        else:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=test["tolerance"],
                            )
                    else:
                        if isinstance(value, dict) and "values" in value:
                            try:
                                assert np.allclose(
                                    result["result"]["observed_value"]["values"],
                                    value["values"],
                                    rtol=RTOL,
                                    atol=ATOL,
                                ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']['values']} not np.allclose to {value['values']}"
                            except TypeError as e:
                                print(e)
                                assert (
                                    result["result"]["observed_value"] == value
                                ), f"{result['result']['observed_value']} != {value}"
                        elif try_allclose:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {value}"
                        else:
                            assert (
                                result["result"]["observed_value"] == value
                            ), f"{result['result']['observed_value']} != {value}"

                # NOTE: This is a key used ONLY for testing cases where an expectation is legitimately allowed to return
                # any of multiple possible observed_values. expect_column_values_to_be_of_type is one such expectation.
                elif key == "observed_value_list":
                    assert result["result"]["observed_value"] in value

                elif key == "unexpected_index_list":
                    if isinstance(data_asset, (SqlAlchemyDataset, SparkDFDataset)):
                        pass
                    elif isinstance(data_asset, (SqlAlchemyBatchData, SparkDFBatchData)):
                        pass
                    else:
>                       assert (
                            result["result"]["unexpected_index_list"] == value
                        ), f"{result['result']['unexpected_index_list']} != {value}"
E                       AssertionError: [{'x': 1, 'pk_index': 0}] != [0]

great_expectations/self_check/util.py:3075: AssertionError
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 4064.25it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 402.41it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 557.70it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 310.26it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 428.01it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 326.60it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 512.61it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 477.50it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 475.58it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 473.34it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_not_be_in_set:basic_negative_strings_set_all_character_values] _

test_case = {'expectation_type': 'expect_column_values_to_not_be_in_set', 'pk_column': True, 'skip': False, 'test': {'exact_match_...ut': {'success': False, 'unexpected_index_list': [0, 1, 2], 'unexpected_list': ['hello', 'jello', 'mello']}, ...}, ...}

    @pytest.mark.order(index=0)
    @pytest.mark.integration
    @pytest.mark.slow  # 12.68s
    def test_case_runner_v3_api(test_case):
        if test_case["skip"]:
            pytest.skip()

        # Note: this should never be done in practice, but we are wiping expectations to reuse batches during testing.
        # test_case["batch"]._initialize_expectations()
        if "parse_strings_as_datetimes" in test_case["test"]["in"]:
            with pytest.deprecated_call():
                evaluate_json_test_v3_api(
                    validator=test_case["validator_with_data"],
                    expectation_type=test_case["expectation_type"],
                    test=test_case["test"],
                    pk_column=test_case["pk_column"],
                )
        else:
>           evaluate_json_test_v3_api(
                validator=test_case["validator_with_data"],
                expectation_type=test_case["expectation_type"],
                test=test_case["test"],
                pk_column=test_case["pk_column"],
            )

tests/test_definitions/test_expectations_v3_api.py:418:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
great_expectations/self_check/util.py:2901: in evaluate_json_test_v3_api
    check_json_test_result(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

test = {'exact_match_out': False, 'in': {'column': 'z', 'value_set': ['hello', 'jello', 'mello']}, 'input': {'column': 'z', '...]}, 'out': {'success': False, 'unexpected_index_list': [0, 1, 2], 'unexpected_list': ['hello', 'jello', 'mello']}, ...}
result = {'exception_info': {'exception_message': None, 'exception_traceback': None, 'raised_exception': False}, 'expectation_c...d_counts': [{'count': 1, 'value': 'hello'}, {'count': 1, 'value': 'jello'}, {'count': 1, 'value': 'mello'}], ...}, ...}
data_asset = <great_expectations.execution_engine.pandas_batch_data.PandasBatchData object at 0x7f78062e4d90>
pk_column = True

    def check_json_test_result(  # noqa: C901 - 52
        test, result, data_asset=None, pk_column=False
    ) -> None:

        # check for id_pk results in cases where pk_column is true and unexpected_index_list already exists
        # this will work for testing since result_format is COMPLETE
        if pk_column:
            if not result["success"]:
                if "unexpected_index_list" in result["result"]:
                    assert "unexpected_index_query" in result["result"]

        if "unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )
            elif "unexpected_list" in test["output"]:
                (
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )

        if "partial_unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "partial_unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )
            elif "partial_unexpected_list" in test["output"]:
                (
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )

        # Determine if np.allclose(..) might be needed for float comparison
        try_allclose = False
        if "observed_value" in test["output"]:
            if RX_FLOAT.match(repr(test["output"]["observed_value"])):
                try_allclose = True

        # Check results
        if test["exact_match_out"] is True:
            if "result" in result and "observed_value" in result["result"]:
                if isinstance(result["result"]["observed_value"], (np.floating, float)):
                    assert np.allclose(
                        result["result"]["observed_value"],
                        expectationValidationResultSchema.load(test["output"])["result"][
                            "observed_value"
                        ],
                        rtol=RTOL,
                        atol=ATOL,
                    ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {expectationValidationResultSchema.load(test['output'])['result']['observed_value']}"
                else:
                    assert result == expectationValidationResultSchema.load(
                        test["output"]
                    ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
            else:
                assert result == expectationValidationResultSchema.load(
                    test["output"]
                ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
        else:
            # Convert result to json since our tests are reading from json so cannot easily contain richer types (e.g. NaN)
            # NOTE - 20191031 - JPC - we may eventually want to change these tests as we update our view on how
            # representations, serializations, and objects should interact and how much of that is shown to the user.
            result = result.to_json_dict()
            for key, value in test["output"].items():
                # Apply our great expectations-specific test logic

                if key == "success":
                    if isinstance(value, (np.floating, float)):
                        try:
                            assert np.allclose(
                                result["success"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['success']} not np.allclose to {value}"
                        except TypeError:
                            assert (
                                result["success"] == value
                            ), f"{result['success']} != {value}"
                    else:
                        assert result["success"] == value, f"{result['success']} != {value}"

                elif key == "observed_value":
                    if "tolerance" in test:
                        if isinstance(value, dict):
                            assert set(result["result"]["observed_value"].keys()) == set(
                                value.keys()
                            ), f"{set(result['result']['observed_value'].keys())} != {set(value.keys())}"
                            for k, v in value.items():
                                assert np.allclose(
                                    result["result"]["observed_value"][k],
                                    v,
                                    rtol=test["tolerance"],
                                )
                        else:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=test["tolerance"],
                            )
                    else:
                        if isinstance(value, dict) and "values" in value:
                            try:
                                assert np.allclose(
                                    result["result"]["observed_value"]["values"],
                                    value["values"],
                                    rtol=RTOL,
                                    atol=ATOL,
                                ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']['values']} not np.allclose to {value['values']}"
                            except TypeError as e:
                                print(e)
                                assert (
                                    result["result"]["observed_value"] == value
                                ), f"{result['result']['observed_value']} != {value}"
                        elif try_allclose:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {value}"
                        else:
                            assert (
                                result["result"]["observed_value"] == value
                            ), f"{result['result']['observed_value']} != {value}"

                # NOTE: This is a key used ONLY for testing cases where an expectation is legitimately allowed to return
                # any of multiple possible observed_values. expect_column_values_to_be_of_type is one such expectation.
                elif key == "observed_value_list":
                    assert result["result"]["observed_value"] in value

                elif key == "unexpected_index_list":
                    if isinstance(data_asset, (SqlAlchemyDataset, SparkDFDataset)):
                        pass
                    elif isinstance(data_asset, (SqlAlchemyBatchData, SparkDFBatchData)):
                        pass
                    else:
>                       assert (
                            result["result"]["unexpected_index_list"] == value
                        ), f"{result['result']['unexpected_index_list']} != {value}"
E                       AssertionError: [{'z': 'hello', 'pk_index': 0}, {'z': 'jello', 'pk_index': 1}, {'z': 'mello', 'pk_index': 2}] != [0, 1, 2]

great_expectations/self_check/util.py:3075: AssertionError
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 2793.41it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 350.83it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 498.75it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 293.34it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 414.80it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 322.08it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 507.95it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 473.53it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 471.68it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 469.51it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_not_be_in_set:negative_test_float_set_two_out_of_three_column_values_included_mostly] _

test_case = {'expectation_type': 'expect_column_values_to_not_be_in_set', 'pk_column': True, 'skip': False, 'test': {'exact_match_...et': [1.1, 2.2]}, 'out': {'success': False, 'unexpected_index_list': [0, 1], 'unexpected_list': [1.1, 2.2]}, ...}, ...}

    @pytest.mark.order(index=0)
    @pytest.mark.integration
    @pytest.mark.slow  # 12.68s
    def test_case_runner_v3_api(test_case):
        if test_case["skip"]:
            pytest.skip()

        # Note: this should never be done in practice, but we are wiping expectations to reuse batches during testing.
        # test_case["batch"]._initialize_expectations()
        if "parse_strings_as_datetimes" in test_case["test"]["in"]:
            with pytest.deprecated_call():
                evaluate_json_test_v3_api(
                    validator=test_case["validator_with_data"],
                    expectation_type=test_case["expectation_type"],
                    test=test_case["test"],
                    pk_column=test_case["pk_column"],
                )
        else:
>           evaluate_json_test_v3_api(
                validator=test_case["validator_with_data"],
                expectation_type=test_case["expectation_type"],
                test=test_case["test"],
                pk_column=test_case["pk_column"],
            )

tests/test_definitions/test_expectations_v3_api.py:418:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
great_expectations/self_check/util.py:2901: in evaluate_json_test_v3_api
    check_json_test_result(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

test = {'exact_match_out': False, 'in': {'column': 'y', 'mostly': 0.65, 'value_set': [1.1, 2.2]}, 'input': {'column': 'y', 'm...alue_set': [1.1, 2.2]}, 'out': {'success': False, 'unexpected_index_list': [0, 1], 'unexpected_list': [1.1, 2.2]}, ...}
result = {'exception_info': {'exception_message': None, 'exception_traceback': None, 'raised_exception': False}, 'expectation_c...issing_percent': 0.0, 'partial_unexpected_counts': [{'count': 1, 'value': 1.1}, {'count': 1, 'value': 2.2}], ...}, ...}
data_asset = <great_expectations.execution_engine.pandas_batch_data.PandasBatchData object at 0x7f78062e4d90>
pk_column = True

    def check_json_test_result(  # noqa: C901 - 52
        test, result, data_asset=None, pk_column=False
    ) -> None:

        # check for id_pk results in cases where pk_column is true and unexpected_index_list already exists
        # this will work for testing since result_format is COMPLETE
        if pk_column:
            if not result["success"]:
                if "unexpected_index_list" in result["result"]:
                    assert "unexpected_index_query" in result["result"]

        if "unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )
            elif "unexpected_list" in test["output"]:
                (
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )

        if "partial_unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "partial_unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )
            elif "partial_unexpected_list" in test["output"]:
                (
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )

        # Determine if np.allclose(..) might be needed for float comparison
        try_allclose = False
        if "observed_value" in test["output"]:
            if RX_FLOAT.match(repr(test["output"]["observed_value"])):
                try_allclose = True

        # Check results
        if test["exact_match_out"] is True:
            if "result" in result and "observed_value" in result["result"]:
                if isinstance(result["result"]["observed_value"], (np.floating, float)):
                    assert np.allclose(
                        result["result"]["observed_value"],
                        expectationValidationResultSchema.load(test["output"])["result"][
                            "observed_value"
                        ],
                        rtol=RTOL,
                        atol=ATOL,
                    ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {expectationValidationResultSchema.load(test['output'])['result']['observed_value']}"
                else:
                    assert result == expectationValidationResultSchema.load(
                        test["output"]
                    ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
            else:
                assert result == expectationValidationResultSchema.load(
                    test["output"]
                ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
        else:
            # Convert result to json since our tests are reading from json so cannot easily contain richer types (e.g. NaN)
            # NOTE - 20191031 - JPC - we may eventually want to change these tests as we update our view on how
            # representations, serializations, and objects should interact and how much of that is shown to the user.
            result = result.to_json_dict()
            for key, value in test["output"].items():
                # Apply our great expectations-specific test logic

                if key == "success":
                    if isinstance(value, (np.floating, float)):
                        try:
                            assert np.allclose(
                                result["success"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['success']} not np.allclose to {value}"
                        except TypeError:
                            assert (
                                result["success"] == value
                            ), f"{result['success']} != {value}"
                    else:
                        assert result["success"] == value, f"{result['success']} != {value}"

                elif key == "observed_value":
                    if "tolerance" in test:
                        if isinstance(value, dict):
                            assert set(result["result"]["observed_value"].keys()) == set(
                                value.keys()
                            ), f"{set(result['result']['observed_value'].keys())} != {set(value.keys())}"
                            for k, v in value.items():
                                assert np.allclose(
                                    result["result"]["observed_value"][k],
                                    v,
                                    rtol=test["tolerance"],
                                )
                        else:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=test["tolerance"],
                            )
                    else:
                        if isinstance(value, dict) and "values" in value:
                            try:
                                assert np.allclose(
                                    result["result"]["observed_value"]["values"],
                                    value["values"],
                                    rtol=RTOL,
                                    atol=ATOL,
                                ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']['values']} not np.allclose to {value['values']}"
                            except TypeError as e:
                                print(e)
                                assert (
                                    result["result"]["observed_value"] == value
                                ), f"{result['result']['observed_value']} != {value}"
                        elif try_allclose:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {value}"
                        else:
                            assert (
                                result["result"]["observed_value"] == value
                            ), f"{result['result']['observed_value']} != {value}"

                # NOTE: This is a key used ONLY for testing cases where an expectation is legitimately allowed to return
                # any of multiple possible observed_values. expect_column_values_to_be_of_type is one such expectation.
                elif key == "observed_value_list":
                    assert result["result"]["observed_value"] in value

                elif key == "unexpected_index_list":
                    if isinstance(data_asset, (SqlAlchemyDataset, SparkDFDataset)):
                        pass
                    elif isinstance(data_asset, (SqlAlchemyBatchData, SparkDFBatchData)):
                        pass
                    else:
>                       assert (
                            result["result"]["unexpected_index_list"] == value
                        ), f"{result['result']['unexpected_index_list']} != {value}"
E                       AssertionError: [{'y': 1.1, 'pk_index': 0}, {'y': 2.2, 'pk_index': 1}] != [0, 1]

great_expectations/self_check/util.py:3075: AssertionError
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 4098.00it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 384.46it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 548.04it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 300.84it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 426.00it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 323.40it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 507.05it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 469.68it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 467.66it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 465.48it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_not_be_in_set:negative_test_float_set_two_out_of_three_column_values_included_mostly_summary_output] _

test_case = {'expectation_type': 'expect_column_values_to_not_be_in_set', 'pk_column': True, 'skip': False, 'test': {'exact_match_...et': [1.1, 2.2]}, 'input': {'column': 'y', 'mostly': 0.65, 'value_set': [1.1, 2.2]}, 'only_for': ['pandas'], ...}, ...}

    @pytest.mark.order(index=0)
    @pytest.mark.integration
    @pytest.mark.slow  # 12.68s
    def test_case_runner_v3_api(test_case):
        if test_case["skip"]:
            pytest.skip()

        # Note: this should never be done in practice, but we are wiping expectations to reuse batches during testing.
        # test_case["batch"]._initialize_expectations()
        if "parse_strings_as_datetimes" in test_case["test"]["in"]:
            with pytest.deprecated_call():
                evaluate_json_test_v3_api(
                    validator=test_case["validator_with_data"],
                    expectation_type=test_case["expectation_type"],
                    test=test_case["test"],
                    pk_column=test_case["pk_column"],
                )
        else:
>           evaluate_json_test_v3_api(
                validator=test_case["validator_with_data"],
                expectation_type=test_case["expectation_type"],
                test=test_case["test"],
                pk_column=test_case["pk_column"],
            )

tests/test_definitions/test_expectations_v3_api.py:418:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
great_expectations/self_check/util.py:2901: in evaluate_json_test_v3_api
    check_json_test_result(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

test = {'exact_match_out': False, 'in': {'column': 'y', 'mostly': 0.65, 'value_set': [1.1, 2.2]}, 'input': {'column': 'y', 'mostly': 0.65, 'value_set': [1.1, 2.2]}, 'only_for': ['pandas'], ...}
result = {'exception_info': {'exception_message': None, 'exception_traceback': None, 'raised_exception': False}, 'expectation_c...issing_percent': 0.0, 'partial_unexpected_counts': [{'count': 1, 'value': 1.1}, {'count': 1, 'value': 2.2}], ...}, ...}
data_asset = <great_expectations.execution_engine.pandas_batch_data.PandasBatchData object at 0x7f78062e4d90>
pk_column = True

    def check_json_test_result(  # noqa: C901 - 52
        test, result, data_asset=None, pk_column=False
    ) -> None:

        # check for id_pk results in cases where pk_column is true and unexpected_index_list already exists
        # this will work for testing since result_format is COMPLETE
        if pk_column:
            if not result["success"]:
                if "unexpected_index_list" in result["result"]:
                    assert "unexpected_index_query" in result["result"]

        if "unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )
            elif "unexpected_list" in test["output"]:
                (
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )

        if "partial_unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "partial_unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )
            elif "partial_unexpected_list" in test["output"]:
                (
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )

        # Determine if np.allclose(..) might be needed for float comparison
        try_allclose = False
        if "observed_value" in test["output"]:
            if RX_FLOAT.match(repr(test["output"]["observed_value"])):
                try_allclose = True

        # Check results
        if test["exact_match_out"] is True:
            if "result" in result and "observed_value" in result["result"]:
                if isinstance(result["result"]["observed_value"], (np.floating, float)):
                    assert np.allclose(
                        result["result"]["observed_value"],
                        expectationValidationResultSchema.load(test["output"])["result"][
                            "observed_value"
                        ],
                        rtol=RTOL,
                        atol=ATOL,
                    ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {expectationValidationResultSchema.load(test['output'])['result']['observed_value']}"
                else:
                    assert result == expectationValidationResultSchema.load(
                        test["output"]
                    ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
            else:
                assert result == expectationValidationResultSchema.load(
                    test["output"]
                ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
        else:
            # Convert result to json since our tests are reading from json so cannot easily contain richer types (e.g. NaN)
            # NOTE - 20191031 - JPC - we may eventually want to change these tests as we update our view on how
            # representations, serializations, and objects should interact and how much of that is shown to the user.
            result = result.to_json_dict()
            for key, value in test["output"].items():
                # Apply our great expectations-specific test logic

                if key == "success":
                    if isinstance(value, (np.floating, float)):
                        try:
                            assert np.allclose(
                                result["success"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['success']} not np.allclose to {value}"
                        except TypeError:
                            assert (
                                result["success"] == value
                            ), f"{result['success']} != {value}"
                    else:
                        assert result["success"] == value, f"{result['success']} != {value}"

                elif key == "observed_value":
                    if "tolerance" in test:
                        if isinstance(value, dict):
                            assert set(result["result"]["observed_value"].keys()) == set(
                                value.keys()
                            ), f"{set(result['result']['observed_value'].keys())} != {set(value.keys())}"
                            for k, v in value.items():
                                assert np.allclose(
                                    result["result"]["observed_value"][k],
                                    v,
                                    rtol=test["tolerance"],
                                )
                        else:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=test["tolerance"],
                            )
                    else:
                        if isinstance(value, dict) and "values" in value:
                            try:
                                assert np.allclose(
                                    result["result"]["observed_value"]["values"],
                                    value["values"],
                                    rtol=RTOL,
                                    atol=ATOL,
                                ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']['values']} not np.allclose to {value['values']}"
                            except TypeError as e:
                                print(e)
                                assert (
                                    result["result"]["observed_value"] == value
                                ), f"{result['result']['observed_value']} != {value}"
                        elif try_allclose:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {value}"
                        else:
                            assert (
                                result["result"]["observed_value"] == value
                            ), f"{result['result']['observed_value']} != {value}"

                # NOTE: This is a key used ONLY for testing cases where an expectation is legitimately allowed to return
                # any of multiple possible observed_values. expect_column_values_to_be_of_type is one such expectation.
                elif key == "observed_value_list":
                    assert result["result"]["observed_value"] in value

                elif key == "unexpected_index_list":
                    if isinstance(data_asset, (SqlAlchemyDataset, SparkDFDataset)):
                        pass
                    elif isinstance(data_asset, (SqlAlchemyBatchData, SparkDFBatchData)):
                        pass
                    else:
>                       assert (
                            result["result"]["unexpected_index_list"] == value
                        ), f"{result['result']['unexpected_index_list']} != {value}"
E                       AssertionError: [{'y': 1.1, 'pk_index': 0}, {'y': 2.2, 'pk_index': 1}] != [0, 1]

great_expectations/self_check/util.py:3075: AssertionError
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 3958.76it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 394.39it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 562.21it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 310.33it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 438.48it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 331.43it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 521.19it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 479.02it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 476.62it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 474.29it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_not_be_in_set:positive_test_float_set_two_out_of_three_column_values_included_mostly] _

test_case = {'expectation_type': 'expect_column_values_to_not_be_in_set', 'pk_column': True, 'skip': False, 'test': {'exact_match_...et': [1.1, 2.2]}, 'out': {'success': False, 'unexpected_index_list': [0, 1], 'unexpected_list': [1.1, 2.2]}, ...}, ...}

    @pytest.mark.order(index=0)
    @pytest.mark.integration
    @pytest.mark.slow  # 12.68s
    def test_case_runner_v3_api(test_case):
        if test_case["skip"]:
            pytest.skip()

        # Note: this should never be done in practice, but we are wiping expectations to reuse batches during testing.
        # test_case["batch"]._initialize_expectations()
        if "parse_strings_as_datetimes" in test_case["test"]["in"]:
            with pytest.deprecated_call():
                evaluate_json_test_v3_api(
                    validator=test_case["validator_with_data"],
                    expectation_type=test_case["expectation_type"],
                    test=test_case["test"],
                    pk_column=test_case["pk_column"],
                )
        else:
>           evaluate_json_test_v3_api(
                validator=test_case["validator_with_data"],
                expectation_type=test_case["expectation_type"],
                test=test_case["test"],
                pk_column=test_case["pk_column"],
            )

tests/test_definitions/test_expectations_v3_api.py:418:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
great_expectations/self_check/util.py:2901: in evaluate_json_test_v3_api
    check_json_test_result(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

test = {'exact_match_out': False, 'in': {'column': 'y', 'mostly': 0.7, 'value_set': [1.1, 2.2]}, 'input': {'column': 'y', 'mo...alue_set': [1.1, 2.2]}, 'out': {'success': False, 'unexpected_index_list': [0, 1], 'unexpected_list': [1.1, 2.2]}, ...}
result = {'exception_info': {'exception_message': None, 'exception_traceback': None, 'raised_exception': False}, 'expectation_c...issing_percent': 0.0, 'partial_unexpected_counts': [{'count': 1, 'value': 1.1}, {'count': 1, 'value': 2.2}], ...}, ...}
data_asset = <great_expectations.execution_engine.pandas_batch_data.PandasBatchData object at 0x7f78062e4d90>
pk_column = True

    def check_json_test_result(  # noqa: C901 - 52
        test, result, data_asset=None, pk_column=False
    ) -> None:

        # check for id_pk results in cases where pk_column is true and unexpected_index_list already exists
        # this will work for testing since result_format is COMPLETE
        if pk_column:
            if not result["success"]:
                if "unexpected_index_list" in result["result"]:
                    assert "unexpected_index_query" in result["result"]

        if "unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )
            elif "unexpected_list" in test["output"]:
                (
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )

        if "partial_unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "partial_unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )
            elif "partial_unexpected_list" in test["output"]:
                (
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )

        # Determine if np.allclose(..) might be needed for float comparison
        try_allclose = False
        if "observed_value" in test["output"]:
            if RX_FLOAT.match(repr(test["output"]["observed_value"])):
                try_allclose = True

        # Check results
        if test["exact_match_out"] is True:
            if "result" in result and "observed_value" in result["result"]:
                if isinstance(result["result"]["observed_value"], (np.floating, float)):
                    assert np.allclose(
                        result["result"]["observed_value"],
                        expectationValidationResultSchema.load(test["output"])["result"][
                            "observed_value"
                        ],
                        rtol=RTOL,
                        atol=ATOL,
                    ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {expectationValidationResultSchema.load(test['output'])['result']['observed_value']}"
                else:
                    assert result == expectationValidationResultSchema.load(
                        test["output"]
                    ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
            else:
                assert result == expectationValidationResultSchema.load(
                    test["output"]
                ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
        else:
            # Convert result to json since our tests are reading from json so cannot easily contain richer types (e.g. NaN)
            # NOTE - 20191031 - JPC - we may eventually want to change these tests as we update our view on how
            # representations, serializations, and objects should interact and how much of that is shown to the user.
            result = result.to_json_dict()
            for key, value in test["output"].items():
                # Apply our great expectations-specific test logic

                if key == "success":
                    if isinstance(value, (np.floating, float)):
                        try:
                            assert np.allclose(
                                result["success"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['success']} not np.allclose to {value}"
                        except TypeError:
                            assert (
                                result["success"] == value
                            ), f"{result['success']} != {value}"
                    else:
                        assert result["success"] == value, f"{result['success']} != {value}"

                elif key == "observed_value":
                    if "tolerance" in test:
                        if isinstance(value, dict):
                            assert set(result["result"]["observed_value"].keys()) == set(
                                value.keys()
                            ), f"{set(result['result']['observed_value'].keys())} != {set(value.keys())}"
                            for k, v in value.items():
                                assert np.allclose(
                                    result["result"]["observed_value"][k],
                                    v,
                                    rtol=test["tolerance"],
                                )
                        else:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=test["tolerance"],
                            )
                    else:
                        if isinstance(value, dict) and "values" in value:
                            try:
                                assert np.allclose(
                                    result["result"]["observed_value"]["values"],
                                    value["values"],
                                    rtol=RTOL,
                                    atol=ATOL,
                                ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']['values']} not np.allclose to {value['values']}"
                            except TypeError as e:
                                print(e)
                                assert (
                                    result["result"]["observed_value"] == value
                                ), f"{result['result']['observed_value']} != {value}"
                        elif try_allclose:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {value}"
                        else:
                            assert (
                                result["result"]["observed_value"] == value
                            ), f"{result['result']['observed_value']} != {value}"

                # NOTE: This is a key used ONLY for testing cases where an expectation is legitimately allowed to return
                # any of multiple possible observed_values. expect_column_values_to_be_of_type is one such expectation.
                elif key == "observed_value_list":
                    assert result["result"]["observed_value"] in value

                elif key == "unexpected_index_list":
                    if isinstance(data_asset, (SqlAlchemyDataset, SparkDFDataset)):
                        pass
                    elif isinstance(data_asset, (SqlAlchemyBatchData, SparkDFBatchData)):
                        pass
                    else:
>                       assert (
                            result["result"]["unexpected_index_list"] == value
                        ), f"{result['result']['unexpected_index_list']} != {value}"
E                       AssertionError: [{'y': 1.1, 'pk_index': 0}, {'y': 2.2, 'pk_index': 1}] != [0, 1]

great_expectations/self_check/util.py:3075: AssertionError
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 2469.42it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 281.89it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 398.57it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 231.90it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 335.10it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 257.72it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 411.37it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 383.87it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 382.53it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 380.99it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_in_set:negative_test_case_exclude_existing_column_value] _

test_case = {'expectation_type': 'expect_column_values_to_be_in_set', 'pk_column': True, 'skip': False, 'test': {'exact_match_out'...lumn': 'x', 'value_set': [2, 4]}, 'include_in_gallery': True, 'input': {'column': 'x', 'value_set': [2, 4]}, ...}, ...}

    @pytest.mark.order(index=0)
    @pytest.mark.integration
    @pytest.mark.slow  # 12.68s
    def test_case_runner_v3_api(test_case):
        if test_case["skip"]:
            pytest.skip()

        # Note: this should never be done in practice, but we are wiping expectations to reuse batches during testing.
        # test_case["batch"]._initialize_expectations()
        if "parse_strings_as_datetimes" in test_case["test"]["in"]:
            with pytest.deprecated_call():
                evaluate_json_test_v3_api(
                    validator=test_case["validator_with_data"],
                    expectation_type=test_case["expectation_type"],
                    test=test_case["test"],
                    pk_column=test_case["pk_column"],
                )
        else:
>           evaluate_json_test_v3_api(
                validator=test_case["validator_with_data"],
                expectation_type=test_case["expectation_type"],
                test=test_case["test"],
                pk_column=test_case["pk_column"],
            )

tests/test_definitions/test_expectations_v3_api.py:418:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
great_expectations/self_check/util.py:2901: in evaluate_json_test_v3_api
    check_json_test_result(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

test = {'exact_match_out': False, 'in': {'column': 'x', 'value_set': [2, 4]}, 'include_in_gallery': True, 'input': {'column': 'x', 'value_set': [2, 4]}, ...}
result = {'exception_info': {'exception_message': None, 'exception_traceback': None, 'raised_exception': False}, 'expectation_c...nt': 3, 'missing_count': 0, 'missing_percent': 0.0, 'partial_unexpected_counts': [{'count': 1, 'value': 1}], ...}, ...}
data_asset = <great_expectations.execution_engine.pandas_batch_data.PandasBatchData object at 0x7f78062a3340>
pk_column = True

    def check_json_test_result(  # noqa: C901 - 52
        test, result, data_asset=None, pk_column=False
    ) -> None:

        # check for id_pk results in cases where pk_column is true and unexpected_index_list already exists
        # this will work for testing since result_format is COMPLETE
        if pk_column:
            if not result["success"]:
                if "unexpected_index_list" in result["result"]:
                    assert "unexpected_index_query" in result["result"]

        if "unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )
            elif "unexpected_list" in test["output"]:
                (
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )

        if "partial_unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "partial_unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )
            elif "partial_unexpected_list" in test["output"]:
                (
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )

        # Determine if np.allclose(..) might be needed for float comparison
        try_allclose = False
        if "observed_value" in test["output"]:
            if RX_FLOAT.match(repr(test["output"]["observed_value"])):
                try_allclose = True

        # Check results
        if test["exact_match_out"] is True:
            if "result" in result and "observed_value" in result["result"]:
                if isinstance(result["result"]["observed_value"], (np.floating, float)):
                    assert np.allclose(
                        result["result"]["observed_value"],
                        expectationValidationResultSchema.load(test["output"])["result"][
                            "observed_value"
                        ],
                        rtol=RTOL,
                        atol=ATOL,
                    ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {expectationValidationResultSchema.load(test['output'])['result']['observed_value']}"
                else:
                    assert result == expectationValidationResultSchema.load(
                        test["output"]
                    ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
            else:
                assert result == expectationValidationResultSchema.load(
                    test["output"]
                ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
        else:
            # Convert result to json since our tests are reading from json so cannot easily contain richer types (e.g. NaN)
            # NOTE - 20191031 - JPC - we may eventually want to change these tests as we update our view on how
            # representations, serializations, and objects should interact and how much of that is shown to the user.
            result = result.to_json_dict()
            for key, value in test["output"].items():
                # Apply our great expectations-specific test logic

                if key == "success":
                    if isinstance(value, (np.floating, float)):
                        try:
                            assert np.allclose(
                                result["success"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['success']} not np.allclose to {value}"
                        except TypeError:
                            assert (
                                result["success"] == value
                            ), f"{result['success']} != {value}"
                    else:
                        assert result["success"] == value, f"{result['success']} != {value}"

                elif key == "observed_value":
                    if "tolerance" in test:
                        if isinstance(value, dict):
                            assert set(result["result"]["observed_value"].keys()) == set(
                                value.keys()
                            ), f"{set(result['result']['observed_value'].keys())} != {set(value.keys())}"
                            for k, v in value.items():
                                assert np.allclose(
                                    result["result"]["observed_value"][k],
                                    v,
                                    rtol=test["tolerance"],
                                )
                        else:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=test["tolerance"],
                            )
                    else:
                        if isinstance(value, dict) and "values" in value:
                            try:
                                assert np.allclose(
                                    result["result"]["observed_value"]["values"],
                                    value["values"],
                                    rtol=RTOL,
                                    atol=ATOL,
                                ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']['values']} not np.allclose to {value['values']}"
                            except TypeError as e:
                                print(e)
                                assert (
                                    result["result"]["observed_value"] == value
                                ), f"{result['result']['observed_value']} != {value}"
                        elif try_allclose:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {value}"
                        else:
                            assert (
                                result["result"]["observed_value"] == value
                            ), f"{result['result']['observed_value']} != {value}"

                # NOTE: This is a key used ONLY for testing cases where an expectation is legitimately allowed to return
                # any of multiple possible observed_values. expect_column_values_to_be_of_type is one such expectation.
                elif key == "observed_value_list":
                    assert result["result"]["observed_value"] in value

                elif key == "unexpected_index_list":
                    if isinstance(data_asset, (SqlAlchemyDataset, SparkDFDataset)):
                        pass
                    elif isinstance(data_asset, (SqlAlchemyBatchData, SparkDFBatchData)):
                        pass
                    else:
>                       assert (
                            result["result"]["unexpected_index_list"] == value
                        ), f"{result['result']['unexpected_index_list']} != {value}"
E                       AssertionError: [{'x': 1, 'pk_index': 0}] != [0]

great_expectations/self_check/util.py:3075: AssertionError
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 4009.85it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 414.87it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 589.28it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 321.54it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 460.66it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 353.51it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 556.20it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 515.33it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 513.11it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 510.54it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_in_set:negative_empty_values_set] _

test_case = {'expectation_type': 'expect_column_values_to_be_in_set', 'pk_column': True, 'skip': False, 'test': {'exact_match_out'...alue_set': []}, 'out': {'success': False, 'unexpected_index_list': [0, 1, 2], 'unexpected_list': [1, 2, 4]}, ...}, ...}

    @pytest.mark.order(index=0)
    @pytest.mark.integration
    @pytest.mark.slow  # 12.68s
    def test_case_runner_v3_api(test_case):
        if test_case["skip"]:
            pytest.skip()

        # Note: this should never be done in practice, but we are wiping expectations to reuse batches during testing.
        # test_case["batch"]._initialize_expectations()
        if "parse_strings_as_datetimes" in test_case["test"]["in"]:
            with pytest.deprecated_call():
                evaluate_json_test_v3_api(
                    validator=test_case["validator_with_data"],
                    expectation_type=test_case["expectation_type"],
                    test=test_case["test"],
                    pk_column=test_case["pk_column"],
                )
        else:
>           evaluate_json_test_v3_api(
                validator=test_case["validator_with_data"],
                expectation_type=test_case["expectation_type"],
                test=test_case["test"],
                pk_column=test_case["pk_column"],
            )

tests/test_definitions/test_expectations_v3_api.py:418:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
great_expectations/self_check/util.py:2901: in evaluate_json_test_v3_api
    check_json_test_result(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

test = {'exact_match_out': False, 'in': {'column': 'x', 'value_set': []}, 'input': {'column': 'x', 'value_set': []}, 'out': {'success': False, 'unexpected_index_list': [0, 1, 2], 'unexpected_list': [1, 2, 4]}, ...}
result = {'exception_info': {'exception_message': None, 'exception_traceback': None, 'raised_exception': False}, 'expectation_c...'partial_unexpected_counts': [{'count': 1, 'value': 1}, {'count': 1, 'value': 2}, {'count': 1, 'value': 4}], ...}, ...}
data_asset = <great_expectations.execution_engine.pandas_batch_data.PandasBatchData object at 0x7f78062a3340>
pk_column = True

    def check_json_test_result(  # noqa: C901 - 52
        test, result, data_asset=None, pk_column=False
    ) -> None:

        # check for id_pk results in cases where pk_column is true and unexpected_index_list already exists
        # this will work for testing since result_format is COMPLETE
        if pk_column:
            if not result["success"]:
                if "unexpected_index_list" in result["result"]:
                    assert "unexpected_index_query" in result["result"]

        if "unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )
            elif "unexpected_list" in test["output"]:
                (
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )

        if "partial_unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "partial_unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )
            elif "partial_unexpected_list" in test["output"]:
                (
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )

        # Determine if np.allclose(..) might be needed for float comparison
        try_allclose = False
        if "observed_value" in test["output"]:
            if RX_FLOAT.match(repr(test["output"]["observed_value"])):
                try_allclose = True

        # Check results
        if test["exact_match_out"] is True:
            if "result" in result and "observed_value" in result["result"]:
                if isinstance(result["result"]["observed_value"], (np.floating, float)):
                    assert np.allclose(
                        result["result"]["observed_value"],
                        expectationValidationResultSchema.load(test["output"])["result"][
                            "observed_value"
                        ],
                        rtol=RTOL,
                        atol=ATOL,
                    ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {expectationValidationResultSchema.load(test['output'])['result']['observed_value']}"
                else:
                    assert result == expectationValidationResultSchema.load(
                        test["output"]
                    ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
            else:
                assert result == expectationValidationResultSchema.load(
                    test["output"]
                ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
        else:
            # Convert result to json since our tests are reading from json so cannot easily contain richer types (e.g. NaN)
            # NOTE - 20191031 - JPC - we may eventually want to change these tests as we update our view on how
            # representations, serializations, and objects should interact and how much of that is shown to the user.
            result = result.to_json_dict()
            for key, value in test["output"].items():
                # Apply our great expectations-specific test logic

                if key == "success":
                    if isinstance(value, (np.floating, float)):
                        try:
                            assert np.allclose(
                                result["success"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['success']} not np.allclose to {value}"
                        except TypeError:
                            assert (
                                result["success"] == value
                            ), f"{result['success']} != {value}"
                    else:
                        assert result["success"] == value, f"{result['success']} != {value}"

                elif key == "observed_value":
                    if "tolerance" in test:
                        if isinstance(value, dict):
                            assert set(result["result"]["observed_value"].keys()) == set(
                                value.keys()
                            ), f"{set(result['result']['observed_value'].keys())} != {set(value.keys())}"
                            for k, v in value.items():
                                assert np.allclose(
                                    result["result"]["observed_value"][k],
                                    v,
                                    rtol=test["tolerance"],
                                )
                        else:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=test["tolerance"],
                            )
                    else:
                        if isinstance(value, dict) and "values" in value:
                            try:
                                assert np.allclose(
                                    result["result"]["observed_value"]["values"],
                                    value["values"],
                                    rtol=RTOL,
                                    atol=ATOL,
                                ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']['values']} not np.allclose to {value['values']}"
                            except TypeError as e:
                                print(e)
                                assert (
                                    result["result"]["observed_value"] == value
                                ), f"{result['result']['observed_value']} != {value}"
                        elif try_allclose:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {value}"
                        else:
                            assert (
                                result["result"]["observed_value"] == value
                            ), f"{result['result']['observed_value']} != {value}"

                # NOTE: This is a key used ONLY for testing cases where an expectation is legitimately allowed to return
                # any of multiple possible observed_values. expect_column_values_to_be_of_type is one such expectation.
                elif key == "observed_value_list":
                    assert result["result"]["observed_value"] in value

                elif key == "unexpected_index_list":
                    if isinstance(data_asset, (SqlAlchemyDataset, SparkDFDataset)):
                        pass
                    elif isinstance(data_asset, (SqlAlchemyBatchData, SparkDFBatchData)):
                        pass
                    else:
>                       assert (
                            result["result"]["unexpected_index_list"] == value
                        ), f"{result['result']['unexpected_index_list']} != {value}"
E                       AssertionError: [{'x': 1, 'pk_index': 0}, {'x': 2, 'pk_index': 1}, {'x': 4, 'pk_index': 2}] != [0, 1, 2]

great_expectations/self_check/util.py:3075: AssertionError
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 3782.06it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 405.03it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 574.48it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 326.52it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 465.51it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 357.99it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 553.13it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 513.32it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 511.01it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 508.44it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_in_set:negative_test_strings_set_extra_value_in_column] _

test_case = {'expectation_type': 'expect_column_values_to_be_in_set', 'pk_column': True, 'skip': False, 'test': {'exact_match_out'... ['hello', 'jello']}, 'out': {'success': False, 'unexpected_index_list': [2], 'unexpected_list': ['mello']}, ...}, ...}

    @pytest.mark.order(index=0)
    @pytest.mark.integration
    @pytest.mark.slow  # 12.68s
    def test_case_runner_v3_api(test_case):
        if test_case["skip"]:
            pytest.skip()

        # Note: this should never be done in practice, but we are wiping expectations to reuse batches during testing.
        # test_case["batch"]._initialize_expectations()
        if "parse_strings_as_datetimes" in test_case["test"]["in"]:
            with pytest.deprecated_call():
                evaluate_json_test_v3_api(
                    validator=test_case["validator_with_data"],
                    expectation_type=test_case["expectation_type"],
                    test=test_case["test"],
                    pk_column=test_case["pk_column"],
                )
        else:
>           evaluate_json_test_v3_api(
                validator=test_case["validator_with_data"],
                expectation_type=test_case["expectation_type"],
                test=test_case["test"],
                pk_column=test_case["pk_column"],
            )

tests/test_definitions/test_expectations_v3_api.py:418:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
great_expectations/self_check/util.py:2901: in evaluate_json_test_v3_api
    check_json_test_result(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

test = {'exact_match_out': False, 'in': {'column': 'z', 'value_set': ['hello', 'jello']}, 'input': {'column': 'z', 'value_set': ['hello', 'jello']}, 'out': {'success': False, 'unexpected_index_list': [2], 'unexpected_list': ['mello']}, ...}
result = {'exception_info': {'exception_message': None, 'exception_traceback': None, 'raised_exception': False}, 'expectation_c..., 'missing_count': 0, 'missing_percent': 0.0, 'partial_unexpected_counts': [{'count': 1, 'value': 'mello'}], ...}, ...}
data_asset = <great_expectations.execution_engine.pandas_batch_data.PandasBatchData object at 0x7f78062a3340>
pk_column = True

    def check_json_test_result(  # noqa: C901 - 52
        test, result, data_asset=None, pk_column=False
    ) -> None:

        # check for id_pk results in cases where pk_column is true and unexpected_index_list already exists
        # this will work for testing since result_format is COMPLETE
        if pk_column:
            if not result["success"]:
                if "unexpected_index_list" in result["result"]:
                    assert "unexpected_index_query" in result["result"]

        if "unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )
            elif "unexpected_list" in test["output"]:
                (
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )

        if "partial_unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "partial_unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )
            elif "partial_unexpected_list" in test["output"]:
                (
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )

        # Determine if np.allclose(..) might be needed for float comparison
        try_allclose = False
        if "observed_value" in test["output"]:
            if RX_FLOAT.match(repr(test["output"]["observed_value"])):
                try_allclose = True

        # Check results
        if test["exact_match_out"] is True:
            if "result" in result and "observed_value" in result["result"]:
                if isinstance(result["result"]["observed_value"], (np.floating, float)):
                    assert np.allclose(
                        result["result"]["observed_value"],
                        expectationValidationResultSchema.load(test["output"])["result"][
                            "observed_value"
                        ],
                        rtol=RTOL,
                        atol=ATOL,
                    ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {expectationValidationResultSchema.load(test['output'])['result']['observed_value']}"
                else:
                    assert result == expectationValidationResultSchema.load(
                        test["output"]
                    ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
            else:
                assert result == expectationValidationResultSchema.load(
                    test["output"]
                ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
        else:
            # Convert result to json since our tests are reading from json so cannot easily contain richer types (e.g. NaN)
            # NOTE - 20191031 - JPC - we may eventually want to change these tests as we update our view on how
            # representations, serializations, and objects should interact and how much of that is shown to the user.
            result = result.to_json_dict()
            for key, value in test["output"].items():
                # Apply our great expectations-specific test logic

                if key == "success":
                    if isinstance(value, (np.floating, float)):
                        try:
                            assert np.allclose(
                                result["success"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['success']} not np.allclose to {value}"
                        except TypeError:
                            assert (
                                result["success"] == value
                            ), f"{result['success']} != {value}"
                    else:
                        assert result["success"] == value, f"{result['success']} != {value}"

                elif key == "observed_value":
                    if "tolerance" in test:
                        if isinstance(value, dict):
                            assert set(result["result"]["observed_value"].keys()) == set(
                                value.keys()
                            ), f"{set(result['result']['observed_value'].keys())} != {set(value.keys())}"
                            for k, v in value.items():
                                assert np.allclose(
                                    result["result"]["observed_value"][k],
                                    v,
                                    rtol=test["tolerance"],
                                )
                        else:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=test["tolerance"],
                            )
                    else:
                        if isinstance(value, dict) and "values" in value:
                            try:
                                assert np.allclose(
                                    result["result"]["observed_value"]["values"],
                                    value["values"],
                                    rtol=RTOL,
                                    atol=ATOL,
                                ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']['values']} not np.allclose to {value['values']}"
                            except TypeError as e:
                                print(e)
                                assert (
                                    result["result"]["observed_value"] == value
                                ), f"{result['result']['observed_value']} != {value}"
                        elif try_allclose:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {value}"
                        else:
                            assert (
                                result["result"]["observed_value"] == value
                            ), f"{result['result']['observed_value']} != {value}"

                # NOTE: This is a key used ONLY for testing cases where an expectation is legitimately allowed to return
                # any of multiple possible observed_values. expect_column_values_to_be_of_type is one such expectation.
                elif key == "observed_value_list":
                    assert result["result"]["observed_value"] in value

                elif key == "unexpected_index_list":
                    if isinstance(data_asset, (SqlAlchemyDataset, SparkDFDataset)):
                        pass
                    elif isinstance(data_asset, (SqlAlchemyBatchData, SparkDFBatchData)):
                        pass
                    else:
>                       assert (
                            result["result"]["unexpected_index_list"] == value
                        ), f"{result['result']['unexpected_index_list']} != {value}"
E                       AssertionError: [{'z': 'mello', 'pk_index': 2}] != [2]

great_expectations/self_check/util.py:3075: AssertionError
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 4142.52it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 403.65it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 572.31it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 322.13it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 449.69it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 335.24it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 515.70it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 478.50it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 476.28it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 473.95it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_in_set:negative_test_numbers_set_no_matching_values] _

test_case = {'expectation_type': 'expect_column_values_to_be_in_set', 'pk_column': True, 'skip': False, 'test': {'exact_match_out'...lue_set': [3]}, 'out': {'success': False, 'unexpected_index_list': [0, 1, 2], 'unexpected_list': [1, 2, 4]}, ...}, ...}

    @pytest.mark.order(index=0)
    @pytest.mark.integration
    @pytest.mark.slow  # 12.68s
    def test_case_runner_v3_api(test_case):
        if test_case["skip"]:
            pytest.skip()

        # Note: this should never be done in practice, but we are wiping expectations to reuse batches during testing.
        # test_case["batch"]._initialize_expectations()
        if "parse_strings_as_datetimes" in test_case["test"]["in"]:
            with pytest.deprecated_call():
                evaluate_json_test_v3_api(
                    validator=test_case["validator_with_data"],
                    expectation_type=test_case["expectation_type"],
                    test=test_case["test"],
                    pk_column=test_case["pk_column"],
                )
        else:
>           evaluate_json_test_v3_api(
                validator=test_case["validator_with_data"],
                expectation_type=test_case["expectation_type"],
                test=test_case["test"],
                pk_column=test_case["pk_column"],
            )

tests/test_definitions/test_expectations_v3_api.py:418:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
great_expectations/self_check/util.py:2901: in evaluate_json_test_v3_api
    check_json_test_result(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

test = {'exact_match_out': False, 'in': {'column': 'x', 'value_set': [3]}, 'input': {'column': 'x', 'value_set': [3]}, 'out': {'success': False, 'unexpected_index_list': [0, 1, 2], 'unexpected_list': [1, 2, 4]}, ...}
result = {'exception_info': {'exception_message': None, 'exception_traceback': None, 'raised_exception': False}, 'expectation_c...'partial_unexpected_counts': [{'count': 1, 'value': 1}, {'count': 1, 'value': 2}, {'count': 1, 'value': 4}], ...}, ...}
data_asset = <great_expectations.execution_engine.pandas_batch_data.PandasBatchData object at 0x7f78062a3340>
pk_column = True

    def check_json_test_result(  # noqa: C901 - 52
        test, result, data_asset=None, pk_column=False
    ) -> None:

        # check for id_pk results in cases where pk_column is true and unexpected_index_list already exists
        # this will work for testing since result_format is COMPLETE
        if pk_column:
            if not result["success"]:
                if "unexpected_index_list" in result["result"]:
                    assert "unexpected_index_query" in result["result"]

        if "unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )
            elif "unexpected_list" in test["output"]:
                (
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )

        if "partial_unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "partial_unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )
            elif "partial_unexpected_list" in test["output"]:
                (
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )

        # Determine if np.allclose(..) might be needed for float comparison
        try_allclose = False
        if "observed_value" in test["output"]:
            if RX_FLOAT.match(repr(test["output"]["observed_value"])):
                try_allclose = True

        # Check results
        if test["exact_match_out"] is True:
            if "result" in result and "observed_value" in result["result"]:
                if isinstance(result["result"]["observed_value"], (np.floating, float)):
                    assert np.allclose(
                        result["result"]["observed_value"],
                        expectationValidationResultSchema.load(test["output"])["result"][
                            "observed_value"
                        ],
                        rtol=RTOL,
                        atol=ATOL,
                    ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {expectationValidationResultSchema.load(test['output'])['result']['observed_value']}"
                else:
                    assert result == expectationValidationResultSchema.load(
                        test["output"]
                    ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
            else:
                assert result == expectationValidationResultSchema.load(
                    test["output"]
                ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
        else:
            # Convert result to json since our tests are reading from json so cannot easily contain richer types (e.g. NaN)
            # NOTE - 20191031 - JPC - we may eventually want to change these tests as we update our view on how
            # representations, serializations, and objects should interact and how much of that is shown to the user.
            result = result.to_json_dict()
            for key, value in test["output"].items():
                # Apply our great expectations-specific test logic

                if key == "success":
                    if isinstance(value, (np.floating, float)):
                        try:
                            assert np.allclose(
                                result["success"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['success']} not np.allclose to {value}"
                        except TypeError:
                            assert (
                                result["success"] == value
                            ), f"{result['success']} != {value}"
                    else:
                        assert result["success"] == value, f"{result['success']} != {value}"

                elif key == "observed_value":
                    if "tolerance" in test:
                        if isinstance(value, dict):
                            assert set(result["result"]["observed_value"].keys()) == set(
                                value.keys()
                            ), f"{set(result['result']['observed_value'].keys())} != {set(value.keys())}"
                            for k, v in value.items():
                                assert np.allclose(
                                    result["result"]["observed_value"][k],
                                    v,
                                    rtol=test["tolerance"],
                                )
                        else:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=test["tolerance"],
                            )
                    else:
                        if isinstance(value, dict) and "values" in value:
                            try:
                                assert np.allclose(
                                    result["result"]["observed_value"]["values"],
                                    value["values"],
                                    rtol=RTOL,
                                    atol=ATOL,
                                ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']['values']} not np.allclose to {value['values']}"
                            except TypeError as e:
                                print(e)
                                assert (
                                    result["result"]["observed_value"] == value
                                ), f"{result['result']['observed_value']} != {value}"
                        elif try_allclose:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {value}"
                        else:
                            assert (
                                result["result"]["observed_value"] == value
                            ), f"{result['result']['observed_value']} != {value}"

                # NOTE: This is a key used ONLY for testing cases where an expectation is legitimately allowed to return
                # any of multiple possible observed_values. expect_column_values_to_be_of_type is one such expectation.
                elif key == "observed_value_list":
                    assert result["result"]["observed_value"] in value

                elif key == "unexpected_index_list":
                    if isinstance(data_asset, (SqlAlchemyDataset, SparkDFDataset)):
                        pass
                    elif isinstance(data_asset, (SqlAlchemyBatchData, SparkDFBatchData)):
                        pass
                    else:
>                       assert (
                            result["result"]["unexpected_index_list"] == value
                        ), f"{result['result']['unexpected_index_list']} != {value}"
E                       AssertionError: [{'x': 1, 'pk_index': 0}, {'x': 2, 'pk_index': 1}, {'x': 4, 'pk_index': 2}] != [0, 1, 2]

great_expectations/self_check/util.py:3075: AssertionError
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 2597.90it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 360.23it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 512.54it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 281.48it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 406.37it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 319.80it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 513.03it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 476.76it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 474.63it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 472.39it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_in_set:negative_test_float_set] _

test_case = {'expectation_type': 'expect_column_values_to_be_in_set', 'pk_column': True, 'skip': False, 'test': {'exact_match_out'..., 5.51]}, 'out': {'success': False, 'unexpected_index_list': [0, 1, 2], 'unexpected_list': [1.1, 2.2, 5.5]}, ...}, ...}

    @pytest.mark.order(index=0)
    @pytest.mark.integration
    @pytest.mark.slow  # 12.68s
    def test_case_runner_v3_api(test_case):
        if test_case["skip"]:
            pytest.skip()

        # Note: this should never be done in practice, but we are wiping expectations to reuse batches during testing.
        # test_case["batch"]._initialize_expectations()
        if "parse_strings_as_datetimes" in test_case["test"]["in"]:
            with pytest.deprecated_call():
                evaluate_json_test_v3_api(
                    validator=test_case["validator_with_data"],
                    expectation_type=test_case["expectation_type"],
                    test=test_case["test"],
                    pk_column=test_case["pk_column"],
                )
        else:
>           evaluate_json_test_v3_api(
                validator=test_case["validator_with_data"],
                expectation_type=test_case["expectation_type"],
                test=test_case["test"],
                pk_column=test_case["pk_column"],
            )

tests/test_definitions/test_expectations_v3_api.py:418:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
great_expectations/self_check/util.py:2901: in evaluate_json_test_v3_api
    check_json_test_result(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

test = {'exact_match_out': False, 'in': {'column': 'y', 'value_set': [1.11, 2.22, 5.51]}, 'input': {'column': 'y', 'value_set..., 2.22, 5.51]}, 'out': {'success': False, 'unexpected_index_list': [0, 1, 2], 'unexpected_list': [1.1, 2.2, 5.5]}, ...}
result = {'exception_info': {'exception_message': None, 'exception_traceback': None, 'raised_exception': False}, 'expectation_c...al_unexpected_counts': [{'count': 1, 'value': 1.1}, {'count': 1, 'value': 2.2}, {'count': 1, 'value': 5.5}], ...}, ...}
data_asset = <great_expectations.execution_engine.pandas_batch_data.PandasBatchData object at 0x7f78062a3340>
pk_column = True

    def check_json_test_result(  # noqa: C901 - 52
        test, result, data_asset=None, pk_column=False
    ) -> None:

        # check for id_pk results in cases where pk_column is true and unexpected_index_list already exists
        # this will work for testing since result_format is COMPLETE
        if pk_column:
            if not result["success"]:
                if "unexpected_index_list" in result["result"]:
                    assert "unexpected_index_query" in result["result"]

        if "unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )
            elif "unexpected_list" in test["output"]:
                (
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )

        if "partial_unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "partial_unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )
            elif "partial_unexpected_list" in test["output"]:
                (
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )

        # Determine if np.allclose(..) might be needed for float comparison
        try_allclose = False
        if "observed_value" in test["output"]:
            if RX_FLOAT.match(repr(test["output"]["observed_value"])):
                try_allclose = True

        # Check results
        if test["exact_match_out"] is True:
            if "result" in result and "observed_value" in result["result"]:
                if isinstance(result["result"]["observed_value"], (np.floating, float)):
                    assert np.allclose(
                        result["result"]["observed_value"],
                        expectationValidationResultSchema.load(test["output"])["result"][
                            "observed_value"
                        ],
                        rtol=RTOL,
                        atol=ATOL,
                    ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {expectationValidationResultSchema.load(test['output'])['result']['observed_value']}"
                else:
                    assert result == expectationValidationResultSchema.load(
                        test["output"]
                    ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
            else:
                assert result == expectationValidationResultSchema.load(
                    test["output"]
                ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
        else:
            # Convert result to json since our tests are reading from json so cannot easily contain richer types (e.g. NaN)
            # NOTE - 20191031 - JPC - we may eventually want to change these tests as we update our view on how
            # representations, serializations, and objects should interact and how much of that is shown to the user.
            result = result.to_json_dict()
            for key, value in test["output"].items():
                # Apply our great expectations-specific test logic

                if key == "success":
                    if isinstance(value, (np.floating, float)):
                        try:
                            assert np.allclose(
                                result["success"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['success']} not np.allclose to {value}"
                        except TypeError:
                            assert (
                                result["success"] == value
                            ), f"{result['success']} != {value}"
                    else:
                        assert result["success"] == value, f"{result['success']} != {value}"

                elif key == "observed_value":
                    if "tolerance" in test:
                        if isinstance(value, dict):
                            assert set(result["result"]["observed_value"].keys()) == set(
                                value.keys()
                            ), f"{set(result['result']['observed_value'].keys())} != {set(value.keys())}"
                            for k, v in value.items():
                                assert np.allclose(
                                    result["result"]["observed_value"][k],
                                    v,
                                    rtol=test["tolerance"],
                                )
                        else:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=test["tolerance"],
                            )
                    else:
                        if isinstance(value, dict) and "values" in value:
                            try:
                                assert np.allclose(
                                    result["result"]["observed_value"]["values"],
                                    value["values"],
                                    rtol=RTOL,
                                    atol=ATOL,
                                ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']['values']} not np.allclose to {value['values']}"
                            except TypeError as e:
                                print(e)
                                assert (
                                    result["result"]["observed_value"] == value
                                ), f"{result['result']['observed_value']} != {value}"
                        elif try_allclose:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {value}"
                        else:
                            assert (
                                result["result"]["observed_value"] == value
                            ), f"{result['result']['observed_value']} != {value}"

                # NOTE: This is a key used ONLY for testing cases where an expectation is legitimately allowed to return
                # any of multiple possible observed_values. expect_column_values_to_be_of_type is one such expectation.
                elif key == "observed_value_list":
                    assert result["result"]["observed_value"] in value

                elif key == "unexpected_index_list":
                    if isinstance(data_asset, (SqlAlchemyDataset, SparkDFDataset)):
                        pass
                    elif isinstance(data_asset, (SqlAlchemyBatchData, SparkDFBatchData)):
                        pass
                    else:
>                       assert (
                            result["result"]["unexpected_index_list"] == value
                        ), f"{result['result']['unexpected_index_list']} != {value}"
E                       AssertionError: [{'y': 1.1, 'pk_index': 0}, {'y': 2.2, 'pk_index': 1}, {'y': 5.5, 'pk_index': 2}] != [0, 1, 2]

great_expectations/self_check/util.py:3075: AssertionError
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 4190.11it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 393.00it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 561.56it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 304.04it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 435.16it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 330.80it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 524.43it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 484.99it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 482.55it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 479.73it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_in_set:test_conditional_expectation_fails] _

test_case = {'expectation_type': 'expect_column_values_to_be_in_set', 'pk_column': True, 'skip': False, 'test': {'exact_match_out'... True, 'column': 'z', 'condition_parser': 'pandas', 'row_condition': 'x == 1', ...}, 'only_for': ['pandas'], ...}, ...}

    @pytest.mark.order(index=0)
    @pytest.mark.integration
    @pytest.mark.slow  # 12.68s
    def test_case_runner_v3_api(test_case):
        if test_case["skip"]:
            pytest.skip()

        # Note: this should never be done in practice, but we are wiping expectations to reuse batches during testing.
        # test_case["batch"]._initialize_expectations()
        if "parse_strings_as_datetimes" in test_case["test"]["in"]:
            with pytest.deprecated_call():
                evaluate_json_test_v3_api(
                    validator=test_case["validator_with_data"],
                    expectation_type=test_case["expectation_type"],
                    test=test_case["test"],
                    pk_column=test_case["pk_column"],
                )
        else:
>           evaluate_json_test_v3_api(
                validator=test_case["validator_with_data"],
                expectation_type=test_case["expectation_type"],
                test=test_case["test"],
                pk_column=test_case["pk_column"],
            )

tests/test_definitions/test_expectations_v3_api.py:418:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
great_expectations/self_check/util.py:2901: in evaluate_json_test_v3_api
    check_json_test_result(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

test = {'exact_match_out': False, 'in': {'catch_exceptions': True, 'column': 'z', 'condition_parser': 'pandas', 'row_conditio...ions': True, 'column': 'z', 'condition_parser': 'pandas', 'row_condition': 'x == 1', ...}, 'only_for': ['pandas'], ...}
result = {'exception_info': {'exception_message': None, 'exception_traceback': None, 'raised_exception': False}, 'expectation_c..., 'missing_count': 0, 'missing_percent': 0.0, 'partial_unexpected_counts': [{'count': 1, 'value': 'hello'}], ...}, ...}
data_asset = <great_expectations.execution_engine.pandas_batch_data.PandasBatchData object at 0x7f78062a3340>
pk_column = True

    def check_json_test_result(  # noqa: C901 - 52
        test, result, data_asset=None, pk_column=False
    ) -> None:

        # check for id_pk results in cases where pk_column is true and unexpected_index_list already exists
        # this will work for testing since result_format is COMPLETE
        if pk_column:
            if not result["success"]:
                if "unexpected_index_list" in result["result"]:
                    assert "unexpected_index_query" in result["result"]

        if "unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )
            elif "unexpected_list" in test["output"]:
                (
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )

        if "partial_unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "partial_unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )
            elif "partial_unexpected_list" in test["output"]:
                (
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )

        # Determine if np.allclose(..) might be needed for float comparison
        try_allclose = False
        if "observed_value" in test["output"]:
            if RX_FLOAT.match(repr(test["output"]["observed_value"])):
                try_allclose = True

        # Check results
        if test["exact_match_out"] is True:
            if "result" in result and "observed_value" in result["result"]:
                if isinstance(result["result"]["observed_value"], (np.floating, float)):
                    assert np.allclose(
                        result["result"]["observed_value"],
                        expectationValidationResultSchema.load(test["output"])["result"][
                            "observed_value"
                        ],
                        rtol=RTOL,
                        atol=ATOL,
                    ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {expectationValidationResultSchema.load(test['output'])['result']['observed_value']}"
                else:
                    assert result == expectationValidationResultSchema.load(
                        test["output"]
                    ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
            else:
                assert result == expectationValidationResultSchema.load(
                    test["output"]
                ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
        else:
            # Convert result to json since our tests are reading from json so cannot easily contain richer types (e.g. NaN)
            # NOTE - 20191031 - JPC - we may eventually want to change these tests as we update our view on how
            # representations, serializations, and objects should interact and how much of that is shown to the user.
            result = result.to_json_dict()
            for key, value in test["output"].items():
                # Apply our great expectations-specific test logic

                if key == "success":
                    if isinstance(value, (np.floating, float)):
                        try:
                            assert np.allclose(
                                result["success"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['success']} not np.allclose to {value}"
                        except TypeError:
                            assert (
                                result["success"] == value
                            ), f"{result['success']} != {value}"
                    else:
                        assert result["success"] == value, f"{result['success']} != {value}"

                elif key == "observed_value":
                    if "tolerance" in test:
                        if isinstance(value, dict):
                            assert set(result["result"]["observed_value"].keys()) == set(
                                value.keys()
                            ), f"{set(result['result']['observed_value'].keys())} != {set(value.keys())}"
                            for k, v in value.items():
                                assert np.allclose(
                                    result["result"]["observed_value"][k],
                                    v,
                                    rtol=test["tolerance"],
                                )
                        else:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=test["tolerance"],
                            )
                    else:
                        if isinstance(value, dict) and "values" in value:
                            try:
                                assert np.allclose(
                                    result["result"]["observed_value"]["values"],
                                    value["values"],
                                    rtol=RTOL,
                                    atol=ATOL,
                                ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']['values']} not np.allclose to {value['values']}"
                            except TypeError as e:
                                print(e)
                                assert (
                                    result["result"]["observed_value"] == value
                                ), f"{result['result']['observed_value']} != {value}"
                        elif try_allclose:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {value}"
                        else:
                            assert (
                                result["result"]["observed_value"] == value
                            ), f"{result['result']['observed_value']} != {value}"

                # NOTE: This is a key used ONLY for testing cases where an expectation is legitimately allowed to return
                # any of multiple possible observed_values. expect_column_values_to_be_of_type is one such expectation.
                elif key == "observed_value_list":
                    assert result["result"]["observed_value"] in value

                elif key == "unexpected_index_list":
                    if isinstance(data_asset, (SqlAlchemyDataset, SparkDFDataset)):
                        pass
                    elif isinstance(data_asset, (SqlAlchemyBatchData, SparkDFBatchData)):
                        pass
                    else:
>                       assert (
                            result["result"]["unexpected_index_list"] == value
                        ), f"{result['result']['unexpected_index_list']} != {value}"
E                       AssertionError: [{'z': 'hello', 'pk_index': 0}] != [0]

great_expectations/self_check/util.py:3075: AssertionError
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 893.64it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 382.92it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 552.39it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 363.64it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 448.35it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 364.99it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 506.43it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 468.26it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 465.81it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 463.52it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_in_set:basic_negative_test_case_datetime_set] _

test_case = {'expectation_type': 'expect_column_values_to_be_in_set', 'pk_column': True, 'skip': False, 'test': {'exact_match_out'...:01']}, 'out': {'success': False, 'unexpected_index_list': [0], 'unexpected_list': ['2018-01-01T00:00:00']}, ...}, ...}

    @pytest.mark.order(index=0)
    @pytest.mark.integration
    @pytest.mark.slow  # 12.68s
    def test_case_runner_v3_api(test_case):
        if test_case["skip"]:
            pytest.skip()

        # Note: this should never be done in practice, but we are wiping expectations to reuse batches during testing.
        # test_case["batch"]._initialize_expectations()
        if "parse_strings_as_datetimes" in test_case["test"]["in"]:
            with pytest.deprecated_call():
>               evaluate_json_test_v3_api(
                    validator=test_case["validator_with_data"],
                    expectation_type=test_case["expectation_type"],
                    test=test_case["test"],
                    pk_column=test_case["pk_column"],
                )

tests/test_definitions/test_expectations_v3_api.py:411:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
great_expectations/self_check/util.py:2901: in evaluate_json_test_v3_api
    check_json_test_result(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

test = {'exact_match_out': False, 'in': {'column': 'dates', 'parse_strings_as_datetimes': True, 'value_set': ['2018-01-02', '... 00:34:01']}, 'out': {'success': False, 'unexpected_index_list': [0], 'unexpected_list': ['2018-01-01T00:00:00']}, ...}
result = {'exception_info': {'exception_message': None, 'exception_traceback': None, 'raised_exception': False}, 'expectation_c...nt': 0, 'missing_percent': 0.0, 'partial_unexpected_counts': [{'count': 1, 'value': '2018-01-01T00:00:00'}], ...}, ...}
data_asset = <great_expectations.execution_engine.pandas_batch_data.PandasBatchData object at 0x7f78063d7700>
pk_column = True

    def check_json_test_result(  # noqa: C901 - 52
        test, result, data_asset=None, pk_column=False
    ) -> None:

        # check for id_pk results in cases where pk_column is true and unexpected_index_list already exists
        # this will work for testing since result_format is COMPLETE
        if pk_column:
            if not result["success"]:
                if "unexpected_index_list" in result["result"]:
                    assert "unexpected_index_query" in result["result"]

        if "unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )
            elif "unexpected_list" in test["output"]:
                (
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )

        if "partial_unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "partial_unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )
            elif "partial_unexpected_list" in test["output"]:
                (
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )

        # Determine if np.allclose(..) might be needed for float comparison
        try_allclose = False
        if "observed_value" in test["output"]:
            if RX_FLOAT.match(repr(test["output"]["observed_value"])):
                try_allclose = True

        # Check results
        if test["exact_match_out"] is True:
            if "result" in result and "observed_value" in result["result"]:
                if isinstance(result["result"]["observed_value"], (np.floating, float)):
                    assert np.allclose(
                        result["result"]["observed_value"],
                        expectationValidationResultSchema.load(test["output"])["result"][
                            "observed_value"
                        ],
                        rtol=RTOL,
                        atol=ATOL,
                    ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {expectationValidationResultSchema.load(test['output'])['result']['observed_value']}"
                else:
                    assert result == expectationValidationResultSchema.load(
                        test["output"]
                    ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
            else:
                assert result == expectationValidationResultSchema.load(
                    test["output"]
                ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
        else:
            # Convert result to json since our tests are reading from json so cannot easily contain richer types (e.g. NaN)
            # NOTE - 20191031 - JPC - we may eventually want to change these tests as we update our view on how
            # representations, serializations, and objects should interact and how much of that is shown to the user.
            result = result.to_json_dict()
            for key, value in test["output"].items():
                # Apply our great expectations-specific test logic

                if key == "success":
                    if isinstance(value, (np.floating, float)):
                        try:
                            assert np.allclose(
                                result["success"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['success']} not np.allclose to {value}"
                        except TypeError:
                            assert (
                                result["success"] == value
                            ), f"{result['success']} != {value}"
                    else:
                        assert result["success"] == value, f"{result['success']} != {value}"

                elif key == "observed_value":
                    if "tolerance" in test:
                        if isinstance(value, dict):
                            assert set(result["result"]["observed_value"].keys()) == set(
                                value.keys()
                            ), f"{set(result['result']['observed_value'].keys())} != {set(value.keys())}"
                            for k, v in value.items():
                                assert np.allclose(
                                    result["result"]["observed_value"][k],
                                    v,
                                    rtol=test["tolerance"],
                                )
                        else:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=test["tolerance"],
                            )
                    else:
                        if isinstance(value, dict) and "values" in value:
                            try:
                                assert np.allclose(
                                    result["result"]["observed_value"]["values"],
                                    value["values"],
                                    rtol=RTOL,
                                    atol=ATOL,
                                ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']['values']} not np.allclose to {value['values']}"
                            except TypeError as e:
                                print(e)
                                assert (
                                    result["result"]["observed_value"] == value
                                ), f"{result['result']['observed_value']} != {value}"
                        elif try_allclose:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {value}"
                        else:
                            assert (
                                result["result"]["observed_value"] == value
                            ), f"{result['result']['observed_value']} != {value}"

                # NOTE: This is a key used ONLY for testing cases where an expectation is legitimately allowed to return
                # any of multiple possible observed_values. expect_column_values_to_be_of_type is one such expectation.
                elif key == "observed_value_list":
                    assert result["result"]["observed_value"] in value

                elif key == "unexpected_index_list":
                    if isinstance(data_asset, (SqlAlchemyDataset, SparkDFDataset)):
                        pass
                    elif isinstance(data_asset, (SqlAlchemyBatchData, SparkDFBatchData)):
                        pass
                    else:
>                       assert (
                            result["result"]["unexpected_index_list"] == value
                        ), f"{result['result']['unexpected_index_list']} != {value}"
E                       AssertionError: [{'dates': '2018-01-01T00:00:00', 'pk_index': 0}] != [0]

great_expectations/self_check/util.py:3075: AssertionError
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 4106.02it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 408.09it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 577.28it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 325.45it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 448.59it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 346.07it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 524.90it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 486.76it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 484.45it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 482.07it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_decreasing:positive_test_with_mostly] _

test_case = {'expectation_type': 'expect_column_values_to_be_decreasing', 'pk_column': True, 'skip': False, 'test': {'exact_match_..., 'in': {'column': 'w', 'mostly': 0.6}, 'include_in_gallery': True, 'input': {'column': 'w', 'mostly': 0.6}, ...}, ...}

    @pytest.mark.order(index=0)
    @pytest.mark.integration
    @pytest.mark.slow  # 12.68s
    def test_case_runner_v3_api(test_case):
        if test_case["skip"]:
            pytest.skip()

        # Note: this should never be done in practice, but we are wiping expectations to reuse batches during testing.
        # test_case["batch"]._initialize_expectations()
        if "parse_strings_as_datetimes" in test_case["test"]["in"]:
            with pytest.deprecated_call():
                evaluate_json_test_v3_api(
                    validator=test_case["validator_with_data"],
                    expectation_type=test_case["expectation_type"],
                    test=test_case["test"],
                    pk_column=test_case["pk_column"],
                )
        else:
>           evaluate_json_test_v3_api(
                validator=test_case["validator_with_data"],
                expectation_type=test_case["expectation_type"],
                test=test_case["test"],
                pk_column=test_case["pk_column"],
            )

tests/test_definitions/test_expectations_v3_api.py:418:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
great_expectations/self_check/util.py:2901: in evaluate_json_test_v3_api
    check_json_test_result(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

test = {'exact_match_out': False, 'in': {'column': 'w', 'mostly': 0.6}, 'include_in_gallery': True, 'input': {'column': 'w', 'mostly': 0.6}, ...}
result = {'exception_info': {'exception_message': None, 'exception_traceback': None, 'raised_exception': False}, 'expectation_c...'partial_unexpected_counts': [{'count': 1, 'value': 2}, {'count': 1, 'value': 3}, {'count': 1, 'value': 7}], ...}, ...}
data_asset = <great_expectations.execution_engine.pandas_batch_data.PandasBatchData object at 0x7f78062d8460>
pk_column = True

    def check_json_test_result(  # noqa: C901 - 52
        test, result, data_asset=None, pk_column=False
    ) -> None:

        # check for id_pk results in cases where pk_column is true and unexpected_index_list already exists
        # this will work for testing since result_format is COMPLETE
        if pk_column:
            if not result["success"]:
                if "unexpected_index_list" in result["result"]:
                    assert "unexpected_index_query" in result["result"]

        if "unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )
            elif "unexpected_list" in test["output"]:
                (
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )

        if "partial_unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "partial_unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )
            elif "partial_unexpected_list" in test["output"]:
                (
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )

        # Determine if np.allclose(..) might be needed for float comparison
        try_allclose = False
        if "observed_value" in test["output"]:
            if RX_FLOAT.match(repr(test["output"]["observed_value"])):
                try_allclose = True

        # Check results
        if test["exact_match_out"] is True:
            if "result" in result and "observed_value" in result["result"]:
                if isinstance(result["result"]["observed_value"], (np.floating, float)):
                    assert np.allclose(
                        result["result"]["observed_value"],
                        expectationValidationResultSchema.load(test["output"])["result"][
                            "observed_value"
                        ],
                        rtol=RTOL,
                        atol=ATOL,
                    ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {expectationValidationResultSchema.load(test['output'])['result']['observed_value']}"
                else:
                    assert result == expectationValidationResultSchema.load(
                        test["output"]
                    ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
            else:
                assert result == expectationValidationResultSchema.load(
                    test["output"]
                ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
        else:
            # Convert result to json since our tests are reading from json so cannot easily contain richer types (e.g. NaN)
            # NOTE - 20191031 - JPC - we may eventually want to change these tests as we update our view on how
            # representations, serializations, and objects should interact and how much of that is shown to the user.
            result = result.to_json_dict()
            for key, value in test["output"].items():
                # Apply our great expectations-specific test logic

                if key == "success":
                    if isinstance(value, (np.floating, float)):
                        try:
                            assert np.allclose(
                                result["success"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['success']} not np.allclose to {value}"
                        except TypeError:
                            assert (
                                result["success"] == value
                            ), f"{result['success']} != {value}"
                    else:
                        assert result["success"] == value, f"{result['success']} != {value}"

                elif key == "observed_value":
                    if "tolerance" in test:
                        if isinstance(value, dict):
                            assert set(result["result"]["observed_value"].keys()) == set(
                                value.keys()
                            ), f"{set(result['result']['observed_value'].keys())} != {set(value.keys())}"
                            for k, v in value.items():
                                assert np.allclose(
                                    result["result"]["observed_value"][k],
                                    v,
                                    rtol=test["tolerance"],
                                )
                        else:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=test["tolerance"],
                            )
                    else:
                        if isinstance(value, dict) and "values" in value:
                            try:
                                assert np.allclose(
                                    result["result"]["observed_value"]["values"],
                                    value["values"],
                                    rtol=RTOL,
                                    atol=ATOL,
                                ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']['values']} not np.allclose to {value['values']}"
                            except TypeError as e:
                                print(e)
                                assert (
                                    result["result"]["observed_value"] == value
                                ), f"{result['result']['observed_value']} != {value}"
                        elif try_allclose:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {value}"
                        else:
                            assert (
                                result["result"]["observed_value"] == value
                            ), f"{result['result']['observed_value']} != {value}"

                # NOTE: This is a key used ONLY for testing cases where an expectation is legitimately allowed to return
                # any of multiple possible observed_values. expect_column_values_to_be_of_type is one such expectation.
                elif key == "observed_value_list":
                    assert result["result"]["observed_value"] in value

                elif key == "unexpected_index_list":
                    if isinstance(data_asset, (SqlAlchemyDataset, SparkDFDataset)):
                        pass
                    elif isinstance(data_asset, (SqlAlchemyBatchData, SparkDFBatchData)):
                        pass
                    else:
>                       assert (
                            result["result"]["unexpected_index_list"] == value
                        ), f"{result['result']['unexpected_index_list']} != {value}"
E                       AssertionError: [{'w': 2, 'pk_index': 1}, {'w': 3, 'pk_index': 2}, {'w': 7, 'pk_index': 3}] != [1, 2, 3]

great_expectations/self_check/util.py:3075: AssertionError
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 3907.13it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 415.63it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 588.70it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 313.48it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 337.22it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 275.56it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 455.63it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 426.13it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 422.99it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 421.04it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_decreasing:basic_negative_test] _

test_case = {'expectation_type': 'expect_column_values_to_be_decreasing', 'pk_column': True, 'skip': False, 'test': {'exact_match_out': False, 'in': {'column': 'y'}, 'include_in_gallery': True, 'input': {'column': 'y'}, ...}, ...}

    @pytest.mark.order(index=0)
    @pytest.mark.integration
    @pytest.mark.slow  # 12.68s
    def test_case_runner_v3_api(test_case):
        if test_case["skip"]:
            pytest.skip()

        # Note: this should never be done in practice, but we are wiping expectations to reuse batches during testing.
        # test_case["batch"]._initialize_expectations()
        if "parse_strings_as_datetimes" in test_case["test"]["in"]:
            with pytest.deprecated_call():
                evaluate_json_test_v3_api(
                    validator=test_case["validator_with_data"],
                    expectation_type=test_case["expectation_type"],
                    test=test_case["test"],
                    pk_column=test_case["pk_column"],
                )
        else:
>           evaluate_json_test_v3_api(
                validator=test_case["validator_with_data"],
                expectation_type=test_case["expectation_type"],
                test=test_case["test"],
                pk_column=test_case["pk_column"],
            )

tests/test_definitions/test_expectations_v3_api.py:418:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
great_expectations/self_check/util.py:2901: in evaluate_json_test_v3_api
    check_json_test_result(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

test = {'exact_match_out': False, 'in': {'column': 'y'}, 'include_in_gallery': True, 'input': {'column': 'y'}, ...}
result = {'exception_info': {'exception_message': None, 'exception_traceback': None, 'raised_exception': False}, 'expectation_c...'partial_unexpected_counts': [{'count': 1, 'value': 2}, {'count': 1, 'value': 3}, {'count': 1, 'value': 4}], ...}, ...}
data_asset = <great_expectations.execution_engine.pandas_batch_data.PandasBatchData object at 0x7f78062d8460>
pk_column = True

    def check_json_test_result(  # noqa: C901 - 52
        test, result, data_asset=None, pk_column=False
    ) -> None:

        # check for id_pk results in cases where pk_column is true and unexpected_index_list already exists
        # this will work for testing since result_format is COMPLETE
        if pk_column:
            if not result["success"]:
                if "unexpected_index_list" in result["result"]:
                    assert "unexpected_index_query" in result["result"]

        if "unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )
            elif "unexpected_list" in test["output"]:
                (
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )

        if "partial_unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "partial_unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )
            elif "partial_unexpected_list" in test["output"]:
                (
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )

        # Determine if np.allclose(..) might be needed for float comparison
        try_allclose = False
        if "observed_value" in test["output"]:
            if RX_FLOAT.match(repr(test["output"]["observed_value"])):
                try_allclose = True

        # Check results
        if test["exact_match_out"] is True:
            if "result" in result and "observed_value" in result["result"]:
                if isinstance(result["result"]["observed_value"], (np.floating, float)):
                    assert np.allclose(
                        result["result"]["observed_value"],
                        expectationValidationResultSchema.load(test["output"])["result"][
                            "observed_value"
                        ],
                        rtol=RTOL,
                        atol=ATOL,
                    ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {expectationValidationResultSchema.load(test['output'])['result']['observed_value']}"
                else:
                    assert result == expectationValidationResultSchema.load(
                        test["output"]
                    ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
            else:
                assert result == expectationValidationResultSchema.load(
                    test["output"]
                ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
        else:
            # Convert result to json since our tests are reading from json so cannot easily contain richer types (e.g. NaN)
            # NOTE - 20191031 - JPC - we may eventually want to change these tests as we update our view on how
            # representations, serializations, and objects should interact and how much of that is shown to the user.
            result = result.to_json_dict()
            for key, value in test["output"].items():
                # Apply our great expectations-specific test logic

                if key == "success":
                    if isinstance(value, (np.floating, float)):
                        try:
                            assert np.allclose(
                                result["success"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['success']} not np.allclose to {value}"
                        except TypeError:
                            assert (
                                result["success"] == value
                            ), f"{result['success']} != {value}"
                    else:
                        assert result["success"] == value, f"{result['success']} != {value}"

                elif key == "observed_value":
                    if "tolerance" in test:
                        if isinstance(value, dict):
                            assert set(result["result"]["observed_value"].keys()) == set(
                                value.keys()
                            ), f"{set(result['result']['observed_value'].keys())} != {set(value.keys())}"
                            for k, v in value.items():
                                assert np.allclose(
                                    result["result"]["observed_value"][k],
                                    v,
                                    rtol=test["tolerance"],
                                )
                        else:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=test["tolerance"],
                            )
                    else:
                        if isinstance(value, dict) and "values" in value:
                            try:
                                assert np.allclose(
                                    result["result"]["observed_value"]["values"],
                                    value["values"],
                                    rtol=RTOL,
                                    atol=ATOL,
                                ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']['values']} not np.allclose to {value['values']}"
                            except TypeError as e:
                                print(e)
                                assert (
                                    result["result"]["observed_value"] == value
                                ), f"{result['result']['observed_value']} != {value}"
                        elif try_allclose:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {value}"
                        else:
                            assert (
                                result["result"]["observed_value"] == value
                            ), f"{result['result']['observed_value']} != {value}"

                # NOTE: This is a key used ONLY for testing cases where an expectation is legitimately allowed to return
                # any of multiple possible observed_values. expect_column_values_to_be_of_type is one such expectation.
                elif key == "observed_value_list":
                    assert result["result"]["observed_value"] in value

                elif key == "unexpected_index_list":
                    if isinstance(data_asset, (SqlAlchemyDataset, SparkDFDataset)):
                        pass
                    elif isinstance(data_asset, (SqlAlchemyBatchData, SparkDFBatchData)):
                        pass
                    else:
>                       assert (
                            result["result"]["unexpected_index_list"] == value
                        ), f"{result['result']['unexpected_index_list']} != {value}"
E                       AssertionError: [{'y': 2, 'pk_index': 3}, {'y': 3, 'pk_index': 6}, {'y': 4, 'pk_index': 9}] != [3, 6, 9]

great_expectations/self_check/util.py:3075: AssertionError
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 4130.28it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 413.39it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 586.04it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 329.89it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 433.02it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 335.05it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 528.04it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 491.98it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 489.89it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 487.52it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_decreasing:basic_negative_test_with_strictly] _

test_case = {'expectation_type': 'expect_column_values_to_be_decreasing', 'pk_column': True, 'skip': False, 'test': {'exact_match_...cess': False, 'unexpected_index_list': [1, 2, 3, 4, 5, 6, ...], 'unexpected_list': [1, 1, 2, 2, 2, 3, ...]}, ...}, ...}

    @pytest.mark.order(index=0)
    @pytest.mark.integration
    @pytest.mark.slow  # 12.68s
    def test_case_runner_v3_api(test_case):
        if test_case["skip"]:
            pytest.skip()

        # Note: this should never be done in practice, but we are wiping expectations to reuse batches during testing.
        # test_case["batch"]._initialize_expectations()
        if "parse_strings_as_datetimes" in test_case["test"]["in"]:
            with pytest.deprecated_call():
                evaluate_json_test_v3_api(
                    validator=test_case["validator_with_data"],
                    expectation_type=test_case["expectation_type"],
                    test=test_case["test"],
                    pk_column=test_case["pk_column"],
                )
        else:
>           evaluate_json_test_v3_api(
                validator=test_case["validator_with_data"],
                expectation_type=test_case["expectation_type"],
                test=test_case["test"],
                pk_column=test_case["pk_column"],
            )

tests/test_definitions/test_expectations_v3_api.py:418:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
great_expectations/self_check/util.py:2901: in evaluate_json_test_v3_api
    check_json_test_result(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

test = {'exact_match_out': False, 'in': {'column': 'y', 'strictly': True}, 'input': {'column': 'y', 'strictly': True}, 'out': {'success': False, 'unexpected_index_list': [1, 2, 3, 4, 5, 6, ...], 'unexpected_list': [1, 1, 2, 2, 2, 3, ...]}, ...}
result = {'exception_info': {'exception_message': None, 'exception_traceback': None, 'raised_exception': False}, 'expectation_c...': [{'count': 3, 'value': 2}, {'count': 3, 'value': 3}, {'count': 2, 'value': 1}, {'count': 1, 'value': 4}], ...}, ...}
data_asset = <great_expectations.execution_engine.pandas_batch_data.PandasBatchData object at 0x7f78062d8460>
pk_column = True

    def check_json_test_result(  # noqa: C901 - 52
        test, result, data_asset=None, pk_column=False
    ) -> None:

        # check for id_pk results in cases where pk_column is true and unexpected_index_list already exists
        # this will work for testing since result_format is COMPLETE
        if pk_column:
            if not result["success"]:
                if "unexpected_index_list" in result["result"]:
                    assert "unexpected_index_query" in result["result"]

        if "unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )
            elif "unexpected_list" in test["output"]:
                (
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )

        if "partial_unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "partial_unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )
            elif "partial_unexpected_list" in test["output"]:
                (
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )

        # Determine if np.allclose(..) might be needed for float comparison
        try_allclose = False
        if "observed_value" in test["output"]:
            if RX_FLOAT.match(repr(test["output"]["observed_value"])):
                try_allclose = True

        # Check results
        if test["exact_match_out"] is True:
            if "result" in result and "observed_value" in result["result"]:
                if isinstance(result["result"]["observed_value"], (np.floating, float)):
                    assert np.allclose(
                        result["result"]["observed_value"],
                        expectationValidationResultSchema.load(test["output"])["result"][
                            "observed_value"
                        ],
                        rtol=RTOL,
                        atol=ATOL,
                    ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {expectationValidationResultSchema.load(test['output'])['result']['observed_value']}"
                else:
                    assert result == expectationValidationResultSchema.load(
                        test["output"]
                    ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
            else:
                assert result == expectationValidationResultSchema.load(
                    test["output"]
                ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
        else:
            # Convert result to json since our tests are reading from json so cannot easily contain richer types (e.g. NaN)
            # NOTE - 20191031 - JPC - we may eventually want to change these tests as we update our view on how
            # representations, serializations, and objects should interact and how much of that is shown to the user.
            result = result.to_json_dict()
            for key, value in test["output"].items():
                # Apply our great expectations-specific test logic

                if key == "success":
                    if isinstance(value, (np.floating, float)):
                        try:
                            assert np.allclose(
                                result["success"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['success']} not np.allclose to {value}"
                        except TypeError:
                            assert (
                                result["success"] == value
                            ), f"{result['success']} != {value}"
                    else:
                        assert result["success"] == value, f"{result['success']} != {value}"

                elif key == "observed_value":
                    if "tolerance" in test:
                        if isinstance(value, dict):
                            assert set(result["result"]["observed_value"].keys()) == set(
                                value.keys()
                            ), f"{set(result['result']['observed_value'].keys())} != {set(value.keys())}"
                            for k, v in value.items():
                                assert np.allclose(
                                    result["result"]["observed_value"][k],
                                    v,
                                    rtol=test["tolerance"],
                                )
                        else:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=test["tolerance"],
                            )
                    else:
                        if isinstance(value, dict) and "values" in value:
                            try:
                                assert np.allclose(
                                    result["result"]["observed_value"]["values"],
                                    value["values"],
                                    rtol=RTOL,
                                    atol=ATOL,
                                ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']['values']} not np.allclose to {value['values']}"
                            except TypeError as e:
                                print(e)
                                assert (
                                    result["result"]["observed_value"] == value
                                ), f"{result['result']['observed_value']} != {value}"
                        elif try_allclose:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {value}"
                        else:
                            assert (
                                result["result"]["observed_value"] == value
                            ), f"{result['result']['observed_value']} != {value}"

                # NOTE: This is a key used ONLY for testing cases where an expectation is legitimately allowed to return
                # any of multiple possible observed_values. expect_column_values_to_be_of_type is one such expectation.
                elif key == "observed_value_list":
                    assert result["result"]["observed_value"] in value

                elif key == "unexpected_index_list":
                    if isinstance(data_asset, (SqlAlchemyDataset, SparkDFDataset)):
                        pass
                    elif isinstance(data_asset, (SqlAlchemyBatchData, SparkDFBatchData)):
                        pass
                    else:
>                       assert (
                            result["result"]["unexpected_index_list"] == value
                        ), f"{result['result']['unexpected_index_list']} != {value}"
E                       AssertionError: [{'y': 1, 'pk_index': 1}, {'y': 1, 'pk_index': 2}, {'y': 2, 'pk_index': 3}, {'y': 2, 'pk_index': 4}, {'y': 2, 'pk_index': 5}, {'y': 3, 'pk_index': 6}, {'y': 3, 'pk_index': 7}, {'y': 3, 'pk_index': 8}, {'y': 4, 'pk_index': 9}] != [1, 2, 3, 4, 5, 6, 7, 8, 9]

great_expectations/self_check/util.py:3075: AssertionError
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 4000.29it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 426.99it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 607.28it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 333.78it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 433.62it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 332.93it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 514.22it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 475.33it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 473.06it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 470.76it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_not_match_regex:negative_test_insufficient_mostly_and_one_non_matching_value] _

test_case = {'expectation_type': 'expect_column_values_to_not_match_regex', 'pk_column': True, 'skip': False, 'test': {'exact_matc...y': 0.3, 'regex': '^a'}, 'include_in_gallery': True, 'input': {'column': 'a', 'mostly': 0.3, 'regex': '^a'}, ...}, ...}

    @pytest.mark.order(index=0)
    @pytest.mark.integration
    @pytest.mark.slow  # 12.68s
    def test_case_runner_v3_api(test_case):
        if test_case["skip"]:
            pytest.skip()

        # Note: this should never be done in practice, but we are wiping expectations to reuse batches during testing.
        # test_case["batch"]._initialize_expectations()
        if "parse_strings_as_datetimes" in test_case["test"]["in"]:
            with pytest.deprecated_call():
                evaluate_json_test_v3_api(
                    validator=test_case["validator_with_data"],
                    expectation_type=test_case["expectation_type"],
                    test=test_case["test"],
                    pk_column=test_case["pk_column"],
                )
        else:
>           evaluate_json_test_v3_api(
                validator=test_case["validator_with_data"],
                expectation_type=test_case["expectation_type"],
                test=test_case["test"],
                pk_column=test_case["pk_column"],
            )

tests/test_definitions/test_expectations_v3_api.py:418:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
great_expectations/self_check/util.py:2901: in evaluate_json_test_v3_api
    check_json_test_result(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

test = {'exact_match_out': False, 'in': {'column': 'a', 'mostly': 0.3, 'regex': '^a'}, 'include_in_gallery': True, 'input': {'column': 'a', 'mostly': 0.3, 'regex': '^a'}, ...}
result = {'exception_info': {'exception_message': None, 'exception_traceback': None, 'raised_exception': False}, 'expectation_c... 'value': 'aaa'}, {'count': 1, 'value': 'abb'}, {'count': 1, 'value': 'acc'}, {'count': 1, 'value': 'add'}], ...}, ...}
data_asset = <great_expectations.execution_engine.pandas_batch_data.PandasBatchData object at 0x7f780634c9a0>
pk_column = True

    def check_json_test_result(  # noqa: C901 - 52
        test, result, data_asset=None, pk_column=False
    ) -> None:

        # check for id_pk results in cases where pk_column is true and unexpected_index_list already exists
        # this will work for testing since result_format is COMPLETE
        if pk_column:
            if not result["success"]:
                if "unexpected_index_list" in result["result"]:
                    assert "unexpected_index_query" in result["result"]

        if "unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )
            elif "unexpected_list" in test["output"]:
                (
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )

        if "partial_unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "partial_unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )
            elif "partial_unexpected_list" in test["output"]:
                (
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )

        # Determine if np.allclose(..) might be needed for float comparison
        try_allclose = False
        if "observed_value" in test["output"]:
            if RX_FLOAT.match(repr(test["output"]["observed_value"])):
                try_allclose = True

        # Check results
        if test["exact_match_out"] is True:
            if "result" in result and "observed_value" in result["result"]:
                if isinstance(result["result"]["observed_value"], (np.floating, float)):
                    assert np.allclose(
                        result["result"]["observed_value"],
                        expectationValidationResultSchema.load(test["output"])["result"][
                            "observed_value"
                        ],
                        rtol=RTOL,
                        atol=ATOL,
                    ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {expectationValidationResultSchema.load(test['output'])['result']['observed_value']}"
                else:
                    assert result == expectationValidationResultSchema.load(
                        test["output"]
                    ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
            else:
                assert result == expectationValidationResultSchema.load(
                    test["output"]
                ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
        else:
            # Convert result to json since our tests are reading from json so cannot easily contain richer types (e.g. NaN)
            # NOTE - 20191031 - JPC - we may eventually want to change these tests as we update our view on how
            # representations, serializations, and objects should interact and how much of that is shown to the user.
            result = result.to_json_dict()
            for key, value in test["output"].items():
                # Apply our great expectations-specific test logic

                if key == "success":
                    if isinstance(value, (np.floating, float)):
                        try:
                            assert np.allclose(
                                result["success"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['success']} not np.allclose to {value}"
                        except TypeError:
                            assert (
                                result["success"] == value
                            ), f"{result['success']} != {value}"
                    else:
                        assert result["success"] == value, f"{result['success']} != {value}"

                elif key == "observed_value":
                    if "tolerance" in test:
                        if isinstance(value, dict):
                            assert set(result["result"]["observed_value"].keys()) == set(
                                value.keys()
                            ), f"{set(result['result']['observed_value'].keys())} != {set(value.keys())}"
                            for k, v in value.items():
                                assert np.allclose(
                                    result["result"]["observed_value"][k],
                                    v,
                                    rtol=test["tolerance"],
                                )
                        else:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=test["tolerance"],
                            )
                    else:
                        if isinstance(value, dict) and "values" in value:
                            try:
                                assert np.allclose(
                                    result["result"]["observed_value"]["values"],
                                    value["values"],
                                    rtol=RTOL,
                                    atol=ATOL,
                                ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']['values']} not np.allclose to {value['values']}"
                            except TypeError as e:
                                print(e)
                                assert (
                                    result["result"]["observed_value"] == value
                                ), f"{result['result']['observed_value']} != {value}"
                        elif try_allclose:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {value}"
                        else:
                            assert (
                                result["result"]["observed_value"] == value
                            ), f"{result['result']['observed_value']} != {value}"

                # NOTE: This is a key used ONLY for testing cases where an expectation is legitimately allowed to return
                # any of multiple possible observed_values. expect_column_values_to_be_of_type is one such expectation.
                elif key == "observed_value_list":
                    assert result["result"]["observed_value"] in value

                elif key == "unexpected_index_list":
                    if isinstance(data_asset, (SqlAlchemyDataset, SparkDFDataset)):
                        pass
                    elif isinstance(data_asset, (SqlAlchemyBatchData, SparkDFBatchData)):
                        pass
                    else:
>                       assert (
                            result["result"]["unexpected_index_list"] == value
                        ), f"{result['result']['unexpected_index_list']} != {value}"
E                       AssertionError: [{'a': 'aaa', 'pk_index': 0}, {'a': 'abb', 'pk_index': 1}, {'a': 'acc', 'pk_index': 2}, {'a': 'add', 'pk_index': 3}] != [0, 1, 2, 3]

great_expectations/self_check/util.py:3075: AssertionError
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 3604.90it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 388.65it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 547.46it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 324.36it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 443.03it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 345.80it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 546.80it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 510.10it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 507.90it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 505.36it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_not_match_regex:positive_test_exact_mostly_w_one_non_matching_value] _

test_case = {'expectation_type': 'expect_column_values_to_not_match_regex', 'pk_column': True, 'skip': False, 'test': {'exact_matc...y': 0.2, 'regex': '^a'}, 'include_in_gallery': True, 'input': {'column': 'a', 'mostly': 0.2, 'regex': '^a'}, ...}, ...}

    @pytest.mark.order(index=0)
    @pytest.mark.integration
    @pytest.mark.slow  # 12.68s
    def test_case_runner_v3_api(test_case):
        if test_case["skip"]:
            pytest.skip()

        # Note: this should never be done in practice, but we are wiping expectations to reuse batches during testing.
        # test_case["batch"]._initialize_expectations()
        if "parse_strings_as_datetimes" in test_case["test"]["in"]:
            with pytest.deprecated_call():
                evaluate_json_test_v3_api(
                    validator=test_case["validator_with_data"],
                    expectation_type=test_case["expectation_type"],
                    test=test_case["test"],
                    pk_column=test_case["pk_column"],
                )
        else:
>           evaluate_json_test_v3_api(
                validator=test_case["validator_with_data"],
                expectation_type=test_case["expectation_type"],
                test=test_case["test"],
                pk_column=test_case["pk_column"],
            )

tests/test_definitions/test_expectations_v3_api.py:418:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
great_expectations/self_check/util.py:2901: in evaluate_json_test_v3_api
    check_json_test_result(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

test = {'exact_match_out': False, 'in': {'column': 'a', 'mostly': 0.2, 'regex': '^a'}, 'include_in_gallery': True, 'input': {'column': 'a', 'mostly': 0.2, 'regex': '^a'}, ...}
result = {'exception_info': {'exception_message': None, 'exception_traceback': None, 'raised_exception': False}, 'expectation_c... 'value': 'aaa'}, {'count': 1, 'value': 'abb'}, {'count': 1, 'value': 'acc'}, {'count': 1, 'value': 'add'}], ...}, ...}
data_asset = <great_expectations.execution_engine.pandas_batch_data.PandasBatchData object at 0x7f780634c9a0>
pk_column = True

    def check_json_test_result(  # noqa: C901 - 52
        test, result, data_asset=None, pk_column=False
    ) -> None:

        # check for id_pk results in cases where pk_column is true and unexpected_index_list already exists
        # this will work for testing since result_format is COMPLETE
        if pk_column:
            if not result["success"]:
                if "unexpected_index_list" in result["result"]:
                    assert "unexpected_index_query" in result["result"]

        if "unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )
            elif "unexpected_list" in test["output"]:
                (
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )

        if "partial_unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "partial_unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )
            elif "partial_unexpected_list" in test["output"]:
                (
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )

        # Determine if np.allclose(..) might be needed for float comparison
        try_allclose = False
        if "observed_value" in test["output"]:
            if RX_FLOAT.match(repr(test["output"]["observed_value"])):
                try_allclose = True

        # Check results
        if test["exact_match_out"] is True:
            if "result" in result and "observed_value" in result["result"]:
                if isinstance(result["result"]["observed_value"], (np.floating, float)):
                    assert np.allclose(
                        result["result"]["observed_value"],
                        expectationValidationResultSchema.load(test["output"])["result"][
                            "observed_value"
                        ],
                        rtol=RTOL,
                        atol=ATOL,
                    ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {expectationValidationResultSchema.load(test['output'])['result']['observed_value']}"
                else:
                    assert result == expectationValidationResultSchema.load(
                        test["output"]
                    ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
            else:
                assert result == expectationValidationResultSchema.load(
                    test["output"]
                ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
        else:
            # Convert result to json since our tests are reading from json so cannot easily contain richer types (e.g. NaN)
            # NOTE - 20191031 - JPC - we may eventually want to change these tests as we update our view on how
            # representations, serializations, and objects should interact and how much of that is shown to the user.
            result = result.to_json_dict()
            for key, value in test["output"].items():
                # Apply our great expectations-specific test logic

                if key == "success":
                    if isinstance(value, (np.floating, float)):
                        try:
                            assert np.allclose(
                                result["success"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['success']} not np.allclose to {value}"
                        except TypeError:
                            assert (
                                result["success"] == value
                            ), f"{result['success']} != {value}"
                    else:
                        assert result["success"] == value, f"{result['success']} != {value}"

                elif key == "observed_value":
                    if "tolerance" in test:
                        if isinstance(value, dict):
                            assert set(result["result"]["observed_value"].keys()) == set(
                                value.keys()
                            ), f"{set(result['result']['observed_value'].keys())} != {set(value.keys())}"
                            for k, v in value.items():
                                assert np.allclose(
                                    result["result"]["observed_value"][k],
                                    v,
                                    rtol=test["tolerance"],
                                )
                        else:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=test["tolerance"],
                            )
                    else:
                        if isinstance(value, dict) and "values" in value:
                            try:
                                assert np.allclose(
                                    result["result"]["observed_value"]["values"],
                                    value["values"],
                                    rtol=RTOL,
                                    atol=ATOL,
                                ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']['values']} not np.allclose to {value['values']}"
                            except TypeError as e:
                                print(e)
                                assert (
                                    result["result"]["observed_value"] == value
                                ), f"{result['result']['observed_value']} != {value}"
                        elif try_allclose:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {value}"
                        else:
                            assert (
                                result["result"]["observed_value"] == value
                            ), f"{result['result']['observed_value']} != {value}"

                # NOTE: This is a key used ONLY for testing cases where an expectation is legitimately allowed to return
                # any of multiple possible observed_values. expect_column_values_to_be_of_type is one such expectation.
                elif key == "observed_value_list":
                    assert result["result"]["observed_value"] in value

                elif key == "unexpected_index_list":
                    if isinstance(data_asset, (SqlAlchemyDataset, SparkDFDataset)):
                        pass
                    elif isinstance(data_asset, (SqlAlchemyBatchData, SparkDFBatchData)):
                        pass
                    else:
>                       assert (
                            result["result"]["unexpected_index_list"] == value
                        ), f"{result['result']['unexpected_index_list']} != {value}"
E                       AssertionError: [{'a': 'aaa', 'pk_index': 0}, {'a': 'abb', 'pk_index': 1}, {'a': 'acc', 'pk_index': 2}, {'a': 'add', 'pk_index': 3}] != [0, 1, 2, 3]

great_expectations/self_check/util.py:3075: AssertionError
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 3389.34it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 431.78it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 610.85it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 345.66it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 482.58it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 370.92it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 577.43it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 536.28it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 533.82it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 530.95it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_not_match_regex:positive_test_sufficient_mostly_w_one_non_matching_value] _

test_case = {'expectation_type': 'expect_column_values_to_not_match_regex', 'pk_column': True, 'skip': False, 'test': {'exact_matc...: {'success': True, 'unexpected_index_list': [0, 1, 2, 3], 'unexpected_list': ['aaa', 'abb', 'acc', 'add']}, ...}, ...}

    @pytest.mark.order(index=0)
    @pytest.mark.integration
    @pytest.mark.slow  # 12.68s
    def test_case_runner_v3_api(test_case):
        if test_case["skip"]:
            pytest.skip()

        # Note: this should never be done in practice, but we are wiping expectations to reuse batches during testing.
        # test_case["batch"]._initialize_expectations()
        if "parse_strings_as_datetimes" in test_case["test"]["in"]:
            with pytest.deprecated_call():
                evaluate_json_test_v3_api(
                    validator=test_case["validator_with_data"],
                    expectation_type=test_case["expectation_type"],
                    test=test_case["test"],
                    pk_column=test_case["pk_column"],
                )
        else:
>           evaluate_json_test_v3_api(
                validator=test_case["validator_with_data"],
                expectation_type=test_case["expectation_type"],
                test=test_case["test"],
                pk_column=test_case["pk_column"],
            )

tests/test_definitions/test_expectations_v3_api.py:418:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
great_expectations/self_check/util.py:2901: in evaluate_json_test_v3_api
    check_json_test_result(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

test = {'exact_match_out': False, 'in': {'column': 'a', 'mostly': 0.1, 'regex': '^a'}, 'input': {'column': 'a', 'mostly': 0.1... 'out': {'success': True, 'unexpected_index_list': [0, 1, 2, 3], 'unexpected_list': ['aaa', 'abb', 'acc', 'add']}, ...}
result = {'exception_info': {'exception_message': None, 'exception_traceback': None, 'raised_exception': False}, 'expectation_c... 'value': 'aaa'}, {'count': 1, 'value': 'abb'}, {'count': 1, 'value': 'acc'}, {'count': 1, 'value': 'add'}], ...}, ...}
data_asset = <great_expectations.execution_engine.pandas_batch_data.PandasBatchData object at 0x7f780634c9a0>
pk_column = True

    def check_json_test_result(  # noqa: C901 - 52
        test, result, data_asset=None, pk_column=False
    ) -> None:

        # check for id_pk results in cases where pk_column is true and unexpected_index_list already exists
        # this will work for testing since result_format is COMPLETE
        if pk_column:
            if not result["success"]:
                if "unexpected_index_list" in result["result"]:
                    assert "unexpected_index_query" in result["result"]

        if "unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )
            elif "unexpected_list" in test["output"]:
                (
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )

        if "partial_unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "partial_unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )
            elif "partial_unexpected_list" in test["output"]:
                (
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )

        # Determine if np.allclose(..) might be needed for float comparison
        try_allclose = False
        if "observed_value" in test["output"]:
            if RX_FLOAT.match(repr(test["output"]["observed_value"])):
                try_allclose = True

        # Check results
        if test["exact_match_out"] is True:
            if "result" in result and "observed_value" in result["result"]:
                if isinstance(result["result"]["observed_value"], (np.floating, float)):
                    assert np.allclose(
                        result["result"]["observed_value"],
                        expectationValidationResultSchema.load(test["output"])["result"][
                            "observed_value"
                        ],
                        rtol=RTOL,
                        atol=ATOL,
                    ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {expectationValidationResultSchema.load(test['output'])['result']['observed_value']}"
                else:
                    assert result == expectationValidationResultSchema.load(
                        test["output"]
                    ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
            else:
                assert result == expectationValidationResultSchema.load(
                    test["output"]
                ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
        else:
            # Convert result to json since our tests are reading from json so cannot easily contain richer types (e.g. NaN)
            # NOTE - 20191031 - JPC - we may eventually want to change these tests as we update our view on how
            # representations, serializations, and objects should interact and how much of that is shown to the user.
            result = result.to_json_dict()
            for key, value in test["output"].items():
                # Apply our great expectations-specific test logic

                if key == "success":
                    if isinstance(value, (np.floating, float)):
                        try:
                            assert np.allclose(
                                result["success"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['success']} not np.allclose to {value}"
                        except TypeError:
                            assert (
                                result["success"] == value
                            ), f"{result['success']} != {value}"
                    else:
                        assert result["success"] == value, f"{result['success']} != {value}"

                elif key == "observed_value":
                    if "tolerance" in test:
                        if isinstance(value, dict):
                            assert set(result["result"]["observed_value"].keys()) == set(
                                value.keys()
                            ), f"{set(result['result']['observed_value'].keys())} != {set(value.keys())}"
                            for k, v in value.items():
                                assert np.allclose(
                                    result["result"]["observed_value"][k],
                                    v,
                                    rtol=test["tolerance"],
                                )
                        else:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=test["tolerance"],
                            )
                    else:
                        if isinstance(value, dict) and "values" in value:
                            try:
                                assert np.allclose(
                                    result["result"]["observed_value"]["values"],
                                    value["values"],
                                    rtol=RTOL,
                                    atol=ATOL,
                                ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']['values']} not np.allclose to {value['values']}"
                            except TypeError as e:
                                print(e)
                                assert (
                                    result["result"]["observed_value"] == value
                                ), f"{result['result']['observed_value']} != {value}"
                        elif try_allclose:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {value}"
                        else:
                            assert (
                                result["result"]["observed_value"] == value
                            ), f"{result['result']['observed_value']} != {value}"

                # NOTE: This is a key used ONLY for testing cases where an expectation is legitimately allowed to return
                # any of multiple possible observed_values. expect_column_values_to_be_of_type is one such expectation.
                elif key == "observed_value_list":
                    assert result["result"]["observed_value"] in value

                elif key == "unexpected_index_list":
                    if isinstance(data_asset, (SqlAlchemyDataset, SparkDFDataset)):
                        pass
                    elif isinstance(data_asset, (SqlAlchemyBatchData, SparkDFBatchData)):
                        pass
                    else:
>                       assert (
                            result["result"]["unexpected_index_list"] == value
                        ), f"{result['result']['unexpected_index_list']} != {value}"
E                       AssertionError: [{'a': 'aaa', 'pk_index': 0}, {'a': 'abb', 'pk_index': 1}, {'a': 'acc', 'pk_index': 2}, {'a': 'add', 'pk_index': 3}] != [0, 1, 2, 3]

great_expectations/self_check/util.py:3075: AssertionError
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 3975.64it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 444.45it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 625.14it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 339.59it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 473.03it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 366.55it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 580.29it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 539.40it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 537.06it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 534.24it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_not_match_regex:negative_test_one_missing_value_and_insufficent_mostly] _

test_case = {'expectation_type': 'expect_column_values_to_not_match_regex', 'pk_column': True, 'skip': False, 'test': {'exact_matc...'}, 'out': {'success': False, 'unexpected_index_list': [0, 1, 2], 'unexpected_list': ['aaa', 'abb', 'acc']}, ...}, ...}

    @pytest.mark.order(index=0)
    @pytest.mark.integration
    @pytest.mark.slow  # 12.68s
    def test_case_runner_v3_api(test_case):
        if test_case["skip"]:
            pytest.skip()

        # Note: this should never be done in practice, but we are wiping expectations to reuse batches during testing.
        # test_case["batch"]._initialize_expectations()
        if "parse_strings_as_datetimes" in test_case["test"]["in"]:
            with pytest.deprecated_call():
                evaluate_json_test_v3_api(
                    validator=test_case["validator_with_data"],
                    expectation_type=test_case["expectation_type"],
                    test=test_case["test"],
                    pk_column=test_case["pk_column"],
                )
        else:
>           evaluate_json_test_v3_api(
                validator=test_case["validator_with_data"],
                expectation_type=test_case["expectation_type"],
                test=test_case["test"],
                pk_column=test_case["pk_column"],
            )

tests/test_definitions/test_expectations_v3_api.py:418:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
great_expectations/self_check/util.py:2901: in evaluate_json_test_v3_api
    check_json_test_result(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

test = {'exact_match_out': False, 'in': {'column': 'b', 'mostly': 0.5, 'regex': '^a'}, 'input': {'column': 'b', 'mostly': 0.5...': '^a'}, 'out': {'success': False, 'unexpected_index_list': [0, 1, 2], 'unexpected_list': ['aaa', 'abb', 'acc']}, ...}
result = {'exception_info': {'exception_message': None, 'exception_traceback': None, 'raised_exception': False}, 'expectation_c...xpected_counts': [{'count': 1, 'value': 'aaa'}, {'count': 1, 'value': 'abb'}, {'count': 1, 'value': 'acc'}], ...}, ...}
data_asset = <great_expectations.execution_engine.pandas_batch_data.PandasBatchData object at 0x7f780634c9a0>
pk_column = True

    def check_json_test_result(  # noqa: C901 - 52
        test, result, data_asset=None, pk_column=False
    ) -> None:

        # check for id_pk results in cases where pk_column is true and unexpected_index_list already exists
        # this will work for testing since result_format is COMPLETE
        if pk_column:
            if not result["success"]:
                if "unexpected_index_list" in result["result"]:
                    assert "unexpected_index_query" in result["result"]

        if "unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )
            elif "unexpected_list" in test["output"]:
                (
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )

        if "partial_unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "partial_unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )
            elif "partial_unexpected_list" in test["output"]:
                (
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )

        # Determine if np.allclose(..) might be needed for float comparison
        try_allclose = False
        if "observed_value" in test["output"]:
            if RX_FLOAT.match(repr(test["output"]["observed_value"])):
                try_allclose = True

        # Check results
        if test["exact_match_out"] is True:
            if "result" in result and "observed_value" in result["result"]:
                if isinstance(result["result"]["observed_value"], (np.floating, float)):
                    assert np.allclose(
                        result["result"]["observed_value"],
                        expectationValidationResultSchema.load(test["output"])["result"][
                            "observed_value"
                        ],
                        rtol=RTOL,
                        atol=ATOL,
                    ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {expectationValidationResultSchema.load(test['output'])['result']['observed_value']}"
                else:
                    assert result == expectationValidationResultSchema.load(
                        test["output"]
                    ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
            else:
                assert result == expectationValidationResultSchema.load(
                    test["output"]
                ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
        else:
            # Convert result to json since our tests are reading from json so cannot easily contain richer types (e.g. NaN)
            # NOTE - 20191031 - JPC - we may eventually want to change these tests as we update our view on how
            # representations, serializations, and objects should interact and how much of that is shown to the user.
            result = result.to_json_dict()
            for key, value in test["output"].items():
                # Apply our great expectations-specific test logic

                if key == "success":
                    if isinstance(value, (np.floating, float)):
                        try:
                            assert np.allclose(
                                result["success"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['success']} not np.allclose to {value}"
                        except TypeError:
                            assert (
                                result["success"] == value
                            ), f"{result['success']} != {value}"
                    else:
                        assert result["success"] == value, f"{result['success']} != {value}"

                elif key == "observed_value":
                    if "tolerance" in test:
                        if isinstance(value, dict):
                            assert set(result["result"]["observed_value"].keys()) == set(
                                value.keys()
                            ), f"{set(result['result']['observed_value'].keys())} != {set(value.keys())}"
                            for k, v in value.items():
                                assert np.allclose(
                                    result["result"]["observed_value"][k],
                                    v,
                                    rtol=test["tolerance"],
                                )
                        else:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=test["tolerance"],
                            )
                    else:
                        if isinstance(value, dict) and "values" in value:
                            try:
                                assert np.allclose(
                                    result["result"]["observed_value"]["values"],
                                    value["values"],
                                    rtol=RTOL,
                                    atol=ATOL,
                                ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']['values']} not np.allclose to {value['values']}"
                            except TypeError as e:
                                print(e)
                                assert (
                                    result["result"]["observed_value"] == value
                                ), f"{result['result']['observed_value']} != {value}"
                        elif try_allclose:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {value}"
                        else:
                            assert (
                                result["result"]["observed_value"] == value
                            ), f"{result['result']['observed_value']} != {value}"

                # NOTE: This is a key used ONLY for testing cases where an expectation is legitimately allowed to return
                # any of multiple possible observed_values. expect_column_values_to_be_of_type is one such expectation.
                elif key == "observed_value_list":
                    assert result["result"]["observed_value"] in value

                elif key == "unexpected_index_list":
                    if isinstance(data_asset, (SqlAlchemyDataset, SparkDFDataset)):
                        pass
                    elif isinstance(data_asset, (SqlAlchemyBatchData, SparkDFBatchData)):
                        pass
                    else:
>                       assert (
                            result["result"]["unexpected_index_list"] == value
                        ), f"{result['result']['unexpected_index_list']} != {value}"
E                       AssertionError: [{'b': 'aaa', 'pk_index': 0}, {'b': 'abb', 'pk_index': 1}, {'b': 'acc', 'pk_index': 2}] != [0, 1, 2]

great_expectations/self_check/util.py:3075: AssertionError
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 3130.08it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 341.35it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 485.58it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 291.66it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 411.25it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 327.54it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 530.28it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 495.73it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 493.73it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 491.33it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_not_match_regex:negative_test_empty_regex] _

test_case = {'expectation_type': 'expect_column_values_to_not_match_regex', 'pk_column': True, 'skip': False, 'test': {'exact_matc... {'success': False, 'unexpected_index_list': [0, 1, 2, 3], 'unexpected_list': ['aaa', 'abb', 'acc', 'bdd']}, ...}, ...}

    @pytest.mark.order(index=0)
    @pytest.mark.integration
    @pytest.mark.slow  # 12.68s
    def test_case_runner_v3_api(test_case):
        if test_case["skip"]:
            pytest.skip()

        # Note: this should never be done in practice, but we are wiping expectations to reuse batches during testing.
        # test_case["batch"]._initialize_expectations()
        if "parse_strings_as_datetimes" in test_case["test"]["in"]:
            with pytest.deprecated_call():
                evaluate_json_test_v3_api(
                    validator=test_case["validator_with_data"],
                    expectation_type=test_case["expectation_type"],
                    test=test_case["test"],
                    pk_column=test_case["pk_column"],
                )
        else:
>           evaluate_json_test_v3_api(
                validator=test_case["validator_with_data"],
                expectation_type=test_case["expectation_type"],
                test=test_case["test"],
                pk_column=test_case["pk_column"],
            )

tests/test_definitions/test_expectations_v3_api.py:418:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
great_expectations/self_check/util.py:2901: in evaluate_json_test_v3_api
    check_json_test_result(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

test = {'exact_match_out': False, 'in': {'column': 'b', 'regex': ''}, 'input': {'column': 'b', 'regex': ''}, 'out': {'success': False, 'unexpected_index_list': [0, 1, 2, 3], 'unexpected_list': ['aaa', 'abb', 'acc', 'bdd']}, ...}
result = {'exception_info': {'exception_message': None, 'exception_traceback': None, 'raised_exception': False}, 'expectation_c... 'value': 'aaa'}, {'count': 1, 'value': 'abb'}, {'count': 1, 'value': 'acc'}, {'count': 1, 'value': 'bdd'}], ...}, ...}
data_asset = <great_expectations.execution_engine.pandas_batch_data.PandasBatchData object at 0x7f780634c9a0>
pk_column = True

    def check_json_test_result(  # noqa: C901 - 52
        test, result, data_asset=None, pk_column=False
    ) -> None:

        # check for id_pk results in cases where pk_column is true and unexpected_index_list already exists
        # this will work for testing since result_format is COMPLETE
        if pk_column:
            if not result["success"]:
                if "unexpected_index_list" in result["result"]:
                    assert "unexpected_index_query" in result["result"]

        if "unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )
            elif "unexpected_list" in test["output"]:
                (
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )

        if "partial_unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "partial_unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )
            elif "partial_unexpected_list" in test["output"]:
                (
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )

        # Determine if np.allclose(..) might be needed for float comparison
        try_allclose = False
        if "observed_value" in test["output"]:
            if RX_FLOAT.match(repr(test["output"]["observed_value"])):
                try_allclose = True

        # Check results
        if test["exact_match_out"] is True:
            if "result" in result and "observed_value" in result["result"]:
                if isinstance(result["result"]["observed_value"], (np.floating, float)):
                    assert np.allclose(
                        result["result"]["observed_value"],
                        expectationValidationResultSchema.load(test["output"])["result"][
                            "observed_value"
                        ],
                        rtol=RTOL,
                        atol=ATOL,
                    ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {expectationValidationResultSchema.load(test['output'])['result']['observed_value']}"
                else:
                    assert result == expectationValidationResultSchema.load(
                        test["output"]
                    ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
            else:
                assert result == expectationValidationResultSchema.load(
                    test["output"]
                ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
        else:
            # Convert result to json since our tests are reading from json so cannot easily contain richer types (e.g. NaN)
            # NOTE - 20191031 - JPC - we may eventually want to change these tests as we update our view on how
            # representations, serializations, and objects should interact and how much of that is shown to the user.
            result = result.to_json_dict()
            for key, value in test["output"].items():
                # Apply our great expectations-specific test logic

                if key == "success":
                    if isinstance(value, (np.floating, float)):
                        try:
                            assert np.allclose(
                                result["success"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['success']} not np.allclose to {value}"
                        except TypeError:
                            assert (
                                result["success"] == value
                            ), f"{result['success']} != {value}"
                    else:
                        assert result["success"] == value, f"{result['success']} != {value}"

                elif key == "observed_value":
                    if "tolerance" in test:
                        if isinstance(value, dict):
                            assert set(result["result"]["observed_value"].keys()) == set(
                                value.keys()
                            ), f"{set(result['result']['observed_value'].keys())} != {set(value.keys())}"
                            for k, v in value.items():
                                assert np.allclose(
                                    result["result"]["observed_value"][k],
                                    v,
                                    rtol=test["tolerance"],
                                )
                        else:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=test["tolerance"],
                            )
                    else:
                        if isinstance(value, dict) and "values" in value:
                            try:
                                assert np.allclose(
                                    result["result"]["observed_value"]["values"],
                                    value["values"],
                                    rtol=RTOL,
                                    atol=ATOL,
                                ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']['values']} not np.allclose to {value['values']}"
                            except TypeError as e:
                                print(e)
                                assert (
                                    result["result"]["observed_value"] == value
                                ), f"{result['result']['observed_value']} != {value}"
                        elif try_allclose:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {value}"
                        else:
                            assert (
                                result["result"]["observed_value"] == value
                            ), f"{result['result']['observed_value']} != {value}"

                # NOTE: This is a key used ONLY for testing cases where an expectation is legitimately allowed to return
                # any of multiple possible observed_values. expect_column_values_to_be_of_type is one such expectation.
                elif key == "observed_value_list":
                    assert result["result"]["observed_value"] in value

                elif key == "unexpected_index_list":
                    if isinstance(data_asset, (SqlAlchemyDataset, SparkDFDataset)):
                        pass
                    elif isinstance(data_asset, (SqlAlchemyBatchData, SparkDFBatchData)):
                        pass
                    else:
>                       assert (
                            result["result"]["unexpected_index_list"] == value
                        ), f"{result['result']['unexpected_index_list']} != {value}"
E                       AssertionError: [{'b': 'aaa', 'pk_index': 0}, {'b': 'abb', 'pk_index': 1}, {'b': 'acc', 'pk_index': 2}, {'b': 'bdd', 'pk_index': 3}] != [0, 1, 2, 3]

great_expectations/self_check/util.py:3075: AssertionError
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 3508.41it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 419.28it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 590.66it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 330.29it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 461.46it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 358.73it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 563.92it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 522.98it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 520.31it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 517.60it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_not_match_regex:negative_test_match_characters_not_at_the_beginning_of_string_exact_mostly] _

test_case = {'expectation_type': 'expect_column_values_to_not_match_regex', 'pk_column': True, 'skip': False, 'test': {'exact_matc...'regex': 'b'}, 'out': {'success': True, 'unexpected_index_list': [1, 4], 'unexpected_list': ['abb', 'bee']}, ...}, ...}

    @pytest.mark.order(index=0)
    @pytest.mark.integration
    @pytest.mark.slow  # 12.68s
    def test_case_runner_v3_api(test_case):
        if test_case["skip"]:
            pytest.skip()

        # Note: this should never be done in practice, but we are wiping expectations to reuse batches during testing.
        # test_case["batch"]._initialize_expectations()
        if "parse_strings_as_datetimes" in test_case["test"]["in"]:
            with pytest.deprecated_call():
                evaluate_json_test_v3_api(
                    validator=test_case["validator_with_data"],
                    expectation_type=test_case["expectation_type"],
                    test=test_case["test"],
                    pk_column=test_case["pk_column"],
                )
        else:
>           evaluate_json_test_v3_api(
                validator=test_case["validator_with_data"],
                expectation_type=test_case["expectation_type"],
                test=test_case["test"],
                pk_column=test_case["pk_column"],
            )

tests/test_definitions/test_expectations_v3_api.py:418:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
great_expectations/self_check/util.py:2901: in evaluate_json_test_v3_api
    check_json_test_result(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

test = {'exact_match_out': False, 'in': {'column': 'a', 'mostly': 0.6, 'regex': 'b'}, 'input': {'column': 'a', 'mostly': 0.6, 'regex': 'b'}, 'out': {'success': True, 'unexpected_index_list': [1, 4], 'unexpected_list': ['abb', 'bee']}, ...}
result = {'exception_info': {'exception_message': None, 'exception_traceback': None, 'raised_exception': False}, 'expectation_c...ng_percent': 0.0, 'partial_unexpected_counts': [{'count': 1, 'value': 'abb'}, {'count': 1, 'value': 'bee'}], ...}, ...}
data_asset = <great_expectations.execution_engine.pandas_batch_data.PandasBatchData object at 0x7f780634c9a0>
pk_column = True

    def check_json_test_result(  # noqa: C901 - 52
        test, result, data_asset=None, pk_column=False
    ) -> None:

        # check for id_pk results in cases where pk_column is true and unexpected_index_list already exists
        # this will work for testing since result_format is COMPLETE
        if pk_column:
            if not result["success"]:
                if "unexpected_index_list" in result["result"]:
                    assert "unexpected_index_query" in result["result"]

        if "unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )
            elif "unexpected_list" in test["output"]:
                (
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )

        if "partial_unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "partial_unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )
            elif "partial_unexpected_list" in test["output"]:
                (
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )

        # Determine if np.allclose(..) might be needed for float comparison
        try_allclose = False
        if "observed_value" in test["output"]:
            if RX_FLOAT.match(repr(test["output"]["observed_value"])):
                try_allclose = True

        # Check results
        if test["exact_match_out"] is True:
            if "result" in result and "observed_value" in result["result"]:
                if isinstance(result["result"]["observed_value"], (np.floating, float)):
                    assert np.allclose(
                        result["result"]["observed_value"],
                        expectationValidationResultSchema.load(test["output"])["result"][
                            "observed_value"
                        ],
                        rtol=RTOL,
                        atol=ATOL,
                    ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {expectationValidationResultSchema.load(test['output'])['result']['observed_value']}"
                else:
                    assert result == expectationValidationResultSchema.load(
                        test["output"]
                    ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
            else:
                assert result == expectationValidationResultSchema.load(
                    test["output"]
                ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
        else:
            # Convert result to json since our tests are reading from json so cannot easily contain richer types (e.g. NaN)
            # NOTE - 20191031 - JPC - we may eventually want to change these tests as we update our view on how
            # representations, serializations, and objects should interact and how much of that is shown to the user.
            result = result.to_json_dict()
            for key, value in test["output"].items():
                # Apply our great expectations-specific test logic

                if key == "success":
                    if isinstance(value, (np.floating, float)):
                        try:
                            assert np.allclose(
                                result["success"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['success']} not np.allclose to {value}"
                        except TypeError:
                            assert (
                                result["success"] == value
                            ), f"{result['success']} != {value}"
                    else:
                        assert result["success"] == value, f"{result['success']} != {value}"

                elif key == "observed_value":
                    if "tolerance" in test:
                        if isinstance(value, dict):
                            assert set(result["result"]["observed_value"].keys()) == set(
                                value.keys()
                            ), f"{set(result['result']['observed_value'].keys())} != {set(value.keys())}"
                            for k, v in value.items():
                                assert np.allclose(
                                    result["result"]["observed_value"][k],
                                    v,
                                    rtol=test["tolerance"],
                                )
                        else:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=test["tolerance"],
                            )
                    else:
                        if isinstance(value, dict) and "values" in value:
                            try:
                                assert np.allclose(
                                    result["result"]["observed_value"]["values"],
                                    value["values"],
                                    rtol=RTOL,
                                    atol=ATOL,
                                ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']['values']} not np.allclose to {value['values']}"
                            except TypeError as e:
                                print(e)
                                assert (
                                    result["result"]["observed_value"] == value
                                ), f"{result['result']['observed_value']} != {value}"
                        elif try_allclose:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {value}"
                        else:
                            assert (
                                result["result"]["observed_value"] == value
                            ), f"{result['result']['observed_value']} != {value}"

                # NOTE: This is a key used ONLY for testing cases where an expectation is legitimately allowed to return
                # any of multiple possible observed_values. expect_column_values_to_be_of_type is one such expectation.
                elif key == "observed_value_list":
                    assert result["result"]["observed_value"] in value

                elif key == "unexpected_index_list":
                    if isinstance(data_asset, (SqlAlchemyDataset, SparkDFDataset)):
                        pass
                    elif isinstance(data_asset, (SqlAlchemyBatchData, SparkDFBatchData)):
                        pass
                    else:
>                       assert (
                            result["result"]["unexpected_index_list"] == value
                        ), f"{result['result']['unexpected_index_list']} != {value}"
E                       AssertionError: [{'a': 'abb', 'pk_index': 1}, {'a': 'bee', 'pk_index': 4}] != [1, 4]

great_expectations/self_check/util.py:3075: AssertionError
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 2249.56it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 338.47it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 482.38it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 273.77it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 386.24it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 303.89it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 494.56it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 464.70it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 462.96it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 460.93it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_not_match_regex_list:basic_negative_test] _

test_case = {'expectation_type': 'expect_column_values_to_not_match_regex_list', 'pk_column': True, 'skip': False, 'test': {'exact...['[12]+', '[45]+']}, 'include_in_gallery': True, 'input': {'column': 'w', 'regex_list': ['[12]+', '[45]+']}, ...}, ...}

    @pytest.mark.order(index=0)
    @pytest.mark.integration
    @pytest.mark.slow  # 12.68s
    def test_case_runner_v3_api(test_case):
        if test_case["skip"]:
            pytest.skip()

        # Note: this should never be done in practice, but we are wiping expectations to reuse batches during testing.
        # test_case["batch"]._initialize_expectations()
        if "parse_strings_as_datetimes" in test_case["test"]["in"]:
            with pytest.deprecated_call():
                evaluate_json_test_v3_api(
                    validator=test_case["validator_with_data"],
                    expectation_type=test_case["expectation_type"],
                    test=test_case["test"],
                    pk_column=test_case["pk_column"],
                )
        else:
>           evaluate_json_test_v3_api(
                validator=test_case["validator_with_data"],
                expectation_type=test_case["expectation_type"],
                test=test_case["test"],
                pk_column=test_case["pk_column"],
            )

tests/test_definitions/test_expectations_v3_api.py:418:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
great_expectations/self_check/util.py:2901: in evaluate_json_test_v3_api
    check_json_test_result(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

test = {'exact_match_out': False, 'in': {'column': 'w', 'regex_list': ['[12]+', '[45]+']}, 'include_in_gallery': True, 'input': {'column': 'w', 'regex_list': ['[12]+', '[45]+']}, ...}
result = {'exception_info': {'exception_message': None, 'exception_traceback': None, 'raised_exception': False}, 'expectation_c...ue': '222'}, {'count': 1, 'value': '321'}, {'count': 1, 'value': '444'}, {'count': 1, 'value': '456'}, ...], ...}, ...}
data_asset = <great_expectations.execution_engine.pandas_batch_data.PandasBatchData object at 0x7f78064473a0>
pk_column = True

    def check_json_test_result(  # noqa: C901 - 52
        test, result, data_asset=None, pk_column=False
    ) -> None:

        # check for id_pk results in cases where pk_column is true and unexpected_index_list already exists
        # this will work for testing since result_format is COMPLETE
        if pk_column:
            if not result["success"]:
                if "unexpected_index_list" in result["result"]:
                    assert "unexpected_index_query" in result["result"]

        if "unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )
            elif "unexpected_list" in test["output"]:
                (
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )

        if "partial_unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "partial_unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )
            elif "partial_unexpected_list" in test["output"]:
                (
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )

        # Determine if np.allclose(..) might be needed for float comparison
        try_allclose = False
        if "observed_value" in test["output"]:
            if RX_FLOAT.match(repr(test["output"]["observed_value"])):
                try_allclose = True

        # Check results
        if test["exact_match_out"] is True:
            if "result" in result and "observed_value" in result["result"]:
                if isinstance(result["result"]["observed_value"], (np.floating, float)):
                    assert np.allclose(
                        result["result"]["observed_value"],
                        expectationValidationResultSchema.load(test["output"])["result"][
                            "observed_value"
                        ],
                        rtol=RTOL,
                        atol=ATOL,
                    ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {expectationValidationResultSchema.load(test['output'])['result']['observed_value']}"
                else:
                    assert result == expectationValidationResultSchema.load(
                        test["output"]
                    ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
            else:
                assert result == expectationValidationResultSchema.load(
                    test["output"]
                ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
        else:
            # Convert result to json since our tests are reading from json so cannot easily contain richer types (e.g. NaN)
            # NOTE - 20191031 - JPC - we may eventually want to change these tests as we update our view on how
            # representations, serializations, and objects should interact and how much of that is shown to the user.
            result = result.to_json_dict()
            for key, value in test["output"].items():
                # Apply our great expectations-specific test logic

                if key == "success":
                    if isinstance(value, (np.floating, float)):
                        try:
                            assert np.allclose(
                                result["success"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['success']} not np.allclose to {value}"
                        except TypeError:
                            assert (
                                result["success"] == value
                            ), f"{result['success']} != {value}"
                    else:
                        assert result["success"] == value, f"{result['success']} != {value}"

                elif key == "observed_value":
                    if "tolerance" in test:
                        if isinstance(value, dict):
                            assert set(result["result"]["observed_value"].keys()) == set(
                                value.keys()
                            ), f"{set(result['result']['observed_value'].keys())} != {set(value.keys())}"
                            for k, v in value.items():
                                assert np.allclose(
                                    result["result"]["observed_value"][k],
                                    v,
                                    rtol=test["tolerance"],
                                )
                        else:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=test["tolerance"],
                            )
                    else:
                        if isinstance(value, dict) and "values" in value:
                            try:
                                assert np.allclose(
                                    result["result"]["observed_value"]["values"],
                                    value["values"],
                                    rtol=RTOL,
                                    atol=ATOL,
                                ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']['values']} not np.allclose to {value['values']}"
                            except TypeError as e:
                                print(e)
                                assert (
                                    result["result"]["observed_value"] == value
                                ), f"{result['result']['observed_value']} != {value}"
                        elif try_allclose:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {value}"
                        else:
                            assert (
                                result["result"]["observed_value"] == value
                            ), f"{result['result']['observed_value']} != {value}"

                # NOTE: This is a key used ONLY for testing cases where an expectation is legitimately allowed to return
                # any of multiple possible observed_values. expect_column_values_to_be_of_type is one such expectation.
                elif key == "observed_value_list":
                    assert result["result"]["observed_value"] in value

                elif key == "unexpected_index_list":
                    if isinstance(data_asset, (SqlAlchemyDataset, SparkDFDataset)):
                        pass
                    elif isinstance(data_asset, (SqlAlchemyBatchData, SparkDFBatchData)):
                        pass
                    else:
>                       assert (
                            result["result"]["unexpected_index_list"] == value
                        ), f"{result['result']['unexpected_index_list']} != {value}"
E                       AssertionError: [{'w': '111', 'pk_index': 0}, {'w': '222', 'pk_index': 1}, {'w': '123', 'pk_index': 3}, {'w': '321', 'pk_index': 4}, {'w': '444', 'pk_index': 5}, {'w': '456', 'pk_index': 6}, {'w': '654', 'pk_index': 7}, {'w': '555', 'pk_index': 8}] != [0, 1, 3, 4, 5, 6, 7, 8]

great_expectations/self_check/util.py:3075: AssertionError
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 2185.67it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 283.05it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 140.12it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 95.93it/s] Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 105.40it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 96.32it/s]alculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 134.95it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 131.64it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 131.43it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 131.20it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_not_match_regex_list:negative_test_with_more_string-ish_strings] _

test_case = {'expectation_type': 'expect_column_values_to_not_match_regex_list', 'pk_column': True, 'skip': False, 'test': {'exact...mus', 'ovat', 'h.*t']}, 'out': {'success': False, 'unexpected_index_list': [4], 'unexpected_list': ['hat']}, ...}, ...}

    @pytest.mark.order(index=0)
    @pytest.mark.integration
    @pytest.mark.slow  # 12.68s
    def test_case_runner_v3_api(test_case):
        if test_case["skip"]:
            pytest.skip()

        # Note: this should never be done in practice, but we are wiping expectations to reuse batches during testing.
        # test_case["batch"]._initialize_expectations()
        if "parse_strings_as_datetimes" in test_case["test"]["in"]:
            with pytest.deprecated_call():
                evaluate_json_test_v3_api(
                    validator=test_case["validator_with_data"],
                    expectation_type=test_case["expectation_type"],
                    test=test_case["test"],
                    pk_column=test_case["pk_column"],
                )
        else:
>           evaluate_json_test_v3_api(
                validator=test_case["validator_with_data"],
                expectation_type=test_case["expectation_type"],
                test=test_case["test"],
                pk_column=test_case["pk_column"],
            )

tests/test_definitions/test_expectations_v3_api.py:418:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
great_expectations/self_check/util.py:2901: in evaluate_json_test_v3_api
    check_json_test_result(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

test = {'exact_match_out': False, 'in': {'column': 'x', 'regex_list': ['opatomus', 'ovat', 'h.*t']}, 'input': {'column': 'x',...'opatomus', 'ovat', 'h.*t']}, 'out': {'success': False, 'unexpected_index_list': [4], 'unexpected_list': ['hat']}, ...}
result = {'exception_info': {'exception_message': None, 'exception_traceback': None, 'raised_exception': False}, 'expectation_c...10, 'missing_count': 0, 'missing_percent': 0.0, 'partial_unexpected_counts': [{'count': 1, 'value': 'hat'}], ...}, ...}
data_asset = <great_expectations.execution_engine.pandas_batch_data.PandasBatchData object at 0x7f78064473a0>
pk_column = True

    def check_json_test_result(  # noqa: C901 - 52
        test, result, data_asset=None, pk_column=False
    ) -> None:

        # check for id_pk results in cases where pk_column is true and unexpected_index_list already exists
        # this will work for testing since result_format is COMPLETE
        if pk_column:
            if not result["success"]:
                if "unexpected_index_list" in result["result"]:
                    assert "unexpected_index_query" in result["result"]

        if "unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )
            elif "unexpected_list" in test["output"]:
                (
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )

        if "partial_unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "partial_unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )
            elif "partial_unexpected_list" in test["output"]:
                (
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )

        # Determine if np.allclose(..) might be needed for float comparison
        try_allclose = False
        if "observed_value" in test["output"]:
            if RX_FLOAT.match(repr(test["output"]["observed_value"])):
                try_allclose = True

        # Check results
        if test["exact_match_out"] is True:
            if "result" in result and "observed_value" in result["result"]:
                if isinstance(result["result"]["observed_value"], (np.floating, float)):
                    assert np.allclose(
                        result["result"]["observed_value"],
                        expectationValidationResultSchema.load(test["output"])["result"][
                            "observed_value"
                        ],
                        rtol=RTOL,
                        atol=ATOL,
                    ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {expectationValidationResultSchema.load(test['output'])['result']['observed_value']}"
                else:
                    assert result == expectationValidationResultSchema.load(
                        test["output"]
                    ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
            else:
                assert result == expectationValidationResultSchema.load(
                    test["output"]
                ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
        else:
            # Convert result to json since our tests are reading from json so cannot easily contain richer types (e.g. NaN)
            # NOTE - 20191031 - JPC - we may eventually want to change these tests as we update our view on how
            # representations, serializations, and objects should interact and how much of that is shown to the user.
            result = result.to_json_dict()
            for key, value in test["output"].items():
                # Apply our great expectations-specific test logic

                if key == "success":
                    if isinstance(value, (np.floating, float)):
                        try:
                            assert np.allclose(
                                result["success"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['success']} not np.allclose to {value}"
                        except TypeError:
                            assert (
                                result["success"] == value
                            ), f"{result['success']} != {value}"
                    else:
                        assert result["success"] == value, f"{result['success']} != {value}"

                elif key == "observed_value":
                    if "tolerance" in test:
                        if isinstance(value, dict):
                            assert set(result["result"]["observed_value"].keys()) == set(
                                value.keys()
                            ), f"{set(result['result']['observed_value'].keys())} != {set(value.keys())}"
                            for k, v in value.items():
                                assert np.allclose(
                                    result["result"]["observed_value"][k],
                                    v,
                                    rtol=test["tolerance"],
                                )
                        else:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=test["tolerance"],
                            )
                    else:
                        if isinstance(value, dict) and "values" in value:
                            try:
                                assert np.allclose(
                                    result["result"]["observed_value"]["values"],
                                    value["values"],
                                    rtol=RTOL,
                                    atol=ATOL,
                                ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']['values']} not np.allclose to {value['values']}"
                            except TypeError as e:
                                print(e)
                                assert (
                                    result["result"]["observed_value"] == value
                                ), f"{result['result']['observed_value']} != {value}"
                        elif try_allclose:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {value}"
                        else:
                            assert (
                                result["result"]["observed_value"] == value
                            ), f"{result['result']['observed_value']} != {value}"

                # NOTE: This is a key used ONLY for testing cases where an expectation is legitimately allowed to return
                # any of multiple possible observed_values. expect_column_values_to_be_of_type is one such expectation.
                elif key == "observed_value_list":
                    assert result["result"]["observed_value"] in value

                elif key == "unexpected_index_list":
                    if isinstance(data_asset, (SqlAlchemyDataset, SparkDFDataset)):
                        pass
                    elif isinstance(data_asset, (SqlAlchemyBatchData, SparkDFBatchData)):
                        pass
                    else:
>                       assert (
                            result["result"]["unexpected_index_list"] == value
                        ), f"{result['result']['unexpected_index_list']} != {value}"
E                       AssertionError: [{'x': 'hat', 'pk_index': 4}] != [4]

great_expectations/self_check/util.py:3075: AssertionError
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 4072.14it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 418.76it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 593.34it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 333.96it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 420.49it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 332.49it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 533.96it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 494.38it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 491.86it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 489.38it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_increasing:positive_test_with_strictly] _

test_case = {'expectation_type': 'expect_column_values_to_be_increasing', 'pk_column': True, 'skip': False, 'test': {'exact_match_...ut': {'success': False, 'unexpected_index_list': [1, 2, 4, 5, 7, 8], 'unexpected_list': [1, 1, 2, 2, 3, 3]}, ...}, ...}

    @pytest.mark.order(index=0)
    @pytest.mark.integration
    @pytest.mark.slow  # 12.68s
    def test_case_runner_v3_api(test_case):
        if test_case["skip"]:
            pytest.skip()

        # Note: this should never be done in practice, but we are wiping expectations to reuse batches during testing.
        # test_case["batch"]._initialize_expectations()
        if "parse_strings_as_datetimes" in test_case["test"]["in"]:
            with pytest.deprecated_call():
                evaluate_json_test_v3_api(
                    validator=test_case["validator_with_data"],
                    expectation_type=test_case["expectation_type"],
                    test=test_case["test"],
                    pk_column=test_case["pk_column"],
                )
        else:
>           evaluate_json_test_v3_api(
                validator=test_case["validator_with_data"],
                expectation_type=test_case["expectation_type"],
                test=test_case["test"],
                pk_column=test_case["pk_column"],
            )

tests/test_definitions/test_expectations_v3_api.py:418:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
great_expectations/self_check/util.py:2901: in evaluate_json_test_v3_api
    check_json_test_result(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

test = {'exact_match_out': False, 'in': {'column': 'y', 'strictly': True}, 'input': {'column': 'y', 'strictly': True}, 'out': {'success': False, 'unexpected_index_list': [1, 2, 4, 5, 7, 8], 'unexpected_list': [1, 1, 2, 2, 3, 3]}, ...}
result = {'exception_info': {'exception_message': None, 'exception_traceback': None, 'raised_exception': False}, 'expectation_c...'partial_unexpected_counts': [{'count': 2, 'value': 1}, {'count': 2, 'value': 2}, {'count': 2, 'value': 3}], ...}, ...}
data_asset = <great_expectations.execution_engine.pandas_batch_data.PandasBatchData object at 0x7f780645c5b0>
pk_column = True

    def check_json_test_result(  # noqa: C901 - 52
        test, result, data_asset=None, pk_column=False
    ) -> None:

        # check for id_pk results in cases where pk_column is true and unexpected_index_list already exists
        # this will work for testing since result_format is COMPLETE
        if pk_column:
            if not result["success"]:
                if "unexpected_index_list" in result["result"]:
                    assert "unexpected_index_query" in result["result"]

        if "unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )
            elif "unexpected_list" in test["output"]:
                (
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )

        if "partial_unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "partial_unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )
            elif "partial_unexpected_list" in test["output"]:
                (
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )

        # Determine if np.allclose(..) might be needed for float comparison
        try_allclose = False
        if "observed_value" in test["output"]:
            if RX_FLOAT.match(repr(test["output"]["observed_value"])):
                try_allclose = True

        # Check results
        if test["exact_match_out"] is True:
            if "result" in result and "observed_value" in result["result"]:
                if isinstance(result["result"]["observed_value"], (np.floating, float)):
                    assert np.allclose(
                        result["result"]["observed_value"],
                        expectationValidationResultSchema.load(test["output"])["result"][
                            "observed_value"
                        ],
                        rtol=RTOL,
                        atol=ATOL,
                    ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {expectationValidationResultSchema.load(test['output'])['result']['observed_value']}"
                else:
                    assert result == expectationValidationResultSchema.load(
                        test["output"]
                    ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
            else:
                assert result == expectationValidationResultSchema.load(
                    test["output"]
                ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
        else:
            # Convert result to json since our tests are reading from json so cannot easily contain richer types (e.g. NaN)
            # NOTE - 20191031 - JPC - we may eventually want to change these tests as we update our view on how
            # representations, serializations, and objects should interact and how much of that is shown to the user.
            result = result.to_json_dict()
            for key, value in test["output"].items():
                # Apply our great expectations-specific test logic

                if key == "success":
                    if isinstance(value, (np.floating, float)):
                        try:
                            assert np.allclose(
                                result["success"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['success']} not np.allclose to {value}"
                        except TypeError:
                            assert (
                                result["success"] == value
                            ), f"{result['success']} != {value}"
                    else:
                        assert result["success"] == value, f"{result['success']} != {value}"

                elif key == "observed_value":
                    if "tolerance" in test:
                        if isinstance(value, dict):
                            assert set(result["result"]["observed_value"].keys()) == set(
                                value.keys()
                            ), f"{set(result['result']['observed_value'].keys())} != {set(value.keys())}"
                            for k, v in value.items():
                                assert np.allclose(
                                    result["result"]["observed_value"][k],
                                    v,
                                    rtol=test["tolerance"],
                                )
                        else:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=test["tolerance"],
                            )
                    else:
                        if isinstance(value, dict) and "values" in value:
                            try:
                                assert np.allclose(
                                    result["result"]["observed_value"]["values"],
                                    value["values"],
                                    rtol=RTOL,
                                    atol=ATOL,
                                ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']['values']} not np.allclose to {value['values']}"
                            except TypeError as e:
                                print(e)
                                assert (
                                    result["result"]["observed_value"] == value
                                ), f"{result['result']['observed_value']} != {value}"
                        elif try_allclose:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {value}"
                        else:
                            assert (
                                result["result"]["observed_value"] == value
                            ), f"{result['result']['observed_value']} != {value}"

                # NOTE: This is a key used ONLY for testing cases where an expectation is legitimately allowed to return
                # any of multiple possible observed_values. expect_column_values_to_be_of_type is one such expectation.
                elif key == "observed_value_list":
                    assert result["result"]["observed_value"] in value

                elif key == "unexpected_index_list":
                    if isinstance(data_asset, (SqlAlchemyDataset, SparkDFDataset)):
                        pass
                    elif isinstance(data_asset, (SqlAlchemyBatchData, SparkDFBatchData)):
                        pass
                    else:
>                       assert (
                            result["result"]["unexpected_index_list"] == value
                        ), f"{result['result']['unexpected_index_list']} != {value}"
E                       AssertionError: [{'y': 1, 'pk_index': 1}, {'y': 1, 'pk_index': 2}, {'y': 2, 'pk_index': 4}, {'y': 2, 'pk_index': 5}, {'y': 3, 'pk_index': 7}, {'y': 3, 'pk_index': 8}] != [1, 2, 4, 5, 7, 8]

great_expectations/self_check/util.py:3075: AssertionError
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 3460.65it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 427.90it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 608.66it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 339.56it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 447.39it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 342.89it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 542.47it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 505.13it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 503.07it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 500.63it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_increasing:basic_negative_test] _

test_case = {'expectation_type': 'expect_column_values_to_be_increasing', 'pk_column': True, 'skip': False, 'test': {'exact_match_out': False, 'in': {'column': 'w'}, 'include_in_gallery': True, 'input': {'column': 'w'}, ...}, ...}

    @pytest.mark.order(index=0)
    @pytest.mark.integration
    @pytest.mark.slow  # 12.68s
    def test_case_runner_v3_api(test_case):
        if test_case["skip"]:
            pytest.skip()

        # Note: this should never be done in practice, but we are wiping expectations to reuse batches during testing.
        # test_case["batch"]._initialize_expectations()
        if "parse_strings_as_datetimes" in test_case["test"]["in"]:
            with pytest.deprecated_call():
                evaluate_json_test_v3_api(
                    validator=test_case["validator_with_data"],
                    expectation_type=test_case["expectation_type"],
                    test=test_case["test"],
                    pk_column=test_case["pk_column"],
                )
        else:
>           evaluate_json_test_v3_api(
                validator=test_case["validator_with_data"],
                expectation_type=test_case["expectation_type"],
                test=test_case["test"],
                pk_column=test_case["pk_column"],
            )

tests/test_definitions/test_expectations_v3_api.py:418:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
great_expectations/self_check/util.py:2901: in evaluate_json_test_v3_api
    check_json_test_result(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

test = {'exact_match_out': False, 'in': {'column': 'w'}, 'include_in_gallery': True, 'input': {'column': 'w'}, ...}
result = {'exception_info': {'exception_message': None, 'exception_traceback': None, 'raised_exception': False}, 'expectation_c...': [{'count': 1, 'value': 1}, {'count': 1, 'value': 2}, {'count': 1, 'value': 3}, {'count': 1, 'value': 4}], ...}, ...}
data_asset = <great_expectations.execution_engine.pandas_batch_data.PandasBatchData object at 0x7f780645c5b0>
pk_column = True

    def check_json_test_result(  # noqa: C901 - 52
        test, result, data_asset=None, pk_column=False
    ) -> None:

        # check for id_pk results in cases where pk_column is true and unexpected_index_list already exists
        # this will work for testing since result_format is COMPLETE
        if pk_column:
            if not result["success"]:
                if "unexpected_index_list" in result["result"]:
                    assert "unexpected_index_query" in result["result"]

        if "unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )
            elif "unexpected_list" in test["output"]:
                (
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )

        if "partial_unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "partial_unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )
            elif "partial_unexpected_list" in test["output"]:
                (
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )

        # Determine if np.allclose(..) might be needed for float comparison
        try_allclose = False
        if "observed_value" in test["output"]:
            if RX_FLOAT.match(repr(test["output"]["observed_value"])):
                try_allclose = True

        # Check results
        if test["exact_match_out"] is True:
            if "result" in result and "observed_value" in result["result"]:
                if isinstance(result["result"]["observed_value"], (np.floating, float)):
                    assert np.allclose(
                        result["result"]["observed_value"],
                        expectationValidationResultSchema.load(test["output"])["result"][
                            "observed_value"
                        ],
                        rtol=RTOL,
                        atol=ATOL,
                    ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {expectationValidationResultSchema.load(test['output'])['result']['observed_value']}"
                else:
                    assert result == expectationValidationResultSchema.load(
                        test["output"]
                    ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
            else:
                assert result == expectationValidationResultSchema.load(
                    test["output"]
                ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
        else:
            # Convert result to json since our tests are reading from json so cannot easily contain richer types (e.g. NaN)
            # NOTE - 20191031 - JPC - we may eventually want to change these tests as we update our view on how
            # representations, serializations, and objects should interact and how much of that is shown to the user.
            result = result.to_json_dict()
            for key, value in test["output"].items():
                # Apply our great expectations-specific test logic

                if key == "success":
                    if isinstance(value, (np.floating, float)):
                        try:
                            assert np.allclose(
                                result["success"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['success']} not np.allclose to {value}"
                        except TypeError:
                            assert (
                                result["success"] == value
                            ), f"{result['success']} != {value}"
                    else:
                        assert result["success"] == value, f"{result['success']} != {value}"

                elif key == "observed_value":
                    if "tolerance" in test:
                        if isinstance(value, dict):
                            assert set(result["result"]["observed_value"].keys()) == set(
                                value.keys()
                            ), f"{set(result['result']['observed_value'].keys())} != {set(value.keys())}"
                            for k, v in value.items():
                                assert np.allclose(
                                    result["result"]["observed_value"][k],
                                    v,
                                    rtol=test["tolerance"],
                                )
                        else:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=test["tolerance"],
                            )
                    else:
                        if isinstance(value, dict) and "values" in value:
                            try:
                                assert np.allclose(
                                    result["result"]["observed_value"]["values"],
                                    value["values"],
                                    rtol=RTOL,
                                    atol=ATOL,
                                ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']['values']} not np.allclose to {value['values']}"
                            except TypeError as e:
                                print(e)
                                assert (
                                    result["result"]["observed_value"] == value
                                ), f"{result['result']['observed_value']} != {value}"
                        elif try_allclose:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {value}"
                        else:
                            assert (
                                result["result"]["observed_value"] == value
                            ), f"{result['result']['observed_value']} != {value}"

                # NOTE: This is a key used ONLY for testing cases where an expectation is legitimately allowed to return
                # any of multiple possible observed_values. expect_column_values_to_be_of_type is one such expectation.
                elif key == "observed_value_list":
                    assert result["result"]["observed_value"] in value

                elif key == "unexpected_index_list":
                    if isinstance(data_asset, (SqlAlchemyDataset, SparkDFDataset)):
                        pass
                    elif isinstance(data_asset, (SqlAlchemyBatchData, SparkDFBatchData)):
                        pass
                    else:
>                       assert (
                            result["result"]["unexpected_index_list"] == value
                        ), f"{result['result']['unexpected_index_list']} != {value}"
E                       AssertionError: [{'w': 4, 'pk_index': 6}, {'w': 3, 'pk_index': 7}, {'w': 2, 'pk_index': 8}, {'w': 1, 'pk_index': 9}] != [6, 7, 8, 9]

great_expectations/self_check/util.py:3075: AssertionError
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 3256.45it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 406.25it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 578.37it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 322.68it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 415.07it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 310.69it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 492.22it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 460.43it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 458.67it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 456.62it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_increasing:negative_test_with_interspersed_nulls] _

test_case = {'expectation_type': 'expect_column_values_to_be_increasing', 'pk_column': True, 'skip': False, 'test': {'exact_match_..., 'input': {'column': 'b'}, 'out': {'success': False, 'unexpected_index_list': [7], 'unexpected_list': [1]}, ...}, ...}

    @pytest.mark.order(index=0)
    @pytest.mark.integration
    @pytest.mark.slow  # 12.68s
    def test_case_runner_v3_api(test_case):
        if test_case["skip"]:
            pytest.skip()

        # Note: this should never be done in practice, but we are wiping expectations to reuse batches during testing.
        # test_case["batch"]._initialize_expectations()
        if "parse_strings_as_datetimes" in test_case["test"]["in"]:
            with pytest.deprecated_call():
                evaluate_json_test_v3_api(
                    validator=test_case["validator_with_data"],
                    expectation_type=test_case["expectation_type"],
                    test=test_case["test"],
                    pk_column=test_case["pk_column"],
                )
        else:
>           evaluate_json_test_v3_api(
                validator=test_case["validator_with_data"],
                expectation_type=test_case["expectation_type"],
                test=test_case["test"],
                pk_column=test_case["pk_column"],
            )

tests/test_definitions/test_expectations_v3_api.py:418:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
great_expectations/self_check/util.py:2901: in evaluate_json_test_v3_api
    check_json_test_result(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

test = {'exact_match_out': False, 'in': {'column': 'b'}, 'input': {'column': 'b'}, 'out': {'success': False, 'unexpected_index_list': [7], 'unexpected_list': [1]}, ...}
result = {'exception_info': {'exception_message': None, 'exception_traceback': None, 'raised_exception': False}, 'expectation_c... 10, 'missing_count': 7, 'missing_percent': 70.0, 'partial_unexpected_counts': [{'count': 1, 'value': 1.0}], ...}, ...}
data_asset = <great_expectations.execution_engine.pandas_batch_data.PandasBatchData object at 0x7f780645c5b0>
pk_column = True

    def check_json_test_result(  # noqa: C901 - 52
        test, result, data_asset=None, pk_column=False
    ) -> None:

        # check for id_pk results in cases where pk_column is true and unexpected_index_list already exists
        # this will work for testing since result_format is COMPLETE
        if pk_column:
            if not result["success"]:
                if "unexpected_index_list" in result["result"]:
                    assert "unexpected_index_query" in result["result"]

        if "unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )
            elif "unexpected_list" in test["output"]:
                (
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )

        if "partial_unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "partial_unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )
            elif "partial_unexpected_list" in test["output"]:
                (
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )

        # Determine if np.allclose(..) might be needed for float comparison
        try_allclose = False
        if "observed_value" in test["output"]:
            if RX_FLOAT.match(repr(test["output"]["observed_value"])):
                try_allclose = True

        # Check results
        if test["exact_match_out"] is True:
            if "result" in result and "observed_value" in result["result"]:
                if isinstance(result["result"]["observed_value"], (np.floating, float)):
                    assert np.allclose(
                        result["result"]["observed_value"],
                        expectationValidationResultSchema.load(test["output"])["result"][
                            "observed_value"
                        ],
                        rtol=RTOL,
                        atol=ATOL,
                    ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {expectationValidationResultSchema.load(test['output'])['result']['observed_value']}"
                else:
                    assert result == expectationValidationResultSchema.load(
                        test["output"]
                    ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
            else:
                assert result == expectationValidationResultSchema.load(
                    test["output"]
                ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
        else:
            # Convert result to json since our tests are reading from json so cannot easily contain richer types (e.g. NaN)
            # NOTE - 20191031 - JPC - we may eventually want to change these tests as we update our view on how
            # representations, serializations, and objects should interact and how much of that is shown to the user.
            result = result.to_json_dict()
            for key, value in test["output"].items():
                # Apply our great expectations-specific test logic

                if key == "success":
                    if isinstance(value, (np.floating, float)):
                        try:
                            assert np.allclose(
                                result["success"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['success']} not np.allclose to {value}"
                        except TypeError:
                            assert (
                                result["success"] == value
                            ), f"{result['success']} != {value}"
                    else:
                        assert result["success"] == value, f"{result['success']} != {value}"

                elif key == "observed_value":
                    if "tolerance" in test:
                        if isinstance(value, dict):
                            assert set(result["result"]["observed_value"].keys()) == set(
                                value.keys()
                            ), f"{set(result['result']['observed_value'].keys())} != {set(value.keys())}"
                            for k, v in value.items():
                                assert np.allclose(
                                    result["result"]["observed_value"][k],
                                    v,
                                    rtol=test["tolerance"],
                                )
                        else:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=test["tolerance"],
                            )
                    else:
                        if isinstance(value, dict) and "values" in value:
                            try:
                                assert np.allclose(
                                    result["result"]["observed_value"]["values"],
                                    value["values"],
                                    rtol=RTOL,
                                    atol=ATOL,
                                ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']['values']} not np.allclose to {value['values']}"
                            except TypeError as e:
                                print(e)
                                assert (
                                    result["result"]["observed_value"] == value
                                ), f"{result['result']['observed_value']} != {value}"
                        elif try_allclose:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {value}"
                        else:
                            assert (
                                result["result"]["observed_value"] == value
                            ), f"{result['result']['observed_value']} != {value}"

                # NOTE: This is a key used ONLY for testing cases where an expectation is legitimately allowed to return
                # any of multiple possible observed_values. expect_column_values_to_be_of_type is one such expectation.
                elif key == "observed_value_list":
                    assert result["result"]["observed_value"] in value

                elif key == "unexpected_index_list":
                    if isinstance(data_asset, (SqlAlchemyDataset, SparkDFDataset)):
                        pass
                    elif isinstance(data_asset, (SqlAlchemyBatchData, SparkDFBatchData)):
                        pass
                    else:
>                       assert (
                            result["result"]["unexpected_index_list"] == value
                        ), f"{result['result']['unexpected_index_list']} != {value}"
E                       AssertionError: [{'b': 1.0, 'pk_index': 7}] != [7]

great_expectations/self_check/util.py:3075: AssertionError
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 4007.94it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 403.30it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 572.29it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 326.97it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 427.82it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 328.75it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 524.46it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 488.64it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 486.57it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 484.24it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_value_lengths_to_be_between:negative_test_matching_min_max_too_small] _

test_case = {'expectation_type': 'expect_column_value_lengths_to_be_between', 'pk_column': True, 'skip': False, 'test': {'exact_ma...: 8, 'min_value': 4}, 'include_in_gallery': True, 'input': {'column': 's2', 'max_value': 8, 'min_value': 4}, ...}, ...}

    @pytest.mark.order(index=0)
    @pytest.mark.integration
    @pytest.mark.slow  # 12.68s
    def test_case_runner_v3_api(test_case):
        if test_case["skip"]:
            pytest.skip()

        # Note: this should never be done in practice, but we are wiping expectations to reuse batches during testing.
        # test_case["batch"]._initialize_expectations()
        if "parse_strings_as_datetimes" in test_case["test"]["in"]:
            with pytest.deprecated_call():
                evaluate_json_test_v3_api(
                    validator=test_case["validator_with_data"],
                    expectation_type=test_case["expectation_type"],
                    test=test_case["test"],
                    pk_column=test_case["pk_column"],
                )
        else:
>           evaluate_json_test_v3_api(
                validator=test_case["validator_with_data"],
                expectation_type=test_case["expectation_type"],
                test=test_case["test"],
                pk_column=test_case["pk_column"],
            )

tests/test_definitions/test_expectations_v3_api.py:418:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
great_expectations/self_check/util.py:2901: in evaluate_json_test_v3_api
    check_json_test_result(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

test = {'exact_match_out': False, 'in': {'column': 's2', 'max_value': 8, 'min_value': 4}, 'include_in_gallery': True, 'input': {'column': 's2', 'max_value': 8, 'min_value': 4}, ...}
result = {'exception_info': {'exception_message': None, 'exception_traceback': None, 'raised_exception': False}, 'expectation_c...issing_count': 0, 'missing_percent': 0.0, 'partial_unexpected_counts': [{'count': 1, 'value': 'collected'}], ...}, ...}
data_asset = <great_expectations.execution_engine.pandas_batch_data.PandasBatchData object at 0x7f78063cca00>
pk_column = True

    def check_json_test_result(  # noqa: C901 - 52
        test, result, data_asset=None, pk_column=False
    ) -> None:

        # check for id_pk results in cases where pk_column is true and unexpected_index_list already exists
        # this will work for testing since result_format is COMPLETE
        if pk_column:
            if not result["success"]:
                if "unexpected_index_list" in result["result"]:
                    assert "unexpected_index_query" in result["result"]

        if "unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )
            elif "unexpected_list" in test["output"]:
                (
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )

        if "partial_unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "partial_unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )
            elif "partial_unexpected_list" in test["output"]:
                (
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )

        # Determine if np.allclose(..) might be needed for float comparison
        try_allclose = False
        if "observed_value" in test["output"]:
            if RX_FLOAT.match(repr(test["output"]["observed_value"])):
                try_allclose = True

        # Check results
        if test["exact_match_out"] is True:
            if "result" in result and "observed_value" in result["result"]:
                if isinstance(result["result"]["observed_value"], (np.floating, float)):
                    assert np.allclose(
                        result["result"]["observed_value"],
                        expectationValidationResultSchema.load(test["output"])["result"][
                            "observed_value"
                        ],
                        rtol=RTOL,
                        atol=ATOL,
                    ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {expectationValidationResultSchema.load(test['output'])['result']['observed_value']}"
                else:
                    assert result == expectationValidationResultSchema.load(
                        test["output"]
                    ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
            else:
                assert result == expectationValidationResultSchema.load(
                    test["output"]
                ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
        else:
            # Convert result to json since our tests are reading from json so cannot easily contain richer types (e.g. NaN)
            # NOTE - 20191031 - JPC - we may eventually want to change these tests as we update our view on how
            # representations, serializations, and objects should interact and how much of that is shown to the user.
            result = result.to_json_dict()
            for key, value in test["output"].items():
                # Apply our great expectations-specific test logic

                if key == "success":
                    if isinstance(value, (np.floating, float)):
                        try:
                            assert np.allclose(
                                result["success"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['success']} not np.allclose to {value}"
                        except TypeError:
                            assert (
                                result["success"] == value
                            ), f"{result['success']} != {value}"
                    else:
                        assert result["success"] == value, f"{result['success']} != {value}"

                elif key == "observed_value":
                    if "tolerance" in test:
                        if isinstance(value, dict):
                            assert set(result["result"]["observed_value"].keys()) == set(
                                value.keys()
                            ), f"{set(result['result']['observed_value'].keys())} != {set(value.keys())}"
                            for k, v in value.items():
                                assert np.allclose(
                                    result["result"]["observed_value"][k],
                                    v,
                                    rtol=test["tolerance"],
                                )
                        else:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=test["tolerance"],
                            )
                    else:
                        if isinstance(value, dict) and "values" in value:
                            try:
                                assert np.allclose(
                                    result["result"]["observed_value"]["values"],
                                    value["values"],
                                    rtol=RTOL,
                                    atol=ATOL,
                                ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']['values']} not np.allclose to {value['values']}"
                            except TypeError as e:
                                print(e)
                                assert (
                                    result["result"]["observed_value"] == value
                                ), f"{result['result']['observed_value']} != {value}"
                        elif try_allclose:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {value}"
                        else:
                            assert (
                                result["result"]["observed_value"] == value
                            ), f"{result['result']['observed_value']} != {value}"

                # NOTE: This is a key used ONLY for testing cases where an expectation is legitimately allowed to return
                # any of multiple possible observed_values. expect_column_values_to_be_of_type is one such expectation.
                elif key == "observed_value_list":
                    assert result["result"]["observed_value"] in value

                elif key == "unexpected_index_list":
                    if isinstance(data_asset, (SqlAlchemyDataset, SparkDFDataset)):
                        pass
                    elif isinstance(data_asset, (SqlAlchemyBatchData, SparkDFBatchData)):
                        pass
                    else:
>                       assert (
                            result["result"]["unexpected_index_list"] == value
                        ), f"{result['result']['unexpected_index_list']} != {value}"
E                       AssertionError: [{'s2': 'collected', 'pk_index': 2}] != [2]

great_expectations/self_check/util.py:3075: AssertionError
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/11 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/11 [00:00<?, ?it/s]Calculating Metrics:  18%|█▊        | 2/11 [00:00<00:00, 3521.67it/s]Calculating Metrics:  18%|█▊        | 2/11 [00:00<00:00, 305.21it/s]Calculating Metrics:  27%|██▋       | 3/11 [00:00<00:00, 435.73it/s]Calculating Metrics:  27%|██▋       | 3/11 [00:00<00:00, 241.57it/s]Calculating Metrics:  45%|████▌     | 5/11 [00:00<00:00, 354.73it/s]Calculating Metrics:  45%|████▌     | 5/11 [00:00<00:00, 264.76it/s]Calculating Metrics:  64%|██████▎   | 7/11 [00:00<00:00, 338.93it/s]Calculating Metrics:  64%|██████▎   | 7/11 [00:00<00:00, 286.09it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 389.16it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 365.12it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 364.05it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 362.88it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_value_lengths_to_be_between:negative_test_matching_max_min_too_large] _

test_case = {'expectation_type': 'expect_column_value_lengths_to_be_between', 'pk_column': True, 'skip': False, 'test': {'exact_ma...value': 5}, 'out': {'success': False, 'unexpected_index_list': [0, 1], 'unexpected_list': ['calm', 'cool']}, ...}, ...}

    @pytest.mark.order(index=0)
    @pytest.mark.integration
    @pytest.mark.slow  # 12.68s
    def test_case_runner_v3_api(test_case):
        if test_case["skip"]:
            pytest.skip()

        # Note: this should never be done in practice, but we are wiping expectations to reuse batches during testing.
        # test_case["batch"]._initialize_expectations()
        if "parse_strings_as_datetimes" in test_case["test"]["in"]:
            with pytest.deprecated_call():
                evaluate_json_test_v3_api(
                    validator=test_case["validator_with_data"],
                    expectation_type=test_case["expectation_type"],
                    test=test_case["test"],
                    pk_column=test_case["pk_column"],
                )
        else:
>           evaluate_json_test_v3_api(
                validator=test_case["validator_with_data"],
                expectation_type=test_case["expectation_type"],
                test=test_case["test"],
                pk_column=test_case["pk_column"],
            )

tests/test_definitions/test_expectations_v3_api.py:418:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
great_expectations/self_check/util.py:2901: in evaluate_json_test_v3_api
    check_json_test_result(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

test = {'exact_match_out': False, 'in': {'column': 's2', 'max_value': 9, 'min_value': 5}, 'input': {'column': 's2', 'max_valu... 'min_value': 5}, 'out': {'success': False, 'unexpected_index_list': [0, 1], 'unexpected_list': ['calm', 'cool']}, ...}
result = {'exception_info': {'exception_message': None, 'exception_traceback': None, 'raised_exception': False}, 'expectation_c..._percent': 0.0, 'partial_unexpected_counts': [{'count': 1, 'value': 'calm'}, {'count': 1, 'value': 'cool'}], ...}, ...}
data_asset = <great_expectations.execution_engine.pandas_batch_data.PandasBatchData object at 0x7f78063cca00>
pk_column = True

    def check_json_test_result(  # noqa: C901 - 52
        test, result, data_asset=None, pk_column=False
    ) -> None:

        # check for id_pk results in cases where pk_column is true and unexpected_index_list already exists
        # this will work for testing since result_format is COMPLETE
        if pk_column:
            if not result["success"]:
                if "unexpected_index_list" in result["result"]:
                    assert "unexpected_index_query" in result["result"]

        if "unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )
            elif "unexpected_list" in test["output"]:
                (
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )

        if "partial_unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "partial_unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )
            elif "partial_unexpected_list" in test["output"]:
                (
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )

        # Determine if np.allclose(..) might be needed for float comparison
        try_allclose = False
        if "observed_value" in test["output"]:
            if RX_FLOAT.match(repr(test["output"]["observed_value"])):
                try_allclose = True

        # Check results
        if test["exact_match_out"] is True:
            if "result" in result and "observed_value" in result["result"]:
                if isinstance(result["result"]["observed_value"], (np.floating, float)):
                    assert np.allclose(
                        result["result"]["observed_value"],
                        expectationValidationResultSchema.load(test["output"])["result"][
                            "observed_value"
                        ],
                        rtol=RTOL,
                        atol=ATOL,
                    ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {expectationValidationResultSchema.load(test['output'])['result']['observed_value']}"
                else:
                    assert result == expectationValidationResultSchema.load(
                        test["output"]
                    ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
            else:
                assert result == expectationValidationResultSchema.load(
                    test["output"]
                ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
        else:
            # Convert result to json since our tests are reading from json so cannot easily contain richer types (e.g. NaN)
            # NOTE - 20191031 - JPC - we may eventually want to change these tests as we update our view on how
            # representations, serializations, and objects should interact and how much of that is shown to the user.
            result = result.to_json_dict()
            for key, value in test["output"].items():
                # Apply our great expectations-specific test logic

                if key == "success":
                    if isinstance(value, (np.floating, float)):
                        try:
                            assert np.allclose(
                                result["success"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['success']} not np.allclose to {value}"
                        except TypeError:
                            assert (
                                result["success"] == value
                            ), f"{result['success']} != {value}"
                    else:
                        assert result["success"] == value, f"{result['success']} != {value}"

                elif key == "observed_value":
                    if "tolerance" in test:
                        if isinstance(value, dict):
                            assert set(result["result"]["observed_value"].keys()) == set(
                                value.keys()
                            ), f"{set(result['result']['observed_value'].keys())} != {set(value.keys())}"
                            for k, v in value.items():
                                assert np.allclose(
                                    result["result"]["observed_value"][k],
                                    v,
                                    rtol=test["tolerance"],
                                )
                        else:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=test["tolerance"],
                            )
                    else:
                        if isinstance(value, dict) and "values" in value:
                            try:
                                assert np.allclose(
                                    result["result"]["observed_value"]["values"],
                                    value["values"],
                                    rtol=RTOL,
                                    atol=ATOL,
                                ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']['values']} not np.allclose to {value['values']}"
                            except TypeError as e:
                                print(e)
                                assert (
                                    result["result"]["observed_value"] == value
                                ), f"{result['result']['observed_value']} != {value}"
                        elif try_allclose:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {value}"
                        else:
                            assert (
                                result["result"]["observed_value"] == value
                            ), f"{result['result']['observed_value']} != {value}"

                # NOTE: This is a key used ONLY for testing cases where an expectation is legitimately allowed to return
                # any of multiple possible observed_values. expect_column_values_to_be_of_type is one such expectation.
                elif key == "observed_value_list":
                    assert result["result"]["observed_value"] in value

                elif key == "unexpected_index_list":
                    if isinstance(data_asset, (SqlAlchemyDataset, SparkDFDataset)):
                        pass
                    elif isinstance(data_asset, (SqlAlchemyBatchData, SparkDFBatchData)):
                        pass
                    else:
>                       assert (
                            result["result"]["unexpected_index_list"] == value
                        ), f"{result['result']['unexpected_index_list']} != {value}"
E                       AssertionError: [{'s2': 'cool', 'pk_index': 0}, {'s2': 'calm', 'pk_index': 1}] != [0, 1]

great_expectations/self_check/util.py:3075: AssertionError
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/11 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/11 [00:00<?, ?it/s]Calculating Metrics:  18%|█▊        | 2/11 [00:00<00:00, 3992.67it/s]Calculating Metrics:  18%|█▊        | 2/11 [00:00<00:00, 319.59it/s]Calculating Metrics:  27%|██▋       | 3/11 [00:00<00:00, 457.05it/s]Calculating Metrics:  27%|██▋       | 3/11 [00:00<00:00, 250.92it/s]Calculating Metrics:  45%|████▌     | 5/11 [00:00<00:00, 365.77it/s]Calculating Metrics:  45%|████▌     | 5/11 [00:00<00:00, 243.57it/s]Calculating Metrics:  64%|██████▎   | 7/11 [00:00<00:00, 302.91it/s]Calculating Metrics:  64%|██████▎   | 7/11 [00:00<00:00, 253.81it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 348.95it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 328.15it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 327.15it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 326.02it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_value_lengths_to_be_between:negative_test_with_max_lt_min] _

test_case = {'expectation_type': 'expect_column_value_lengths_to_be_between', 'pk_column': True, 'skip': False, 'test': {'exact_ma... 'unexpected_index_list': [0, 1, 2, 3, 4], 'unexpected_list': ['sassy', 'sexy', 'silly', 'slimy', 'smart']}, ...}, ...}

    @pytest.mark.order(index=0)
    @pytest.mark.integration
    @pytest.mark.slow  # 12.68s
    def test_case_runner_v3_api(test_case):
        if test_case["skip"]:
            pytest.skip()

        # Note: this should never be done in practice, but we are wiping expectations to reuse batches during testing.
        # test_case["batch"]._initialize_expectations()
        if "parse_strings_as_datetimes" in test_case["test"]["in"]:
            with pytest.deprecated_call():
                evaluate_json_test_v3_api(
                    validator=test_case["validator_with_data"],
                    expectation_type=test_case["expectation_type"],
                    test=test_case["test"],
                    pk_column=test_case["pk_column"],
                )
        else:
>           evaluate_json_test_v3_api(
                validator=test_case["validator_with_data"],
                expectation_type=test_case["expectation_type"],
                test=test_case["test"],
                pk_column=test_case["pk_column"],
            )

tests/test_definitions/test_expectations_v3_api.py:418:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
great_expectations/self_check/util.py:2901: in evaluate_json_test_v3_api
    check_json_test_result(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

test = {'exact_match_out': False, 'in': {'column': 's1', 'max_value': 0, 'min_value': 1}, 'input': {'column': 's1', 'max_valu...False, 'unexpected_index_list': [0, 1, 2, 3, 4], 'unexpected_list': ['sassy', 'sexy', 'silly', 'slimy', 'smart']}, ...}
result = {'exception_info': {'exception_message': None, 'exception_traceback': None, 'raised_exception': False}, 'expectation_c...': 'sexy'}, {'count': 1, 'value': 'silly'}, {'count': 1, 'value': 'slimy'}, {'count': 1, 'value': 'smart'}], ...}, ...}
data_asset = <great_expectations.execution_engine.pandas_batch_data.PandasBatchData object at 0x7f78063cca00>
pk_column = True

    def check_json_test_result(  # noqa: C901 - 52
        test, result, data_asset=None, pk_column=False
    ) -> None:

        # check for id_pk results in cases where pk_column is true and unexpected_index_list already exists
        # this will work for testing since result_format is COMPLETE
        if pk_column:
            if not result["success"]:
                if "unexpected_index_list" in result["result"]:
                    assert "unexpected_index_query" in result["result"]

        if "unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )
            elif "unexpected_list" in test["output"]:
                (
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )

        if "partial_unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "partial_unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )
            elif "partial_unexpected_list" in test["output"]:
                (
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )

        # Determine if np.allclose(..) might be needed for float comparison
        try_allclose = False
        if "observed_value" in test["output"]:
            if RX_FLOAT.match(repr(test["output"]["observed_value"])):
                try_allclose = True

        # Check results
        if test["exact_match_out"] is True:
            if "result" in result and "observed_value" in result["result"]:
                if isinstance(result["result"]["observed_value"], (np.floating, float)):
                    assert np.allclose(
                        result["result"]["observed_value"],
                        expectationValidationResultSchema.load(test["output"])["result"][
                            "observed_value"
                        ],
                        rtol=RTOL,
                        atol=ATOL,
                    ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {expectationValidationResultSchema.load(test['output'])['result']['observed_value']}"
                else:
                    assert result == expectationValidationResultSchema.load(
                        test["output"]
                    ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
            else:
                assert result == expectationValidationResultSchema.load(
                    test["output"]
                ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
        else:
            # Convert result to json since our tests are reading from json so cannot easily contain richer types (e.g. NaN)
            # NOTE - 20191031 - JPC - we may eventually want to change these tests as we update our view on how
            # representations, serializations, and objects should interact and how much of that is shown to the user.
            result = result.to_json_dict()
            for key, value in test["output"].items():
                # Apply our great expectations-specific test logic

                if key == "success":
                    if isinstance(value, (np.floating, float)):
                        try:
                            assert np.allclose(
                                result["success"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['success']} not np.allclose to {value}"
                        except TypeError:
                            assert (
                                result["success"] == value
                            ), f"{result['success']} != {value}"
                    else:
                        assert result["success"] == value, f"{result['success']} != {value}"

                elif key == "observed_value":
                    if "tolerance" in test:
                        if isinstance(value, dict):
                            assert set(result["result"]["observed_value"].keys()) == set(
                                value.keys()
                            ), f"{set(result['result']['observed_value'].keys())} != {set(value.keys())}"
                            for k, v in value.items():
                                assert np.allclose(
                                    result["result"]["observed_value"][k],
                                    v,
                                    rtol=test["tolerance"],
                                )
                        else:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=test["tolerance"],
                            )
                    else:
                        if isinstance(value, dict) and "values" in value:
                            try:
                                assert np.allclose(
                                    result["result"]["observed_value"]["values"],
                                    value["values"],
                                    rtol=RTOL,
                                    atol=ATOL,
                                ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']['values']} not np.allclose to {value['values']}"
                            except TypeError as e:
                                print(e)
                                assert (
                                    result["result"]["observed_value"] == value
                                ), f"{result['result']['observed_value']} != {value}"
                        elif try_allclose:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {value}"
                        else:
                            assert (
                                result["result"]["observed_value"] == value
                            ), f"{result['result']['observed_value']} != {value}"

                # NOTE: This is a key used ONLY for testing cases where an expectation is legitimately allowed to return
                # any of multiple possible observed_values. expect_column_values_to_be_of_type is one such expectation.
                elif key == "observed_value_list":
                    assert result["result"]["observed_value"] in value

                elif key == "unexpected_index_list":
                    if isinstance(data_asset, (SqlAlchemyDataset, SparkDFDataset)):
                        pass
                    elif isinstance(data_asset, (SqlAlchemyBatchData, SparkDFBatchData)):
                        pass
                    else:
>                       assert (
                            result["result"]["unexpected_index_list"] == value
                        ), f"{result['result']['unexpected_index_list']} != {value}"
E                       AssertionError: [{'s1': 'smart', 'pk_index': 0}, {'s1': 'silly', 'pk_index': 1}, {'s1': 'sassy', 'pk_index': 2}, {'s1': 'slimy', 'pk_index': 3}, {'s1': 'sexy', 'pk_index': 4}] != [0, 1, 2, 3, 4]

great_expectations/self_check/util.py:3075: AssertionError
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/11 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/11 [00:00<?, ?it/s]Calculating Metrics:  18%|█▊        | 2/11 [00:00<00:00, 2542.00it/s]Calculating Metrics:  18%|█▊        | 2/11 [00:00<00:00, 229.18it/s]Calculating Metrics:  27%|██▋       | 3/11 [00:00<00:00, 331.06it/s]Calculating Metrics:  27%|██▋       | 3/11 [00:00<00:00, 206.74it/s]Calculating Metrics:  45%|████▌     | 5/11 [00:00<00:00, 308.85it/s]Calculating Metrics:  45%|████▌     | 5/11 [00:00<00:00, 235.94it/s]Calculating Metrics:  64%|██████▎   | 7/11 [00:00<00:00, 304.63it/s]Calculating Metrics:  64%|██████▎   | 7/11 [00:00<00:00, 261.84it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 361.48it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 339.94it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 338.90it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 337.81it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_value_lengths_to_be_between:test_conditional_expectation_fails] _

test_case = {'expectation_type': 'expect_column_value_lengths_to_be_between', 'pk_column': True, 'skip': False, 'test': {'exact_ma...ceptions': True, 'column': 's1', 'condition_parser': 'pandas', 'max_value': 4, ...}, 'only_for': ['pandas'], ...}, ...}

    @pytest.mark.order(index=0)
    @pytest.mark.integration
    @pytest.mark.slow  # 12.68s
    def test_case_runner_v3_api(test_case):
        if test_case["skip"]:
            pytest.skip()

        # Note: this should never be done in practice, but we are wiping expectations to reuse batches during testing.
        # test_case["batch"]._initialize_expectations()
        if "parse_strings_as_datetimes" in test_case["test"]["in"]:
            with pytest.deprecated_call():
                evaluate_json_test_v3_api(
                    validator=test_case["validator_with_data"],
                    expectation_type=test_case["expectation_type"],
                    test=test_case["test"],
                    pk_column=test_case["pk_column"],
                )
        else:
>           evaluate_json_test_v3_api(
                validator=test_case["validator_with_data"],
                expectation_type=test_case["expectation_type"],
                test=test_case["test"],
                pk_column=test_case["pk_column"],
            )

tests/test_definitions/test_expectations_v3_api.py:418:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
great_expectations/self_check/util.py:2901: in evaluate_json_test_v3_api
    check_json_test_result(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

test = {'exact_match_out': False, 'in': {'catch_exceptions': True, 'column': 's1', 'condition_parser': 'pandas', 'max_value':...tch_exceptions': True, 'column': 's1', 'condition_parser': 'pandas', 'max_value': 4, ...}, 'only_for': ['pandas'], ...}
result = {'exception_info': {'exception_message': None, 'exception_traceback': None, 'raised_exception': False}, 'expectation_c..., 'missing_count': 0, 'missing_percent': 0.0, 'partial_unexpected_counts': [{'count': 1, 'value': 'slimy'}], ...}, ...}
data_asset = <great_expectations.execution_engine.pandas_batch_data.PandasBatchData object at 0x7f78063cca00>
pk_column = True

    def check_json_test_result(  # noqa: C901 - 52
        test, result, data_asset=None, pk_column=False
    ) -> None:

        # check for id_pk results in cases where pk_column is true and unexpected_index_list already exists
        # this will work for testing since result_format is COMPLETE
        if pk_column:
            if not result["success"]:
                if "unexpected_index_list" in result["result"]:
                    assert "unexpected_index_query" in result["result"]

        if "unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )
            elif "unexpected_list" in test["output"]:
                (
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )

        if "partial_unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "partial_unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )
            elif "partial_unexpected_list" in test["output"]:
                (
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )

        # Determine if np.allclose(..) might be needed for float comparison
        try_allclose = False
        if "observed_value" in test["output"]:
            if RX_FLOAT.match(repr(test["output"]["observed_value"])):
                try_allclose = True

        # Check results
        if test["exact_match_out"] is True:
            if "result" in result and "observed_value" in result["result"]:
                if isinstance(result["result"]["observed_value"], (np.floating, float)):
                    assert np.allclose(
                        result["result"]["observed_value"],
                        expectationValidationResultSchema.load(test["output"])["result"][
                            "observed_value"
                        ],
                        rtol=RTOL,
                        atol=ATOL,
                    ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {expectationValidationResultSchema.load(test['output'])['result']['observed_value']}"
                else:
                    assert result == expectationValidationResultSchema.load(
                        test["output"]
                    ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
            else:
                assert result == expectationValidationResultSchema.load(
                    test["output"]
                ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
        else:
            # Convert result to json since our tests are reading from json so cannot easily contain richer types (e.g. NaN)
            # NOTE - 20191031 - JPC - we may eventually want to change these tests as we update our view on how
            # representations, serializations, and objects should interact and how much of that is shown to the user.
            result = result.to_json_dict()
            for key, value in test["output"].items():
                # Apply our great expectations-specific test logic

                if key == "success":
                    if isinstance(value, (np.floating, float)):
                        try:
                            assert np.allclose(
                                result["success"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['success']} not np.allclose to {value}"
                        except TypeError:
                            assert (
                                result["success"] == value
                            ), f"{result['success']} != {value}"
                    else:
                        assert result["success"] == value, f"{result['success']} != {value}"

                elif key == "observed_value":
                    if "tolerance" in test:
                        if isinstance(value, dict):
                            assert set(result["result"]["observed_value"].keys()) == set(
                                value.keys()
                            ), f"{set(result['result']['observed_value'].keys())} != {set(value.keys())}"
                            for k, v in value.items():
                                assert np.allclose(
                                    result["result"]["observed_value"][k],
                                    v,
                                    rtol=test["tolerance"],
                                )
                        else:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=test["tolerance"],
                            )
                    else:
                        if isinstance(value, dict) and "values" in value:
                            try:
                                assert np.allclose(
                                    result["result"]["observed_value"]["values"],
                                    value["values"],
                                    rtol=RTOL,
                                    atol=ATOL,
                                ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']['values']} not np.allclose to {value['values']}"
                            except TypeError as e:
                                print(e)
                                assert (
                                    result["result"]["observed_value"] == value
                                ), f"{result['result']['observed_value']} != {value}"
                        elif try_allclose:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {value}"
                        else:
                            assert (
                                result["result"]["observed_value"] == value
                            ), f"{result['result']['observed_value']} != {value}"

                # NOTE: This is a key used ONLY for testing cases where an expectation is legitimately allowed to return
                # any of multiple possible observed_values. expect_column_values_to_be_of_type is one such expectation.
                elif key == "observed_value_list":
                    assert result["result"]["observed_value"] in value

                elif key == "unexpected_index_list":
                    if isinstance(data_asset, (SqlAlchemyDataset, SparkDFDataset)):
                        pass
                    elif isinstance(data_asset, (SqlAlchemyBatchData, SparkDFBatchData)):
                        pass
                    else:
>                       assert (
                            result["result"]["unexpected_index_list"] == value
                        ), f"{result['result']['unexpected_index_list']} != {value}"
E                       AssertionError: [{'s1': 'slimy', 'pk_index': 3}] != [3]

great_expectations/self_check/util.py:3075: AssertionError
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/11 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/11 [00:00<?, ?it/s]Calculating Metrics:  18%|█▊        | 2/11 [00:00<00:00, 788.33it/s]Calculating Metrics:  18%|█▊        | 2/11 [00:00<00:00, 298.55it/s]Calculating Metrics:  27%|██▋       | 3/11 [00:00<00:00, 433.46it/s]Calculating Metrics:  27%|██▋       | 3/11 [00:00<00:00, 273.82it/s]Calculating Metrics:  45%|████▌     | 5/11 [00:00<00:00, 354.03it/s]Calculating Metrics:  45%|████▌     | 5/11 [00:00<00:00, 279.26it/s]Calculating Metrics:  64%|██████▎   | 7/11 [00:00<00:00, 345.71it/s]Calculating Metrics:  64%|██████▎   | 7/11 [00:00<00:00, 298.87it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 365.87it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 348.19it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 347.04it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 345.85it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_unique:basic_negative_case_all_non_unique_character_values] _

test_case = {'expectation_type': 'expect_column_values_to_be_unique', 'pk_column': True, 'skip': False, 'test': {'exact_match_out': False, 'in': {'column': 'a'}, 'include_in_gallery': True, 'input': {'column': 'a'}, ...}, ...}

    @pytest.mark.order(index=0)
    @pytest.mark.integration
    @pytest.mark.slow  # 12.68s
    def test_case_runner_v3_api(test_case):
        if test_case["skip"]:
            pytest.skip()

        # Note: this should never be done in practice, but we are wiping expectations to reuse batches during testing.
        # test_case["batch"]._initialize_expectations()
        if "parse_strings_as_datetimes" in test_case["test"]["in"]:
            with pytest.deprecated_call():
                evaluate_json_test_v3_api(
                    validator=test_case["validator_with_data"],
                    expectation_type=test_case["expectation_type"],
                    test=test_case["test"],
                    pk_column=test_case["pk_column"],
                )
        else:
>           evaluate_json_test_v3_api(
                validator=test_case["validator_with_data"],
                expectation_type=test_case["expectation_type"],
                test=test_case["test"],
                pk_column=test_case["pk_column"],
            )

tests/test_definitions/test_expectations_v3_api.py:418:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
great_expectations/self_check/util.py:2901: in evaluate_json_test_v3_api
    check_json_test_result(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

test = {'exact_match_out': False, 'in': {'column': 'a'}, 'include_in_gallery': True, 'input': {'column': 'a'}, ...}
result = {'exception_info': {'exception_message': None, 'exception_traceback': None, 'raised_exception': False}, 'expectation_c...: 4, 'missing_count': 1, 'missing_percent': 25.0, 'partial_unexpected_counts': [{'count': 3, 'value': '2'}], ...}, ...}
data_asset = <great_expectations.execution_engine.pandas_batch_data.PandasBatchData object at 0x7f7806497b50>
pk_column = True

    def check_json_test_result(  # noqa: C901 - 52
        test, result, data_asset=None, pk_column=False
    ) -> None:

        # check for id_pk results in cases where pk_column is true and unexpected_index_list already exists
        # this will work for testing since result_format is COMPLETE
        if pk_column:
            if not result["success"]:
                if "unexpected_index_list" in result["result"]:
                    assert "unexpected_index_query" in result["result"]

        if "unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )
            elif "unexpected_list" in test["output"]:
                (
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )

        if "partial_unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "partial_unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )
            elif "partial_unexpected_list" in test["output"]:
                (
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )

        # Determine if np.allclose(..) might be needed for float comparison
        try_allclose = False
        if "observed_value" in test["output"]:
            if RX_FLOAT.match(repr(test["output"]["observed_value"])):
                try_allclose = True

        # Check results
        if test["exact_match_out"] is True:
            if "result" in result and "observed_value" in result["result"]:
                if isinstance(result["result"]["observed_value"], (np.floating, float)):
                    assert np.allclose(
                        result["result"]["observed_value"],
                        expectationValidationResultSchema.load(test["output"])["result"][
                            "observed_value"
                        ],
                        rtol=RTOL,
                        atol=ATOL,
                    ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {expectationValidationResultSchema.load(test['output'])['result']['observed_value']}"
                else:
                    assert result == expectationValidationResultSchema.load(
                        test["output"]
                    ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
            else:
                assert result == expectationValidationResultSchema.load(
                    test["output"]
                ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
        else:
            # Convert result to json since our tests are reading from json so cannot easily contain richer types (e.g. NaN)
            # NOTE - 20191031 - JPC - we may eventually want to change these tests as we update our view on how
            # representations, serializations, and objects should interact and how much of that is shown to the user.
            result = result.to_json_dict()
            for key, value in test["output"].items():
                # Apply our great expectations-specific test logic

                if key == "success":
                    if isinstance(value, (np.floating, float)):
                        try:
                            assert np.allclose(
                                result["success"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['success']} not np.allclose to {value}"
                        except TypeError:
                            assert (
                                result["success"] == value
                            ), f"{result['success']} != {value}"
                    else:
                        assert result["success"] == value, f"{result['success']} != {value}"

                elif key == "observed_value":
                    if "tolerance" in test:
                        if isinstance(value, dict):
                            assert set(result["result"]["observed_value"].keys()) == set(
                                value.keys()
                            ), f"{set(result['result']['observed_value'].keys())} != {set(value.keys())}"
                            for k, v in value.items():
                                assert np.allclose(
                                    result["result"]["observed_value"][k],
                                    v,
                                    rtol=test["tolerance"],
                                )
                        else:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=test["tolerance"],
                            )
                    else:
                        if isinstance(value, dict) and "values" in value:
                            try:
                                assert np.allclose(
                                    result["result"]["observed_value"]["values"],
                                    value["values"],
                                    rtol=RTOL,
                                    atol=ATOL,
                                ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']['values']} not np.allclose to {value['values']}"
                            except TypeError as e:
                                print(e)
                                assert (
                                    result["result"]["observed_value"] == value
                                ), f"{result['result']['observed_value']} != {value}"
                        elif try_allclose:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {value}"
                        else:
                            assert (
                                result["result"]["observed_value"] == value
                            ), f"{result['result']['observed_value']} != {value}"

                # NOTE: This is a key used ONLY for testing cases where an expectation is legitimately allowed to return
                # any of multiple possible observed_values. expect_column_values_to_be_of_type is one such expectation.
                elif key == "observed_value_list":
                    assert result["result"]["observed_value"] in value

                elif key == "unexpected_index_list":
                    if isinstance(data_asset, (SqlAlchemyDataset, SparkDFDataset)):
                        pass
                    elif isinstance(data_asset, (SqlAlchemyBatchData, SparkDFBatchData)):
                        pass
                    else:
>                       assert (
                            result["result"]["unexpected_index_list"] == value
                        ), f"{result['result']['unexpected_index_list']} != {value}"
E                       AssertionError: [{'a': '2', 'pk_index': 0}, {'a': '2', 'pk_index': 1}, {'a': '2', 'pk_index': 2}] != [0, 1, 2]

great_expectations/self_check/util.py:3075: AssertionError
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 3985.09it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 466.32it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 652.03it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 365.54it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 509.69it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 375.77it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 532.54it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 482.88it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 479.15it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 476.60it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_unique:multi_type_column_contains_2_and_quoted_2_suppressed_for_sqalchemy] _

test_case = {'expectation_type': 'expect_column_values_to_be_unique', 'pk_column': True, 'skip': False, 'test': {'exact_match_out'...: 'c', 'mostly': 0.3}, 'out': {'success': True, 'unexpected_index_list': [0, 1], 'unexpected_list': [1, 1]}, ...}, ...}

    @pytest.mark.order(index=0)
    @pytest.mark.integration
    @pytest.mark.slow  # 12.68s
    def test_case_runner_v3_api(test_case):
        if test_case["skip"]:
            pytest.skip()

        # Note: this should never be done in practice, but we are wiping expectations to reuse batches during testing.
        # test_case["batch"]._initialize_expectations()
        if "parse_strings_as_datetimes" in test_case["test"]["in"]:
            with pytest.deprecated_call():
                evaluate_json_test_v3_api(
                    validator=test_case["validator_with_data"],
                    expectation_type=test_case["expectation_type"],
                    test=test_case["test"],
                    pk_column=test_case["pk_column"],
                )
        else:
>           evaluate_json_test_v3_api(
                validator=test_case["validator_with_data"],
                expectation_type=test_case["expectation_type"],
                test=test_case["test"],
                pk_column=test_case["pk_column"],
            )

tests/test_definitions/test_expectations_v3_api.py:418:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
great_expectations/self_check/util.py:2901: in evaluate_json_test_v3_api
    check_json_test_result(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

test = {'exact_match_out': False, 'in': {'column': 'c', 'mostly': 0.3}, 'input': {'column': 'c', 'mostly': 0.3}, 'out': {'success': True, 'unexpected_index_list': [0, 1], 'unexpected_list': [1, 1]}, ...}
result = {'exception_info': {'exception_message': None, 'exception_traceback': None, 'raised_exception': False}, 'expectation_c...: 4, 'missing_count': 1, 'missing_percent': 25.0, 'partial_unexpected_counts': [{'count': 2, 'value': 1.0}], ...}, ...}
data_asset = <great_expectations.execution_engine.pandas_batch_data.PandasBatchData object at 0x7f7806497b50>
pk_column = True

    def check_json_test_result(  # noqa: C901 - 52
        test, result, data_asset=None, pk_column=False
    ) -> None:

        # check for id_pk results in cases where pk_column is true and unexpected_index_list already exists
        # this will work for testing since result_format is COMPLETE
        if pk_column:
            if not result["success"]:
                if "unexpected_index_list" in result["result"]:
                    assert "unexpected_index_query" in result["result"]

        if "unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )
            elif "unexpected_list" in test["output"]:
                (
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )

        if "partial_unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "partial_unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )
            elif "partial_unexpected_list" in test["output"]:
                (
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )

        # Determine if np.allclose(..) might be needed for float comparison
        try_allclose = False
        if "observed_value" in test["output"]:
            if RX_FLOAT.match(repr(test["output"]["observed_value"])):
                try_allclose = True

        # Check results
        if test["exact_match_out"] is True:
            if "result" in result and "observed_value" in result["result"]:
                if isinstance(result["result"]["observed_value"], (np.floating, float)):
                    assert np.allclose(
                        result["result"]["observed_value"],
                        expectationValidationResultSchema.load(test["output"])["result"][
                            "observed_value"
                        ],
                        rtol=RTOL,
                        atol=ATOL,
                    ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {expectationValidationResultSchema.load(test['output'])['result']['observed_value']}"
                else:
                    assert result == expectationValidationResultSchema.load(
                        test["output"]
                    ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
            else:
                assert result == expectationValidationResultSchema.load(
                    test["output"]
                ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
        else:
            # Convert result to json since our tests are reading from json so cannot easily contain richer types (e.g. NaN)
            # NOTE - 20191031 - JPC - we may eventually want to change these tests as we update our view on how
            # representations, serializations, and objects should interact and how much of that is shown to the user.
            result = result.to_json_dict()
            for key, value in test["output"].items():
                # Apply our great expectations-specific test logic

                if key == "success":
                    if isinstance(value, (np.floating, float)):
                        try:
                            assert np.allclose(
                                result["success"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['success']} not np.allclose to {value}"
                        except TypeError:
                            assert (
                                result["success"] == value
                            ), f"{result['success']} != {value}"
                    else:
                        assert result["success"] == value, f"{result['success']} != {value}"

                elif key == "observed_value":
                    if "tolerance" in test:
                        if isinstance(value, dict):
                            assert set(result["result"]["observed_value"].keys()) == set(
                                value.keys()
                            ), f"{set(result['result']['observed_value'].keys())} != {set(value.keys())}"
                            for k, v in value.items():
                                assert np.allclose(
                                    result["result"]["observed_value"][k],
                                    v,
                                    rtol=test["tolerance"],
                                )
                        else:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=test["tolerance"],
                            )
                    else:
                        if isinstance(value, dict) and "values" in value:
                            try:
                                assert np.allclose(
                                    result["result"]["observed_value"]["values"],
                                    value["values"],
                                    rtol=RTOL,
                                    atol=ATOL,
                                ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']['values']} not np.allclose to {value['values']}"
                            except TypeError as e:
                                print(e)
                                assert (
                                    result["result"]["observed_value"] == value
                                ), f"{result['result']['observed_value']} != {value}"
                        elif try_allclose:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {value}"
                        else:
                            assert (
                                result["result"]["observed_value"] == value
                            ), f"{result['result']['observed_value']} != {value}"

                # NOTE: This is a key used ONLY for testing cases where an expectation is legitimately allowed to return
                # any of multiple possible observed_values. expect_column_values_to_be_of_type is one such expectation.
                elif key == "observed_value_list":
                    assert result["result"]["observed_value"] in value

                elif key == "unexpected_index_list":
                    if isinstance(data_asset, (SqlAlchemyDataset, SparkDFDataset)):
                        pass
                    elif isinstance(data_asset, (SqlAlchemyBatchData, SparkDFBatchData)):
                        pass
                    else:
>                       assert (
                            result["result"]["unexpected_index_list"] == value
                        ), f"{result['result']['unexpected_index_list']} != {value}"
E                       AssertionError: [{'c': 1.0, 'pk_index': 0}, {'c': 1.0, 'pk_index': 1}] != [0, 1]

great_expectations/self_check/util.py:3075: AssertionError
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 2328.23it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 335.29it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 472.58it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 277.27it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 401.61it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 328.75it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 531.15it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 500.23it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 498.18it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 495.76it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_unique:positive_case_using_mostly] _

test_case = {'expectation_type': 'expect_column_values_to_be_unique', 'pk_column': True, 'skip': False, 'test': {'exact_match_out'...: 'c', 'mostly': 0.3}, 'out': {'success': True, 'unexpected_index_list': [0, 1], 'unexpected_list': [1, 1]}, ...}, ...}

    @pytest.mark.order(index=0)
    @pytest.mark.integration
    @pytest.mark.slow  # 12.68s
    def test_case_runner_v3_api(test_case):
        if test_case["skip"]:
            pytest.skip()

        # Note: this should never be done in practice, but we are wiping expectations to reuse batches during testing.
        # test_case["batch"]._initialize_expectations()
        if "parse_strings_as_datetimes" in test_case["test"]["in"]:
            with pytest.deprecated_call():
                evaluate_json_test_v3_api(
                    validator=test_case["validator_with_data"],
                    expectation_type=test_case["expectation_type"],
                    test=test_case["test"],
                    pk_column=test_case["pk_column"],
                )
        else:
>           evaluate_json_test_v3_api(
                validator=test_case["validator_with_data"],
                expectation_type=test_case["expectation_type"],
                test=test_case["test"],
                pk_column=test_case["pk_column"],
            )

tests/test_definitions/test_expectations_v3_api.py:418:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
great_expectations/self_check/util.py:2901: in evaluate_json_test_v3_api
    check_json_test_result(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

test = {'exact_match_out': False, 'in': {'column': 'c', 'mostly': 0.3}, 'input': {'column': 'c', 'mostly': 0.3}, 'out': {'success': True, 'unexpected_index_list': [0, 1], 'unexpected_list': [1, 1]}, ...}
result = {'exception_info': {'exception_message': None, 'exception_traceback': None, 'raised_exception': False}, 'expectation_c...: 4, 'missing_count': 1, 'missing_percent': 25.0, 'partial_unexpected_counts': [{'count': 2, 'value': 1.0}], ...}, ...}
data_asset = <great_expectations.execution_engine.pandas_batch_data.PandasBatchData object at 0x7f7806497b50>
pk_column = True

    def check_json_test_result(  # noqa: C901 - 52
        test, result, data_asset=None, pk_column=False
    ) -> None:

        # check for id_pk results in cases where pk_column is true and unexpected_index_list already exists
        # this will work for testing since result_format is COMPLETE
        if pk_column:
            if not result["success"]:
                if "unexpected_index_list" in result["result"]:
                    assert "unexpected_index_query" in result["result"]

        if "unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )
            elif "unexpected_list" in test["output"]:
                (
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )

        if "partial_unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "partial_unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )
            elif "partial_unexpected_list" in test["output"]:
                (
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )

        # Determine if np.allclose(..) might be needed for float comparison
        try_allclose = False
        if "observed_value" in test["output"]:
            if RX_FLOAT.match(repr(test["output"]["observed_value"])):
                try_allclose = True

        # Check results
        if test["exact_match_out"] is True:
            if "result" in result and "observed_value" in result["result"]:
                if isinstance(result["result"]["observed_value"], (np.floating, float)):
                    assert np.allclose(
                        result["result"]["observed_value"],
                        expectationValidationResultSchema.load(test["output"])["result"][
                            "observed_value"
                        ],
                        rtol=RTOL,
                        atol=ATOL,
                    ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {expectationValidationResultSchema.load(test['output'])['result']['observed_value']}"
                else:
                    assert result == expectationValidationResultSchema.load(
                        test["output"]
                    ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
            else:
                assert result == expectationValidationResultSchema.load(
                    test["output"]
                ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
        else:
            # Convert result to json since our tests are reading from json so cannot easily contain richer types (e.g. NaN)
            # NOTE - 20191031 - JPC - we may eventually want to change these tests as we update our view on how
            # representations, serializations, and objects should interact and how much of that is shown to the user.
            result = result.to_json_dict()
            for key, value in test["output"].items():
                # Apply our great expectations-specific test logic

                if key == "success":
                    if isinstance(value, (np.floating, float)):
                        try:
                            assert np.allclose(
                                result["success"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['success']} not np.allclose to {value}"
                        except TypeError:
                            assert (
                                result["success"] == value
                            ), f"{result['success']} != {value}"
                    else:
                        assert result["success"] == value, f"{result['success']} != {value}"

                elif key == "observed_value":
                    if "tolerance" in test:
                        if isinstance(value, dict):
                            assert set(result["result"]["observed_value"].keys()) == set(
                                value.keys()
                            ), f"{set(result['result']['observed_value'].keys())} != {set(value.keys())}"
                            for k, v in value.items():
                                assert np.allclose(
                                    result["result"]["observed_value"][k],
                                    v,
                                    rtol=test["tolerance"],
                                )
                        else:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=test["tolerance"],
                            )
                    else:
                        if isinstance(value, dict) and "values" in value:
                            try:
                                assert np.allclose(
                                    result["result"]["observed_value"]["values"],
                                    value["values"],
                                    rtol=RTOL,
                                    atol=ATOL,
                                ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']['values']} not np.allclose to {value['values']}"
                            except TypeError as e:
                                print(e)
                                assert (
                                    result["result"]["observed_value"] == value
                                ), f"{result['result']['observed_value']} != {value}"
                        elif try_allclose:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {value}"
                        else:
                            assert (
                                result["result"]["observed_value"] == value
                            ), f"{result['result']['observed_value']} != {value}"

                # NOTE: This is a key used ONLY for testing cases where an expectation is legitimately allowed to return
                # any of multiple possible observed_values. expect_column_values_to_be_of_type is one such expectation.
                elif key == "observed_value_list":
                    assert result["result"]["observed_value"] in value

                elif key == "unexpected_index_list":
                    if isinstance(data_asset, (SqlAlchemyDataset, SparkDFDataset)):
                        pass
                    elif isinstance(data_asset, (SqlAlchemyBatchData, SparkDFBatchData)):
                        pass
                    else:
>                       assert (
                            result["result"]["unexpected_index_list"] == value
                        ), f"{result['result']['unexpected_index_list']} != {value}"
E                       AssertionError: [{'c': 1.0, 'pk_index': 0}, {'c': 1.0, 'pk_index': 1}] != [0, 1]

great_expectations/self_check/util.py:3075: AssertionError
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 3839.18it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 467.07it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 652.47it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 374.35it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 522.68it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 403.16it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 618.44it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 574.35it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 571.00it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 567.66it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_unique:negative_case_using_mostly] _

test_case = {'expectation_type': 'expect_column_values_to_be_unique', 'pk_column': True, 'skip': False, 'test': {'exact_match_out'... 'c', 'mostly': 0.4}, 'out': {'success': False, 'unexpected_index_list': [0, 1], 'unexpected_list': [1, 1]}, ...}, ...}

    @pytest.mark.order(index=0)
    @pytest.mark.integration
    @pytest.mark.slow  # 12.68s
    def test_case_runner_v3_api(test_case):
        if test_case["skip"]:
            pytest.skip()

        # Note: this should never be done in practice, but we are wiping expectations to reuse batches during testing.
        # test_case["batch"]._initialize_expectations()
        if "parse_strings_as_datetimes" in test_case["test"]["in"]:
            with pytest.deprecated_call():
                evaluate_json_test_v3_api(
                    validator=test_case["validator_with_data"],
                    expectation_type=test_case["expectation_type"],
                    test=test_case["test"],
                    pk_column=test_case["pk_column"],
                )
        else:
>           evaluate_json_test_v3_api(
                validator=test_case["validator_with_data"],
                expectation_type=test_case["expectation_type"],
                test=test_case["test"],
                pk_column=test_case["pk_column"],
            )

tests/test_definitions/test_expectations_v3_api.py:418:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
great_expectations/self_check/util.py:2901: in evaluate_json_test_v3_api
    check_json_test_result(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

test = {'exact_match_out': False, 'in': {'column': 'c', 'mostly': 0.4}, 'input': {'column': 'c', 'mostly': 0.4}, 'out': {'success': False, 'unexpected_index_list': [0, 1], 'unexpected_list': [1, 1]}, ...}
result = {'exception_info': {'exception_message': None, 'exception_traceback': None, 'raised_exception': False}, 'expectation_c...: 4, 'missing_count': 1, 'missing_percent': 25.0, 'partial_unexpected_counts': [{'count': 2, 'value': 1.0}], ...}, ...}
data_asset = <great_expectations.execution_engine.pandas_batch_data.PandasBatchData object at 0x7f7806497b50>
pk_column = True

    def check_json_test_result(  # noqa: C901 - 52
        test, result, data_asset=None, pk_column=False
    ) -> None:

        # check for id_pk results in cases where pk_column is true and unexpected_index_list already exists
        # this will work for testing since result_format is COMPLETE
        if pk_column:
            if not result["success"]:
                if "unexpected_index_list" in result["result"]:
                    assert "unexpected_index_query" in result["result"]

        if "unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )
            elif "unexpected_list" in test["output"]:
                (
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )

        if "partial_unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "partial_unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )
            elif "partial_unexpected_list" in test["output"]:
                (
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )

        # Determine if np.allclose(..) might be needed for float comparison
        try_allclose = False
        if "observed_value" in test["output"]:
            if RX_FLOAT.match(repr(test["output"]["observed_value"])):
                try_allclose = True

        # Check results
        if test["exact_match_out"] is True:
            if "result" in result and "observed_value" in result["result"]:
                if isinstance(result["result"]["observed_value"], (np.floating, float)):
                    assert np.allclose(
                        result["result"]["observed_value"],
                        expectationValidationResultSchema.load(test["output"])["result"][
                            "observed_value"
                        ],
                        rtol=RTOL,
                        atol=ATOL,
                    ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {expectationValidationResultSchema.load(test['output'])['result']['observed_value']}"
                else:
                    assert result == expectationValidationResultSchema.load(
                        test["output"]
                    ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
            else:
                assert result == expectationValidationResultSchema.load(
                    test["output"]
                ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
        else:
            # Convert result to json since our tests are reading from json so cannot easily contain richer types (e.g. NaN)
            # NOTE - 20191031 - JPC - we may eventually want to change these tests as we update our view on how
            # representations, serializations, and objects should interact and how much of that is shown to the user.
            result = result.to_json_dict()
            for key, value in test["output"].items():
                # Apply our great expectations-specific test logic

                if key == "success":
                    if isinstance(value, (np.floating, float)):
                        try:
                            assert np.allclose(
                                result["success"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['success']} not np.allclose to {value}"
                        except TypeError:
                            assert (
                                result["success"] == value
                            ), f"{result['success']} != {value}"
                    else:
                        assert result["success"] == value, f"{result['success']} != {value}"

                elif key == "observed_value":
                    if "tolerance" in test:
                        if isinstance(value, dict):
                            assert set(result["result"]["observed_value"].keys()) == set(
                                value.keys()
                            ), f"{set(result['result']['observed_value'].keys())} != {set(value.keys())}"
                            for k, v in value.items():
                                assert np.allclose(
                                    result["result"]["observed_value"][k],
                                    v,
                                    rtol=test["tolerance"],
                                )
                        else:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=test["tolerance"],
                            )
                    else:
                        if isinstance(value, dict) and "values" in value:
                            try:
                                assert np.allclose(
                                    result["result"]["observed_value"]["values"],
                                    value["values"],
                                    rtol=RTOL,
                                    atol=ATOL,
                                ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']['values']} not np.allclose to {value['values']}"
                            except TypeError as e:
                                print(e)
                                assert (
                                    result["result"]["observed_value"] == value
                                ), f"{result['result']['observed_value']} != {value}"
                        elif try_allclose:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {value}"
                        else:
                            assert (
                                result["result"]["observed_value"] == value
                            ), f"{result['result']['observed_value']} != {value}"

                # NOTE: This is a key used ONLY for testing cases where an expectation is legitimately allowed to return
                # any of multiple possible observed_values. expect_column_values_to_be_of_type is one such expectation.
                elif key == "observed_value_list":
                    assert result["result"]["observed_value"] in value

                elif key == "unexpected_index_list":
                    if isinstance(data_asset, (SqlAlchemyDataset, SparkDFDataset)):
                        pass
                    elif isinstance(data_asset, (SqlAlchemyBatchData, SparkDFBatchData)):
                        pass
                    else:
>                       assert (
                            result["result"]["unexpected_index_list"] == value
                        ), f"{result['result']['unexpected_index_list']} != {value}"
E                       AssertionError: [{'c': 1.0, 'pk_index': 0}, {'c': 1.0, 'pk_index': 1}] != [0, 1]

great_expectations/self_check/util.py:3075: AssertionError
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 3804.36it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 473.83it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 664.04it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 307.61it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 396.29it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 309.18it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 492.45it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 461.00it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 458.85it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 456.71it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_unique:negative_case_non_unique_numeric_values] _

test_case = {'expectation_type': 'expect_column_values_to_be_unique', 'pk_column': True, 'skip': False, 'test': {'exact_match_out'...ut': {'column': 'c'}, 'out': {'success': False, 'unexpected_index_list': [0, 1], 'unexpected_list': [1, 1]}, ...}, ...}

    @pytest.mark.order(index=0)
    @pytest.mark.integration
    @pytest.mark.slow  # 12.68s
    def test_case_runner_v3_api(test_case):
        if test_case["skip"]:
            pytest.skip()

        # Note: this should never be done in practice, but we are wiping expectations to reuse batches during testing.
        # test_case["batch"]._initialize_expectations()
        if "parse_strings_as_datetimes" in test_case["test"]["in"]:
            with pytest.deprecated_call():
                evaluate_json_test_v3_api(
                    validator=test_case["validator_with_data"],
                    expectation_type=test_case["expectation_type"],
                    test=test_case["test"],
                    pk_column=test_case["pk_column"],
                )
        else:
>           evaluate_json_test_v3_api(
                validator=test_case["validator_with_data"],
                expectation_type=test_case["expectation_type"],
                test=test_case["test"],
                pk_column=test_case["pk_column"],
            )

tests/test_definitions/test_expectations_v3_api.py:418:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
great_expectations/self_check/util.py:2901: in evaluate_json_test_v3_api
    check_json_test_result(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

test = {'exact_match_out': False, 'in': {'column': 'c'}, 'input': {'column': 'c'}, 'out': {'success': False, 'unexpected_index_list': [0, 1], 'unexpected_list': [1, 1]}, ...}
result = {'exception_info': {'exception_message': None, 'exception_traceback': None, 'raised_exception': False}, 'expectation_c...: 4, 'missing_count': 1, 'missing_percent': 25.0, 'partial_unexpected_counts': [{'count': 2, 'value': 1.0}], ...}, ...}
data_asset = <great_expectations.execution_engine.pandas_batch_data.PandasBatchData object at 0x7f7806497b50>
pk_column = True

    def check_json_test_result(  # noqa: C901 - 52
        test, result, data_asset=None, pk_column=False
    ) -> None:

        # check for id_pk results in cases where pk_column is true and unexpected_index_list already exists
        # this will work for testing since result_format is COMPLETE
        if pk_column:
            if not result["success"]:
                if "unexpected_index_list" in result["result"]:
                    assert "unexpected_index_query" in result["result"]

        if "unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )
            elif "unexpected_list" in test["output"]:
                (
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )

        if "partial_unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "partial_unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )
            elif "partial_unexpected_list" in test["output"]:
                (
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )

        # Determine if np.allclose(..) might be needed for float comparison
        try_allclose = False
        if "observed_value" in test["output"]:
            if RX_FLOAT.match(repr(test["output"]["observed_value"])):
                try_allclose = True

        # Check results
        if test["exact_match_out"] is True:
            if "result" in result and "observed_value" in result["result"]:
                if isinstance(result["result"]["observed_value"], (np.floating, float)):
                    assert np.allclose(
                        result["result"]["observed_value"],
                        expectationValidationResultSchema.load(test["output"])["result"][
                            "observed_value"
                        ],
                        rtol=RTOL,
                        atol=ATOL,
                    ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {expectationValidationResultSchema.load(test['output'])['result']['observed_value']}"
                else:
                    assert result == expectationValidationResultSchema.load(
                        test["output"]
                    ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
            else:
                assert result == expectationValidationResultSchema.load(
                    test["output"]
                ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
        else:
            # Convert result to json since our tests are reading from json so cannot easily contain richer types (e.g. NaN)
            # NOTE - 20191031 - JPC - we may eventually want to change these tests as we update our view on how
            # representations, serializations, and objects should interact and how much of that is shown to the user.
            result = result.to_json_dict()
            for key, value in test["output"].items():
                # Apply our great expectations-specific test logic

                if key == "success":
                    if isinstance(value, (np.floating, float)):
                        try:
                            assert np.allclose(
                                result["success"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['success']} not np.allclose to {value}"
                        except TypeError:
                            assert (
                                result["success"] == value
                            ), f"{result['success']} != {value}"
                    else:
                        assert result["success"] == value, f"{result['success']} != {value}"

                elif key == "observed_value":
                    if "tolerance" in test:
                        if isinstance(value, dict):
                            assert set(result["result"]["observed_value"].keys()) == set(
                                value.keys()
                            ), f"{set(result['result']['observed_value'].keys())} != {set(value.keys())}"
                            for k, v in value.items():
                                assert np.allclose(
                                    result["result"]["observed_value"][k],
                                    v,
                                    rtol=test["tolerance"],
                                )
                        else:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=test["tolerance"],
                            )
                    else:
                        if isinstance(value, dict) and "values" in value:
                            try:
                                assert np.allclose(
                                    result["result"]["observed_value"]["values"],
                                    value["values"],
                                    rtol=RTOL,
                                    atol=ATOL,
                                ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']['values']} not np.allclose to {value['values']}"
                            except TypeError as e:
                                print(e)
                                assert (
                                    result["result"]["observed_value"] == value
                                ), f"{result['result']['observed_value']} != {value}"
                        elif try_allclose:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {value}"
                        else:
                            assert (
                                result["result"]["observed_value"] == value
                            ), f"{result['result']['observed_value']} != {value}"

                # NOTE: This is a key used ONLY for testing cases where an expectation is legitimately allowed to return
                # any of multiple possible observed_values. expect_column_values_to_be_of_type is one such expectation.
                elif key == "observed_value_list":
                    assert result["result"]["observed_value"] in value

                elif key == "unexpected_index_list":
                    if isinstance(data_asset, (SqlAlchemyDataset, SparkDFDataset)):
                        pass
                    elif isinstance(data_asset, (SqlAlchemyBatchData, SparkDFBatchData)):
                        pass
                    else:
>                       assert (
                            result["result"]["unexpected_index_list"] == value
                        ), f"{result['result']['unexpected_index_list']} != {value}"
E                       AssertionError: [{'c': 1.0, 'pk_index': 0}, {'c': 1.0, 'pk_index': 1}] != [0, 1]

great_expectations/self_check/util.py:3075: AssertionError
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 4082.05it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 477.09it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 669.80it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 379.36it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 518.99it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 402.15it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 616.85it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 569.66it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 566.16it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 562.78it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_unique:negative_multiple_duplicate_values] _

test_case = {'expectation_type': 'expect_column_values_to_be_unique', 'pk_column': True, 'skip': False, 'test': {'exact_match_out'..., 'out': {'success': False, 'unexpected_index_list': [0, 1, 2, 3], 'unexpected_list': ['1', '1', '2', '2']}, ...}, ...}

    @pytest.mark.order(index=0)
    @pytest.mark.integration
    @pytest.mark.slow  # 12.68s
    def test_case_runner_v3_api(test_case):
        if test_case["skip"]:
            pytest.skip()

        # Note: this should never be done in practice, but we are wiping expectations to reuse batches during testing.
        # test_case["batch"]._initialize_expectations()
        if "parse_strings_as_datetimes" in test_case["test"]["in"]:
            with pytest.deprecated_call():
                evaluate_json_test_v3_api(
                    validator=test_case["validator_with_data"],
                    expectation_type=test_case["expectation_type"],
                    test=test_case["test"],
                    pk_column=test_case["pk_column"],
                )
        else:
>           evaluate_json_test_v3_api(
                validator=test_case["validator_with_data"],
                expectation_type=test_case["expectation_type"],
                test=test_case["test"],
                pk_column=test_case["pk_column"],
            )

tests/test_definitions/test_expectations_v3_api.py:418:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
great_expectations/self_check/util.py:2901: in evaluate_json_test_v3_api
    check_json_test_result(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

test = {'exact_match_out': False, 'in': {'column': 'mult_dup'}, 'input': {'column': 'mult_dup'}, 'out': {'success': False, 'unexpected_index_list': [0, 1, 2, 3], 'unexpected_list': ['1', '1', '2', '2']}, ...}
result = {'exception_info': {'exception_message': None, 'exception_traceback': None, 'raised_exception': False}, 'expectation_c...issing_percent': 0.0, 'partial_unexpected_counts': [{'count': 2, 'value': '1'}, {'count': 2, 'value': '2'}], ...}, ...}
data_asset = <great_expectations.execution_engine.pandas_batch_data.PandasBatchData object at 0x7f7806497b50>
pk_column = True

    def check_json_test_result(  # noqa: C901 - 52
        test, result, data_asset=None, pk_column=False
    ) -> None:

        # check for id_pk results in cases where pk_column is true and unexpected_index_list already exists
        # this will work for testing since result_format is COMPLETE
        if pk_column:
            if not result["success"]:
                if "unexpected_index_list" in result["result"]:
                    assert "unexpected_index_query" in result["result"]

        if "unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )
            elif "unexpected_list" in test["output"]:
                (
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )

        if "partial_unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "partial_unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )
            elif "partial_unexpected_list" in test["output"]:
                (
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )

        # Determine if np.allclose(..) might be needed for float comparison
        try_allclose = False
        if "observed_value" in test["output"]:
            if RX_FLOAT.match(repr(test["output"]["observed_value"])):
                try_allclose = True

        # Check results
        if test["exact_match_out"] is True:
            if "result" in result and "observed_value" in result["result"]:
                if isinstance(result["result"]["observed_value"], (np.floating, float)):
                    assert np.allclose(
                        result["result"]["observed_value"],
                        expectationValidationResultSchema.load(test["output"])["result"][
                            "observed_value"
                        ],
                        rtol=RTOL,
                        atol=ATOL,
                    ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {expectationValidationResultSchema.load(test['output'])['result']['observed_value']}"
                else:
                    assert result == expectationValidationResultSchema.load(
                        test["output"]
                    ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
            else:
                assert result == expectationValidationResultSchema.load(
                    test["output"]
                ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
        else:
            # Convert result to json since our tests are reading from json so cannot easily contain richer types (e.g. NaN)
            # NOTE - 20191031 - JPC - we may eventually want to change these tests as we update our view on how
            # representations, serializations, and objects should interact and how much of that is shown to the user.
            result = result.to_json_dict()
            for key, value in test["output"].items():
                # Apply our great expectations-specific test logic

                if key == "success":
                    if isinstance(value, (np.floating, float)):
                        try:
                            assert np.allclose(
                                result["success"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['success']} not np.allclose to {value}"
                        except TypeError:
                            assert (
                                result["success"] == value
                            ), f"{result['success']} != {value}"
                    else:
                        assert result["success"] == value, f"{result['success']} != {value}"

                elif key == "observed_value":
                    if "tolerance" in test:
                        if isinstance(value, dict):
                            assert set(result["result"]["observed_value"].keys()) == set(
                                value.keys()
                            ), f"{set(result['result']['observed_value'].keys())} != {set(value.keys())}"
                            for k, v in value.items():
                                assert np.allclose(
                                    result["result"]["observed_value"][k],
                                    v,
                                    rtol=test["tolerance"],
                                )
                        else:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=test["tolerance"],
                            )
                    else:
                        if isinstance(value, dict) and "values" in value:
                            try:
                                assert np.allclose(
                                    result["result"]["observed_value"]["values"],
                                    value["values"],
                                    rtol=RTOL,
                                    atol=ATOL,
                                ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']['values']} not np.allclose to {value['values']}"
                            except TypeError as e:
                                print(e)
                                assert (
                                    result["result"]["observed_value"] == value
                                ), f"{result['result']['observed_value']} != {value}"
                        elif try_allclose:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {value}"
                        else:
                            assert (
                                result["result"]["observed_value"] == value
                            ), f"{result['result']['observed_value']} != {value}"

                # NOTE: This is a key used ONLY for testing cases where an expectation is legitimately allowed to return
                # any of multiple possible observed_values. expect_column_values_to_be_of_type is one such expectation.
                elif key == "observed_value_list":
                    assert result["result"]["observed_value"] in value

                elif key == "unexpected_index_list":
                    if isinstance(data_asset, (SqlAlchemyDataset, SparkDFDataset)):
                        pass
                    elif isinstance(data_asset, (SqlAlchemyBatchData, SparkDFBatchData)):
                        pass
                    else:
>                       assert (
                            result["result"]["unexpected_index_list"] == value
                        ), f"{result['result']['unexpected_index_list']} != {value}"
E                       AssertionError: [{'mult_dup': '1', 'pk_index': 0}, {'mult_dup': '1', 'pk_index': 1}, {'mult_dup': '2', 'pk_index': 2}, {'mult_dup': '2', 'pk_index': 3}] != [0, 1, 2, 3]

great_expectations/self_check/util.py:3075: AssertionError
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 4048.56it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 455.68it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 637.46it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 368.69it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 503.78it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 377.70it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 533.59it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 496.62it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 493.38it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 489.65it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_match_json_schema:basic_negative_test] _

test_case = {'expectation_type': 'expect_column_values_to_match_json_schema', 'pk_column': True, 'skip': False, 'test': {'exact_ma...rue, 'input': {'column': 'x', 'json_schema': {'properties': {'a': {'type': 'integer'}}, 'required': ['b']}}, ...}, ...}

    @pytest.mark.order(index=0)
    @pytest.mark.integration
    @pytest.mark.slow  # 12.68s
    def test_case_runner_v3_api(test_case):
        if test_case["skip"]:
            pytest.skip()

        # Note: this should never be done in practice, but we are wiping expectations to reuse batches during testing.
        # test_case["batch"]._initialize_expectations()
        if "parse_strings_as_datetimes" in test_case["test"]["in"]:
            with pytest.deprecated_call():
                evaluate_json_test_v3_api(
                    validator=test_case["validator_with_data"],
                    expectation_type=test_case["expectation_type"],
                    test=test_case["test"],
                    pk_column=test_case["pk_column"],
                )
        else:
>           evaluate_json_test_v3_api(
                validator=test_case["validator_with_data"],
                expectation_type=test_case["expectation_type"],
                test=test_case["test"],
                pk_column=test_case["pk_column"],
            )

tests/test_definitions/test_expectations_v3_api.py:418:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
great_expectations/self_check/util.py:2901: in evaluate_json_test_v3_api
    check_json_test_result(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

test = {'exact_match_out': False, 'in': {'column': 'x', 'json_schema': {'properties': {'a': {'type': 'integer'}}, 'required':...ry': True, 'input': {'column': 'x', 'json_schema': {'properties': {'a': {'type': 'integer'}}, 'required': ['b']}}, ...}
result = {'exception_info': {'exception_message': None, 'exception_traceback': None, 'raised_exception': False}, 'expectation_c...2}'}, {'count': 1, 'value': '{"a":3}'}, {'count': 1, 'value': '{"a":4}'}, {'count': 1, 'value': '{"a":5}'}], ...}, ...}
data_asset = <great_expectations.execution_engine.pandas_batch_data.PandasBatchData object at 0x7f78063498b0>
pk_column = True

    def check_json_test_result(  # noqa: C901 - 52
        test, result, data_asset=None, pk_column=False
    ) -> None:

        # check for id_pk results in cases where pk_column is true and unexpected_index_list already exists
        # this will work for testing since result_format is COMPLETE
        if pk_column:
            if not result["success"]:
                if "unexpected_index_list" in result["result"]:
                    assert "unexpected_index_query" in result["result"]

        if "unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )
            elif "unexpected_list" in test["output"]:
                (
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )

        if "partial_unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "partial_unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )
            elif "partial_unexpected_list" in test["output"]:
                (
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )

        # Determine if np.allclose(..) might be needed for float comparison
        try_allclose = False
        if "observed_value" in test["output"]:
            if RX_FLOAT.match(repr(test["output"]["observed_value"])):
                try_allclose = True

        # Check results
        if test["exact_match_out"] is True:
            if "result" in result and "observed_value" in result["result"]:
                if isinstance(result["result"]["observed_value"], (np.floating, float)):
                    assert np.allclose(
                        result["result"]["observed_value"],
                        expectationValidationResultSchema.load(test["output"])["result"][
                            "observed_value"
                        ],
                        rtol=RTOL,
                        atol=ATOL,
                    ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {expectationValidationResultSchema.load(test['output'])['result']['observed_value']}"
                else:
                    assert result == expectationValidationResultSchema.load(
                        test["output"]
                    ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
            else:
                assert result == expectationValidationResultSchema.load(
                    test["output"]
                ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
        else:
            # Convert result to json since our tests are reading from json so cannot easily contain richer types (e.g. NaN)
            # NOTE - 20191031 - JPC - we may eventually want to change these tests as we update our view on how
            # representations, serializations, and objects should interact and how much of that is shown to the user.
            result = result.to_json_dict()
            for key, value in test["output"].items():
                # Apply our great expectations-specific test logic

                if key == "success":
                    if isinstance(value, (np.floating, float)):
                        try:
                            assert np.allclose(
                                result["success"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['success']} not np.allclose to {value}"
                        except TypeError:
                            assert (
                                result["success"] == value
                            ), f"{result['success']} != {value}"
                    else:
                        assert result["success"] == value, f"{result['success']} != {value}"

                elif key == "observed_value":
                    if "tolerance" in test:
                        if isinstance(value, dict):
                            assert set(result["result"]["observed_value"].keys()) == set(
                                value.keys()
                            ), f"{set(result['result']['observed_value'].keys())} != {set(value.keys())}"
                            for k, v in value.items():
                                assert np.allclose(
                                    result["result"]["observed_value"][k],
                                    v,
                                    rtol=test["tolerance"],
                                )
                        else:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=test["tolerance"],
                            )
                    else:
                        if isinstance(value, dict) and "values" in value:
                            try:
                                assert np.allclose(
                                    result["result"]["observed_value"]["values"],
                                    value["values"],
                                    rtol=RTOL,
                                    atol=ATOL,
                                ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']['values']} not np.allclose to {value['values']}"
                            except TypeError as e:
                                print(e)
                                assert (
                                    result["result"]["observed_value"] == value
                                ), f"{result['result']['observed_value']} != {value}"
                        elif try_allclose:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {value}"
                        else:
                            assert (
                                result["result"]["observed_value"] == value
                            ), f"{result['result']['observed_value']} != {value}"

                # NOTE: This is a key used ONLY for testing cases where an expectation is legitimately allowed to return
                # any of multiple possible observed_values. expect_column_values_to_be_of_type is one such expectation.
                elif key == "observed_value_list":
                    assert result["result"]["observed_value"] in value

                elif key == "unexpected_index_list":
                    if isinstance(data_asset, (SqlAlchemyDataset, SparkDFDataset)):
                        pass
                    elif isinstance(data_asset, (SqlAlchemyBatchData, SparkDFBatchData)):
                        pass
                    else:
>                       assert (
                            result["result"]["unexpected_index_list"] == value
                        ), f"{result['result']['unexpected_index_list']} != {value}"
E                       AssertionError: [{'x': '{"a":1}', 'pk_index': 0}, {'x': '{"a":2}', 'pk_index': 1}, {'x': '{"a":3}', 'pk_index': 2}, {'x': '{"a":4}', 'pk_index': 3}, {'x': '{"a":5}', 'pk_index': 4}] != [0, 1, 2, 3, 4]

great_expectations/self_check/util.py:3075: AssertionError
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 3876.44it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 395.26it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 557.83it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 311.10it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 233.15it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 201.39it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 346.44it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 327.86it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 326.69it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 325.57it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_null:negative_no_missing_values] _

test_case = {'expectation_type': 'expect_column_values_to_be_null', 'pk_column': True, 'skip': False, 'test': {'exact_match_out': False, 'in': {'column': 'no_null'}, 'include_in_gallery': True, 'input': {'column': 'no_null'}, ...}, ...}

    @pytest.mark.order(index=0)
    @pytest.mark.integration
    @pytest.mark.slow  # 12.68s
    def test_case_runner_v3_api(test_case):
        if test_case["skip"]:
            pytest.skip()

        # Note: this should never be done in practice, but we are wiping expectations to reuse batches during testing.
        # test_case["batch"]._initialize_expectations()
        if "parse_strings_as_datetimes" in test_case["test"]["in"]:
            with pytest.deprecated_call():
                evaluate_json_test_v3_api(
                    validator=test_case["validator_with_data"],
                    expectation_type=test_case["expectation_type"],
                    test=test_case["test"],
                    pk_column=test_case["pk_column"],
                )
        else:
>           evaluate_json_test_v3_api(
                validator=test_case["validator_with_data"],
                expectation_type=test_case["expectation_type"],
                test=test_case["test"],
                pk_column=test_case["pk_column"],
            )

tests/test_definitions/test_expectations_v3_api.py:418:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
great_expectations/self_check/util.py:2901: in evaluate_json_test_v3_api
    check_json_test_result(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

test = {'exact_match_out': False, 'in': {'column': 'no_null'}, 'include_in_gallery': True, 'input': {'column': 'no_null'}, ...}
result = {'exception_info': {'exception_message': None, 'exception_traceback': None, 'raised_exception': False}, 'expectation_c...ll': 'c', 'pk_index': 2}, {'no_null': 'd', 'pk_index': 3}], 'partial_unexpected_list': ['a', 'b', 'c', 'd'], ...}, ...}
data_asset = <great_expectations.execution_engine.pandas_batch_data.PandasBatchData object at 0x7f7806462b20>
pk_column = True

    def check_json_test_result(  # noqa: C901 - 52
        test, result, data_asset=None, pk_column=False
    ) -> None:

        # check for id_pk results in cases where pk_column is true and unexpected_index_list already exists
        # this will work for testing since result_format is COMPLETE
        if pk_column:
            if not result["success"]:
                if "unexpected_index_list" in result["result"]:
                    assert "unexpected_index_query" in result["result"]

        if "unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )
            elif "unexpected_list" in test["output"]:
                (
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )

        if "partial_unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "partial_unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )
            elif "partial_unexpected_list" in test["output"]:
                (
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )

        # Determine if np.allclose(..) might be needed for float comparison
        try_allclose = False
        if "observed_value" in test["output"]:
            if RX_FLOAT.match(repr(test["output"]["observed_value"])):
                try_allclose = True

        # Check results
        if test["exact_match_out"] is True:
            if "result" in result and "observed_value" in result["result"]:
                if isinstance(result["result"]["observed_value"], (np.floating, float)):
                    assert np.allclose(
                        result["result"]["observed_value"],
                        expectationValidationResultSchema.load(test["output"])["result"][
                            "observed_value"
                        ],
                        rtol=RTOL,
                        atol=ATOL,
                    ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {expectationValidationResultSchema.load(test['output'])['result']['observed_value']}"
                else:
                    assert result == expectationValidationResultSchema.load(
                        test["output"]
                    ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
            else:
                assert result == expectationValidationResultSchema.load(
                    test["output"]
                ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
        else:
            # Convert result to json since our tests are reading from json so cannot easily contain richer types (e.g. NaN)
            # NOTE - 20191031 - JPC - we may eventually want to change these tests as we update our view on how
            # representations, serializations, and objects should interact and how much of that is shown to the user.
            result = result.to_json_dict()
            for key, value in test["output"].items():
                # Apply our great expectations-specific test logic

                if key == "success":
                    if isinstance(value, (np.floating, float)):
                        try:
                            assert np.allclose(
                                result["success"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['success']} not np.allclose to {value}"
                        except TypeError:
                            assert (
                                result["success"] == value
                            ), f"{result['success']} != {value}"
                    else:
                        assert result["success"] == value, f"{result['success']} != {value}"

                elif key == "observed_value":
                    if "tolerance" in test:
                        if isinstance(value, dict):
                            assert set(result["result"]["observed_value"].keys()) == set(
                                value.keys()
                            ), f"{set(result['result']['observed_value'].keys())} != {set(value.keys())}"
                            for k, v in value.items():
                                assert np.allclose(
                                    result["result"]["observed_value"][k],
                                    v,
                                    rtol=test["tolerance"],
                                )
                        else:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=test["tolerance"],
                            )
                    else:
                        if isinstance(value, dict) and "values" in value:
                            try:
                                assert np.allclose(
                                    result["result"]["observed_value"]["values"],
                                    value["values"],
                                    rtol=RTOL,
                                    atol=ATOL,
                                ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']['values']} not np.allclose to {value['values']}"
                            except TypeError as e:
                                print(e)
                                assert (
                                    result["result"]["observed_value"] == value
                                ), f"{result['result']['observed_value']} != {value}"
                        elif try_allclose:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {value}"
                        else:
                            assert (
                                result["result"]["observed_value"] == value
                            ), f"{result['result']['observed_value']} != {value}"

                # NOTE: This is a key used ONLY for testing cases where an expectation is legitimately allowed to return
                # any of multiple possible observed_values. expect_column_values_to_be_of_type is one such expectation.
                elif key == "observed_value_list":
                    assert result["result"]["observed_value"] in value

                elif key == "unexpected_index_list":
                    if isinstance(data_asset, (SqlAlchemyDataset, SparkDFDataset)):
                        pass
                    elif isinstance(data_asset, (SqlAlchemyBatchData, SparkDFBatchData)):
                        pass
                    else:
>                       assert (
                            result["result"]["unexpected_index_list"] == value
                        ), f"{result['result']['unexpected_index_list']} != {value}"
E                       AssertionError: [{'no_null': 'a', 'pk_index': 0}, {'no_null': 'b', 'pk_index': 1}, {'no_null': 'c', 'pk_index': 2}, {'no_null': 'd', 'pk_index': 3}] != [0, 1, 2, 3]

great_expectations/self_check/util.py:3075: AssertionError
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/8 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/8 [00:00<?, ?it/s]Calculating Metrics:  25%|██▌       | 2/8 [00:00<00:00, 3930.93it/s]Calculating Metrics:  25%|██▌       | 2/8 [00:00<00:00, 614.46it/s]Calculating Metrics:  38%|███▊      | 3/8 [00:00<00:00, 852.56it/s]Calculating Metrics:  38%|███▊      | 3/8 [00:00<00:00, 482.57it/s]Calculating Metrics:  50%|█████     | 4/8 [00:00<00:00, 575.78it/s]Calculating Metrics:  50%|█████     | 4/8 [00:00<00:00, 430.85it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 665.12it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 614.78it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 610.18it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 605.42it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_null:positive_mostly] _

test_case = {'expectation_type': 'expect_column_values_to_be_null', 'pk_column': True, 'skip': False, 'test': {'exact_match_out': ...hree_null', 'mostly': 0.75}, 'out': {'success': True, 'unexpected_index_list': [3], 'unexpected_list': [5]}, ...}, ...}

    @pytest.mark.order(index=0)
    @pytest.mark.integration
    @pytest.mark.slow  # 12.68s
    def test_case_runner_v3_api(test_case):
        if test_case["skip"]:
            pytest.skip()

        # Note: this should never be done in practice, but we are wiping expectations to reuse batches during testing.
        # test_case["batch"]._initialize_expectations()
        if "parse_strings_as_datetimes" in test_case["test"]["in"]:
            with pytest.deprecated_call():
                evaluate_json_test_v3_api(
                    validator=test_case["validator_with_data"],
                    expectation_type=test_case["expectation_type"],
                    test=test_case["test"],
                    pk_column=test_case["pk_column"],
                )
        else:
>           evaluate_json_test_v3_api(
                validator=test_case["validator_with_data"],
                expectation_type=test_case["expectation_type"],
                test=test_case["test"],
                pk_column=test_case["pk_column"],
            )

tests/test_definitions/test_expectations_v3_api.py:418:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
great_expectations/self_check/util.py:2901: in evaluate_json_test_v3_api
    check_json_test_result(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

test = {'exact_match_out': False, 'in': {'column': 'three_null', 'mostly': 0.75}, 'input': {'column': 'three_null', 'mostly': 0.75}, 'out': {'success': True, 'unexpected_index_list': [3], 'unexpected_list': [5]}, ...}
result = {'exception_info': {'exception_message': None, 'exception_traceback': None, 'raised_exception': False}, 'expectation_c...}], 'partial_unexpected_index_list': [{'pk_index': 3, 'three_null': 5.0}], 'partial_unexpected_list': [5.0], ...}, ...}
data_asset = <great_expectations.execution_engine.pandas_batch_data.PandasBatchData object at 0x7f7806462b20>
pk_column = True

    def check_json_test_result(  # noqa: C901 - 52
        test, result, data_asset=None, pk_column=False
    ) -> None:

        # check for id_pk results in cases where pk_column is true and unexpected_index_list already exists
        # this will work for testing since result_format is COMPLETE
        if pk_column:
            if not result["success"]:
                if "unexpected_index_list" in result["result"]:
                    assert "unexpected_index_query" in result["result"]

        if "unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )
            elif "unexpected_list" in test["output"]:
                (
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )

        if "partial_unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "partial_unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )
            elif "partial_unexpected_list" in test["output"]:
                (
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )

        # Determine if np.allclose(..) might be needed for float comparison
        try_allclose = False
        if "observed_value" in test["output"]:
            if RX_FLOAT.match(repr(test["output"]["observed_value"])):
                try_allclose = True

        # Check results
        if test["exact_match_out"] is True:
            if "result" in result and "observed_value" in result["result"]:
                if isinstance(result["result"]["observed_value"], (np.floating, float)):
                    assert np.allclose(
                        result["result"]["observed_value"],
                        expectationValidationResultSchema.load(test["output"])["result"][
                            "observed_value"
                        ],
                        rtol=RTOL,
                        atol=ATOL,
                    ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {expectationValidationResultSchema.load(test['output'])['result']['observed_value']}"
                else:
                    assert result == expectationValidationResultSchema.load(
                        test["output"]
                    ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
            else:
                assert result == expectationValidationResultSchema.load(
                    test["output"]
                ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
        else:
            # Convert result to json since our tests are reading from json so cannot easily contain richer types (e.g. NaN)
            # NOTE - 20191031 - JPC - we may eventually want to change these tests as we update our view on how
            # representations, serializations, and objects should interact and how much of that is shown to the user.
            result = result.to_json_dict()
            for key, value in test["output"].items():
                # Apply our great expectations-specific test logic

                if key == "success":
                    if isinstance(value, (np.floating, float)):
                        try:
                            assert np.allclose(
                                result["success"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['success']} not np.allclose to {value}"
                        except TypeError:
                            assert (
                                result["success"] == value
                            ), f"{result['success']} != {value}"
                    else:
                        assert result["success"] == value, f"{result['success']} != {value}"

                elif key == "observed_value":
                    if "tolerance" in test:
                        if isinstance(value, dict):
                            assert set(result["result"]["observed_value"].keys()) == set(
                                value.keys()
                            ), f"{set(result['result']['observed_value'].keys())} != {set(value.keys())}"
                            for k, v in value.items():
                                assert np.allclose(
                                    result["result"]["observed_value"][k],
                                    v,
                                    rtol=test["tolerance"],
                                )
                        else:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=test["tolerance"],
                            )
                    else:
                        if isinstance(value, dict) and "values" in value:
                            try:
                                assert np.allclose(
                                    result["result"]["observed_value"]["values"],
                                    value["values"],
                                    rtol=RTOL,
                                    atol=ATOL,
                                ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']['values']} not np.allclose to {value['values']}"
                            except TypeError as e:
                                print(e)
                                assert (
                                    result["result"]["observed_value"] == value
                                ), f"{result['result']['observed_value']} != {value}"
                        elif try_allclose:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {value}"
                        else:
                            assert (
                                result["result"]["observed_value"] == value
                            ), f"{result['result']['observed_value']} != {value}"

                # NOTE: This is a key used ONLY for testing cases where an expectation is legitimately allowed to return
                # any of multiple possible observed_values. expect_column_values_to_be_of_type is one such expectation.
                elif key == "observed_value_list":
                    assert result["result"]["observed_value"] in value

                elif key == "unexpected_index_list":
                    if isinstance(data_asset, (SqlAlchemyDataset, SparkDFDataset)):
                        pass
                    elif isinstance(data_asset, (SqlAlchemyBatchData, SparkDFBatchData)):
                        pass
                    else:
>                       assert (
                            result["result"]["unexpected_index_list"] == value
                        ), f"{result['result']['unexpected_index_list']} != {value}"
E                       AssertionError: [{'three_null': 5.0, 'pk_index': 3}] != [3]

great_expectations/self_check/util.py:3075: AssertionError
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/8 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/8 [00:00<?, ?it/s]Calculating Metrics:  25%|██▌       | 2/8 [00:00<00:00, 4007.94it/s]Calculating Metrics:  25%|██▌       | 2/8 [00:00<00:00, 622.30it/s]Calculating Metrics:  38%|███▊      | 3/8 [00:00<00:00, 863.32it/s]Calculating Metrics:  38%|███▊      | 3/8 [00:00<00:00, 497.53it/s]Calculating Metrics:  50%|█████     | 4/8 [00:00<00:00, 599.53it/s]Calculating Metrics:  50%|█████     | 4/8 [00:00<00:00, 449.90it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 696.25it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 642.72it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 637.97it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 632.85it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_null:negative_mostly] _

test_case = {'expectation_type': 'expect_column_values_to_be_null', 'pk_column': True, 'skip': False, 'test': {'exact_match_out': ...hree_null', 'mostly': 0.8}, 'out': {'success': False, 'unexpected_index_list': [3], 'unexpected_list': [5]}, ...}, ...}

    @pytest.mark.order(index=0)
    @pytest.mark.integration
    @pytest.mark.slow  # 12.68s
    def test_case_runner_v3_api(test_case):
        if test_case["skip"]:
            pytest.skip()

        # Note: this should never be done in practice, but we are wiping expectations to reuse batches during testing.
        # test_case["batch"]._initialize_expectations()
        if "parse_strings_as_datetimes" in test_case["test"]["in"]:
            with pytest.deprecated_call():
                evaluate_json_test_v3_api(
                    validator=test_case["validator_with_data"],
                    expectation_type=test_case["expectation_type"],
                    test=test_case["test"],
                    pk_column=test_case["pk_column"],
                )
        else:
>           evaluate_json_test_v3_api(
                validator=test_case["validator_with_data"],
                expectation_type=test_case["expectation_type"],
                test=test_case["test"],
                pk_column=test_case["pk_column"],
            )

tests/test_definitions/test_expectations_v3_api.py:418:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
great_expectations/self_check/util.py:2901: in evaluate_json_test_v3_api
    check_json_test_result(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

test = {'exact_match_out': False, 'in': {'column': 'three_null', 'mostly': 0.8}, 'input': {'column': 'three_null', 'mostly': 0.8}, 'out': {'success': False, 'unexpected_index_list': [3], 'unexpected_list': [5]}, ...}
result = {'exception_info': {'exception_message': None, 'exception_traceback': None, 'raised_exception': False}, 'expectation_c...}], 'partial_unexpected_index_list': [{'pk_index': 3, 'three_null': 5.0}], 'partial_unexpected_list': [5.0], ...}, ...}
data_asset = <great_expectations.execution_engine.pandas_batch_data.PandasBatchData object at 0x7f7806462b20>
pk_column = True

    def check_json_test_result(  # noqa: C901 - 52
        test, result, data_asset=None, pk_column=False
    ) -> None:

        # check for id_pk results in cases where pk_column is true and unexpected_index_list already exists
        # this will work for testing since result_format is COMPLETE
        if pk_column:
            if not result["success"]:
                if "unexpected_index_list" in result["result"]:
                    assert "unexpected_index_query" in result["result"]

        if "unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )
            elif "unexpected_list" in test["output"]:
                (
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )

        if "partial_unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "partial_unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )
            elif "partial_unexpected_list" in test["output"]:
                (
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )

        # Determine if np.allclose(..) might be needed for float comparison
        try_allclose = False
        if "observed_value" in test["output"]:
            if RX_FLOAT.match(repr(test["output"]["observed_value"])):
                try_allclose = True

        # Check results
        if test["exact_match_out"] is True:
            if "result" in result and "observed_value" in result["result"]:
                if isinstance(result["result"]["observed_value"], (np.floating, float)):
                    assert np.allclose(
                        result["result"]["observed_value"],
                        expectationValidationResultSchema.load(test["output"])["result"][
                            "observed_value"
                        ],
                        rtol=RTOL,
                        atol=ATOL,
                    ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {expectationValidationResultSchema.load(test['output'])['result']['observed_value']}"
                else:
                    assert result == expectationValidationResultSchema.load(
                        test["output"]
                    ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
            else:
                assert result == expectationValidationResultSchema.load(
                    test["output"]
                ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
        else:
            # Convert result to json since our tests are reading from json so cannot easily contain richer types (e.g. NaN)
            # NOTE - 20191031 - JPC - we may eventually want to change these tests as we update our view on how
            # representations, serializations, and objects should interact and how much of that is shown to the user.
            result = result.to_json_dict()
            for key, value in test["output"].items():
                # Apply our great expectations-specific test logic

                if key == "success":
                    if isinstance(value, (np.floating, float)):
                        try:
                            assert np.allclose(
                                result["success"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['success']} not np.allclose to {value}"
                        except TypeError:
                            assert (
                                result["success"] == value
                            ), f"{result['success']} != {value}"
                    else:
                        assert result["success"] == value, f"{result['success']} != {value}"

                elif key == "observed_value":
                    if "tolerance" in test:
                        if isinstance(value, dict):
                            assert set(result["result"]["observed_value"].keys()) == set(
                                value.keys()
                            ), f"{set(result['result']['observed_value'].keys())} != {set(value.keys())}"
                            for k, v in value.items():
                                assert np.allclose(
                                    result["result"]["observed_value"][k],
                                    v,
                                    rtol=test["tolerance"],
                                )
                        else:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=test["tolerance"],
                            )
                    else:
                        if isinstance(value, dict) and "values" in value:
                            try:
                                assert np.allclose(
                                    result["result"]["observed_value"]["values"],
                                    value["values"],
                                    rtol=RTOL,
                                    atol=ATOL,
                                ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']['values']} not np.allclose to {value['values']}"
                            except TypeError as e:
                                print(e)
                                assert (
                                    result["result"]["observed_value"] == value
                                ), f"{result['result']['observed_value']} != {value}"
                        elif try_allclose:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {value}"
                        else:
                            assert (
                                result["result"]["observed_value"] == value
                            ), f"{result['result']['observed_value']} != {value}"

                # NOTE: This is a key used ONLY for testing cases where an expectation is legitimately allowed to return
                # any of multiple possible observed_values. expect_column_values_to_be_of_type is one such expectation.
                elif key == "observed_value_list":
                    assert result["result"]["observed_value"] in value

                elif key == "unexpected_index_list":
                    if isinstance(data_asset, (SqlAlchemyDataset, SparkDFDataset)):
                        pass
                    elif isinstance(data_asset, (SqlAlchemyBatchData, SparkDFBatchData)):
                        pass
                    else:
>                       assert (
                            result["result"]["unexpected_index_list"] == value
                        ), f"{result['result']['unexpected_index_list']} != {value}"
E                       AssertionError: [{'three_null': 5.0, 'pk_index': 3}] != [3]

great_expectations/self_check/util.py:3075: AssertionError
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/8 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/8 [00:00<?, ?it/s]Calculating Metrics:  25%|██▌       | 2/8 [00:00<00:00, 3723.31it/s]Calculating Metrics:  25%|██▌       | 2/8 [00:00<00:00, 605.68it/s]Calculating Metrics:  38%|███▊      | 3/8 [00:00<00:00, 836.07it/s]Calculating Metrics:  38%|███▊      | 3/8 [00:00<00:00, 485.19it/s]Calculating Metrics:  50%|█████     | 4/8 [00:00<00:00, 565.99it/s]Calculating Metrics:  50%|█████     | 4/8 [00:00<00:00, 358.23it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 517.94it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 482.12it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 478.49it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 475.32it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_null:negative_case_with_75percent_non_null_values_no_mostly] _

test_case = {'expectation_type': 'expect_column_values_to_be_null', 'pk_column': True, 'skip': False, 'test': {'exact_match_out': ...': 'one_null'}, 'out': {'success': False, 'unexpected_index_list': [0, 1, 3], 'unexpected_list': [4, 5, 7]}, ...}, ...}

    @pytest.mark.order(index=0)
    @pytest.mark.integration
    @pytest.mark.slow  # 12.68s
    def test_case_runner_v3_api(test_case):
        if test_case["skip"]:
            pytest.skip()

        # Note: this should never be done in practice, but we are wiping expectations to reuse batches during testing.
        # test_case["batch"]._initialize_expectations()
        if "parse_strings_as_datetimes" in test_case["test"]["in"]:
            with pytest.deprecated_call():
                evaluate_json_test_v3_api(
                    validator=test_case["validator_with_data"],
                    expectation_type=test_case["expectation_type"],
                    test=test_case["test"],
                    pk_column=test_case["pk_column"],
                )
        else:
>           evaluate_json_test_v3_api(
                validator=test_case["validator_with_data"],
                expectation_type=test_case["expectation_type"],
                test=test_case["test"],
                pk_column=test_case["pk_column"],
            )

tests/test_definitions/test_expectations_v3_api.py:418:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
great_expectations/self_check/util.py:2901: in evaluate_json_test_v3_api
    check_json_test_result(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

test = {'exact_match_out': False, 'in': {'column': 'one_null'}, 'input': {'column': 'one_null'}, 'out': {'success': False, 'unexpected_index_list': [0, 1, 3], 'unexpected_list': [4, 5, 7]}, ...}
result = {'exception_info': {'exception_message': None, 'exception_traceback': None, 'raised_exception': False}, 'expectation_c...e_null': 5.0, 'pk_index': 1}, {'one_null': 7.0, 'pk_index': 3}], 'partial_unexpected_list': [4.0, 5.0, 7.0], ...}, ...}
data_asset = <great_expectations.execution_engine.pandas_batch_data.PandasBatchData object at 0x7f7806462b20>
pk_column = True

    def check_json_test_result(  # noqa: C901 - 52
        test, result, data_asset=None, pk_column=False
    ) -> None:

        # check for id_pk results in cases where pk_column is true and unexpected_index_list already exists
        # this will work for testing since result_format is COMPLETE
        if pk_column:
            if not result["success"]:
                if "unexpected_index_list" in result["result"]:
                    assert "unexpected_index_query" in result["result"]

        if "unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )
            elif "unexpected_list" in test["output"]:
                (
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )

        if "partial_unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "partial_unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )
            elif "partial_unexpected_list" in test["output"]:
                (
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )

        # Determine if np.allclose(..) might be needed for float comparison
        try_allclose = False
        if "observed_value" in test["output"]:
            if RX_FLOAT.match(repr(test["output"]["observed_value"])):
                try_allclose = True

        # Check results
        if test["exact_match_out"] is True:
            if "result" in result and "observed_value" in result["result"]:
                if isinstance(result["result"]["observed_value"], (np.floating, float)):
                    assert np.allclose(
                        result["result"]["observed_value"],
                        expectationValidationResultSchema.load(test["output"])["result"][
                            "observed_value"
                        ],
                        rtol=RTOL,
                        atol=ATOL,
                    ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {expectationValidationResultSchema.load(test['output'])['result']['observed_value']}"
                else:
                    assert result == expectationValidationResultSchema.load(
                        test["output"]
                    ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
            else:
                assert result == expectationValidationResultSchema.load(
                    test["output"]
                ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
        else:
            # Convert result to json since our tests are reading from json so cannot easily contain richer types (e.g. NaN)
            # NOTE - 20191031 - JPC - we may eventually want to change these tests as we update our view on how
            # representations, serializations, and objects should interact and how much of that is shown to the user.
            result = result.to_json_dict()
            for key, value in test["output"].items():
                # Apply our great expectations-specific test logic

                if key == "success":
                    if isinstance(value, (np.floating, float)):
                        try:
                            assert np.allclose(
                                result["success"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['success']} not np.allclose to {value}"
                        except TypeError:
                            assert (
                                result["success"] == value
                            ), f"{result['success']} != {value}"
                    else:
                        assert result["success"] == value, f"{result['success']} != {value}"

                elif key == "observed_value":
                    if "tolerance" in test:
                        if isinstance(value, dict):
                            assert set(result["result"]["observed_value"].keys()) == set(
                                value.keys()
                            ), f"{set(result['result']['observed_value'].keys())} != {set(value.keys())}"
                            for k, v in value.items():
                                assert np.allclose(
                                    result["result"]["observed_value"][k],
                                    v,
                                    rtol=test["tolerance"],
                                )
                        else:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=test["tolerance"],
                            )
                    else:
                        if isinstance(value, dict) and "values" in value:
                            try:
                                assert np.allclose(
                                    result["result"]["observed_value"]["values"],
                                    value["values"],
                                    rtol=RTOL,
                                    atol=ATOL,
                                ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']['values']} not np.allclose to {value['values']}"
                            except TypeError as e:
                                print(e)
                                assert (
                                    result["result"]["observed_value"] == value
                                ), f"{result['result']['observed_value']} != {value}"
                        elif try_allclose:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {value}"
                        else:
                            assert (
                                result["result"]["observed_value"] == value
                            ), f"{result['result']['observed_value']} != {value}"

                # NOTE: This is a key used ONLY for testing cases where an expectation is legitimately allowed to return
                # any of multiple possible observed_values. expect_column_values_to_be_of_type is one such expectation.
                elif key == "observed_value_list":
                    assert result["result"]["observed_value"] in value

                elif key == "unexpected_index_list":
                    if isinstance(data_asset, (SqlAlchemyDataset, SparkDFDataset)):
                        pass
                    elif isinstance(data_asset, (SqlAlchemyBatchData, SparkDFBatchData)):
                        pass
                    else:
>                       assert (
                            result["result"]["unexpected_index_list"] == value
                        ), f"{result['result']['unexpected_index_list']} != {value}"
E                       AssertionError: [{'one_null': 4.0, 'pk_index': 0}, {'one_null': 5.0, 'pk_index': 1}, {'one_null': 7.0, 'pk_index': 3}] != [0, 1, 3]

great_expectations/self_check/util.py:3075: AssertionError
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/8 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/8 [00:00<?, ?it/s]Calculating Metrics:  25%|██▌       | 2/8 [00:00<00:00, 3705.22it/s]Calculating Metrics:  25%|██▌       | 2/8 [00:00<00:00, 573.23it/s]Calculating Metrics:  38%|███▊      | 3/8 [00:00<00:00, 794.53it/s]Calculating Metrics:  38%|███▊      | 3/8 [00:00<00:00, 466.57it/s]Calculating Metrics:  50%|█████     | 4/8 [00:00<00:00, 560.23it/s]Calculating Metrics:  50%|█████     | 4/8 [00:00<00:00, 427.49it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 677.24it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 627.36it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 623.06it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 618.24it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_in_type_list:negative_test_string_and_int_values_summary_output] _

test_case = {'expectation_type': 'expect_column_values_to_be_in_type_list', 'pk_column': True, 'skip': False, 'test': {'exact_matc...lumn': 's', 'type_list': ['bool']}, 'input': {'column': 's', 'type_list': ['bool']}, 'only_for': ['pandas'], ...}, ...}

    @pytest.mark.order(index=0)
    @pytest.mark.integration
    @pytest.mark.slow  # 12.68s
    def test_case_runner_v3_api(test_case):
        if test_case["skip"]:
            pytest.skip()

        # Note: this should never be done in practice, but we are wiping expectations to reuse batches during testing.
        # test_case["batch"]._initialize_expectations()
        if "parse_strings_as_datetimes" in test_case["test"]["in"]:
            with pytest.deprecated_call():
                evaluate_json_test_v3_api(
                    validator=test_case["validator_with_data"],
                    expectation_type=test_case["expectation_type"],
                    test=test_case["test"],
                    pk_column=test_case["pk_column"],
                )
        else:
>           evaluate_json_test_v3_api(
                validator=test_case["validator_with_data"],
                expectation_type=test_case["expectation_type"],
                test=test_case["test"],
                pk_column=test_case["pk_column"],
            )

tests/test_definitions/test_expectations_v3_api.py:418:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
great_expectations/self_check/util.py:2901: in evaluate_json_test_v3_api
    check_json_test_result(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

test = {'exact_match_out': False, 'in': {'column': 's', 'type_list': ['bool']}, 'input': {'column': 's', 'type_list': ['bool']}, 'only_for': ['pandas'], ...}
result = {'exception_info': {'exception_message': None, 'exception_traceback': None, 'raised_exception': False}, 'expectation_c...ected_counts': [{'count': 1, 'value': '1'}, {'count': 1, 'value': 'hello'}, {'count': 1, 'value': 'jello'}], ...}, ...}
data_asset = <great_expectations.execution_engine.pandas_batch_data.PandasBatchData object at 0x7f7806610160>
pk_column = True

    def check_json_test_result(  # noqa: C901 - 52
        test, result, data_asset=None, pk_column=False
    ) -> None:

        # check for id_pk results in cases where pk_column is true and unexpected_index_list already exists
        # this will work for testing since result_format is COMPLETE
        if pk_column:
            if not result["success"]:
                if "unexpected_index_list" in result["result"]:
                    assert "unexpected_index_query" in result["result"]

        if "unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )
            elif "unexpected_list" in test["output"]:
                (
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )

        if "partial_unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "partial_unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )
            elif "partial_unexpected_list" in test["output"]:
                (
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )

        # Determine if np.allclose(..) might be needed for float comparison
        try_allclose = False
        if "observed_value" in test["output"]:
            if RX_FLOAT.match(repr(test["output"]["observed_value"])):
                try_allclose = True

        # Check results
        if test["exact_match_out"] is True:
            if "result" in result and "observed_value" in result["result"]:
                if isinstance(result["result"]["observed_value"], (np.floating, float)):
                    assert np.allclose(
                        result["result"]["observed_value"],
                        expectationValidationResultSchema.load(test["output"])["result"][
                            "observed_value"
                        ],
                        rtol=RTOL,
                        atol=ATOL,
                    ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {expectationValidationResultSchema.load(test['output'])['result']['observed_value']}"
                else:
                    assert result == expectationValidationResultSchema.load(
                        test["output"]
                    ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
            else:
                assert result == expectationValidationResultSchema.load(
                    test["output"]
                ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
        else:
            # Convert result to json since our tests are reading from json so cannot easily contain richer types (e.g. NaN)
            # NOTE - 20191031 - JPC - we may eventually want to change these tests as we update our view on how
            # representations, serializations, and objects should interact and how much of that is shown to the user.
            result = result.to_json_dict()
            for key, value in test["output"].items():
                # Apply our great expectations-specific test logic

                if key == "success":
                    if isinstance(value, (np.floating, float)):
                        try:
                            assert np.allclose(
                                result["success"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['success']} not np.allclose to {value}"
                        except TypeError:
                            assert (
                                result["success"] == value
                            ), f"{result['success']} != {value}"
                    else:
                        assert result["success"] == value, f"{result['success']} != {value}"

                elif key == "observed_value":
                    if "tolerance" in test:
                        if isinstance(value, dict):
                            assert set(result["result"]["observed_value"].keys()) == set(
                                value.keys()
                            ), f"{set(result['result']['observed_value'].keys())} != {set(value.keys())}"
                            for k, v in value.items():
                                assert np.allclose(
                                    result["result"]["observed_value"][k],
                                    v,
                                    rtol=test["tolerance"],
                                )
                        else:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=test["tolerance"],
                            )
                    else:
                        if isinstance(value, dict) and "values" in value:
                            try:
                                assert np.allclose(
                                    result["result"]["observed_value"]["values"],
                                    value["values"],
                                    rtol=RTOL,
                                    atol=ATOL,
                                ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']['values']} not np.allclose to {value['values']}"
                            except TypeError as e:
                                print(e)
                                assert (
                                    result["result"]["observed_value"] == value
                                ), f"{result['result']['observed_value']} != {value}"
                        elif try_allclose:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {value}"
                        else:
                            assert (
                                result["result"]["observed_value"] == value
                            ), f"{result['result']['observed_value']} != {value}"

                # NOTE: This is a key used ONLY for testing cases where an expectation is legitimately allowed to return
                # any of multiple possible observed_values. expect_column_values_to_be_of_type is one such expectation.
                elif key == "observed_value_list":
                    assert result["result"]["observed_value"] in value

                elif key == "unexpected_index_list":
                    if isinstance(data_asset, (SqlAlchemyDataset, SparkDFDataset)):
                        pass
                    elif isinstance(data_asset, (SqlAlchemyBatchData, SparkDFBatchData)):
                        pass
                    else:
>                       assert (
                            result["result"]["unexpected_index_list"] == value
                        ), f"{result['result']['unexpected_index_list']} != {value}"
E                       AssertionError: [{'s': 'hello', 'pk_index': 0}, {'s': 'jello', 'pk_index': 1}, {'s': '1', 'pk_index': 2}] != [0, 1, 2]

great_expectations/self_check/util.py:3075: AssertionError
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 3489.44it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 405.03it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 568.49it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 320.04it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 453.14it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 347.54it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 547.43it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 510.05it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 507.93it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 505.41it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_in_type_list:negative_test_string_and_int_values_complete_output] _

test_case = {'expectation_type': 'expect_column_values_to_be_in_type_list', 'pk_column': True, 'skip': False, 'test': {'exact_matc...lumn': 's', 'type_list': ['bool']}, 'input': {'column': 's', 'type_list': ['bool']}, 'only_for': ['pandas'], ...}, ...}

    @pytest.mark.order(index=0)
    @pytest.mark.integration
    @pytest.mark.slow  # 12.68s
    def test_case_runner_v3_api(test_case):
        if test_case["skip"]:
            pytest.skip()

        # Note: this should never be done in practice, but we are wiping expectations to reuse batches during testing.
        # test_case["batch"]._initialize_expectations()
        if "parse_strings_as_datetimes" in test_case["test"]["in"]:
            with pytest.deprecated_call():
                evaluate_json_test_v3_api(
                    validator=test_case["validator_with_data"],
                    expectation_type=test_case["expectation_type"],
                    test=test_case["test"],
                    pk_column=test_case["pk_column"],
                )
        else:
>           evaluate_json_test_v3_api(
                validator=test_case["validator_with_data"],
                expectation_type=test_case["expectation_type"],
                test=test_case["test"],
                pk_column=test_case["pk_column"],
            )

tests/test_definitions/test_expectations_v3_api.py:418:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
great_expectations/self_check/util.py:2901: in evaluate_json_test_v3_api
    check_json_test_result(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

test = {'exact_match_out': False, 'in': {'column': 's', 'type_list': ['bool']}, 'input': {'column': 's', 'type_list': ['bool']}, 'only_for': ['pandas'], ...}
result = {'exception_info': {'exception_message': None, 'exception_traceback': None, 'raised_exception': False}, 'expectation_c...ected_counts': [{'count': 1, 'value': '1'}, {'count': 1, 'value': 'hello'}, {'count': 1, 'value': 'jello'}], ...}, ...}
data_asset = <great_expectations.execution_engine.pandas_batch_data.PandasBatchData object at 0x7f7806610160>
pk_column = True

    def check_json_test_result(  # noqa: C901 - 52
        test, result, data_asset=None, pk_column=False
    ) -> None:

        # check for id_pk results in cases where pk_column is true and unexpected_index_list already exists
        # this will work for testing since result_format is COMPLETE
        if pk_column:
            if not result["success"]:
                if "unexpected_index_list" in result["result"]:
                    assert "unexpected_index_query" in result["result"]

        if "unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )
            elif "unexpected_list" in test["output"]:
                (
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )

        if "partial_unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "partial_unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )
            elif "partial_unexpected_list" in test["output"]:
                (
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )

        # Determine if np.allclose(..) might be needed for float comparison
        try_allclose = False
        if "observed_value" in test["output"]:
            if RX_FLOAT.match(repr(test["output"]["observed_value"])):
                try_allclose = True

        # Check results
        if test["exact_match_out"] is True:
            if "result" in result and "observed_value" in result["result"]:
                if isinstance(result["result"]["observed_value"], (np.floating, float)):
                    assert np.allclose(
                        result["result"]["observed_value"],
                        expectationValidationResultSchema.load(test["output"])["result"][
                            "observed_value"
                        ],
                        rtol=RTOL,
                        atol=ATOL,
                    ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {expectationValidationResultSchema.load(test['output'])['result']['observed_value']}"
                else:
                    assert result == expectationValidationResultSchema.load(
                        test["output"]
                    ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
            else:
                assert result == expectationValidationResultSchema.load(
                    test["output"]
                ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
        else:
            # Convert result to json since our tests are reading from json so cannot easily contain richer types (e.g. NaN)
            # NOTE - 20191031 - JPC - we may eventually want to change these tests as we update our view on how
            # representations, serializations, and objects should interact and how much of that is shown to the user.
            result = result.to_json_dict()
            for key, value in test["output"].items():
                # Apply our great expectations-specific test logic

                if key == "success":
                    if isinstance(value, (np.floating, float)):
                        try:
                            assert np.allclose(
                                result["success"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['success']} not np.allclose to {value}"
                        except TypeError:
                            assert (
                                result["success"] == value
                            ), f"{result['success']} != {value}"
                    else:
                        assert result["success"] == value, f"{result['success']} != {value}"

                elif key == "observed_value":
                    if "tolerance" in test:
                        if isinstance(value, dict):
                            assert set(result["result"]["observed_value"].keys()) == set(
                                value.keys()
                            ), f"{set(result['result']['observed_value'].keys())} != {set(value.keys())}"
                            for k, v in value.items():
                                assert np.allclose(
                                    result["result"]["observed_value"][k],
                                    v,
                                    rtol=test["tolerance"],
                                )
                        else:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=test["tolerance"],
                            )
                    else:
                        if isinstance(value, dict) and "values" in value:
                            try:
                                assert np.allclose(
                                    result["result"]["observed_value"]["values"],
                                    value["values"],
                                    rtol=RTOL,
                                    atol=ATOL,
                                ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']['values']} not np.allclose to {value['values']}"
                            except TypeError as e:
                                print(e)
                                assert (
                                    result["result"]["observed_value"] == value
                                ), f"{result['result']['observed_value']} != {value}"
                        elif try_allclose:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {value}"
                        else:
                            assert (
                                result["result"]["observed_value"] == value
                            ), f"{result['result']['observed_value']} != {value}"

                # NOTE: This is a key used ONLY for testing cases where an expectation is legitimately allowed to return
                # any of multiple possible observed_values. expect_column_values_to_be_of_type is one such expectation.
                elif key == "observed_value_list":
                    assert result["result"]["observed_value"] in value

                elif key == "unexpected_index_list":
                    if isinstance(data_asset, (SqlAlchemyDataset, SparkDFDataset)):
                        pass
                    elif isinstance(data_asset, (SqlAlchemyBatchData, SparkDFBatchData)):
                        pass
                    else:
>                       assert (
                            result["result"]["unexpected_index_list"] == value
                        ), f"{result['result']['unexpected_index_list']} != {value}"
E                       AssertionError: [{'s': 'hello', 'pk_index': 0}, {'s': 'jello', 'pk_index': 1}, {'s': '1', 'pk_index': 2}] != [0, 1, 2]

great_expectations/self_check/util.py:3075: AssertionError
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 4015.61it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 433.74it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 613.50it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 336.86it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 480.72it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 369.74it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 577.27it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 535.90it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 533.50it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 530.73it/s]
_ test_case_runner_v3_api[pandas/multicolumn_map_expectations/expect_compound_columns_to_be_unique:unexpected_values_with_unexpected_index_list] _

test_case = {'expectation_type': 'expect_compound_columns_to_be_unique', 'pk_column': True, 'skip': False, 'test': {'exact_match_out': False, 'in': {'column_list': ['a', 'b']}, 'input': {'column_list': ['a', 'b']}, 'only_for': ['pandas'], ...}, ...}

    @pytest.mark.order(index=0)
    @pytest.mark.integration
    @pytest.mark.slow  # 12.68s
    def test_case_runner_v3_api(test_case):
        if test_case["skip"]:
            pytest.skip()

        # Note: this should never be done in practice, but we are wiping expectations to reuse batches during testing.
        # test_case["batch"]._initialize_expectations()
        if "parse_strings_as_datetimes" in test_case["test"]["in"]:
            with pytest.deprecated_call():
                evaluate_json_test_v3_api(
                    validator=test_case["validator_with_data"],
                    expectation_type=test_case["expectation_type"],
                    test=test_case["test"],
                    pk_column=test_case["pk_column"],
                )
        else:
>           evaluate_json_test_v3_api(
                validator=test_case["validator_with_data"],
                expectation_type=test_case["expectation_type"],
                test=test_case["test"],
                pk_column=test_case["pk_column"],
            )

tests/test_definitions/test_expectations_v3_api.py:418:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
great_expectations/self_check/util.py:2901: in evaluate_json_test_v3_api
    check_json_test_result(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

test = {'exact_match_out': False, 'in': {'column_list': ['a', 'b']}, 'input': {'column_list': ['a', 'b']}, 'only_for': ['pandas'], ...}
result = {'exception_info': {'exception_message': None, 'exception_traceback': None, 'raised_exception': False}, 'expectation_c...}, {'count': 2, 'value': [1.0, 2.0]}, {'count': 2, 'value': [2.0, 1.0]}, {'count': 2, 'value': [2.0, 2.0]}], ...}, ...}
data_asset = <great_expectations.execution_engine.pandas_batch_data.PandasBatchData object at 0x7f7807aac7f0>
pk_column = True

    def check_json_test_result(  # noqa: C901 - 52
        test, result, data_asset=None, pk_column=False
    ) -> None:

        # check for id_pk results in cases where pk_column is true and unexpected_index_list already exists
        # this will work for testing since result_format is COMPLETE
        if pk_column:
            if not result["success"]:
                if "unexpected_index_list" in result["result"]:
                    assert "unexpected_index_query" in result["result"]

        if "unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )
            elif "unexpected_list" in test["output"]:
                (
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )

        if "partial_unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "partial_unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )
            elif "partial_unexpected_list" in test["output"]:
                (
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )

        # Determine if np.allclose(..) might be needed for float comparison
        try_allclose = False
        if "observed_value" in test["output"]:
            if RX_FLOAT.match(repr(test["output"]["observed_value"])):
                try_allclose = True

        # Check results
        if test["exact_match_out"] is True:
            if "result" in result and "observed_value" in result["result"]:
                if isinstance(result["result"]["observed_value"], (np.floating, float)):
                    assert np.allclose(
                        result["result"]["observed_value"],
                        expectationValidationResultSchema.load(test["output"])["result"][
                            "observed_value"
                        ],
                        rtol=RTOL,
                        atol=ATOL,
                    ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {expectationValidationResultSchema.load(test['output'])['result']['observed_value']}"
                else:
                    assert result == expectationValidationResultSchema.load(
                        test["output"]
                    ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
            else:
                assert result == expectationValidationResultSchema.load(
                    test["output"]
                ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
        else:
            # Convert result to json since our tests are reading from json so cannot easily contain richer types (e.g. NaN)
            # NOTE - 20191031 - JPC - we may eventually want to change these tests as we update our view on how
            # representations, serializations, and objects should interact and how much of that is shown to the user.
            result = result.to_json_dict()
            for key, value in test["output"].items():
                # Apply our great expectations-specific test logic

                if key == "success":
                    if isinstance(value, (np.floating, float)):
                        try:
                            assert np.allclose(
                                result["success"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['success']} not np.allclose to {value}"
                        except TypeError:
                            assert (
                                result["success"] == value
                            ), f"{result['success']} != {value}"
                    else:
                        assert result["success"] == value, f"{result['success']} != {value}"

                elif key == "observed_value":
                    if "tolerance" in test:
                        if isinstance(value, dict):
                            assert set(result["result"]["observed_value"].keys()) == set(
                                value.keys()
                            ), f"{set(result['result']['observed_value'].keys())} != {set(value.keys())}"
                            for k, v in value.items():
                                assert np.allclose(
                                    result["result"]["observed_value"][k],
                                    v,
                                    rtol=test["tolerance"],
                                )
                        else:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=test["tolerance"],
                            )
                    else:
                        if isinstance(value, dict) and "values" in value:
                            try:
                                assert np.allclose(
                                    result["result"]["observed_value"]["values"],
                                    value["values"],
                                    rtol=RTOL,
                                    atol=ATOL,
                                ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']['values']} not np.allclose to {value['values']}"
                            except TypeError as e:
                                print(e)
                                assert (
                                    result["result"]["observed_value"] == value
                                ), f"{result['result']['observed_value']} != {value}"
                        elif try_allclose:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {value}"
                        else:
                            assert (
                                result["result"]["observed_value"] == value
                            ), f"{result['result']['observed_value']} != {value}"

                # NOTE: This is a key used ONLY for testing cases where an expectation is legitimately allowed to return
                # any of multiple possible observed_values. expect_column_values_to_be_of_type is one such expectation.
                elif key == "observed_value_list":
                    assert result["result"]["observed_value"] in value

                elif key == "unexpected_index_list":
                    if isinstance(data_asset, (SqlAlchemyDataset, SparkDFDataset)):
                        pass
                    elif isinstance(data_asset, (SqlAlchemyBatchData, SparkDFBatchData)):
                        pass
                    else:
>                       assert (
                            result["result"]["unexpected_index_list"] == value
                        ), f"{result['result']['unexpected_index_list']} != {value}"
E                       AssertionError: [{'a': 1.0, 'pk_index': 0, 'b': 1.0}, {'a': 1.0, 'pk_index': 1, 'b': 2.0}, {'a': 1.0, 'pk_index': 2, 'b': 1.0}, {'a': 1.0, 'pk_index': 3, 'b': 2.0}, {'a': 1.0, 'pk_index': 4, 'b': 1.0}, {'a': 2.0, 'pk_index': 5, 'b': 2.0}, {'a': 2.0, 'pk_index': 6, 'b': 1.0}, {'a': 2.0, 'pk_index': 7, 'b': 2.0}, {'a': 2.0, 'pk_index': 8, 'b': 1.0}] != [0, 1, 2, 3, 4, 5, 6, 7, 8]

great_expectations/self_check/util.py:3075: AssertionError
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:  22%|██▏       | 2/9 [00:00<00:00, 4167.22it/s]Calculating Metrics:  22%|██▏       | 2/9 [00:00<00:00, 514.26it/s]Calculating Metrics:  33%|███▎      | 3/9 [00:00<00:00, 724.28it/s]Calculating Metrics:  33%|███▎      | 3/9 [00:00<00:00, 399.77it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 405.18it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 315.72it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 436.02it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 410.69it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 408.63it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 406.67it/s]
_ test_case_runner_v3_api[pandas/multicolumn_map_expectations/expect_compound_columns_to_be_unique:unexpected_values_exact_match_out_with_unexpected_index_list] _

test_case = {'expectation_type': 'expect_compound_columns_to_be_unique', 'pk_column': True, 'skip': False, 'test': {'exact_match_out': True, 'in': {'column_list': ['a', 'b']}, 'input': {'column_list': ['a', 'b']}, 'only_for': ['pandas'], ...}, ...}

    @pytest.mark.order(index=0)
    @pytest.mark.integration
    @pytest.mark.slow  # 12.68s
    def test_case_runner_v3_api(test_case):
        if test_case["skip"]:
            pytest.skip()

        # Note: this should never be done in practice, but we are wiping expectations to reuse batches during testing.
        # test_case["batch"]._initialize_expectations()
        if "parse_strings_as_datetimes" in test_case["test"]["in"]:
            with pytest.deprecated_call():
                evaluate_json_test_v3_api(
                    validator=test_case["validator_with_data"],
                    expectation_type=test_case["expectation_type"],
                    test=test_case["test"],
                    pk_column=test_case["pk_column"],
                )
        else:
>           evaluate_json_test_v3_api(
                validator=test_case["validator_with_data"],
                expectation_type=test_case["expectation_type"],
                test=test_case["test"],
                pk_column=test_case["pk_column"],
            )

tests/test_definitions/test_expectations_v3_api.py:418:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
great_expectations/self_check/util.py:2901: in evaluate_json_test_v3_api
    check_json_test_result(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

test = {'exact_match_out': True, 'in': {'column_list': ['a', 'b']}, 'input': {'column_list': ['a', 'b']}, 'only_for': ['pandas'], ...}
result = {
  "expectation_config": {
    "kwargs": {
      "result_format": {
        "result_format": "COMPLETE",
        "une..."exception_info": {
    "raised_exception": false,
    "exception_traceback": null,
    "exception_message": null
  }
}
data_asset = <great_expectations.execution_engine.pandas_batch_data.PandasBatchData object at 0x7f7807aac7f0>
pk_column = True

    def check_json_test_result(  # noqa: C901 - 52
        test, result, data_asset=None, pk_column=False
    ) -> None:

        # check for id_pk results in cases where pk_column is true and unexpected_index_list already exists
        # this will work for testing since result_format is COMPLETE
        if pk_column:
            if not result["success"]:
                if "unexpected_index_list" in result["result"]:
                    assert "unexpected_index_query" in result["result"]

        if "unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )
            elif "unexpected_list" in test["output"]:
                (
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )

        if "partial_unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "partial_unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )
            elif "partial_unexpected_list" in test["output"]:
                (
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )

        # Determine if np.allclose(..) might be needed for float comparison
        try_allclose = False
        if "observed_value" in test["output"]:
            if RX_FLOAT.match(repr(test["output"]["observed_value"])):
                try_allclose = True

        # Check results
        if test["exact_match_out"] is True:
            if "result" in result and "observed_value" in result["result"]:
                if isinstance(result["result"]["observed_value"], (np.floating, float)):
                    assert np.allclose(
                        result["result"]["observed_value"],
                        expectationValidationResultSchema.load(test["output"])["result"][
                            "observed_value"
                        ],
                        rtol=RTOL,
                        atol=ATOL,
                    ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {expectationValidationResultSchema.load(test['output'])['result']['observed_value']}"
                else:
                    assert result == expectationValidationResultSchema.load(
                        test["output"]
                    ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
            else:
>               assert result == expectationValidationResultSchema.load(
                    test["output"]
                ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
E               AssertionError: {
E                 "expectation_config": {
E                   "kwargs": {
E                     "result_format": {
E                       "result_format": "COMPLETE",
E                       "unexpected_index_column_names": [
E                         "pk_index"
E                       ]
E                     },
E                     "include_config": false,
E                     "column_list": [
E                       "a",
E                       "b"
E                     ],
E                     "batch_id": "94abdcd5f03e290f9c946ec4a6e7a794"
E                   },
E                   "expectation_type": "expect_compound_columns_to_be_unique",
E                   "meta": {}
E                 },
E                 "result": {
E                   "element_count": 10,
E                   "unexpected_count": 9,
E                   "unexpected_percent": 100.0,
E                   "partial_unexpected_list": [
E                     {
E                       "a": 1.0,
E                       "b": 1.0
E                     },
E                     {
E                       "a": 1.0,
E                       "b": 1.0
E                     },
E                     {
E                       "a": 1.0,
E                       "b": 1.0
E                     },
E                     {
E                       "a": 1.0,
E                       "b": 2.0
E                     },
E                     {
E                       "a": 1.0,
E                       "b": 2.0
E                     },
E                     {
E                       "a": 2.0,
E                       "b": 1.0
E                     },
E                     {
E                       "a": 2.0,
E                       "b": 1.0
E                     },
E                     {
E                       "a": 2.0,
E                       "b": 2.0
E                     },
E                     {
E                       "a": 2.0,
E                       "b": 2.0
E                     }
E                   ],
E                   "unexpected_index_column_names": [
E                     "pk_index"
E                   ],
E                   "missing_count": 1,
E                   "missing_percent": 10.0,
E                   "unexpected_percent_total": 90.0,
E                   "unexpected_percent_nonmissing": 100.0,
E                   "partial_unexpected_index_list": [
E                     {
E                       "a": 1.0,
E                       "pk_index": 0,
E                       "b": 1.0
E                     },
E                     {
E                       "a": 1.0,
E                       "pk_index": 1,
E                       "b": 2.0
E                     },
E                     {
E                       "a": 1.0,
E                       "pk_index": 2,
E                       "b": 1.0
E                     },
E                     {
E                       "a": 1.0,
E                       "pk_index": 3,
E                       "b": 2.0
E                     },
E                     {
E                       "a": 1.0,
E                       "pk_index": 4,
E                       "b": 1.0
E                     },
E                     {
E                       "a": 2.0,
E                       "pk_index": 5,
E                       "b": 2.0
E                     },
E                     {
E                       "a": 2.0,
E                       "pk_index": 6,
E                       "b": 1.0
E                     },
E                     {
E                       "a": 2.0,
E                       "pk_index": 7,
E                       "b": 2.0
E                     },
E                     {
E                       "a": 2.0,
E                       "pk_index": 8,
E                       "b": 1.0
E                     }
E                   ],
E                   "partial_unexpected_counts": [
E                     {
E                       "value": [
E                         1.0,
E                         1.0
E                       ],
E                       "count": 3
E                     },
E                     {
E                       "value": [
E                         1.0,
E                         2.0
E                       ],
E                       "count": 2
E                     },
E                     {
E                       "value": [
E                         2.0,
E                         1.0
E                       ],
E                       "count": 2
E                     },
E                     {
E                       "value": [
E                         2.0,
E                         2.0
E                       ],
E                       "count": 2
E                     }
E                   ],
E                   "unexpected_list": [
E                     {
E                       "a": 1.0,
E                       "b": 1.0
E                     },
E                     {
E                       "a": 1.0,
E                       "b": 1.0
E                     },
E                     {
E                       "a": 1.0,
E                       "b": 1.0
E                     },
E                     {
E                       "a": 1.0,
E                       "b": 2.0
E                     },
E                     {
E                       "a": 1.0,
E                       "b": 2.0
E                     },
E                     {
E                       "a": 2.0,
E                       "b": 1.0
E                     },
E                     {
E                       "a": 2.0,
E                       "b": 1.0
E                     },
E                     {
E                       "a": 2.0,
E                       "b": 2.0
E                     },
E                     {
E                       "a": 2.0,
E                       "b": 2.0
E                     }
E                   ],
E                   "unexpected_index_list": [
E                     {
E                       "a": 1.0,
E                       "pk_index": 0,
E                       "b": 1.0
E                     },
E                     {
E                       "a": 1.0,
E                       "pk_index": 1,
E                       "b": 2.0
E                     },
E                     {
E                       "a": 1.0,
E                       "pk_index": 2,
E                       "b": 1.0
E                     },
E                     {
E                       "a": 1.0,
E                       "pk_index": 3,
E                       "b": 2.0
E                     },
E                     {
E                       "a": 1.0,
E                       "pk_index": 4,
E                       "b": 1.0
E                     },
E                     {
E                       "a": 2.0,
E                       "pk_index": 5,
E                       "b": 2.0
E                     },
E                     {
E                       "a": 2.0,
E                       "pk_index": 6,
E                       "b": 1.0
E                     },
E                     {
E                       "a": 2.0,
E                       "pk_index": 7,
E                       "b": 2.0
E                     },
E                     {
E                       "a": 2.0,
E                       "pk_index": 8,
E                       "b": 1.0
E                     }
E                   ],
E                   "unexpected_index_query": [
E                     0,
E                     1,
E                     2,
E                     3,
E                     4,
E                     5,
E                     6,
E                     7,
E                     8
E                   ]
E                 },
E                 "success": false,
E                 "meta": {},
E                 "exception_info": {
E                   "raised_exception": false,
E                   "exception_traceback": null,
E                   "exception_message": null
E                 }
E               } != {
E                 "expectation_config": null,
E                 "result": {
E                   "element_count": 10,
E                   "missing_count": 1,
E                   "missing_percent": 10.0,
E                   "unexpected_count": 9,
E                   "unexpected_percent": 100.0,
E                   "unexpected_percent_total": 90.0,
E                   "unexpected_percent_nonmissing": 100.0,
E                   "partial_unexpected_list": [
E                     {
E                       "a": 1.0,
E                       "b": 1.0
E                     },
E                     {
E                       "a": 1.0,
E                       "b": 1.0
E                     },
E                     {
E                       "a": 1.0,
E                       "b": 1.0
E                     },
E                     {
E                       "a": 1.0,
E                       "b": 2.0
E                     },
E                     {
E                       "a": 1.0,
E                       "b": 2.0
E                     },
E                     {
E                       "a": 2.0,
E                       "b": 1.0
E                     },
E                     {
E                       "a": 2.0,
E                       "b": 1.0
E                     },
E                     {
E                       "a": 2.0,
E                       "b": 2.0
E                     },
E                     {
E                       "a": 2.0,
E                       "b": 2.0
E                     }
E                   ],
E                   "partial_unexpected_index_list": [
E                     0,
E                     1,
E                     2,
E                     3,
E                     4,
E                     5,
E                     6,
E                     7,
E                     8
E                   ],
E                   "details": {
E                     "partial_unexpected_counts_error": "partial_unexpected_counts requested, but requires a hashable type"
E                   },
E                   "unexpected_list": [
E                     {
E                       "a": 1.0,
E                       "b": 1.0
E                     },
E                     {
E                       "a": 1.0,
E                       "b": 1.0
E                     },
E                     {
E                       "a": 1.0,
E                       "b": 1.0
E                     },
E                     {
E                       "a": 1.0,
E                       "b": 2.0
E                     },
E                     {
E                       "a": 1.0,
E                       "b": 2.0
E                     },
E                     {
E                       "a": 2.0,
E                       "b": 1.0
E                     },
E                     {
E                       "a": 2.0,
E                       "b": 1.0
E                     },
E                     {
E                       "a": 2.0,
E                       "b": 2.0
E                     },
E                     {
E                       "a": 2.0,
E                       "b": 2.0
E                     }
E                   ],
E                   "unexpected_index_list": [
E                     0,
E                     1,
E                     2,
E                     3,
E                     4,
E                     5,
E                     6,
E                     7,
E                     8
E                   ]
E                 },
E                 "success": false,
E                 "meta": {},
E                 "exception_info": {
E                   "raised_exception": false,
E                   "exception_traceback": null,
E                   "exception_message": null
E                 }
E               }

great_expectations/self_check/util.py:2993: AssertionError
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:  22%|██▏       | 2/9 [00:00<00:00, 4236.67it/s]Calculating Metrics:  22%|██▏       | 2/9 [00:00<00:00, 513.32it/s]Calculating Metrics:  33%|███▎      | 3/9 [00:00<00:00, 720.13it/s]Calculating Metrics:  33%|███▎      | 3/9 [00:00<00:00, 391.24it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 379.43it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 296.76it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 428.33it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 405.46it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 403.80it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 402.04it/s]
_ test_case_runner_v3_api[pandas/multicolumn_map_expectations/expect_select_column_values_to_be_unique_within_record:unexpected_values] _

test_case = {'expectation_type': 'expect_select_column_values_to_be_unique_within_record', 'pk_column': True, 'skip': False, 'test... False, 'in': {'column_list': ['a', 'b']}, 'include_in_gallery': True, 'input': {'column_list': ['a', 'b']}, ...}, ...}

    @pytest.mark.order(index=0)
    @pytest.mark.integration
    @pytest.mark.slow  # 12.68s
    def test_case_runner_v3_api(test_case):
        if test_case["skip"]:
            pytest.skip()

        # Note: this should never be done in practice, but we are wiping expectations to reuse batches during testing.
        # test_case["batch"]._initialize_expectations()
        if "parse_strings_as_datetimes" in test_case["test"]["in"]:
            with pytest.deprecated_call():
                evaluate_json_test_v3_api(
                    validator=test_case["validator_with_data"],
                    expectation_type=test_case["expectation_type"],
                    test=test_case["test"],
                    pk_column=test_case["pk_column"],
                )
        else:
>           evaluate_json_test_v3_api(
                validator=test_case["validator_with_data"],
                expectation_type=test_case["expectation_type"],
                test=test_case["test"],
                pk_column=test_case["pk_column"],
            )

tests/test_definitions/test_expectations_v3_api.py:418:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
great_expectations/self_check/util.py:2901: in evaluate_json_test_v3_api
    check_json_test_result(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

test = {'exact_match_out': False, 'in': {'column_list': ['a', 'b']}, 'include_in_gallery': True, 'input': {'column_list': ['a', 'b']}, ...}
result = {'exception_info': {'exception_message': None, 'exception_traceback': None, 'raised_exception': False}, 'expectation_c...: 10.0, 'partial_unexpected_counts': [{'count': 3, 'value': [1.0, 1.0]}, {'count': 2, 'value': [2.0, 2.0]}], ...}, ...}
data_asset = <great_expectations.execution_engine.pandas_batch_data.PandasBatchData object at 0x7f7807c68cd0>
pk_column = True

    def check_json_test_result(  # noqa: C901 - 52
        test, result, data_asset=None, pk_column=False
    ) -> None:

        # check for id_pk results in cases where pk_column is true and unexpected_index_list already exists
        # this will work for testing since result_format is COMPLETE
        if pk_column:
            if not result["success"]:
                if "unexpected_index_list" in result["result"]:
                    assert "unexpected_index_query" in result["result"]

        if "unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )
            elif "unexpected_list" in test["output"]:
                (
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )

        if "partial_unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "partial_unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )
            elif "partial_unexpected_list" in test["output"]:
                (
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )

        # Determine if np.allclose(..) might be needed for float comparison
        try_allclose = False
        if "observed_value" in test["output"]:
            if RX_FLOAT.match(repr(test["output"]["observed_value"])):
                try_allclose = True

        # Check results
        if test["exact_match_out"] is True:
            if "result" in result and "observed_value" in result["result"]:
                if isinstance(result["result"]["observed_value"], (np.floating, float)):
                    assert np.allclose(
                        result["result"]["observed_value"],
                        expectationValidationResultSchema.load(test["output"])["result"][
                            "observed_value"
                        ],
                        rtol=RTOL,
                        atol=ATOL,
                    ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {expectationValidationResultSchema.load(test['output'])['result']['observed_value']}"
                else:
                    assert result == expectationValidationResultSchema.load(
                        test["output"]
                    ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
            else:
                assert result == expectationValidationResultSchema.load(
                    test["output"]
                ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
        else:
            # Convert result to json since our tests are reading from json so cannot easily contain richer types (e.g. NaN)
            # NOTE - 20191031 - JPC - we may eventually want to change these tests as we update our view on how
            # representations, serializations, and objects should interact and how much of that is shown to the user.
            result = result.to_json_dict()
            for key, value in test["output"].items():
                # Apply our great expectations-specific test logic

                if key == "success":
                    if isinstance(value, (np.floating, float)):
                        try:
                            assert np.allclose(
                                result["success"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['success']} not np.allclose to {value}"
                        except TypeError:
                            assert (
                                result["success"] == value
                            ), f"{result['success']} != {value}"
                    else:
                        assert result["success"] == value, f"{result['success']} != {value}"

                elif key == "observed_value":
                    if "tolerance" in test:
                        if isinstance(value, dict):
                            assert set(result["result"]["observed_value"].keys()) == set(
                                value.keys()
                            ), f"{set(result['result']['observed_value'].keys())} != {set(value.keys())}"
                            for k, v in value.items():
                                assert np.allclose(
                                    result["result"]["observed_value"][k],
                                    v,
                                    rtol=test["tolerance"],
                                )
                        else:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=test["tolerance"],
                            )
                    else:
                        if isinstance(value, dict) and "values" in value:
                            try:
                                assert np.allclose(
                                    result["result"]["observed_value"]["values"],
                                    value["values"],
                                    rtol=RTOL,
                                    atol=ATOL,
                                ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']['values']} not np.allclose to {value['values']}"
                            except TypeError as e:
                                print(e)
                                assert (
                                    result["result"]["observed_value"] == value
                                ), f"{result['result']['observed_value']} != {value}"
                        elif try_allclose:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {value}"
                        else:
                            assert (
                                result["result"]["observed_value"] == value
                            ), f"{result['result']['observed_value']} != {value}"

                # NOTE: This is a key used ONLY for testing cases where an expectation is legitimately allowed to return
                # any of multiple possible observed_values. expect_column_values_to_be_of_type is one such expectation.
                elif key == "observed_value_list":
                    assert result["result"]["observed_value"] in value

                elif key == "unexpected_index_list":
                    if isinstance(data_asset, (SqlAlchemyDataset, SparkDFDataset)):
                        pass
                    elif isinstance(data_asset, (SqlAlchemyBatchData, SparkDFBatchData)):
                        pass
                    else:
>                       assert (
                            result["result"]["unexpected_index_list"] == value
                        ), f"{result['result']['unexpected_index_list']} != {value}"
E                       AssertionError: [{'a': 1.0, 'pk_index': 0, 'b': 1.0}, {'a': 1.0, 'pk_index': 2, 'b': 1.0}, {'a': 1.0, 'pk_index': 4, 'b': 1.0}, {'a': 2.0, 'pk_index': 5, 'b': 2.0}, {'a': 2.0, 'pk_index': 7, 'b': 2.0}] != [0, 2, 4, 5, 7]

great_expectations/self_check/util.py:3075: AssertionError
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:  22%|██▏       | 2/9 [00:00<00:00, 3874.65it/s]Calculating Metrics:  22%|██▏       | 2/9 [00:00<00:00, 486.86it/s]Calculating Metrics:  33%|███▎      | 3/9 [00:00<00:00, 684.45it/s]Calculating Metrics:  33%|███▎      | 3/9 [00:00<00:00, 391.64it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 383.65it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 301.48it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 435.26it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 412.18it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 410.62it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 408.79it/s]
_ test_case_runner_v3_api[pandas/multicolumn_map_expectations/expect_select_column_values_to_be_unique_within_record:unexpected_values_exact_match_out_with_index_list] _

test_case = {'expectation_type': 'expect_select_column_values_to_be_unique_within_record', 'pk_column': True, 'skip': False, 'test...out': True, 'in': {'column_list': ['a', 'b']}, 'input': {'column_list': ['a', 'b']}, 'only_for': ['pandas'], ...}, ...}

    @pytest.mark.order(index=0)
    @pytest.mark.integration
    @pytest.mark.slow  # 12.68s
    def test_case_runner_v3_api(test_case):
        if test_case["skip"]:
            pytest.skip()

        # Note: this should never be done in practice, but we are wiping expectations to reuse batches during testing.
        # test_case["batch"]._initialize_expectations()
        if "parse_strings_as_datetimes" in test_case["test"]["in"]:
            with pytest.deprecated_call():
                evaluate_json_test_v3_api(
                    validator=test_case["validator_with_data"],
                    expectation_type=test_case["expectation_type"],
                    test=test_case["test"],
                    pk_column=test_case["pk_column"],
                )
        else:
>           evaluate_json_test_v3_api(
                validator=test_case["validator_with_data"],
                expectation_type=test_case["expectation_type"],
                test=test_case["test"],
                pk_column=test_case["pk_column"],
            )

tests/test_definitions/test_expectations_v3_api.py:418:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
great_expectations/self_check/util.py:2901: in evaluate_json_test_v3_api
    check_json_test_result(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

test = {'exact_match_out': True, 'in': {'column_list': ['a', 'b']}, 'input': {'column_list': ['a', 'b']}, 'only_for': ['pandas'], ...}
result = {
  "expectation_config": {
    "kwargs": {
      "result_format": {
        "result_format": "COMPLETE",
        "une..."exception_info": {
    "raised_exception": false,
    "exception_traceback": null,
    "exception_message": null
  }
}
data_asset = <great_expectations.execution_engine.pandas_batch_data.PandasBatchData object at 0x7f7807c68cd0>
pk_column = True

    def check_json_test_result(  # noqa: C901 - 52
        test, result, data_asset=None, pk_column=False
    ) -> None:

        # check for id_pk results in cases where pk_column is true and unexpected_index_list already exists
        # this will work for testing since result_format is COMPLETE
        if pk_column:
            if not result["success"]:
                if "unexpected_index_list" in result["result"]:
                    assert "unexpected_index_query" in result["result"]

        if "unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )
            elif "unexpected_list" in test["output"]:
                (
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )

        if "partial_unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "partial_unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )
            elif "partial_unexpected_list" in test["output"]:
                (
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )

        # Determine if np.allclose(..) might be needed for float comparison
        try_allclose = False
        if "observed_value" in test["output"]:
            if RX_FLOAT.match(repr(test["output"]["observed_value"])):
                try_allclose = True

        # Check results
        if test["exact_match_out"] is True:
            if "result" in result and "observed_value" in result["result"]:
                if isinstance(result["result"]["observed_value"], (np.floating, float)):
                    assert np.allclose(
                        result["result"]["observed_value"],
                        expectationValidationResultSchema.load(test["output"])["result"][
                            "observed_value"
                        ],
                        rtol=RTOL,
                        atol=ATOL,
                    ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {expectationValidationResultSchema.load(test['output'])['result']['observed_value']}"
                else:
                    assert result == expectationValidationResultSchema.load(
                        test["output"]
                    ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
            else:
>               assert result == expectationValidationResultSchema.load(
                    test["output"]
                ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
E               AssertionError: {
E                 "expectation_config": {
E                   "kwargs": {
E                     "result_format": {
E                       "result_format": "COMPLETE",
E                       "unexpected_index_column_names": [
E                         "pk_index"
E                       ]
E                     },
E                     "include_config": false,
E                     "column_list": [
E                       "a",
E                       "b"
E                     ],
E                     "batch_id": "94abdcd5f03e290f9c946ec4a6e7a794"
E                   },
E                   "expectation_type": "expect_select_column_values_to_be_unique_within_record",
E                   "meta": {}
E                 },
E                 "result": {
E                   "element_count": 10,
E                   "unexpected_count": 5,
E                   "unexpected_percent": 55.55555555555556,
E                   "partial_unexpected_list": [
E                     {
E                       "a": 1.0,
E                       "b": 1.0
E                     },
E                     {
E                       "a": 1.0,
E                       "b": 1.0
E                     },
E                     {
E                       "a": 1.0,
E                       "b": 1.0
E                     },
E                     {
E                       "a": 2.0,
E                       "b": 2.0
E                     },
E                     {
E                       "a": 2.0,
E                       "b": 2.0
E                     }
E                   ],
E                   "unexpected_index_column_names": [
E                     "pk_index"
E                   ],
E                   "missing_count": 1,
E                   "missing_percent": 10.0,
E                   "unexpected_percent_total": 50.0,
E                   "unexpected_percent_nonmissing": 55.55555555555556,
E                   "partial_unexpected_index_list": [
E                     {
E                       "a": 1.0,
E                       "pk_index": 0,
E                       "b": 1.0
E                     },
E                     {
E                       "a": 1.0,
E                       "pk_index": 2,
E                       "b": 1.0
E                     },
E                     {
E                       "a": 1.0,
E                       "pk_index": 4,
E                       "b": 1.0
E                     },
E                     {
E                       "a": 2.0,
E                       "pk_index": 5,
E                       "b": 2.0
E                     },
E                     {
E                       "a": 2.0,
E                       "pk_index": 7,
E                       "b": 2.0
E                     }
E                   ],
E                   "partial_unexpected_counts": [
E                     {
E                       "value": [
E                         1.0,
E                         1.0
E                       ],
E                       "count": 3
E                     },
E                     {
E                       "value": [
E                         2.0,
E                         2.0
E                       ],
E                       "count": 2
E                     }
E                   ],
E                   "unexpected_list": [
E                     {
E                       "a": 1.0,
E                       "b": 1.0
E                     },
E                     {
E                       "a": 1.0,
E                       "b": 1.0
E                     },
E                     {
E                       "a": 1.0,
E                       "b": 1.0
E                     },
E                     {
E                       "a": 2.0,
E                       "b": 2.0
E                     },
E                     {
E                       "a": 2.0,
E                       "b": 2.0
E                     }
E                   ],
E                   "unexpected_index_list": [
E                     {
E                       "a": 1.0,
E                       "pk_index": 0,
E                       "b": 1.0
E                     },
E                     {
E                       "a": 1.0,
E                       "pk_index": 2,
E                       "b": 1.0
E                     },
E                     {
E                       "a": 1.0,
E                       "pk_index": 4,
E                       "b": 1.0
E                     },
E                     {
E                       "a": 2.0,
E                       "pk_index": 5,
E                       "b": 2.0
E                     },
E                     {
E                       "a": 2.0,
E                       "pk_index": 7,
E                       "b": 2.0
E                     }
E                   ],
E                   "unexpected_index_query": [
E                     0,
E                     2,
E                     4,
E                     5,
E                     7
E                   ]
E                 },
E                 "success": false,
E                 "meta": {},
E                 "exception_info": {
E                   "raised_exception": false,
E                   "exception_traceback": null,
E                   "exception_message": null
E                 }
E               } != {
E                 "expectation_config": null,
E                 "result": {
E                   "element_count": 10,
E                   "missing_count": 1,
E                   "missing_percent": 10.0,
E                   "unexpected_count": 5,
E                   "unexpected_percent": 55.55555555555556,
E                   "unexpected_percent_total": 50.0,
E                   "unexpected_percent_nonmissing": 55.55555555555556,
E                   "partial_unexpected_list": [
E                     {
E                       "a": 1.0,
E                       "b": 1.0
E                     },
E                     {
E                       "a": 1.0,
E                       "b": 1.0
E                     },
E                     {
E                       "a": 1.0,
E                       "b": 1.0
E                     },
E                     {
E                       "a": 2.0,
E                       "b": 2.0
E                     },
E                     {
E                       "a": 2.0,
E                       "b": 2.0
E                     }
E                   ],
E                   "partial_unexpected_index_list": [
E                     0,
E                     2,
E                     4,
E                     5,
E                     7
E                   ],
E                   "details": {
E                     "partial_unexpected_counts_error": "partial_unexpected_counts requested, but requires a hashable type"
E                   },
E                   "unexpected_list": [
E                     {
E                       "a": 1.0,
E                       "b": 1.0
E                     },
E                     {
E                       "a": 1.0,
E                       "b": 1.0
E                     },
E                     {
E                       "a": 1.0,
E                       "b": 1.0
E                     },
E                     {
E                       "a": 2.0,
E                       "b": 2.0
E                     },
E                     {
E                       "a": 2.0,
E                       "b": 2.0
E                     }
E                   ],
E                   "unexpected_index_list": [
E                     0,
E                     2,
E                     4,
E                     5,
E                     7
E                   ]
E                 },
E                 "success": false,
E                 "meta": {},
E                 "exception_info": {
E                   "raised_exception": false,
E                   "exception_traceback": null,
E                   "exception_message": null
E                 }
E               }

great_expectations/self_check/util.py:2993: AssertionError
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:  22%|██▏       | 2/9 [00:00<00:00, 4165.15it/s]Calculating Metrics:  22%|██▏       | 2/9 [00:00<00:00, 515.97it/s]Calculating Metrics:  33%|███▎      | 3/9 [00:00<00:00, 726.54it/s]Calculating Metrics:  33%|███▎      | 3/9 [00:00<00:00, 408.66it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 397.69it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 309.17it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 448.34it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 424.17it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 422.51it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 420.58it/s]
_ test_case_runner_v3_api[pandas/multicolumn_map_expectations/expect_multicolumn_sum_to_equal:basic_negative_test] _

test_case = {'expectation_type': 'expect_multicolumn_sum_to_equal', 'pk_column': True, 'skip': False, 'test': {'exact_match_out': ...a', 'b'], 'sum_total': 1}, 'include_in_gallery': True, 'input': {'column_list': ['a', 'b'], 'sum_total': 1}, ...}, ...}

    @pytest.mark.order(index=0)
    @pytest.mark.integration
    @pytest.mark.slow  # 12.68s
    def test_case_runner_v3_api(test_case):
        if test_case["skip"]:
            pytest.skip()

        # Note: this should never be done in practice, but we are wiping expectations to reuse batches during testing.
        # test_case["batch"]._initialize_expectations()
        if "parse_strings_as_datetimes" in test_case["test"]["in"]:
            with pytest.deprecated_call():
                evaluate_json_test_v3_api(
                    validator=test_case["validator_with_data"],
                    expectation_type=test_case["expectation_type"],
                    test=test_case["test"],
                    pk_column=test_case["pk_column"],
                )
        else:
>           evaluate_json_test_v3_api(
                validator=test_case["validator_with_data"],
                expectation_type=test_case["expectation_type"],
                test=test_case["test"],
                pk_column=test_case["pk_column"],
            )

tests/test_definitions/test_expectations_v3_api.py:418:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
great_expectations/self_check/util.py:2901: in evaluate_json_test_v3_api
    check_json_test_result(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

test = {'exact_match_out': False, 'in': {'column_list': ['a', 'b'], 'sum_total': 1}, 'include_in_gallery': True, 'input': {'column_list': ['a', 'b'], 'sum_total': 1}, ...}
result = {'exception_info': {'exception_message': None, 'exception_traceback': None, 'raised_exception': False}, 'expectation_c... [1, 1]}, {'count': 1, 'value': [1, 2]}, {'count': 1, 'value': [2, 0]}, {'count': 1, 'value': [3, 0]}, ...], ...}, ...}
data_asset = <great_expectations.execution_engine.pandas_batch_data.PandasBatchData object at 0x7f7807cdd430>
pk_column = True

    def check_json_test_result(  # noqa: C901 - 52
        test, result, data_asset=None, pk_column=False
    ) -> None:

        # check for id_pk results in cases where pk_column is true and unexpected_index_list already exists
        # this will work for testing since result_format is COMPLETE
        if pk_column:
            if not result["success"]:
                if "unexpected_index_list" in result["result"]:
                    assert "unexpected_index_query" in result["result"]

        if "unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )
            elif "unexpected_list" in test["output"]:
                (
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )

        if "partial_unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "partial_unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )
            elif "partial_unexpected_list" in test["output"]:
                (
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )

        # Determine if np.allclose(..) might be needed for float comparison
        try_allclose = False
        if "observed_value" in test["output"]:
            if RX_FLOAT.match(repr(test["output"]["observed_value"])):
                try_allclose = True

        # Check results
        if test["exact_match_out"] is True:
            if "result" in result and "observed_value" in result["result"]:
                if isinstance(result["result"]["observed_value"], (np.floating, float)):
                    assert np.allclose(
                        result["result"]["observed_value"],
                        expectationValidationResultSchema.load(test["output"])["result"][
                            "observed_value"
                        ],
                        rtol=RTOL,
                        atol=ATOL,
                    ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {expectationValidationResultSchema.load(test['output'])['result']['observed_value']}"
                else:
                    assert result == expectationValidationResultSchema.load(
                        test["output"]
                    ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
            else:
                assert result == expectationValidationResultSchema.load(
                    test["output"]
                ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
        else:
            # Convert result to json since our tests are reading from json so cannot easily contain richer types (e.g. NaN)
            # NOTE - 20191031 - JPC - we may eventually want to change these tests as we update our view on how
            # representations, serializations, and objects should interact and how much of that is shown to the user.
            result = result.to_json_dict()
            for key, value in test["output"].items():
                # Apply our great expectations-specific test logic

                if key == "success":
                    if isinstance(value, (np.floating, float)):
                        try:
                            assert np.allclose(
                                result["success"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['success']} not np.allclose to {value}"
                        except TypeError:
                            assert (
                                result["success"] == value
                            ), f"{result['success']} != {value}"
                    else:
                        assert result["success"] == value, f"{result['success']} != {value}"

                elif key == "observed_value":
                    if "tolerance" in test:
                        if isinstance(value, dict):
                            assert set(result["result"]["observed_value"].keys()) == set(
                                value.keys()
                            ), f"{set(result['result']['observed_value'].keys())} != {set(value.keys())}"
                            for k, v in value.items():
                                assert np.allclose(
                                    result["result"]["observed_value"][k],
                                    v,
                                    rtol=test["tolerance"],
                                )
                        else:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=test["tolerance"],
                            )
                    else:
                        if isinstance(value, dict) and "values" in value:
                            try:
                                assert np.allclose(
                                    result["result"]["observed_value"]["values"],
                                    value["values"],
                                    rtol=RTOL,
                                    atol=ATOL,
                                ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']['values']} not np.allclose to {value['values']}"
                            except TypeError as e:
                                print(e)
                                assert (
                                    result["result"]["observed_value"] == value
                                ), f"{result['result']['observed_value']} != {value}"
                        elif try_allclose:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {value}"
                        else:
                            assert (
                                result["result"]["observed_value"] == value
                            ), f"{result['result']['observed_value']} != {value}"

                # NOTE: This is a key used ONLY for testing cases where an expectation is legitimately allowed to return
                # any of multiple possible observed_values. expect_column_values_to_be_of_type is one such expectation.
                elif key == "observed_value_list":
                    assert result["result"]["observed_value"] in value

                elif key == "unexpected_index_list":
                    if isinstance(data_asset, (SqlAlchemyDataset, SparkDFDataset)):
                        pass
                    elif isinstance(data_asset, (SqlAlchemyBatchData, SparkDFBatchData)):
                        pass
                    else:
>                       assert (
                            result["result"]["unexpected_index_list"] == value
                        ), f"{result['result']['unexpected_index_list']} != {value}"
E                       AssertionError: [{'a': 1, 'pk_index': 0, 'b': 1}, {'a': 4, 'pk_index': 2, 'b': 0}, {'a': 0, 'pk_index': 3, 'b': 0}, {'a': 1, 'pk_index': 4, 'b': 2}, {'a': 3, 'pk_index': 7, 'b': 0}, {'a': 2, 'pk_index': 8, 'b': 0}, {'a': -3, 'pk_index': 9, 'b': 3}] != [0, 2, 3, 4, 7, 8, 9]

great_expectations/self_check/util.py:3075: AssertionError
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:  22%|██▏       | 2/9 [00:00<00:00, 3975.64it/s]Calculating Metrics:  22%|██▏       | 2/9 [00:00<00:00, 441.41it/s]Calculating Metrics:  33%|███▎      | 3/9 [00:00<00:00, 620.21it/s]Calculating Metrics:  33%|███▎      | 3/9 [00:00<00:00, 342.38it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 353.76it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 273.15it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 420.32it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 394.49it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 392.99it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 391.32it/s]
_ test_case_runner_v3_api[pandas/multicolumn_map_expectations/expect_multicolumn_sum_to_equal:negative_test_with_ignore_if_any_are_missing] _

test_case = {'expectation_type': 'expect_multicolumn_sum_to_equal', 'pk_column': True, 'skip': False, 'test': {'exact_match_out': ..._list': [1, 4, 7], 'unexpected_list': [{'w': 0.4, 'y': 0.8}, {'w': 0.56, 'y': 0.48}, {'w': 0.8, 'y': 0.4}]}, ...}, ...}

    @pytest.mark.order(index=0)
    @pytest.mark.integration
    @pytest.mark.slow  # 12.68s
    def test_case_runner_v3_api(test_case):
        if test_case["skip"]:
            pytest.skip()

        # Note: this should never be done in practice, but we are wiping expectations to reuse batches during testing.
        # test_case["batch"]._initialize_expectations()
        if "parse_strings_as_datetimes" in test_case["test"]["in"]:
            with pytest.deprecated_call():
                evaluate_json_test_v3_api(
                    validator=test_case["validator_with_data"],
                    expectation_type=test_case["expectation_type"],
                    test=test_case["test"],
                    pk_column=test_case["pk_column"],
                )
        else:
>           evaluate_json_test_v3_api(
                validator=test_case["validator_with_data"],
                expectation_type=test_case["expectation_type"],
                test=test_case["test"],
                pk_column=test_case["pk_column"],
            )

tests/test_definitions/test_expectations_v3_api.py:418:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
great_expectations/self_check/util.py:2901: in evaluate_json_test_v3_api
    check_json_test_result(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

test = {'exact_match_out': False, 'in': {'column_list': ['w', 'y'], 'ignore_row_if': 'any_value_is_missing', 'sum_total': 1},..._index_list': [1, 4, 7], 'unexpected_list': [{'w': 0.4, 'y': 0.8}, {'w': 0.56, 'y': 0.48}, {'w': 0.8, 'y': 0.4}]}, ...}
result = {'exception_info': {'exception_message': None, 'exception_traceback': None, 'raised_exception': False}, 'expectation_c...[{'count': 1, 'value': [0.4, 0.8]}, {'count': 1, 'value': [0.56, 0.48]}, {'count': 1, 'value': [0.8, 0.4]}], ...}, ...}
data_asset = <great_expectations.execution_engine.pandas_batch_data.PandasBatchData object at 0x7f7807cdd430>
pk_column = True

    def check_json_test_result(  # noqa: C901 - 52
        test, result, data_asset=None, pk_column=False
    ) -> None:

        # check for id_pk results in cases where pk_column is true and unexpected_index_list already exists
        # this will work for testing since result_format is COMPLETE
        if pk_column:
            if not result["success"]:
                if "unexpected_index_list" in result["result"]:
                    assert "unexpected_index_query" in result["result"]

        if "unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )
            elif "unexpected_list" in test["output"]:
                (
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )

        if "partial_unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "partial_unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )
            elif "partial_unexpected_list" in test["output"]:
                (
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )

        # Determine if np.allclose(..) might be needed for float comparison
        try_allclose = False
        if "observed_value" in test["output"]:
            if RX_FLOAT.match(repr(test["output"]["observed_value"])):
                try_allclose = True

        # Check results
        if test["exact_match_out"] is True:
            if "result" in result and "observed_value" in result["result"]:
                if isinstance(result["result"]["observed_value"], (np.floating, float)):
                    assert np.allclose(
                        result["result"]["observed_value"],
                        expectationValidationResultSchema.load(test["output"])["result"][
                            "observed_value"
                        ],
                        rtol=RTOL,
                        atol=ATOL,
                    ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {expectationValidationResultSchema.load(test['output'])['result']['observed_value']}"
                else:
                    assert result == expectationValidationResultSchema.load(
                        test["output"]
                    ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
            else:
                assert result == expectationValidationResultSchema.load(
                    test["output"]
                ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
        else:
            # Convert result to json since our tests are reading from json so cannot easily contain richer types (e.g. NaN)
            # NOTE - 20191031 - JPC - we may eventually want to change these tests as we update our view on how
            # representations, serializations, and objects should interact and how much of that is shown to the user.
            result = result.to_json_dict()
            for key, value in test["output"].items():
                # Apply our great expectations-specific test logic

                if key == "success":
                    if isinstance(value, (np.floating, float)):
                        try:
                            assert np.allclose(
                                result["success"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['success']} not np.allclose to {value}"
                        except TypeError:
                            assert (
                                result["success"] == value
                            ), f"{result['success']} != {value}"
                    else:
                        assert result["success"] == value, f"{result['success']} != {value}"

                elif key == "observed_value":
                    if "tolerance" in test:
                        if isinstance(value, dict):
                            assert set(result["result"]["observed_value"].keys()) == set(
                                value.keys()
                            ), f"{set(result['result']['observed_value'].keys())} != {set(value.keys())}"
                            for k, v in value.items():
                                assert np.allclose(
                                    result["result"]["observed_value"][k],
                                    v,
                                    rtol=test["tolerance"],
                                )
                        else:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=test["tolerance"],
                            )
                    else:
                        if isinstance(value, dict) and "values" in value:
                            try:
                                assert np.allclose(
                                    result["result"]["observed_value"]["values"],
                                    value["values"],
                                    rtol=RTOL,
                                    atol=ATOL,
                                ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']['values']} not np.allclose to {value['values']}"
                            except TypeError as e:
                                print(e)
                                assert (
                                    result["result"]["observed_value"] == value
                                ), f"{result['result']['observed_value']} != {value}"
                        elif try_allclose:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {value}"
                        else:
                            assert (
                                result["result"]["observed_value"] == value
                            ), f"{result['result']['observed_value']} != {value}"

                # NOTE: This is a key used ONLY for testing cases where an expectation is legitimately allowed to return
                # any of multiple possible observed_values. expect_column_values_to_be_of_type is one such expectation.
                elif key == "observed_value_list":
                    assert result["result"]["observed_value"] in value

                elif key == "unexpected_index_list":
                    if isinstance(data_asset, (SqlAlchemyDataset, SparkDFDataset)):
                        pass
                    elif isinstance(data_asset, (SqlAlchemyBatchData, SparkDFBatchData)):
                        pass
                    else:
>                       assert (
                            result["result"]["unexpected_index_list"] == value
                        ), f"{result['result']['unexpected_index_list']} != {value}"
E                       AssertionError: [{'w': 0.4, 'pk_index': 1, 'y': 0.8}, {'w': 0.8, 'pk_index': 4, 'y': 0.4}, {'w': 0.56, 'pk_index': 7, 'y': 0.48}] != [1, 4, 7]

great_expectations/self_check/util.py:3075: AssertionError
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:  22%|██▏       | 2/9 [00:00<00:00, 4074.12it/s]Calculating Metrics:  22%|██▏       | 2/9 [00:00<00:00, 447.82it/s]Calculating Metrics:  33%|███▎      | 3/9 [00:00<00:00, 633.20it/s]Calculating Metrics:  33%|███▎      | 3/9 [00:00<00:00, 343.96it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 360.89it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 274.38it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 410.77it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 386.86it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 385.47it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 383.84it/s]
_ test_case_runner_v3_api[pandas/multicolumn_map_expectations/expect_multicolumn_sum_to_equal:negative_test_different_value] _

test_case = {'expectation_type': 'expect_multicolumn_sum_to_equal', 'pk_column': True, 'skip': False, 'test': {'exact_match_out': ...x': 0.7}, {'w': 0.4, 'x': 0.6}, {'w': 0.45, 'x': 0.55}, {'w': 0.5, 'x': 0.5}, {'w': 0.56, 'x': 0.44}, ...]}, ...}, ...}

    @pytest.mark.order(index=0)
    @pytest.mark.integration
    @pytest.mark.slow  # 12.68s
    def test_case_runner_v3_api(test_case):
        if test_case["skip"]:
            pytest.skip()

        # Note: this should never be done in practice, but we are wiping expectations to reuse batches during testing.
        # test_case["batch"]._initialize_expectations()
        if "parse_strings_as_datetimes" in test_case["test"]["in"]:
            with pytest.deprecated_call():
                evaluate_json_test_v3_api(
                    validator=test_case["validator_with_data"],
                    expectation_type=test_case["expectation_type"],
                    test=test_case["test"],
                    pk_column=test_case["pk_column"],
                )
        else:
>           evaluate_json_test_v3_api(
                validator=test_case["validator_with_data"],
                expectation_type=test_case["expectation_type"],
                test=test_case["test"],
                pk_column=test_case["pk_column"],
            )

tests/test_definitions/test_expectations_v3_api.py:418:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
great_expectations/self_check/util.py:2901: in evaluate_json_test_v3_api
    check_json_test_result(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

test = {'exact_match_out': False, 'in': {'column_list': ['w', 'x'], 'ignore_row_if': 'any_value_is_missing', 'sum_total': 1.1...0.3, 'x': 0.7}, {'w': 0.4, 'x': 0.6}, {'w': 0.45, 'x': 0.55}, {'w': 0.5, 'x': 0.5}, {'w': 0.56, 'x': 0.44}, ...]}, ...}
result = {'exception_info': {'exception_message': None, 'exception_traceback': None, 'raised_exception': False}, 'expectation_c...t': 1, 'value': [0.45, 0.55]}, {'count': 1, 'value': [0.5, 0.5]}, {'count': 1, 'value': [0.56, 0.44]}, ...], ...}, ...}
data_asset = <great_expectations.execution_engine.pandas_batch_data.PandasBatchData object at 0x7f7807cdd430>
pk_column = True

    def check_json_test_result(  # noqa: C901 - 52
        test, result, data_asset=None, pk_column=False
    ) -> None:

        # check for id_pk results in cases where pk_column is true and unexpected_index_list already exists
        # this will work for testing since result_format is COMPLETE
        if pk_column:
            if not result["success"]:
                if "unexpected_index_list" in result["result"]:
                    assert "unexpected_index_query" in result["result"]

        if "unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )
            elif "unexpected_list" in test["output"]:
                (
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )

        if "partial_unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "partial_unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )
            elif "partial_unexpected_list" in test["output"]:
                (
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )

        # Determine if np.allclose(..) might be needed for float comparison
        try_allclose = False
        if "observed_value" in test["output"]:
            if RX_FLOAT.match(repr(test["output"]["observed_value"])):
                try_allclose = True

        # Check results
        if test["exact_match_out"] is True:
            if "result" in result and "observed_value" in result["result"]:
                if isinstance(result["result"]["observed_value"], (np.floating, float)):
                    assert np.allclose(
                        result["result"]["observed_value"],
                        expectationValidationResultSchema.load(test["output"])["result"][
                            "observed_value"
                        ],
                        rtol=RTOL,
                        atol=ATOL,
                    ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {expectationValidationResultSchema.load(test['output'])['result']['observed_value']}"
                else:
                    assert result == expectationValidationResultSchema.load(
                        test["output"]
                    ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
            else:
                assert result == expectationValidationResultSchema.load(
                    test["output"]
                ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
        else:
            # Convert result to json since our tests are reading from json so cannot easily contain richer types (e.g. NaN)
            # NOTE - 20191031 - JPC - we may eventually want to change these tests as we update our view on how
            # representations, serializations, and objects should interact and how much of that is shown to the user.
            result = result.to_json_dict()
            for key, value in test["output"].items():
                # Apply our great expectations-specific test logic

                if key == "success":
                    if isinstance(value, (np.floating, float)):
                        try:
                            assert np.allclose(
                                result["success"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['success']} not np.allclose to {value}"
                        except TypeError:
                            assert (
                                result["success"] == value
                            ), f"{result['success']} != {value}"
                    else:
                        assert result["success"] == value, f"{result['success']} != {value}"

                elif key == "observed_value":
                    if "tolerance" in test:
                        if isinstance(value, dict):
                            assert set(result["result"]["observed_value"].keys()) == set(
                                value.keys()
                            ), f"{set(result['result']['observed_value'].keys())} != {set(value.keys())}"
                            for k, v in value.items():
                                assert np.allclose(
                                    result["result"]["observed_value"][k],
                                    v,
                                    rtol=test["tolerance"],
                                )
                        else:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=test["tolerance"],
                            )
                    else:
                        if isinstance(value, dict) and "values" in value:
                            try:
                                assert np.allclose(
                                    result["result"]["observed_value"]["values"],
                                    value["values"],
                                    rtol=RTOL,
                                    atol=ATOL,
                                ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']['values']} not np.allclose to {value['values']}"
                            except TypeError as e:
                                print(e)
                                assert (
                                    result["result"]["observed_value"] == value
                                ), f"{result['result']['observed_value']} != {value}"
                        elif try_allclose:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {value}"
                        else:
                            assert (
                                result["result"]["observed_value"] == value
                            ), f"{result['result']['observed_value']} != {value}"

                # NOTE: This is a key used ONLY for testing cases where an expectation is legitimately allowed to return
                # any of multiple possible observed_values. expect_column_values_to_be_of_type is one such expectation.
                elif key == "observed_value_list":
                    assert result["result"]["observed_value"] in value

                elif key == "unexpected_index_list":
                    if isinstance(data_asset, (SqlAlchemyDataset, SparkDFDataset)):
                        pass
                    elif isinstance(data_asset, (SqlAlchemyBatchData, SparkDFBatchData)):
                        pass
                    else:
>                       assert (
                            result["result"]["unexpected_index_list"] == value
                        ), f"{result['result']['unexpected_index_list']} != {value}"
E                       AssertionError: [{'w': 0.2, 'pk_index': 0, 'x': 0.8}, {'w': 0.4, 'pk_index': 1, 'x': 0.6}, {'w': 0.3, 'pk_index': 2, 'x': 0.7}, {'w': 0.5, 'pk_index': 3, 'x': 0.5}, {'w': 0.8, 'pk_index': 4, 'x': 0.2}, {'w': 0.7, 'pk_index': 5, 'x': 0.3}, {'w': 0.71, 'pk_index': 6, 'x': 0.29}, {'w': 0.56, 'pk_index': 7, 'x': 0.44}, {'w': 0.45, 'pk_index': 8, 'x': 0.55}, {'w': 1.0, 'pk_index': 9, 'x': 0.0}] != [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]

great_expectations/self_check/util.py:3075: AssertionError
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:  22%|██▏       | 2/9 [00:00<00:00, 4282.09it/s]Calculating Metrics:  22%|██▏       | 2/9 [00:00<00:00, 439.36it/s]Calculating Metrics:  33%|███▎      | 3/9 [00:00<00:00, 624.71it/s]Calculating Metrics:  33%|███▎      | 3/9 [00:00<00:00, 338.64it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 360.30it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 274.80it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 410.13it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 382.34it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 380.63it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 378.90it/s]
_ test_case_runner_v3_api[pandas/column_pair_map_expectations/expect_column_pair_values_to_be_in_set:negative_test_with_nulls] _

test_case = {'expectation_type': 'expect_column_pair_values_to_be_in_set', 'pk_column': True, 'skip': False, 'test': {'exact_match...e, 'input': {'column_A': 'x', 'column_B': 'z', 'value_pairs_set': [[1, 1], [2, 2], [3, 3], [4, 4], [5, 5]]}, ...}, ...}

    @pytest.mark.order(index=0)
    @pytest.mark.integration
    @pytest.mark.slow  # 12.68s
    def test_case_runner_v3_api(test_case):
        if test_case["skip"]:
            pytest.skip()

        # Note: this should never be done in practice, but we are wiping expectations to reuse batches during testing.
        # test_case["batch"]._initialize_expectations()
        if "parse_strings_as_datetimes" in test_case["test"]["in"]:
            with pytest.deprecated_call():
                evaluate_json_test_v3_api(
                    validator=test_case["validator_with_data"],
                    expectation_type=test_case["expectation_type"],
                    test=test_case["test"],
                    pk_column=test_case["pk_column"],
                )
        else:
>           evaluate_json_test_v3_api(
                validator=test_case["validator_with_data"],
                expectation_type=test_case["expectation_type"],
                test=test_case["test"],
                pk_column=test_case["pk_column"],
            )

tests/test_definitions/test_expectations_v3_api.py:418:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
great_expectations/self_check/util.py:2901: in evaluate_json_test_v3_api
    check_json_test_result(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

test = {'exact_match_out': False, 'in': {'column_A': 'x', 'column_B': 'z', 'value_pairs_set': [[1, 1], [2, 2], [3, 3], [4, 4]...': True, 'input': {'column_A': 'x', 'column_B': 'z', 'value_pairs_set': [[1, 1], [2, 2], [3, 3], [4, 4], [5, 5]]}, ...}
result = {'exception_info': {'exception_message': None, 'exception_traceback': None, 'raised_exception': False}, 'expectation_c...e]}, {'count': 1, 'value': [8, None]}, {'count': 1, 'value': [9, None]}, {'count': 1, 'value': [10, None]}], ...}, ...}
data_asset = <great_expectations.execution_engine.pandas_batch_data.PandasBatchData object at 0x7f7807d0d8e0>
pk_column = True

    def check_json_test_result(  # noqa: C901 - 52
        test, result, data_asset=None, pk_column=False
    ) -> None:

        # check for id_pk results in cases where pk_column is true and unexpected_index_list already exists
        # this will work for testing since result_format is COMPLETE
        if pk_column:
            if not result["success"]:
                if "unexpected_index_list" in result["result"]:
                    assert "unexpected_index_query" in result["result"]

        if "unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )
            elif "unexpected_list" in test["output"]:
                (
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )

        if "partial_unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "partial_unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )
            elif "partial_unexpected_list" in test["output"]:
                (
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )

        # Determine if np.allclose(..) might be needed for float comparison
        try_allclose = False
        if "observed_value" in test["output"]:
            if RX_FLOAT.match(repr(test["output"]["observed_value"])):
                try_allclose = True

        # Check results
        if test["exact_match_out"] is True:
            if "result" in result and "observed_value" in result["result"]:
                if isinstance(result["result"]["observed_value"], (np.floating, float)):
                    assert np.allclose(
                        result["result"]["observed_value"],
                        expectationValidationResultSchema.load(test["output"])["result"][
                            "observed_value"
                        ],
                        rtol=RTOL,
                        atol=ATOL,
                    ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {expectationValidationResultSchema.load(test['output'])['result']['observed_value']}"
                else:
                    assert result == expectationValidationResultSchema.load(
                        test["output"]
                    ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
            else:
                assert result == expectationValidationResultSchema.load(
                    test["output"]
                ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
        else:
            # Convert result to json since our tests are reading from json so cannot easily contain richer types (e.g. NaN)
            # NOTE - 20191031 - JPC - we may eventually want to change these tests as we update our view on how
            # representations, serializations, and objects should interact and how much of that is shown to the user.
            result = result.to_json_dict()
            for key, value in test["output"].items():
                # Apply our great expectations-specific test logic

                if key == "success":
                    if isinstance(value, (np.floating, float)):
                        try:
                            assert np.allclose(
                                result["success"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['success']} not np.allclose to {value}"
                        except TypeError:
                            assert (
                                result["success"] == value
                            ), f"{result['success']} != {value}"
                    else:
                        assert result["success"] == value, f"{result['success']} != {value}"

                elif key == "observed_value":
                    if "tolerance" in test:
                        if isinstance(value, dict):
                            assert set(result["result"]["observed_value"].keys()) == set(
                                value.keys()
                            ), f"{set(result['result']['observed_value'].keys())} != {set(value.keys())}"
                            for k, v in value.items():
                                assert np.allclose(
                                    result["result"]["observed_value"][k],
                                    v,
                                    rtol=test["tolerance"],
                                )
                        else:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=test["tolerance"],
                            )
                    else:
                        if isinstance(value, dict) and "values" in value:
                            try:
                                assert np.allclose(
                                    result["result"]["observed_value"]["values"],
                                    value["values"],
                                    rtol=RTOL,
                                    atol=ATOL,
                                ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']['values']} not np.allclose to {value['values']}"
                            except TypeError as e:
                                print(e)
                                assert (
                                    result["result"]["observed_value"] == value
                                ), f"{result['result']['observed_value']} != {value}"
                        elif try_allclose:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {value}"
                        else:
                            assert (
                                result["result"]["observed_value"] == value
                            ), f"{result['result']['observed_value']} != {value}"

                # NOTE: This is a key used ONLY for testing cases where an expectation is legitimately allowed to return
                # any of multiple possible observed_values. expect_column_values_to_be_of_type is one such expectation.
                elif key == "observed_value_list":
                    assert result["result"]["observed_value"] in value

                elif key == "unexpected_index_list":
                    if isinstance(data_asset, (SqlAlchemyDataset, SparkDFDataset)):
                        pass
                    elif isinstance(data_asset, (SqlAlchemyBatchData, SparkDFBatchData)):
                        pass
                    else:
>                       assert (
                            result["result"]["unexpected_index_list"] == value
                        ), f"{result['result']['unexpected_index_list']} != {value}"
E                       AssertionError: [{'x': 6, 'pk_index': 5, 'z': None}, {'x': 7, 'pk_index': 6, 'z': None}, {'x': 8, 'pk_index': 7, 'z': None}, {'x': 9, 'pk_index': 8, 'z': None}, {'x': 10, 'pk_index': 9, 'z': None}] != [5, 6, 7, 8, 9]

great_expectations/self_check/util.py:3075: AssertionError
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 5629.94it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 796.19it/s]Calculating Metrics:  40%|████      | 4/10 [00:00<00:00, 1008.06it/s]Calculating Metrics:  40%|████      | 4/10 [00:00<00:00, 529.12it/s] Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 445.20it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 351.79it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 497.93it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 469.46it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 467.48it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 465.34it/s]
_ test_case_runner_v3_api[pandas/column_pair_map_expectations/expect_column_pair_values_A_to_be_greater_than_B:test_ties] _

test_case = {'expectation_type': 'expect_column_pair_values_A_to_be_greater_than_B', 'pk_column': True, 'skip': False, 'test': {'e..._in_gallery': True, 'input': {'column_A': 'w', 'column_B': 'z', 'ignore_row_if': 'either_value_is_missing'}, ...}, ...}

    @pytest.mark.order(index=0)
    @pytest.mark.integration
    @pytest.mark.slow  # 12.68s
    def test_case_runner_v3_api(test_case):
        if test_case["skip"]:
            pytest.skip()

        # Note: this should never be done in practice, but we are wiping expectations to reuse batches during testing.
        # test_case["batch"]._initialize_expectations()
        if "parse_strings_as_datetimes" in test_case["test"]["in"]:
            with pytest.deprecated_call():
                evaluate_json_test_v3_api(
                    validator=test_case["validator_with_data"],
                    expectation_type=test_case["expectation_type"],
                    test=test_case["test"],
                    pk_column=test_case["pk_column"],
                )
        else:
>           evaluate_json_test_v3_api(
                validator=test_case["validator_with_data"],
                expectation_type=test_case["expectation_type"],
                test=test_case["test"],
                pk_column=test_case["pk_column"],
            )

tests/test_definitions/test_expectations_v3_api.py:418:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
great_expectations/self_check/util.py:2901: in evaluate_json_test_v3_api
    check_json_test_result(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

test = {'exact_match_out': False, 'in': {'column_A': 'w', 'column_B': 'z', 'ignore_row_if': 'either_value_is_missing'}, 'include_in_gallery': True, 'input': {'column_A': 'w', 'column_B': 'z', 'ignore_row_if': 'either_value_is_missing'}, ...}
result = {'exception_info': {'exception_message': None, 'exception_traceback': None, 'raised_exception': False}, 'expectation_c..., 2.0]}, {'count': 1, 'value': [3, 3.0]}, {'count': 1, 'value': [4, 4.0]}, {'count': 1, 'value': [5, 5.0]}], ...}, ...}
data_asset = <great_expectations.execution_engine.pandas_batch_data.PandasBatchData object at 0x7f7807b48070>
pk_column = True

    def check_json_test_result(  # noqa: C901 - 52
        test, result, data_asset=None, pk_column=False
    ) -> None:

        # check for id_pk results in cases where pk_column is true and unexpected_index_list already exists
        # this will work for testing since result_format is COMPLETE
        if pk_column:
            if not result["success"]:
                if "unexpected_index_list" in result["result"]:
                    assert "unexpected_index_query" in result["result"]

        if "unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )
            elif "unexpected_list" in test["output"]:
                (
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )

        if "partial_unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "partial_unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )
            elif "partial_unexpected_list" in test["output"]:
                (
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )

        # Determine if np.allclose(..) might be needed for float comparison
        try_allclose = False
        if "observed_value" in test["output"]:
            if RX_FLOAT.match(repr(test["output"]["observed_value"])):
                try_allclose = True

        # Check results
        if test["exact_match_out"] is True:
            if "result" in result and "observed_value" in result["result"]:
                if isinstance(result["result"]["observed_value"], (np.floating, float)):
                    assert np.allclose(
                        result["result"]["observed_value"],
                        expectationValidationResultSchema.load(test["output"])["result"][
                            "observed_value"
                        ],
                        rtol=RTOL,
                        atol=ATOL,
                    ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {expectationValidationResultSchema.load(test['output'])['result']['observed_value']}"
                else:
                    assert result == expectationValidationResultSchema.load(
                        test["output"]
                    ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
            else:
                assert result == expectationValidationResultSchema.load(
                    test["output"]
                ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
        else:
            # Convert result to json since our tests are reading from json so cannot easily contain richer types (e.g. NaN)
            # NOTE - 20191031 - JPC - we may eventually want to change these tests as we update our view on how
            # representations, serializations, and objects should interact and how much of that is shown to the user.
            result = result.to_json_dict()
            for key, value in test["output"].items():
                # Apply our great expectations-specific test logic

                if key == "success":
                    if isinstance(value, (np.floating, float)):
                        try:
                            assert np.allclose(
                                result["success"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['success']} not np.allclose to {value}"
                        except TypeError:
                            assert (
                                result["success"] == value
                            ), f"{result['success']} != {value}"
                    else:
                        assert result["success"] == value, f"{result['success']} != {value}"

                elif key == "observed_value":
                    if "tolerance" in test:
                        if isinstance(value, dict):
                            assert set(result["result"]["observed_value"].keys()) == set(
                                value.keys()
                            ), f"{set(result['result']['observed_value'].keys())} != {set(value.keys())}"
                            for k, v in value.items():
                                assert np.allclose(
                                    result["result"]["observed_value"][k],
                                    v,
                                    rtol=test["tolerance"],
                                )
                        else:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=test["tolerance"],
                            )
                    else:
                        if isinstance(value, dict) and "values" in value:
                            try:
                                assert np.allclose(
                                    result["result"]["observed_value"]["values"],
                                    value["values"],
                                    rtol=RTOL,
                                    atol=ATOL,
                                ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']['values']} not np.allclose to {value['values']}"
                            except TypeError as e:
                                print(e)
                                assert (
                                    result["result"]["observed_value"] == value
                                ), f"{result['result']['observed_value']} != {value}"
                        elif try_allclose:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {value}"
                        else:
                            assert (
                                result["result"]["observed_value"] == value
                            ), f"{result['result']['observed_value']} != {value}"

                # NOTE: This is a key used ONLY for testing cases where an expectation is legitimately allowed to return
                # any of multiple possible observed_values. expect_column_values_to_be_of_type is one such expectation.
                elif key == "observed_value_list":
                    assert result["result"]["observed_value"] in value

                elif key == "unexpected_index_list":
                    if isinstance(data_asset, (SqlAlchemyDataset, SparkDFDataset)):
                        pass
                    elif isinstance(data_asset, (SqlAlchemyBatchData, SparkDFBatchData)):
                        pass
                    else:
>                       assert (
                            result["result"]["unexpected_index_list"] == value
                        ), f"{result['result']['unexpected_index_list']} != {value}"
E                       AssertionError: [{'w': 1, 'pk_index': 0, 'z': 1.0}, {'w': 2, 'pk_index': 1, 'z': 2.0}, {'w': 3, 'pk_index': 2, 'z': 3.0}, {'w': 4, 'pk_index': 3, 'z': 4.0}, {'w': 5, 'pk_index': 4, 'z': 5.0}] != [0, 1, 2, 3, 4]

great_expectations/self_check/util.py:3075: AssertionError
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:  22%|██▏       | 2/9 [00:00<00:00, 4000.29it/s]Calculating Metrics:  22%|██▏       | 2/9 [00:00<00:00, 351.68it/s]Calculating Metrics:  33%|███▎      | 3/9 [00:00<00:00, 496.29it/s]Calculating Metrics:  33%|███▎      | 3/9 [00:00<00:00, 270.52it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 311.31it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 230.31it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 362.45it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 337.60it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 336.45it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 335.17it/s]
_ test_case_runner_v3_api[pandas/column_pair_map_expectations/expect_column_pair_values_A_to_be_greater_than_B:test_parse_strings_as_datetimes_and_mostly] _

test_case = {'expectation_type': 'expect_column_pair_values_A_to_be_greater_than_B', 'pk_column': True, 'skip': False, 'test': {'e...column_A': 'a', 'column_B': 'b', 'mostly': 0.6, 'parse_strings_as_datetimes': True}, 'only_for': ['pandas'], ...}, ...}

    @pytest.mark.order(index=0)
    @pytest.mark.integration
    @pytest.mark.slow  # 12.68s
    def test_case_runner_v3_api(test_case):
        if test_case["skip"]:
            pytest.skip()

        # Note: this should never be done in practice, but we are wiping expectations to reuse batches during testing.
        # test_case["batch"]._initialize_expectations()
        if "parse_strings_as_datetimes" in test_case["test"]["in"]:
            with pytest.deprecated_call():
>               evaluate_json_test_v3_api(
                    validator=test_case["validator_with_data"],
                    expectation_type=test_case["expectation_type"],
                    test=test_case["test"],
                    pk_column=test_case["pk_column"],
                )

tests/test_definitions/test_expectations_v3_api.py:411:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
great_expectations/self_check/util.py:2901: in evaluate_json_test_v3_api
    check_json_test_result(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

test = {'exact_match_out': False, 'in': {'column_A': 'a', 'column_B': 'b', 'mostly': 0.6, 'parse_strings_as_datetimes': True}...t': {'column_A': 'a', 'column_B': 'b', 'mostly': 0.6, 'parse_strings_as_datetimes': True}, 'only_for': ['pandas'], ...}
result = {'exception_info': {'exception_message': None, 'exception_traceback': None, 'raised_exception': False}, 'expectation_c...counts': [{'count': 1, 'value': ['1/1/2016', '1/1/2016']}, {'count': 1, 'value': ['5/5/2016', '5/1/2020']}], ...}, ...}
data_asset = <great_expectations.execution_engine.pandas_batch_data.PandasBatchData object at 0x7f7807b48070>
pk_column = True

    def check_json_test_result(  # noqa: C901 - 52
        test, result, data_asset=None, pk_column=False
    ) -> None:

        # check for id_pk results in cases where pk_column is true and unexpected_index_list already exists
        # this will work for testing since result_format is COMPLETE
        if pk_column:
            if not result["success"]:
                if "unexpected_index_list" in result["result"]:
                    assert "unexpected_index_query" in result["result"]

        if "unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )
            elif "unexpected_list" in test["output"]:
                (
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )

        if "partial_unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "partial_unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )
            elif "partial_unexpected_list" in test["output"]:
                (
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )

        # Determine if np.allclose(..) might be needed for float comparison
        try_allclose = False
        if "observed_value" in test["output"]:
            if RX_FLOAT.match(repr(test["output"]["observed_value"])):
                try_allclose = True

        # Check results
        if test["exact_match_out"] is True:
            if "result" in result and "observed_value" in result["result"]:
                if isinstance(result["result"]["observed_value"], (np.floating, float)):
                    assert np.allclose(
                        result["result"]["observed_value"],
                        expectationValidationResultSchema.load(test["output"])["result"][
                            "observed_value"
                        ],
                        rtol=RTOL,
                        atol=ATOL,
                    ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {expectationValidationResultSchema.load(test['output'])['result']['observed_value']}"
                else:
                    assert result == expectationValidationResultSchema.load(
                        test["output"]
                    ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
            else:
                assert result == expectationValidationResultSchema.load(
                    test["output"]
                ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
        else:
            # Convert result to json since our tests are reading from json so cannot easily contain richer types (e.g. NaN)
            # NOTE - 20191031 - JPC - we may eventually want to change these tests as we update our view on how
            # representations, serializations, and objects should interact and how much of that is shown to the user.
            result = result.to_json_dict()
            for key, value in test["output"].items():
                # Apply our great expectations-specific test logic

                if key == "success":
                    if isinstance(value, (np.floating, float)):
                        try:
                            assert np.allclose(
                                result["success"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['success']} not np.allclose to {value}"
                        except TypeError:
                            assert (
                                result["success"] == value
                            ), f"{result['success']} != {value}"
                    else:
                        assert result["success"] == value, f"{result['success']} != {value}"

                elif key == "observed_value":
                    if "tolerance" in test:
                        if isinstance(value, dict):
                            assert set(result["result"]["observed_value"].keys()) == set(
                                value.keys()
                            ), f"{set(result['result']['observed_value'].keys())} != {set(value.keys())}"
                            for k, v in value.items():
                                assert np.allclose(
                                    result["result"]["observed_value"][k],
                                    v,
                                    rtol=test["tolerance"],
                                )
                        else:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=test["tolerance"],
                            )
                    else:
                        if isinstance(value, dict) and "values" in value:
                            try:
                                assert np.allclose(
                                    result["result"]["observed_value"]["values"],
                                    value["values"],
                                    rtol=RTOL,
                                    atol=ATOL,
                                ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']['values']} not np.allclose to {value['values']}"
                            except TypeError as e:
                                print(e)
                                assert (
                                    result["result"]["observed_value"] == value
                                ), f"{result['result']['observed_value']} != {value}"
                        elif try_allclose:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {value}"
                        else:
                            assert (
                                result["result"]["observed_value"] == value
                            ), f"{result['result']['observed_value']} != {value}"

                # NOTE: This is a key used ONLY for testing cases where an expectation is legitimately allowed to return
                # any of multiple possible observed_values. expect_column_values_to_be_of_type is one such expectation.
                elif key == "observed_value_list":
                    assert result["result"]["observed_value"] in value

                elif key == "unexpected_index_list":
                    if isinstance(data_asset, (SqlAlchemyDataset, SparkDFDataset)):
                        pass
                    elif isinstance(data_asset, (SqlAlchemyBatchData, SparkDFBatchData)):
                        pass
                    else:
>                       assert (
                            result["result"]["unexpected_index_list"] == value
                        ), f"{result['result']['unexpected_index_list']} != {value}"
E                       AssertionError: [{'a': '1/1/2016', 'pk_index': 0, 'b': '1/1/2016'}, {'a': '5/5/2016', 'pk_index': 4, 'b': '5/1/2020'}] != [0, 4]

great_expectations/self_check/util.py:3075: AssertionError
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:  22%|██▏       | 2/9 [00:00<00:00, 4082.05it/s]Calculating Metrics:  22%|██▏       | 2/9 [00:00<00:00, 357.52it/s]Calculating Metrics:  33%|███▎      | 3/9 [00:00<00:00, 510.21it/s]Calculating Metrics:  33%|███▎      | 3/9 [00:00<00:00, 256.74it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 268.20it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 208.47it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 326.63it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 306.92it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 306.02it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 305.00it/s]
_ test_case_runner_v3_api[pandas/column_pair_map_expectations/expect_column_pair_values_A_to_be_greater_than_B:test_mostly] _

test_case = {'expectation_type': 'expect_column_pair_values_A_to_be_greater_than_B', 'pk_column': True, 'skip': False, 'test': {'e...stly': 0.8}, 'out': {'success': True, 'unexpected_index_list': [3, 4], 'unexpected_list': [[5, 6], [6, 7]]}, ...}, ...}

    @pytest.mark.order(index=0)
    @pytest.mark.integration
    @pytest.mark.slow  # 12.68s
    def test_case_runner_v3_api(test_case):
        if test_case["skip"]:
            pytest.skip()

        # Note: this should never be done in practice, but we are wiping expectations to reuse batches during testing.
        # test_case["batch"]._initialize_expectations()
        if "parse_strings_as_datetimes" in test_case["test"]["in"]:
            with pytest.deprecated_call():
                evaluate_json_test_v3_api(
                    validator=test_case["validator_with_data"],
                    expectation_type=test_case["expectation_type"],
                    test=test_case["test"],
                    pk_column=test_case["pk_column"],
                )
        else:
>           evaluate_json_test_v3_api(
                validator=test_case["validator_with_data"],
                expectation_type=test_case["expectation_type"],
                test=test_case["test"],
                pk_column=test_case["pk_column"],
            )

tests/test_definitions/test_expectations_v3_api.py:418:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
great_expectations/self_check/util.py:2901: in evaluate_json_test_v3_api
    check_json_test_result(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

test = {'exact_match_out': False, 'in': {'column_A': 'x', 'column_B': 'c', 'mostly': 0.8}, 'input': {'column_A': 'x', 'column...', 'mostly': 0.8}, 'out': {'success': True, 'unexpected_index_list': [3, 4], 'unexpected_list': [[5, 6], [6, 7]]}, ...}
result = {'exception_info': {'exception_message': None, 'exception_traceback': None, 'raised_exception': False}, 'expectation_c..._percent': 0.0, 'partial_unexpected_counts': [{'count': 1, 'value': [5, 6]}, {'count': 1, 'value': [6, 7]}], ...}, ...}
data_asset = <great_expectations.execution_engine.pandas_batch_data.PandasBatchData object at 0x7f7807b48070>
pk_column = True

    def check_json_test_result(  # noqa: C901 - 52
        test, result, data_asset=None, pk_column=False
    ) -> None:

        # check for id_pk results in cases where pk_column is true and unexpected_index_list already exists
        # this will work for testing since result_format is COMPLETE
        if pk_column:
            if not result["success"]:
                if "unexpected_index_list" in result["result"]:
                    assert "unexpected_index_query" in result["result"]

        if "unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )
            elif "unexpected_list" in test["output"]:
                (
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )

        if "partial_unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "partial_unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )
            elif "partial_unexpected_list" in test["output"]:
                (
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )

        # Determine if np.allclose(..) might be needed for float comparison
        try_allclose = False
        if "observed_value" in test["output"]:
            if RX_FLOAT.match(repr(test["output"]["observed_value"])):
                try_allclose = True

        # Check results
        if test["exact_match_out"] is True:
            if "result" in result and "observed_value" in result["result"]:
                if isinstance(result["result"]["observed_value"], (np.floating, float)):
                    assert np.allclose(
                        result["result"]["observed_value"],
                        expectationValidationResultSchema.load(test["output"])["result"][
                            "observed_value"
                        ],
                        rtol=RTOL,
                        atol=ATOL,
                    ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {expectationValidationResultSchema.load(test['output'])['result']['observed_value']}"
                else:
                    assert result == expectationValidationResultSchema.load(
                        test["output"]
                    ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
            else:
                assert result == expectationValidationResultSchema.load(
                    test["output"]
                ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
        else:
            # Convert result to json since our tests are reading from json so cannot easily contain richer types (e.g. NaN)
            # NOTE - 20191031 - JPC - we may eventually want to change these tests as we update our view on how
            # representations, serializations, and objects should interact and how much of that is shown to the user.
            result = result.to_json_dict()
            for key, value in test["output"].items():
                # Apply our great expectations-specific test logic

                if key == "success":
                    if isinstance(value, (np.floating, float)):
                        try:
                            assert np.allclose(
                                result["success"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['success']} not np.allclose to {value}"
                        except TypeError:
                            assert (
                                result["success"] == value
                            ), f"{result['success']} != {value}"
                    else:
                        assert result["success"] == value, f"{result['success']} != {value}"

                elif key == "observed_value":
                    if "tolerance" in test:
                        if isinstance(value, dict):
                            assert set(result["result"]["observed_value"].keys()) == set(
                                value.keys()
                            ), f"{set(result['result']['observed_value'].keys())} != {set(value.keys())}"
                            for k, v in value.items():
                                assert np.allclose(
                                    result["result"]["observed_value"][k],
                                    v,
                                    rtol=test["tolerance"],
                                )
                        else:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=test["tolerance"],
                            )
                    else:
                        if isinstance(value, dict) and "values" in value:
                            try:
                                assert np.allclose(
                                    result["result"]["observed_value"]["values"],
                                    value["values"],
                                    rtol=RTOL,
                                    atol=ATOL,
                                ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']['values']} not np.allclose to {value['values']}"
                            except TypeError as e:
                                print(e)
                                assert (
                                    result["result"]["observed_value"] == value
                                ), f"{result['result']['observed_value']} != {value}"
                        elif try_allclose:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {value}"
                        else:
                            assert (
                                result["result"]["observed_value"] == value
                            ), f"{result['result']['observed_value']} != {value}"

                # NOTE: This is a key used ONLY for testing cases where an expectation is legitimately allowed to return
                # any of multiple possible observed_values. expect_column_values_to_be_of_type is one such expectation.
                elif key == "observed_value_list":
                    assert result["result"]["observed_value"] in value

                elif key == "unexpected_index_list":
                    if isinstance(data_asset, (SqlAlchemyDataset, SparkDFDataset)):
                        pass
                    elif isinstance(data_asset, (SqlAlchemyBatchData, SparkDFBatchData)):
                        pass
                    else:
>                       assert (
                            result["result"]["unexpected_index_list"] == value
                        ), f"{result['result']['unexpected_index_list']} != {value}"
E                       AssertionError: [{'x': 5, 'pk_index': 3, 'c': 6}, {'x': 6, 'pk_index': 4, 'c': 7}] != [3, 4]

great_expectations/self_check/util.py:3075: AssertionError
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:  22%|██▏       | 2/9 [00:00<00:00, 2732.45it/s]Calculating Metrics:  22%|██▏       | 2/9 [00:00<00:00, 310.03it/s]Calculating Metrics:  33%|███▎      | 3/9 [00:00<00:00, 442.73it/s]Calculating Metrics:  33%|███▎      | 3/9 [00:00<00:00, 246.49it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 280.92it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 213.85it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 344.86it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 321.16it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 319.96it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 318.80it/s]
=============================== warnings summary ===============================
../../../../Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/inspect.py:351
  /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/inspect.py:351: FutureWarning: pandas.Float64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.
    value = getattr(object, key)

../../../../Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/inspect.py:351
  /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/inspect.py:351: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.
    value = getattr(object, key)

../../../../Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/inspect.py:351
  /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/inspect.py:351: FutureWarning: pandas.UInt64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.
    value = getattr(object, key)

../ENVs/supercon_ge/lib/python3.9/site-packages/botocore/httpsession.py:18
  /Users/work/Development/ENVs/supercon_ge/lib/python3.9/site-packages/botocore/httpsession.py:18: DeprecationWarning: 'urllib3.contrib.pyopenssl' module is deprecated and will be removed in a future release of urllib3 2.x. Read more in this issue: https://github.com/urllib3/urllib3/issues/2680
    from urllib3.contrib.pyopenssl import orig_util_SSLContext as SSLContext

tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_not_be_null:basic_positive_case_complete_result_format]
tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_null:basic_positive_case_basic_result_format]
  /Users/work/Development/great_expectations/great_expectations/expectations/metrics/map_metric_provider.py:1527: FutureWarning: The behavior of `series[i:j]` with an integer-dtype index is deprecated. In a future version, this will be treated as *label-based* indexing, consistent with e.g. `series[i]` lookups. To retain the old behavior, use `series.iloc[i:j]`. To get the future behavior, use `series.loc[i:j]`.
    return list(domain_values[: result_format["partial_unexpected_count"]])

tests/test_definitions/test_expectations_v3_api.py: 39 warnings
  /Users/work/Development/great_expectations/great_expectations/core/expectation_validation_result.py:139: DeprecationWarning: NotImplemented should not be used in a boolean context
    return all(

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
==================================== PASSES ====================================
_ test_case_runner_v3_api[sqlite/multi_table_expectations/expect_table_row_count_to_equal_other_table:basic_positive] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 466.84it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 365.56it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 254.26it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 240.43it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 238.08it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 235.39it/s]
_ test_case_runner_v3_api[sqlite/multi_table_expectations/expect_table_row_count_to_equal_other_table:basic_negative] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 4629.47it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 2036.56it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1342.28it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1218.04it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1181.66it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1142.47it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_value_lengths_to_equal:positive_test_string_one_character_length] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/11 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/11 [00:00<?, ?it/s]Calculating Metrics:  18%|█▊        | 2/11 [00:00<00:00, 1222.65it/s]Calculating Metrics:  18%|█▊        | 2/11 [00:00<00:00, 265.79it/s]Calculating Metrics:  27%|██▋       | 3/11 [00:00<00:00, 380.39it/s]Calculating Metrics:  27%|██▋       | 3/11 [00:00<00:00, 236.97it/s]Calculating Metrics:  45%|████▌     | 5/11 [00:00<00:00, 290.75it/s]Calculating Metrics:  45%|████▌     | 5/11 [00:00<00:00, 238.86it/s]Calculating Metrics:  64%|██████▎   | 7/11 [00:00<00:00, 312.30it/s]Calculating Metrics:  64%|██████▎   | 7/11 [00:00<00:00, 274.34it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 376.67it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 358.57it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 357.52it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 356.36it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_value_lengths_to_equal:negative_test_string_value_is_1_too_high] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/11 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/11 [00:00<?, ?it/s]Calculating Metrics:  18%|█▊        | 2/11 [00:00<00:00, 3968.12it/s]Calculating Metrics:  18%|█▊        | 2/11 [00:00<00:00, 394.31it/s]Calculating Metrics:  27%|██▋       | 3/11 [00:00<00:00, 562.01it/s]Calculating Metrics:  27%|██▋       | 3/11 [00:00<00:00, 308.04it/s]Calculating Metrics:  45%|████▌     | 5/11 [00:00<00:00, 438.49it/s]Calculating Metrics:  45%|████▌     | 5/11 [00:00<00:00, 331.48it/s]Calculating Metrics:  64%|██████▎   | 7/11 [00:00<00:00, 425.94it/s]Calculating Metrics:  64%|██████▎   | 7/11 [00:00<00:00, 361.78it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 449.51it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 423.96it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 422.46it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 420.84it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_value_lengths_to_equal:positive_test_with_missing_value_in_column_mostly] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/11 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/11 [00:00<?, ?it/s]Calculating Metrics:  18%|█▊        | 2/11 [00:00<00:00, 4066.22it/s]Calculating Metrics:  18%|█▊        | 2/11 [00:00<00:00, 398.19it/s]Calculating Metrics:  27%|██▋       | 3/11 [00:00<00:00, 562.54it/s]Calculating Metrics:  27%|██▋       | 3/11 [00:00<00:00, 314.01it/s]Calculating Metrics:  45%|████▌     | 5/11 [00:00<00:00, 445.20it/s]Calculating Metrics:  45%|████▌     | 5/11 [00:00<00:00, 335.20it/s]Calculating Metrics:  64%|██████▎   | 7/11 [00:00<00:00, 426.24it/s]Calculating Metrics:  64%|██████▎   | 7/11 [00:00<00:00, 357.62it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 479.12it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 450.66it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 449.15it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 447.38it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_value_lengths_to_equal:negative_one_length_too_small] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/11 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/11 [00:00<?, ?it/s]Calculating Metrics:  18%|█▊        | 2/11 [00:00<00:00, 4156.89it/s]Calculating Metrics:  18%|█▊        | 2/11 [00:00<00:00, 389.70it/s]Calculating Metrics:  27%|██▋       | 3/11 [00:00<00:00, 554.41it/s]Calculating Metrics:  27%|██▋       | 3/11 [00:00<00:00, 312.53it/s]Calculating Metrics:  45%|████▌     | 5/11 [00:00<00:00, 443.06it/s]Calculating Metrics:  45%|████▌     | 5/11 [00:00<00:00, 333.57it/s]Calculating Metrics:  64%|██████▎   | 7/11 [00:00<00:00, 423.31it/s]Calculating Metrics:  64%|██████▎   | 7/11 [00:00<00:00, 356.76it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 478.90it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 450.43it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 448.90it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 447.11it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_value_lengths_to_equal:positive_one_length_too_small_mostly] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/11 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/11 [00:00<?, ?it/s]Calculating Metrics:  18%|█▊        | 2/11 [00:00<00:00, 3194.44it/s]Calculating Metrics:  18%|█▊        | 2/11 [00:00<00:00, 379.44it/s]Calculating Metrics:  27%|██▋       | 3/11 [00:00<00:00, 537.25it/s]Calculating Metrics:  27%|██▋       | 3/11 [00:00<00:00, 305.77it/s]Calculating Metrics:  45%|████▌     | 5/11 [00:00<00:00, 435.20it/s]Calculating Metrics:  45%|████▌     | 5/11 [00:00<00:00, 328.24it/s]Calculating Metrics:  64%|██████▎   | 7/11 [00:00<00:00, 419.84it/s]Calculating Metrics:  64%|██████▎   | 7/11 [00:00<00:00, 354.90it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 472.47it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 441.45it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 439.51it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 437.62it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_value_lengths_to_equal:test_conditional_expectation_passes] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/11 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/11 [00:00<?, ?it/s]Calculating Metrics:  18%|█▊        | 2/11 [00:00<00:00, 205.71it/s]Calculating Metrics:  18%|█▊        | 2/11 [00:00<00:00, 153.98it/s]Calculating Metrics:  27%|██▋       | 3/11 [00:00<00:00, 226.74it/s]Calculating Metrics:  27%|██▋       | 3/11 [00:00<00:00, 185.32it/s]Calculating Metrics:  45%|████▌     | 5/11 [00:00<00:00, 243.97it/s]Calculating Metrics:  45%|████▌     | 5/11 [00:00<00:00, 215.15it/s]Calculating Metrics:  64%|██████▎   | 7/11 [00:00<00:00, 261.78it/s]Calculating Metrics:  64%|██████▎   | 7/11 [00:00<00:00, 241.43it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 290.06it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 280.15it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 279.40it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 278.52it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_value_lengths_to_equal:test_conditional_expectation_fails] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/11 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/11 [00:00<?, ?it/s]Calculating Metrics:  18%|█▊        | 2/11 [00:00<00:00, 578.17it/s]Calculating Metrics:  18%|█▊        | 2/11 [00:00<00:00, 304.45it/s]Calculating Metrics:  27%|██▋       | 3/11 [00:00<00:00, 442.08it/s]Calculating Metrics:  27%|██▋       | 3/11 [00:00<00:00, 304.46it/s]Calculating Metrics:  45%|████▌     | 5/11 [00:00<00:00, 338.73it/s]Calculating Metrics:  45%|████▌     | 5/11 [00:00<00:00, 279.81it/s]Calculating Metrics:  64%|██████▎   | 7/11 [00:00<00:00, 337.77it/s]Calculating Metrics:  64%|██████▎   | 7/11 [00:00<00:00, 306.24it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 370.68it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 355.95it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 354.94it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 353.77it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_value_lengths_to_equal:test_conditional_expectation_parser_errors] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/11 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/11 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/11 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/11 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/11 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/11 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/11 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/11 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/11 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/11 [00:00<?, ?it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_between:basic_positive_now_test] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 2600.31it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 281.41it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 403.27it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 242.58it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 243.62it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 199.46it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 315.82it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 293.82it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 292.59it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 291.39it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_between:basic_negative_now_test] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 3502.55it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 347.34it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 494.65it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 274.35it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 386.64it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 291.62it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 423.21it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 389.17it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 386.25it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 383.13it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_between:basic_positive_now_timedelta_test] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 3914.42it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 348.62it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 495.64it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 271.72it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 383.38it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 280.60it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 415.49it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 385.65it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 383.03it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 380.68it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_between:basic_negative_now_timedelta_test] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 3890.82it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 348.49it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 489.46it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 238.45it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 322.14it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 235.68it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 361.22it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 334.37it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 332.27it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 329.92it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_between:basic_positive_datetime_test] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 3323.54it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 343.84it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 490.29it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 280.43it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 385.27it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 296.47it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 461.85it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 427.66it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 425.23it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 422.92it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_between:basic_negative_datetime_test] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 3760.02it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 335.29it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 479.86it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 270.18it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 377.36it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 286.09it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 437.93it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 406.49it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 404.13it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 400.82it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_json_parseable:basic_positive_test] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 3081.78it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 381.25it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 526.88it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 303.24it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 403.10it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 311.64it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 483.42it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 450.80it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 448.37it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 445.60it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_json_parseable:positive_test_with_nulls] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 2963.13it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 362.72it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 510.98it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 295.94it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 407.97it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 317.70it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 490.48it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 457.45it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 455.12it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 452.25it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_json_parseable:negative_test_integers] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 3968.12it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 487.91it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 686.02it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 386.56it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 541.37it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 413.53it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 622.82it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 578.00it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 574.59it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 570.97it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_json_parseable:negative_test_string_only] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 3053.73it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 458.09it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 645.44it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 373.37it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 506.42it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 393.12it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 606.05it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 566.45it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 563.74it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 560.35it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_json_parseable:negative_test_null_only] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 2860.08it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 395.71it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 536.84it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 315.78it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 448.46it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 360.43it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 561.54it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 525.65it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 522.32it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 518.34it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_between:basic_positive_test_with_timestamps_tz_informed] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 3675.99it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 317.31it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 448.62it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 255.34it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 344.59it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 273.15it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 445.47it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 415.57it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 413.11it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 410.59it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_between:basic_negative_test_with_timestamps_tz_informed] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 3591.01it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 329.87it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 465.19it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 265.75it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 376.37it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 288.55it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 466.12it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 432.42it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 430.57it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 428.06it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_not_be_null:basic_positive_case_complete_result_format] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/6 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/6 [00:00<?, ?it/s]Calculating Metrics:  33%|███▎      | 2/6 [00:00<00:00, 3795.75it/s]Calculating Metrics:  33%|███▎      | 2/6 [00:00<00:00, 860.28it/s]Calculating Metrics:  50%|█████     | 3/6 [00:00<00:00, 1151.75it/s]Calculating Metrics:  50%|█████     | 3/6 [00:00<00:00, 715.55it/s] Calculating Metrics:  67%|██████▋   | 4/6 [00:00<00:00, 813.84it/s]Calculating Metrics:  67%|██████▋   | 4/6 [00:00<00:00, 639.40it/s]Calculating Metrics: 100%|██████████| 6/6 [00:00<00:00, 651.47it/s]Calculating Metrics: 100%|██████████| 6/6 [00:00<00:00, 610.51it/s]Calculating Metrics: 100%|██████████| 6/6 [00:00<00:00, 605.28it/s]Calculating Metrics: 100%|██████████| 6/6 [00:00<00:00, 598.94it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_not_be_null:negative_all_missing_values] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/8 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/8 [00:00<?, ?it/s]Calculating Metrics:  25%|██▌       | 2/8 [00:00<00:00, 3960.63it/s]Calculating Metrics:  25%|██▌       | 2/8 [00:00<00:00, 615.23it/s]Calculating Metrics:  38%|███▊      | 3/8 [00:00<00:00, 854.70it/s]Calculating Metrics:  38%|███▊      | 3/8 [00:00<00:00, 497.29it/s]Calculating Metrics:  50%|█████     | 4/8 [00:00<00:00, 577.79it/s]Calculating Metrics:  50%|█████     | 4/8 [00:00<00:00, 438.26it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 684.53it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 633.57it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 629.13it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 624.13it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_not_be_null:positive_mostly] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/8 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/8 [00:00<?, ?it/s]Calculating Metrics:  25%|██▌       | 2/8 [00:00<00:00, 3199.32it/s]Calculating Metrics:  25%|██▌       | 2/8 [00:00<00:00, 564.81it/s]Calculating Metrics:  38%|███▊      | 3/8 [00:00<00:00, 733.48it/s]Calculating Metrics:  38%|███▊      | 3/8 [00:00<00:00, 418.80it/s]Calculating Metrics:  50%|█████     | 4/8 [00:00<00:00, 501.94it/s]Calculating Metrics:  50%|█████     | 4/8 [00:00<00:00, 388.99it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 636.63it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 593.38it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 589.57it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 581.89it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_not_be_null:negative_no_mostly_one_missing] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/8 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/8 [00:00<?, ?it/s]Calculating Metrics:  25%|██▌       | 2/8 [00:00<00:00, 3490.89it/s]Calculating Metrics:  25%|██▌       | 2/8 [00:00<00:00, 591.16it/s]Calculating Metrics:  38%|███▊      | 3/8 [00:00<00:00, 817.18it/s]Calculating Metrics:  38%|███▊      | 3/8 [00:00<00:00, 482.68it/s]Calculating Metrics:  50%|█████     | 4/8 [00:00<00:00, 581.88it/s]Calculating Metrics:  50%|█████     | 4/8 [00:00<00:00, 438.73it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 702.53it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 650.56it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 640.00it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 629.28it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_not_be_null:negative_mostly] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/8 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/8 [00:00<?, ?it/s]Calculating Metrics:  25%|██▌       | 2/8 [00:00<00:00, 3943.87it/s]Calculating Metrics:  25%|██▌       | 2/8 [00:00<00:00, 616.54it/s]Calculating Metrics:  38%|███▊      | 3/8 [00:00<00:00, 856.16it/s]Calculating Metrics:  38%|███▊      | 3/8 [00:00<00:00, 485.92it/s]Calculating Metrics:  50%|█████     | 4/8 [00:00<00:00, 581.05it/s]Calculating Metrics:  50%|█████     | 4/8 [00:00<00:00, 436.21it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 686.94it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 636.03it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 631.66it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 622.05it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_not_be_null:positive_case_with_mostly_and_no_unexpected_values] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/8 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/8 [00:00<?, ?it/s]Calculating Metrics:  25%|██▌       | 2/8 [00:00<00:00, 3483.64it/s]Calculating Metrics:  25%|██▌       | 2/8 [00:00<00:00, 553.70it/s]Calculating Metrics:  38%|███▊      | 3/8 [00:00<00:00, 763.34it/s]Calculating Metrics:  38%|███▊      | 3/8 [00:00<00:00, 377.12it/s]Calculating Metrics:  50%|█████     | 4/8 [00:00<00:00, 441.40it/s]Calculating Metrics:  50%|█████     | 4/8 [00:00<00:00, 352.20it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 584.02it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 547.11it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 543.81it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 536.91it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_not_be_null:negative_case_with_75percent_null_values_no_mostly] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/8 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/8 [00:00<?, ?it/s]Calculating Metrics:  25%|██▌       | 2/8 [00:00<00:00, 3731.59it/s]Calculating Metrics:  25%|██▌       | 2/8 [00:00<00:00, 615.59it/s]Calculating Metrics:  38%|███▊      | 3/8 [00:00<00:00, 854.70it/s]Calculating Metrics:  38%|███▊      | 3/8 [00:00<00:00, 494.40it/s]Calculating Metrics:  50%|█████     | 4/8 [00:00<00:00, 594.45it/s]Calculating Metrics:  50%|█████     | 4/8 [00:00<00:00, 447.08it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 699.36it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 642.26it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 636.95it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 631.41it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_match_regex_list:basic_positive_test] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 3760.02it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 422.81it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 601.22it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 330.43it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 428.59it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 332.31it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 532.77it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 493.24it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 490.75it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 488.04it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_match_regex_list:positive_test_with_multiple_regexes] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 2933.08it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 388.13it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 549.95it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 315.65it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 414.11it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 324.34it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 518.35it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 481.83it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 479.78it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 477.33it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_match_regex_list:basic_negative_test] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 3985.09it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 422.49it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 600.73it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 332.31it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 434.10it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 336.00it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 531.52it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 494.24it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 492.13it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 487.99it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_match_regex_list:negative_test_with_more_string-ish_strings] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 3766.78it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 390.69it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 554.12it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 315.03it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 418.07it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 322.94it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 511.99it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 476.06it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 473.24it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 469.60it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_match_regex_list:positive_test_with_match_on__any] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 4072.14it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 425.69it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 593.93it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 324.08it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 420.52it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 328.06it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 527.92it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 483.72it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 479.85it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 476.78it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_match_regex_list:positive_test_column_name_has_space_and_match_on__any] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 3656.76it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 390.93it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 548.04it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 313.84it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 411.01it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 319.92it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 513.90it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 478.84it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 476.80it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 472.50it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_value_z_scores_to_be_less_than:basic_successful_test] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 3794.03it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 301.71it/s]Calculating Metrics:  23%|██▎       | 3/13 [00:00<00:00, 429.63it/s]Calculating Metrics:  23%|██▎       | 3/13 [00:00<00:00, 233.63it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 400.75it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 301.08it/s]Calculating Metrics:  62%|██████▏   | 8/13 [00:00<00:00, 369.71it/s]Calculating Metrics:  62%|██████▏   | 8/13 [00:00<00:00, 309.54it/s]Calculating Metrics:  69%|██████▉   | 9/13 [00:00<00:00, 331.83it/s]Calculating Metrics:  69%|██████▉   | 9/13 [00:00<00:00, 290.68it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 372.09it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 351.93it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 350.37it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 348.49it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_value_z_scores_to_be_less_than:basic_unsuccessful_test] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 3490.89it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 296.08it/s]Calculating Metrics:  23%|██▎       | 3/13 [00:00<00:00, 419.18it/s]Calculating Metrics:  23%|██▎       | 3/13 [00:00<00:00, 227.05it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 402.31it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 301.26it/s]Calculating Metrics:  62%|██████▏   | 8/13 [00:00<00:00, 370.28it/s]Calculating Metrics:  62%|██████▏   | 8/13 [00:00<00:00, 309.10it/s]Calculating Metrics:  69%|██████▉   | 9/13 [00:00<00:00, 333.57it/s]Calculating Metrics:  69%|██████▉   | 9/13 [00:00<00:00, 295.02it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 380.41it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 358.94it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 357.86it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 356.77it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_value_z_scores_to_be_less_than:basic_test_with_outlier] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 4000.29it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 327.22it/s]Calculating Metrics:  23%|██▎       | 3/13 [00:00<00:00, 464.83it/s]Calculating Metrics:  23%|██▎       | 3/13 [00:00<00:00, 249.46it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 438.06it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 328.41it/s]Calculating Metrics:  62%|██████▏   | 8/13 [00:00<00:00, 403.04it/s]Calculating Metrics:  62%|██████▏   | 8/13 [00:00<00:00, 326.44it/s]Calculating Metrics:  69%|██████▉   | 9/13 [00:00<00:00, 345.61it/s]Calculating Metrics:  69%|██████▉   | 9/13 [00:00<00:00, 298.30it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 383.90it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 363.18it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 362.00it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 360.25it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_value_z_scores_to_be_less_than:basic_test_mostly_zero] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 3153.61it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 303.62it/s]Calculating Metrics:  23%|██▎       | 3/13 [00:00<00:00, 435.91it/s]Calculating Metrics:  23%|██▎       | 3/13 [00:00<00:00, 241.72it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 424.87it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 319.52it/s]Calculating Metrics:  62%|██████▏   | 8/13 [00:00<00:00, 394.18it/s]Calculating Metrics:  62%|██████▏   | 8/13 [00:00<00:00, 330.32it/s]Calculating Metrics:  69%|██████▉   | 9/13 [00:00<00:00, 354.44it/s]Calculating Metrics:  69%|██████▉   | 9/13 [00:00<00:00, 311.33it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 401.63it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 380.33it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 379.36it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 377.52it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_value_z_scores_to_be_less_than:basic_test_with_missing] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 3003.44it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 204.84it/s]Calculating Metrics:  23%|██▎       | 3/13 [00:00<00:00, 295.08it/s]Calculating Metrics:  23%|██▎       | 3/13 [00:00<00:00, 165.09it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 288.27it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 235.18it/s]Calculating Metrics:  62%|██████▏   | 8/13 [00:00<00:00, 294.10it/s]Calculating Metrics:  62%|██████▏   | 8/13 [00:00<00:00, 255.25it/s]Calculating Metrics:  69%|██████▉   | 9/13 [00:00<00:00, 276.35it/s]Calculating Metrics:  69%|██████▉   | 9/13 [00:00<00:00, 249.16it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 327.75it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 299.60it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 298.80it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 298.01it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_between:basic_positive_test0] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 2937.19it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 297.27it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 425.17it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 235.66it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 328.23it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 251.83it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 402.95it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 377.89it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 376.68it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 375.31it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_between:another_basic_positive_test0] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 3938.31it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 368.07it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 525.49it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 291.12it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 400.36it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 302.61it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 472.70it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 436.82it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 434.88it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 432.97it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_between:basic_positive_test_with_timestamps0] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 4000.29it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 359.78it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 513.44it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 287.36it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 391.60it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 300.58it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 468.01it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 434.80it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 433.11it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 431.15it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_between:missing_min_value0] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 4072.14it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 341.86it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 488.41it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 270.98it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 389.80it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 294.63it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 453.00it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 419.66it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 418.00it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 416.27it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_between:null_min_value0] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 3164.32it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 277.93it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 396.30it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 202.69it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 290.76it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 199.84it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 322.15it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 301.19it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 300.31it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 299.27it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_between:missing_max_value0] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 3138.27it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 251.31it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 361.35it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 204.16it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 298.91it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 239.56it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 392.00it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 366.12it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 364.93it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 363.59it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_between:null_max_value0] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 3708.49it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 332.67it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 474.61it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 263.80it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 379.73it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 290.24it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 455.54it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 422.15it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 420.20it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 418.18it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_between:basic_negative_test0] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 3419.73it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 352.68it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 501.51it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 284.66it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 399.53it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 303.88it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 476.70it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 442.85it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 441.21it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 439.33it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_between:another_negative_test0] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 2815.91it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 311.38it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 445.18it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 265.18it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 378.73it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 292.55it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 458.02it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 426.53it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 424.97it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 423.17it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_between:basic_negative_test_with_timestamps0] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 3629.86it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 367.71it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 524.48it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 285.01it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 379.20it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 292.82it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 456.47it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 425.01it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 423.46it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 421.72it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_between:positive_test_with_result_format__boolean_only0] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/7 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/7 [00:00<?, ?it/s]Calculating Metrics:  29%|██▊       | 2/7 [00:00<00:00, 4032.98it/s]Calculating Metrics:  29%|██▊       | 2/7 [00:00<00:00, 627.94it/s]Calculating Metrics:  43%|████▎     | 3/7 [00:00<00:00, 859.84it/s]Calculating Metrics:  43%|████▎     | 3/7 [00:00<00:00, 518.22it/s]Calculating Metrics:  71%|███████▏  | 5/7 [00:00<00:00, 636.77it/s]Calculating Metrics:  71%|███████▏  | 5/7 [00:00<00:00, 525.04it/s]Calculating Metrics: 100%|██████████| 7/7 [00:00<00:00, 658.20it/s]Calculating Metrics: 100%|██████████| 7/7 [00:00<00:00, 607.68it/s]Calculating Metrics: 100%|██████████| 7/7 [00:00<00:00, 603.24it/s]Calculating Metrics: 100%|██████████| 7/7 [00:00<00:00, 598.12it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_between:another_positive_test_with_result_format__boolean_only0] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/7 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/7 [00:00<?, ?it/s]Calculating Metrics:  29%|██▊       | 2/7 [00:00<00:00, 4072.14it/s]Calculating Metrics:  29%|██▊       | 2/7 [00:00<00:00, 681.39it/s]Calculating Metrics:  43%|████▎     | 3/7 [00:00<00:00, 939.58it/s]Calculating Metrics:  43%|████▎     | 3/7 [00:00<00:00, 541.43it/s]Calculating Metrics:  71%|███████▏  | 5/7 [00:00<00:00, 669.78it/s]Calculating Metrics:  71%|███████▏  | 5/7 [00:00<00:00, 540.36it/s]Calculating Metrics: 100%|██████████| 7/7 [00:00<00:00, 686.55it/s]Calculating Metrics: 100%|██████████| 7/7 [00:00<00:00, 634.47it/s]Calculating Metrics: 100%|██████████| 7/7 [00:00<00:00, 630.06it/s]Calculating Metrics: 100%|██████████| 7/7 [00:00<00:00, 624.67it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_between:negative_test_with_result_format__boolean_only0] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/7 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/7 [00:00<?, ?it/s]Calculating Metrics:  29%|██▊       | 2/7 [00:00<00:00, 4015.61it/s]Calculating Metrics:  29%|██▊       | 2/7 [00:00<00:00, 641.43it/s]Calculating Metrics:  43%|████▎     | 3/7 [00:00<00:00, 871.33it/s]Calculating Metrics:  43%|████▎     | 3/7 [00:00<00:00, 498.75it/s]Calculating Metrics:  71%|███████▏  | 5/7 [00:00<00:00, 621.56it/s]Calculating Metrics:  71%|███████▏  | 5/7 [00:00<00:00, 513.50it/s]Calculating Metrics: 100%|██████████| 7/7 [00:00<00:00, 651.82it/s]Calculating Metrics: 100%|██████████| 7/7 [00:00<00:00, 595.64it/s]Calculating Metrics: 100%|██████████| 7/7 [00:00<00:00, 590.41it/s]Calculating Metrics: 100%|██████████| 7/7 [00:00<00:00, 584.79it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_between:another_negative_test_with_result_format__boolean_only0] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/7 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/7 [00:00<?, ?it/s]Calculating Metrics:  29%|██▊       | 2/7 [00:00<00:00, 4202.71it/s]Calculating Metrics:  29%|██▊       | 2/7 [00:00<00:00, 689.63it/s]Calculating Metrics:  43%|████▎     | 3/7 [00:00<00:00, 953.25it/s]Calculating Metrics:  43%|████▎     | 3/7 [00:00<00:00, 560.96it/s]Calculating Metrics:  71%|███████▏  | 5/7 [00:00<00:00, 689.94it/s]Calculating Metrics:  71%|███████▏  | 5/7 [00:00<00:00, 564.59it/s]Calculating Metrics: 100%|██████████| 7/7 [00:00<00:00, 715.66it/s]Calculating Metrics: 100%|██████████| 7/7 [00:00<00:00, 655.61it/s]Calculating Metrics: 100%|██████████| 7/7 [00:00<00:00, 649.55it/s]Calculating Metrics: 100%|██████████| 7/7 [00:00<00:00, 643.31it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_between:positive_test_with_mostly0] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 3332.78it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 358.63it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 511.33it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 290.44it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 404.18it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 306.39it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 480.33it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 445.60it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 443.91it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 441.97it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_between:2nd_positive_test_with_mostly0] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 3687.30it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 355.87it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 508.03it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 281.16it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 399.07it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 305.45it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 476.35it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 442.24it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 440.51it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 438.59it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_between:error:_improperly_mixed_types0] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 4114.08it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 377.87it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 540.83it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 294.90it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 133.40it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 100.08it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 80.70it/s] Calculating Metrics:  40%|████      | 4/10 [00:00<00:00, 104.79it/s]Calculating Metrics:  40%|████      | 4/10 [00:00<00:00, 93.59it/s]alculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 114.92it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 105.17it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 104.45it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 104.22it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_between:error:_improperly_mixed_types_again0] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 4031.05it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 342.36it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 486.07it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 274.13it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 158.87it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 116.56it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 92.41it/s] Calculating Metrics:  40%|████      | 4/10 [00:00<00:00, 120.09it/s]Calculating Metrics:  40%|████      | 4/10 [00:00<00:00, 106.16it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 130.25it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 117.89it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 116.99it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 116.70it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_between:error:_improperly_mixed_types_once_more0] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 3929.09it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 365.44it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 522.11it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 289.04it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 175.59it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 127.86it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 100.51it/s]Calculating Metrics:  40%|████      | 4/10 [00:00<00:00, 129.95it/s]Calculating Metrics:  40%|████      | 4/10 [00:00<00:00, 112.72it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 137.91it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 123.29it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 122.13it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 121.71it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_between:error_on_string-to-int_comparisons0] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 2490.68it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 269.44it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 383.58it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 216.42it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 136.67it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 104.36it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 84.22it/s] Calculating Metrics:  40%|████      | 4/10 [00:00<00:00, 109.45it/s]Calculating Metrics:  40%|████      | 4/10 [00:00<00:00, 97.44it/s]alculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 119.63it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 109.12it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 108.35it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 108.10it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_between:test_min_value_is_greater_than_max_value0] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 3628.29it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 357.21it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 508.46it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 286.12it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 175.68it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 126.88it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 98.19it/s] Calculating Metrics:  40%|████      | 4/10 [00:00<00:00, 126.28it/s]Calculating Metrics:  40%|████      | 4/10 [00:00<00:00, 109.03it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 133.33it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 120.31it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 119.36it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 119.06it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_between:test_strict_min_success0] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 2916.76it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 357.34it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 511.00it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 284.55it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 400.01it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 303.84it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 480.33it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 445.86it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 444.19it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 442.28it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_between:test_strict_max_success0] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 2550.50it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 292.48it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 417.25it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 223.73it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 314.13it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 230.98it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 349.64it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 326.15it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 324.94it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 323.60it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_between:test_conditional_expectation_passes0] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 523.83it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 231.99it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 333.90it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 218.75it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 256.34it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 215.37it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 312.03it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 296.54it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 295.53it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 294.44it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_between:test_conditional_expectation_parser_errors0] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_match_regex:positive_test_all_missing_values] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 3169.10it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 349.64it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 492.14it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 294.79it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 415.39it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 329.81it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 528.76it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 493.68it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 491.42it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 489.05it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_match_regex:positive_test_all_missing_values_mostly] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 3775.25it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 437.93it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 616.93it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 318.27it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 431.45it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 314.47it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 504.44it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 472.64it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 470.57it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 468.39it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_match_regex:positive_test_empty_regex] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 4013.69it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 450.83it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 635.98it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 346.93it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 478.36it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 348.01it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 500.04it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 458.61it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 455.93it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 453.59it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_match_regex:positive_test_more_complicated_regex] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 3929.09it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 436.70it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 614.76it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 348.80it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 481.37it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 368.17it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 560.29it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 517.44it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 514.67it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 511.33it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_between:basic_positive_test1] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 3509.88it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 349.54it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 496.70it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 268.92it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 361.72it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 280.46it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 444.98it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 414.22it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 412.60it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 410.81it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_between:another_basic_positive_test1] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 3778.65it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 284.17it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 404.80it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 238.89it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 331.54it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 264.72it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 427.64it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 399.60it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 398.25it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 396.71it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_between:basic_positive_test_with_timestamps1] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 3318.28it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 245.04it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 352.66it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 222.12it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 311.71it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 251.64it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 409.57it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 383.89it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 382.59it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 381.11it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_between:missing_min_value1] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 2460.00it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 318.78it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 455.79it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 256.85it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 363.56it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 280.79it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 447.55it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 414.34it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 412.64it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 410.94it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_between:null_min_value1] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 4000.29it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 356.01it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 510.46it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 277.53it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 396.23it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 299.40it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 467.08it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 431.17it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 429.34it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 427.50it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_between:missing_max_value1] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 3992.67it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 346.88it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 495.70it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 273.48it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 387.30it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 293.72it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 458.97it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 424.65it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 422.91it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 420.98it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_between:null_max_value1] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 3514.29it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 275.57it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 393.18it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 225.60it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 323.43it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 253.32it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 402.58it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 373.19it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 371.56it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 370.13it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_between:positive_test_with_result_format__boolean_only1] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/7 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/7 [00:00<?, ?it/s]Calculating Metrics:  29%|██▊       | 2/7 [00:00<00:00, 3874.65it/s]Calculating Metrics:  29%|██▊       | 2/7 [00:00<00:00, 665.13it/s]Calculating Metrics:  43%|████▎     | 3/7 [00:00<00:00, 918.26it/s]Calculating Metrics:  43%|████▎     | 3/7 [00:00<00:00, 535.72it/s]Calculating Metrics:  71%|███████▏  | 5/7 [00:00<00:00, 659.46it/s]Calculating Metrics:  71%|███████▏  | 5/7 [00:00<00:00, 543.35it/s]Calculating Metrics: 100%|██████████| 7/7 [00:00<00:00, 690.13it/s]Calculating Metrics: 100%|██████████| 7/7 [00:00<00:00, 637.17it/s]Calculating Metrics: 100%|██████████| 7/7 [00:00<00:00, 632.63it/s]Calculating Metrics: 100%|██████████| 7/7 [00:00<00:00, 627.00it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_between:another_positive_test_with_result_format__boolean_only1] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/7 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/7 [00:00<?, ?it/s]Calculating Metrics:  29%|██▊       | 2/7 [00:00<00:00, 4015.61it/s]Calculating Metrics:  29%|██▊       | 2/7 [00:00<00:00, 676.56it/s]Calculating Metrics:  43%|████▎     | 3/7 [00:00<00:00, 894.94it/s]Calculating Metrics:  43%|████▎     | 3/7 [00:00<00:00, 515.63it/s]Calculating Metrics:  71%|███████▏  | 5/7 [00:00<00:00, 627.04it/s]Calculating Metrics:  71%|███████▏  | 5/7 [00:00<00:00, 509.64it/s]Calculating Metrics: 100%|██████████| 7/7 [00:00<00:00, 647.73it/s]Calculating Metrics: 100%|██████████| 7/7 [00:00<00:00, 599.21it/s]Calculating Metrics: 100%|██████████| 7/7 [00:00<00:00, 594.94it/s]Calculating Metrics: 100%|██████████| 7/7 [00:00<00:00, 589.96it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_between:negative_test_with_result_format__boolean_only1] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/7 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/7 [00:00<?, ?it/s]Calculating Metrics:  29%|██▊       | 2/7 [00:00<00:00, 3523.14it/s]Calculating Metrics:  29%|██▊       | 2/7 [00:00<00:00, 647.72it/s]Calculating Metrics:  43%|████▎     | 3/7 [00:00<00:00, 893.67it/s]Calculating Metrics:  43%|████▎     | 3/7 [00:00<00:00, 530.14it/s]Calculating Metrics:  71%|███████▏  | 5/7 [00:00<00:00, 662.25it/s]Calculating Metrics:  71%|███████▏  | 5/7 [00:00<00:00, 545.61it/s]Calculating Metrics: 100%|██████████| 7/7 [00:00<00:00, 687.16it/s]Calculating Metrics: 100%|██████████| 7/7 [00:00<00:00, 633.79it/s]Calculating Metrics: 100%|██████████| 7/7 [00:00<00:00, 628.99it/s]Calculating Metrics: 100%|██████████| 7/7 [00:00<00:00, 623.33it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_between:another_negative_test_with_result_format__boolean_only1] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/7 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/7 [00:00<?, ?it/s]Calculating Metrics:  29%|██▊       | 2/7 [00:00<00:00, 3905.31it/s]Calculating Metrics:  29%|██▊       | 2/7 [00:00<00:00, 667.30it/s]Calculating Metrics:  43%|████▎     | 3/7 [00:00<00:00, 917.92it/s]Calculating Metrics:  43%|████▎     | 3/7 [00:00<00:00, 543.66it/s]Calculating Metrics:  71%|███████▏  | 5/7 [00:00<00:00, 605.03it/s]Calculating Metrics:  71%|███████▏  | 5/7 [00:00<00:00, 471.33it/s]Calculating Metrics: 100%|██████████| 7/7 [00:00<00:00, 597.56it/s]Calculating Metrics: 100%|██████████| 7/7 [00:00<00:00, 553.88it/s]Calculating Metrics: 100%|██████████| 7/7 [00:00<00:00, 549.70it/s]Calculating Metrics: 100%|██████████| 7/7 [00:00<00:00, 544.91it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_between:positive_test_with_mostly1] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 3968.12it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 376.66it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 535.92it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 287.58it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 404.93it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 307.81it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 426.24it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 394.91it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 393.04it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 391.33it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_between:2nd_positive_test_with_mostly1] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 3636.15it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 369.07it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 526.42it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 276.19it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 389.59it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 277.44it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 407.82it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 378.39it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 376.70it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 375.20it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_between:error:_improperly_mixed_types1] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 2116.73it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 225.43it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 324.44it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 198.00it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 132.41it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 102.11it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 82.35it/s] Calculating Metrics:  40%|████      | 4/10 [00:00<00:00, 105.69it/s]Calculating Metrics:  40%|████      | 4/10 [00:00<00:00, 92.89it/s]alculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 113.53it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 100.64it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 99.59it/s] Clculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 99.25it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_between:error:_improperly_mixed_types_again1] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 3328.81it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 288.27it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 406.73it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 215.63it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 127.25it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 93.86it/s] Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 75.02it/s]Calculating Metrics:  40%|████      | 4/10 [00:00<00:00, 97.59it/s]Calculating Metrics:  40%|████      | 4/10 [00:00<00:00, 87.86it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 108.07it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 99.08it/s]alculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 98.38it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 98.16it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_between:error:_improperly_mixed_types_once_more1] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 4098.00it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 376.51it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 535.42it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 288.07it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 175.08it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 125.00it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 97.58it/s] Calculating Metrics:  40%|████      | 4/10 [00:00<00:00, 126.03it/s]Calculating Metrics:  40%|████      | 4/10 [00:00<00:00, 108.61it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 132.83it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 118.95it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 117.89it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 117.56it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_between:error_on_string-to-int_comparisons1] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 4034.92it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 378.44it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 541.25it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 290.00it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 174.14it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 124.17it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 97.93it/s] Calculating Metrics:  40%|████      | 4/10 [00:00<00:00, 127.19it/s]Calculating Metrics:  40%|████      | 4/10 [00:00<00:00, 110.82it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 135.78it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 122.47it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 121.49it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 121.18it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_between:test_min_value_is_greater_than_max_value1] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 3765.08it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 340.93it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 482.99it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 273.92it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 168.55it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 122.79it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 96.21it/s] Calculating Metrics:  40%|████      | 4/10 [00:00<00:00, 124.74it/s]Calculating Metrics:  40%|████      | 4/10 [00:00<00:00, 109.29it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 133.87it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 120.80it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 119.84it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 119.54it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_between:test_strict_min_success1] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 3970.00it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 367.47it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 510.13it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 251.32it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 351.79it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 274.83it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 419.27it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 386.25it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 384.60it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 382.82it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_between:test_strict_max_success1] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 2655.46it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 315.01it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 450.65it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 256.03it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 362.39it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 278.16it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 440.37it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 410.59it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 409.00it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 407.33it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_between:test_conditional_expectation_passes1] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 696.09it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 304.05it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 440.41it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 289.94it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 342.19it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 283.98it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 394.06it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 375.19it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 374.01it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 372.65it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_between:test_conditional_expectation_parser_errors1] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_not_be_in_set:basic_positive_test_case_single_value_not_in_column] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 3802.63it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 389.43it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 550.46it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 302.85it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 390.84it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 308.32it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 497.22it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 461.68it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 459.69it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 457.58it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_not_be_in_set:positive_empty_values_set] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 3442.19it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 400.49it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 568.05it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 320.37it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 402.35it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 319.07it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 518.97it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 484.49it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 482.63it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 480.40it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_not_be_in_set:positive_test_strings_set_extra_value_in_column] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 4096.00it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 425.60it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 605.68it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 333.51it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 468.38it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 353.78it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 560.59it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 519.80it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 517.62it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 515.06it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_not_be_in_set:positive_test_values_set_is_null] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 4048.56it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 385.49it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 549.76it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 296.53it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 424.71it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 322.33it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 515.23it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 475.58it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 473.48it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 470.21it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_of_type:basic_python_int_positive_test] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/1 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/1 [00:00<?, ?it/s]Calculating Metrics: 100%|██████████| 1/1 [00:00<00:00, 3104.59it/s]Calculating Metrics: 100%|██████████| 1/1 [00:00<00:00, 2087.76it/s]Calculating Metrics: 100%|██████████| 1/1 [00:00<00:00, 1791.67it/s]Calculating Metrics: 100%|██████████| 1/1 [00:00<00:00, 1533.57it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_of_type:negative_test_python_ints_are_not_string] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/1 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/1 [00:00<?, ?it/s]Calculating Metrics: 100%|██████████| 1/1 [00:00<00:00, 2644.58it/s]Calculating Metrics: 100%|██████████| 1/1 [00:00<00:00, 1691.93it/s]Calculating Metrics: 100%|██████████| 1/1 [00:00<00:00, 1447.31it/s]Calculating Metrics: 100%|██████████| 1/1 [00:00<00:00, 1242.02it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_of_type:positive_test_pandas_floats] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/1 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/1 [00:00<?, ?it/s]Calculating Metrics: 100%|██████████| 1/1 [00:00<00:00, 3377.06it/s]Calculating Metrics: 100%|██████████| 1/1 [00:00<00:00, 2326.29it/s]Calculating Metrics: 100%|██████████| 1/1 [00:00<00:00, 1983.12it/s]Calculating Metrics: 100%|██████████| 1/1 [00:00<00:00, 1689.21it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_of_type:positive_test_pandas_strings] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 4056.39it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 449.43it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 637.21it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 354.57it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 465.72it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 348.38it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 534.13it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 485.01it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 482.14it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 479.18it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_of_type:negative_test_python_floats_are_not_python_bools] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/1 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/1 [00:00<?, ?it/s]Calculating Metrics: 100%|██████████| 1/1 [00:00<00:00, 3223.91it/s]Calculating Metrics: 100%|██████████| 1/1 [00:00<00:00, 2232.20it/s]Calculating Metrics: 100%|██████████| 1/1 [00:00<00:00, 1911.72it/s]Calculating Metrics: 100%|██████████| 1/1 [00:00<00:00, 1633.30it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_of_type:dtype_object_and_type_object_still_has_aggregate_semantics] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/1 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/1 [00:00<?, ?it/s]Calculating Metrics: 100%|██████████| 1/1 [00:00<00:00, 3246.37it/s]Calculating Metrics: 100%|██████████| 1/1 [00:00<00:00, 2122.62it/s]Calculating Metrics: 100%|██████████| 1/1 [00:00<00:00, 1801.68it/s]Calculating Metrics: 100%|██████████| 1/1 [00:00<00:00, 1533.57it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_of_type:dtype_object_and_type_object_still_has_aggregate_semantics_object_underscore] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/1 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/1 [00:00<?, ?it/s]Calculating Metrics: 100%|██████████| 1/1 [00:00<00:00, 2632.96it/s]Calculating Metrics: 100%|██████████| 1/1 [00:00<00:00, 1828.38it/s]Calculating Metrics: 100%|██████████| 1/1 [00:00<00:00, 1560.38it/s]Calculating Metrics: 100%|██████████| 1/1 [00:00<00:00, 1333.22it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_of_type:dtype_object_and_type_object_still_has_aggregate_semantics_big_o] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/1 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/1 [00:00<?, ?it/s]Calculating Metrics: 100%|██████████| 1/1 [00:00<00:00, 2746.76it/s]Calculating Metrics: 100%|██████████| 1/1 [00:00<00:00, 1875.81it/s]Calculating Metrics: 100%|██████████| 1/1 [00:00<00:00, 1594.79it/s]Calculating Metrics: 100%|██████████| 1/1 [00:00<00:00, 1353.00it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_of_type:positive_pandas_datetime_no_timezone] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/1 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/1 [00:00<?, ?it/s]Calculating Metrics: 100%|██████████| 1/1 [00:00<00:00, 3006.67it/s]Calculating Metrics: 100%|██████████| 1/1 [00:00<00:00, 2046.00it/s]Calculating Metrics: 100%|██████████| 1/1 [00:00<00:00, 1739.65it/s]Calculating Metrics: 100%|██████████| 1/1 [00:00<00:00, 1477.39it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_of_type:positive_pandas_datetime_with_timezone] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/1 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/1 [00:00<?, ?it/s]Calculating Metrics: 100%|██████████| 1/1 [00:00<00:00, 3125.41it/s]Calculating Metrics: 100%|██████████| 1/1 [00:00<00:00, 2155.35it/s]Calculating Metrics: 100%|██████████| 1/1 [00:00<00:00, 1841.22it/s]Calculating Metrics: 100%|██████████| 1/1 [00:00<00:00, 1572.67it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_of_type:negative_pandas_datetime_with_timezone] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/1 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/1 [00:00<?, ?it/s]Calculating Metrics: 100%|██████████| 1/1 [00:00<00:00, 3472.11it/s]Calculating Metrics: 100%|██████████| 1/1 [00:00<00:00, 2368.33it/s]Calculating Metrics: 100%|██████████| 1/1 [00:00<00:00, 2011.66it/s]Calculating Metrics: 100%|██████████| 1/1 [00:00<00:00, 1709.17it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_of_type:negative_pandas_datetime_expected_int] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/1 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/1 [00:00<?, ?it/s]Calculating Metrics: 100%|██████████| 1/1 [00:00<00:00, 3165.51it/s]Calculating Metrics: 100%|██████████| 1/1 [00:00<00:00, 2208.69it/s]Calculating Metrics: 100%|██████████| 1/1 [00:00<00:00, 1891.03it/s]Calculating Metrics: 100%|██████████| 1/1 [00:00<00:00, 1618.17it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_in_set:basic_positive_test_case_number_set] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 3927.25it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 403.86it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 573.59it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 319.42it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 453.10it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 347.49it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 554.57it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 512.37it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 509.76it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 506.99it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_in_set:vacuously_true_empty_value_set] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 4007.94it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 396.17it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 565.30it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 311.33it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 437.55it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 328.92it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 531.71it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 492.12it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 490.12it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 487.78it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_in_set:basic_positive_strings_set] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 4032.98it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 419.56it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 597.03it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 309.92it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 437.03it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 333.42it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 511.66it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 474.30it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 471.90it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 469.60it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_in_set:positive_test_float_set] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 2652.94it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 304.23it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 433.36it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 233.94it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 335.78it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 257.16it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 420.76it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 394.70it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 393.35it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 391.84it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_in_set:test_conditional_expectation_passes] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 684.00it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 337.27it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 487.73it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 334.07it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 399.01it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 313.38it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 447.97it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 427.22it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 425.62it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 423.85it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_in_set:test_conditional_expectation_parser_errors] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_in_set:test_empty_column_should_be_vacuously_true] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 3890.82it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 413.37it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 587.08it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 327.86it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 467.59it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 331.12it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 502.31it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 466.74it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 464.55it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 462.28it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_in_set:basic_positive_test_case_datetime_set] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 3912.60it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 424.44it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 603.99it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 330.17it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 457.45it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 349.01it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 534.67it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 493.97it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 491.40it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 488.97it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_decreasing:basic_positive_test] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 2460.00it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 321.75it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 452.64it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 253.72it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 320.35it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 251.88it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 407.38it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 380.47it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 378.86it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 377.09it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_decreasing:test_empty_column_should_be_vacuously_true] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 3846.22it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 421.50it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 597.00it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 327.01it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 442.56it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 343.27it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 556.08it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 514.85it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 512.58it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 510.00it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_not_match_regex:positive_test_one_missing_value_no_exceptions] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 4183.84it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 432.05it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 606.17it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 337.80it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 469.56it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 363.61it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 576.60it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 532.90it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 530.05it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 527.17it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_not_match_regex:positive_test_all_missing_values] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 4156.89it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 442.27it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 625.80it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 352.39it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 489.05it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 374.16it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 584.29it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 542.36it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 539.81it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 536.91it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_not_match_regex:positive_test_all_missing_values_mostly] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 3482.20it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 433.92it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 615.12it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 348.88it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 494.95it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 373.85it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 591.81it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 549.14it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 546.69it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 543.80it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_dateutil_parseable:positive_test] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 3898.05it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 489.36it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 686.84it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 382.37it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 517.13it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 400.77it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 629.77it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 587.30it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 584.49it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 581.16it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_dateutil_parseable:positive_test_with_nulls] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 4213.26it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 476.87it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 672.95it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 374.64it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 506.85it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 397.11it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 622.12it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 578.77it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 575.64it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 572.35it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_dateutil_parseable:negative_test] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 4064.25it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 468.53it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 660.66it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 369.24it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 512.83it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 400.52it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 627.64it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 585.62it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 582.79it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 579.55it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_dateutil_parseable:test_raising_exception_for_wrong_input_data_type] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 3929.09it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 447.25it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 628.67it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 332.75it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 183.53it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 134.23it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 107.05it/s]Calculating Metrics:  40%|████      | 4/10 [00:00<00:00, 138.99it/s]Calculating Metrics:  40%|████      | 4/10 [00:00<00:00, 125.11it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 153.28it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 140.87it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 139.84it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 139.41it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_not_match_regex_list:basic_positive_test] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 3323.54it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 422.94it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 599.64it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 338.41it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 453.52it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 350.22it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 551.67it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 508.08it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 505.33it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 502.67it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_not_match_regex_list:positive_test_with_multiple_regexes] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 3301.30it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 332.27it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 468.90it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 265.77it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 335.01it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 256.85it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 417.28it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 389.64it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 387.96it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 386.10it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_increasing:basic_positive_test] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 3945.72it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 429.74it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 610.76it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 337.12it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 433.81it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 338.34it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 544.58it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 504.72it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 502.22it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 499.63it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_increasing:2nd_basic_positive_test] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 4074.12it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 420.44it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 595.95it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 328.63it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 424.48it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 332.78it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 534.42it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 495.03it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 492.54it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 490.07it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_increasing:positive_test_with_parse_strings_as_datetimes] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 2669.83it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 341.63it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 486.75it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 264.80it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 278.52it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 226.50it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 374.11it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 351.25it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 349.93it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 348.44it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_increasing:positive_test_with_interspersed_nulls] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 4126.22it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 399.46it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 568.51it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 316.06it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 418.98it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 328.75it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 528.60it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 491.74it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 489.62it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 487.23it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_value_lengths_to_be_between:positive_test_exact_min_and_max] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/11 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/11 [00:00<?, ?it/s]Calculating Metrics:  18%|█▊        | 2/11 [00:00<00:00, 3787.18it/s]Calculating Metrics:  18%|█▊        | 2/11 [00:00<00:00, 327.27it/s]Calculating Metrics:  27%|██▋       | 3/11 [00:00<00:00, 470.95it/s]Calculating Metrics:  27%|██▋       | 3/11 [00:00<00:00, 240.92it/s]Calculating Metrics:  45%|████▌     | 5/11 [00:00<00:00, 344.99it/s]Calculating Metrics:  45%|████▌     | 5/11 [00:00<00:00, 254.96it/s]Calculating Metrics:  64%|██████▎   | 7/11 [00:00<00:00, 325.28it/s]Calculating Metrics:  64%|██████▎   | 7/11 [00:00<00:00, 273.79it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 378.24it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 355.44it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 354.52it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 353.41it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_value_lengths_to_be_between:positive_test_with_null_min_value] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/11 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/11 [00:00<?, ?it/s]Calculating Metrics:  18%|█▊        | 2/11 [00:00<00:00, 4098.00it/s]Calculating Metrics:  18%|█▊        | 2/11 [00:00<00:00, 308.08it/s]Calculating Metrics:  27%|██▋       | 3/11 [00:00<00:00, 443.20it/s]Calculating Metrics:  27%|██▋       | 3/11 [00:00<00:00, 238.13it/s]Calculating Metrics:  45%|████▌     | 5/11 [00:00<00:00, 350.44it/s]Calculating Metrics:  45%|████▌     | 5/11 [00:00<00:00, 253.92it/s]Calculating Metrics:  64%|██████▎   | 7/11 [00:00<00:00, 329.22it/s]Calculating Metrics:  64%|██████▎   | 7/11 [00:00<00:00, 275.57it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 375.13it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 345.70it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 344.38it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 343.08it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_value_lengths_to_be_between:positive_test_with_null_max_value] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/11 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/11 [00:00<?, ?it/s]Calculating Metrics:  18%|█▊        | 2/11 [00:00<00:00, 4056.39it/s]Calculating Metrics:  18%|█▊        | 2/11 [00:00<00:00, 304.42it/s]Calculating Metrics:  27%|██▋       | 3/11 [00:00<00:00, 436.88it/s]Calculating Metrics:  27%|██▋       | 3/11 [00:00<00:00, 239.25it/s]Calculating Metrics:  45%|████▌     | 5/11 [00:00<00:00, 347.12it/s]Calculating Metrics:  45%|████▌     | 5/11 [00:00<00:00, 242.99it/s]Calculating Metrics:  64%|██████▎   | 7/11 [00:00<00:00, 314.53it/s]Calculating Metrics:  64%|██████▎   | 7/11 [00:00<00:00, 265.03it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 361.25it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 338.45it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 337.49it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 336.46it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_value_lengths_to_be_between:positive_test_with_missing_value_in_column] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/11 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/11 [00:00<?, ?it/s]Calculating Metrics:  18%|█▊        | 2/11 [00:00<00:00, 4007.94it/s]Calculating Metrics:  18%|█▊        | 2/11 [00:00<00:00, 328.89it/s]Calculating Metrics:  27%|██▋       | 3/11 [00:00<00:00, 470.28it/s]Calculating Metrics:  27%|██▋       | 3/11 [00:00<00:00, 252.89it/s]Calculating Metrics:  45%|████▌     | 5/11 [00:00<00:00, 357.66it/s]Calculating Metrics:  45%|████▌     | 5/11 [00:00<00:00, 258.13it/s]Calculating Metrics:  64%|██████▎   | 7/11 [00:00<00:00, 328.36it/s]Calculating Metrics:  64%|██████▎   | 7/11 [00:00<00:00, 277.26it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 381.11it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 355.10it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 353.94it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 352.78it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_value_lengths_to_be_between:test_error_handling_values_are_integers] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/11 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/11 [00:00<?, ?it/s]Calculating Metrics:  18%|█▊        | 2/11 [00:00<00:00, 4025.24it/s]Calculating Metrics:  18%|█▊        | 2/11 [00:00<00:00, 324.74it/s]Calculating Metrics:  27%|██▋       | 3/11 [00:00<00:00, 465.19it/s]Calculating Metrics:  27%|██▋       | 3/11 [00:00<00:00, 251.34it/s]Calculating Metrics:  45%|████▌     | 5/11 [00:00<00:00, 368.70it/s]Calculating Metrics:  45%|████▌     | 5/11 [00:00<00:00, 270.93it/s]Calculating Metrics:  64%|██████▎   | 7/11 [00:00<00:00, 347.90it/s]Calculating Metrics:  64%|██████▎   | 7/11 [00:00<00:00, 290.54it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 396.10it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 371.03it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 369.95it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 368.64it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_value_lengths_to_be_between:test_conditional_expectation_passes] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/11 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/11 [00:00<?, ?it/s]Calculating Metrics:  18%|█▊        | 2/11 [00:00<00:00, 811.12it/s]Calculating Metrics:  18%|█▊        | 2/11 [00:00<00:00, 292.80it/s]Calculating Metrics:  27%|██▋       | 3/11 [00:00<00:00, 424.71it/s]Calculating Metrics:  27%|██▋       | 3/11 [00:00<00:00, 267.91it/s]Calculating Metrics:  45%|████▌     | 5/11 [00:00<00:00, 346.19it/s]Calculating Metrics:  45%|████▌     | 5/11 [00:00<00:00, 273.77it/s]Calculating Metrics:  64%|██████▎   | 7/11 [00:00<00:00, 338.07it/s]Calculating Metrics:  64%|██████▎   | 7/11 [00:00<00:00, 295.73it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 376.52it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 357.32it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 356.15it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 354.70it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_value_lengths_to_be_between:test_conditional_expectation_parser_errors] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/11 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/11 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/11 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/11 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/11 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/11 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/11 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/11 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/11 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/11 [00:00<?, ?it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_match_strftime_format:simple_positive_test] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 3701.95it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 428.08it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 606.52it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 340.87it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 477.79it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 368.81it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 577.63it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 534.67it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 531.81it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 528.90it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_match_strftime_format:negative_test_wrong_format] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 3758.34it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 417.70it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 586.73it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 328.84it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 465.41it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 358.49it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 559.72it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 521.13it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 518.88it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 516.29it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_match_strftime_format:positive_test_w_nulls] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 4009.85it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 427.53it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 602.17it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 331.53it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 451.76it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 346.72it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 538.65it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 502.56it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 500.48it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 498.06it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_match_strftime_format:positive_test_w_mostly] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 4048.56it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 449.94it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 638.14it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 351.37it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 481.74it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 350.09it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 540.48it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 488.66it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 485.77it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 483.18it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_match_strftime_format:simple_negative_test] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 4138.44it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 452.58it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 639.38it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 347.10it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 485.00it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 370.92it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 540.27it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 470.43it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 466.33it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 462.42it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_match_strftime_format:negative_test_out_of_bounds_value_for_month] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 3899.86it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 431.40it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 607.43it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 341.65it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 478.29it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 366.35it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 570.74it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 530.31it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 527.65it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 524.88it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_match_strftime_format:negative_test_iso8601] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 4130.28it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 445.33it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 626.92it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 346.41it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 477.45it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 367.10it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 568.83it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 528.06it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 525.42it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 522.60it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_match_strftime_format:test_raising_exception_for_wrong_input_data_type] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 4048.56it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 444.45it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 625.55it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 324.64it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 190.89it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 135.17it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 96.39it/s] Calculating Metrics:  40%|████      | 4/10 [00:00<00:00, 123.62it/s]Calculating Metrics:  40%|████      | 4/10 [00:00<00:00, 106.46it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 130.21it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 119.23it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 118.47it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 118.17it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_unique:basic_positive_test_case] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 3876.44it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 470.58it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 656.73it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 336.40it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 418.27it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 297.53it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 488.19it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 461.49it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 459.69it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 457.60it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_unique:positive_case_multiple_null_values] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 3245.11it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 394.70it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 556.67it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 308.22it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 422.54it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 326.20it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 507.59it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 475.62it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 473.46it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 471.01it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_unique:positive_case_all_null_values] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 3899.86it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 483.83it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 678.00it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 300.67it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 426.34it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 326.31it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 492.45it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 465.03it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 463.11it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 460.90it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_match_json_schema:basic_positive_test] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 3105.74it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 338.25it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 479.31it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 263.02it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 276.43it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 230.93it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 385.83it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 363.13it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 361.70it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 360.13it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_match_json_schema:positive_test_with_a_more_complex_schema] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 3130.08it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 326.58it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 461.83it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 261.23it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 223.76it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 186.83it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 309.53it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 292.68it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 291.63it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 290.57it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_null:basic_positive_case_basic_result_format] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/6 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/6 [00:00<?, ?it/s]Calculating Metrics:  33%|███▎      | 2/6 [00:00<00:00, 3106.89it/s]Calculating Metrics:  33%|███▎      | 2/6 [00:00<00:00, 826.14it/s]Calculating Metrics:  50%|█████     | 3/6 [00:00<00:00, 1110.00it/s]Calculating Metrics:  50%|█████     | 3/6 [00:00<00:00, 697.23it/s] Calculating Metrics:  67%|██████▋   | 4/6 [00:00<00:00, 780.52it/s]Calculating Metrics:  67%|██████▋   | 4/6 [00:00<00:00, 611.08it/s]Calculating Metrics: 100%|██████████| 6/6 [00:00<00:00, 751.80it/s]Calculating Metrics: 100%|██████████| 6/6 [00:00<00:00, 696.73it/s]Calculating Metrics: 100%|██████████| 6/6 [00:00<00:00, 690.08it/s]Calculating Metrics: 100%|██████████| 6/6 [00:00<00:00, 682.39it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_null:positive_case_with_mostly_and_no_unexpected_values] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/8 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/8 [00:00<?, ?it/s]Calculating Metrics:  25%|██▌       | 2/8 [00:00<00:00, 2472.33it/s]Calculating Metrics:  25%|██▌       | 2/8 [00:00<00:00, 449.84it/s]Calculating Metrics:  38%|███▊      | 3/8 [00:00<00:00, 628.93it/s]Calculating Metrics:  38%|███▊      | 3/8 [00:00<00:00, 395.78it/s]Calculating Metrics:  50%|█████     | 4/8 [00:00<00:00, 480.71it/s]Calculating Metrics:  50%|█████     | 4/8 [00:00<00:00, 363.84it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 590.45it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 550.40it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 547.27it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 543.63it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_in_type_list:positive_test_pandas_integer_column] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/1 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/1 [00:00<?, ?it/s]Calculating Metrics: 100%|██████████| 1/1 [00:00<00:00, 2208.69it/s]Calculating Metrics: 100%|██████████| 1/1 [00:00<00:00, 1557.48it/s]Calculating Metrics: 100%|██████████| 1/1 [00:00<00:00, 1342.18it/s]Calculating Metrics: 100%|██████████| 1/1 [00:00<00:00, 1153.23it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_in_type_list:negative_test_pandas_float_values_are_not_strings] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/1 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/1 [00:00<?, ?it/s]Calculating Metrics: 100%|██████████| 1/1 [00:00<00:00, 2557.50it/s]Calculating Metrics: 100%|██████████| 1/1 [00:00<00:00, 1760.09it/s]Calculating Metrics: 100%|██████████| 1/1 [00:00<00:00, 1499.04it/s]Calculating Metrics: 100%|██████████| 1/1 [00:00<00:00, 1276.80it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_in_type_list:positive_test_float_values] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/1 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/1 [00:00<?, ?it/s]Calculating Metrics: 100%|██████████| 1/1 [00:00<00:00, 2631.31it/s]Calculating Metrics: 100%|██████████| 1/1 [00:00<00:00, 1788.62it/s]Calculating Metrics: 100%|██████████| 1/1 [00:00<00:00, 1521.88it/s]Calculating Metrics: 100%|██████████| 1/1 [00:00<00:00, 1272.16it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_in_type_list:positive_test_string_values] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 3231.36it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 289.86it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 411.81it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 244.34it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 351.00it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 283.16it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 468.43it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 440.87it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 439.29it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 437.41it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_in_type_list:positive_test_boolean_values] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/1 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/1 [00:00<?, ?it/s]Calculating Metrics: 100%|██████████| 1/1 [00:00<00:00, 3070.50it/s]Calculating Metrics: 100%|██████████| 1/1 [00:00<00:00, 2152.03it/s]Calculating Metrics: 100%|██████████| 1/1 [00:00<00:00, 1845.27it/s]Calculating Metrics: 100%|██████████| 1/1 [00:00<00:00, 1580.37it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_in_type_list:positive_test_string_and_int_values] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 3860.38it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 392.16it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 553.73it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 322.51it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 460.23it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 354.28it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 555.99it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 517.12it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 514.90it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 512.29it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_in_type_list:positive_test_placeholder_expectation] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/1 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/1 [00:00<?, ?it/s]Calculating Metrics: 100%|██████████| 1/1 [00:00<00:00, 2577.94it/s]Calculating Metrics: 100%|██████████| 1/1 [00:00<00:00, 1818.08it/s]Calculating Metrics: 100%|██████████| 1/1 [00:00<00:00, 1545.43it/s]Calculating Metrics: 100%|██████████| 1/1 [00:00<00:00, 1322.71it/s]
_ test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_value_lengths_to_equal:positive_test_string_one_character_length] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/14 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/14 [00:00<?, ?it/s]Calculating Metrics:  14%|█▍        | 2/14 [00:00<00:00, 1843.25it/s]Calculating Metrics:  14%|█▍        | 2/14 [00:00<00:00, 200.94it/s]Calculating Metrics:  29%|██▊       | 4/14 [00:00<00:00, 354.61it/s]Calculating Metrics:  29%|██▊       | 4/14 [00:00<00:00, 239.74it/s]Calculating Metrics:  43%|████▎     | 6/14 [00:00<00:00, 320.63it/s]Calculating Metrics:  43%|████▎     | 6/14 [00:00<00:00, 255.30it/s]Calculating Metrics:  57%|█████▋    | 8/14 [00:00<00:00, 323.15it/s]Calculating Metrics:  57%|█████▋    | 8/14 [00:00<00:00, 279.22it/s]Calculating Metrics:  93%|█████████▎| 13/14 [00:00<00:00, 372.77it/s]Calculating Metrics:  93%|█████████▎| 13/14 [00:00<00:00, 351.08it/s]Calculating Metrics: 100%|██████████| 14/14 [00:00<00:00, 365.24it/s]Calculating Metrics: 100%|██████████| 14/14 [00:00<00:00, 348.99it/s]Calculating Metrics: 100%|██████████| 14/14 [00:00<00:00, 348.15it/s]Calculating Metrics: 100%|██████████| 14/14 [00:00<00:00, 347.28it/s]
_ test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_value_lengths_to_equal:negative_test_string_value_is_1_too_high] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/14 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/14 [00:00<?, ?it/s]Calculating Metrics:  14%|█▍        | 2/14 [00:00<00:00, 3105.74it/s]Calculating Metrics:  14%|█▍        | 2/14 [00:00<00:00, 324.52it/s]Calculating Metrics:  29%|██▊       | 4/14 [00:00<00:00, 551.88it/s]Calculating Metrics:  29%|██▊       | 4/14 [00:00<00:00, 315.14it/s]Calculating Metrics:  43%|████▎     | 6/14 [00:00<00:00, 426.96it/s]Calculating Metrics:  43%|████▎     | 6/14 [00:00<00:00, 319.10it/s]Calculating Metrics:  57%|█████▋    | 8/14 [00:00<00:00, 401.48it/s]Calculating Metrics:  57%|█████▋    | 8/14 [00:00<00:00, 331.05it/s]Calculating Metrics:  93%|█████████▎| 13/14 [00:00<00:00, 466.75it/s]Calculating Metrics:  93%|█████████▎| 13/14 [00:00<00:00, 431.92it/s]Calculating Metrics: 100%|██████████| 14/14 [00:00<00:00, 448.32it/s]Calculating Metrics: 100%|██████████| 14/14 [00:00<00:00, 423.61it/s]Calculating Metrics: 100%|██████████| 14/14 [00:00<00:00, 422.24it/s]Calculating Metrics: 100%|██████████| 14/14 [00:00<00:00, 420.90it/s]
_ test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_value_lengths_to_equal:positive_test_with_missing_value_in_column_mostly] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/14 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/14 [00:00<?, ?it/s]Calculating Metrics:  14%|█▍        | 2/14 [00:00<00:00, 3150.06it/s]Calculating Metrics:  14%|█▍        | 2/14 [00:00<00:00, 294.55it/s]Calculating Metrics:  29%|██▊       | 4/14 [00:00<00:00, 503.40it/s]Calculating Metrics:  29%|██▊       | 4/14 [00:00<00:00, 296.21it/s]Calculating Metrics:  43%|████▎     | 6/14 [00:00<00:00, 402.81it/s]Calculating Metrics:  43%|████▎     | 6/14 [00:00<00:00, 302.51it/s]Calculating Metrics:  57%|█████▋    | 8/14 [00:00<00:00, 380.23it/s]Calculating Metrics:  57%|█████▋    | 8/14 [00:00<00:00, 321.03it/s]Calculating Metrics:  93%|█████████▎| 13/14 [00:00<00:00, 446.48it/s]Calculating Metrics:  93%|█████████▎| 13/14 [00:00<00:00, 414.83it/s]Calculating Metrics: 100%|██████████| 14/14 [00:00<00:00, 428.75it/s]Calculating Metrics: 100%|██████████| 14/14 [00:00<00:00, 406.85it/s]Calculating Metrics: 100%|██████████| 14/14 [00:00<00:00, 405.89it/s]Calculating Metrics: 100%|██████████| 14/14 [00:00<00:00, 404.77it/s]
_ test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_value_lengths_to_equal:negative_one_length_too_small] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/14 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/14 [00:00<?, ?it/s]Calculating Metrics:  14%|█▍        | 2/14 [00:00<00:00, 3072.75it/s]Calculating Metrics:  14%|█▍        | 2/14 [00:00<00:00, 324.26it/s]Calculating Metrics:  29%|██▊       | 4/14 [00:00<00:00, 562.90it/s]Calculating Metrics:  29%|██▊       | 4/14 [00:00<00:00, 321.93it/s]Calculating Metrics:  43%|████▎     | 6/14 [00:00<00:00, 436.47it/s]Calculating Metrics:  43%|████▎     | 6/14 [00:00<00:00, 323.34it/s]Calculating Metrics:  57%|█████▋    | 8/14 [00:00<00:00, 406.34it/s]Calculating Metrics:  57%|█████▋    | 8/14 [00:00<00:00, 338.11it/s]Calculating Metrics:  93%|█████████▎| 13/14 [00:00<00:00, 467.61it/s]Calculating Metrics:  93%|█████████▎| 13/14 [00:00<00:00, 435.00it/s]Calculating Metrics: 100%|██████████| 14/14 [00:00<00:00, 448.39it/s]Calculating Metrics: 100%|██████████| 14/14 [00:00<00:00, 421.00it/s]Calculating Metrics: 100%|██████████| 14/14 [00:00<00:00, 419.15it/s]Calculating Metrics: 100%|██████████| 14/14 [00:00<00:00, 417.76it/s]
_ test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_value_lengths_to_equal:positive_one_length_too_small_mostly] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/14 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/14 [00:00<?, ?it/s]Calculating Metrics:  14%|█▍        | 2/14 [00:00<00:00, 3215.26it/s]Calculating Metrics:  14%|█▍        | 2/14 [00:00<00:00, 315.91it/s]Calculating Metrics:  29%|██▊       | 4/14 [00:00<00:00, 546.60it/s]Calculating Metrics:  29%|██▊       | 4/14 [00:00<00:00, 305.23it/s]Calculating Metrics:  43%|████▎     | 6/14 [00:00<00:00, 408.61it/s]Calculating Metrics:  43%|████▎     | 6/14 [00:00<00:00, 303.28it/s]Calculating Metrics:  57%|█████▋    | 8/14 [00:00<00:00, 378.38it/s]Calculating Metrics:  57%|█████▋    | 8/14 [00:00<00:00, 317.89it/s]Calculating Metrics:  93%|█████████▎| 13/14 [00:00<00:00, 453.37it/s]Calculating Metrics:  93%|█████████▎| 13/14 [00:00<00:00, 422.42it/s]Calculating Metrics: 100%|██████████| 14/14 [00:00<00:00, 439.92it/s]Calculating Metrics: 100%|██████████| 14/14 [00:00<00:00, 416.75it/s]Calculating Metrics: 100%|██████████| 14/14 [00:00<00:00, 415.72it/s]Calculating Metrics: 100%|██████████| 14/14 [00:00<00:00, 414.52it/s]
_ test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_between:basic_positive_now_test] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 2941.31it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 275.86it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 475.98it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 271.41it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 379.75it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 280.14it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 434.88it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 389.79it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 429.74it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 399.41it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 398.21it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 396.96it/s]
_ test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_between:basic_negative_now_test] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 3194.44it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 274.01it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 483.33it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 275.31it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 383.82it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 282.62it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 432.44it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 383.08it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 414.96it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 387.85it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 386.82it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 385.68it/s]
_ test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_between:basic_positive_now_timedelta_test] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 2984.21it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 265.71it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 460.34it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 267.93it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 374.76it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 278.73it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 438.30it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 391.85it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 431.06it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 402.43it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 401.36it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 400.15it/s]
_ test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_between:basic_negative_now_timedelta_test] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 3170.30it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 273.75it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 481.94it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 274.31it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 383.14it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 282.59it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 445.80it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 396.60it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 438.63it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 405.29it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 403.30it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 401.67it/s]
_ test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_between:basic_positive_datetime_test] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 3138.27it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 291.28it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 504.20it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 280.01it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 387.07it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 289.14it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 444.19it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 397.90it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 433.66it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 405.13it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 403.95it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 402.67it/s]
_ test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_between:basic_negative_datetime_test] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 3030.57it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 276.50it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 482.34it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 268.78it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 374.35it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 278.96it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 437.94it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 389.34it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 426.58it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 398.26it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 397.21it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 395.99it/s]
_ test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_between:basic_positive_test_with_timestamps_tz_informed] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 3145.33it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 281.02it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 475.68it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 261.47it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 355.03it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 272.94it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 418.59it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 378.35it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 413.38it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 387.91it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 386.88it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 385.73it/s]
_ test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_between:basic_negative_test_with_timestamps_tz_informed] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 2781.37it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 289.51it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 493.64it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 282.72it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 385.31it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 291.47it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 458.95it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 408.13it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 447.94it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 418.36it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 417.24it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 415.89it/s]
_ test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_not_be_null:basic_positive_case_complete_result_format] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/8 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/8 [00:00<?, ?it/s]Calculating Metrics:  25%|██▌       | 2/8 [00:00<00:00, 2862.03it/s]Calculating Metrics:  25%|██▌       | 2/8 [00:00<00:00, 643.69it/s]Calculating Metrics:  50%|█████     | 4/8 [00:00<00:00, 913.69it/s]Calculating Metrics:  50%|█████     | 4/8 [00:00<00:00, 618.45it/s]Calculating Metrics:  62%|██████▎   | 5/8 [00:00<00:00, 714.48it/s]Calculating Metrics:  62%|██████▎   | 5/8 [00:00<00:00, 569.68it/s]Calculating Metrics:  88%|████████▊ | 7/8 [00:00<00:00, 683.67it/s]Calculating Metrics:  88%|████████▊ | 7/8 [00:00<00:00, 617.13it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 629.18it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 588.85it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 584.85it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 580.52it/s]
_ test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_not_be_null:negative_all_missing_values] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/8 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/8 [00:00<?, ?it/s]Calculating Metrics:  25%|██▌       | 2/8 [00:00<00:00, 3226.39it/s]Calculating Metrics:  25%|██▌       | 2/8 [00:00<00:00, 660.68it/s]Calculating Metrics:  50%|█████     | 4/8 [00:00<00:00, 998.76it/s]Calculating Metrics:  50%|█████     | 4/8 [00:00<00:00, 658.89it/s]Calculating Metrics:  62%|██████▎   | 5/8 [00:00<00:00, 758.38it/s]Calculating Metrics:  62%|██████▎   | 5/8 [00:00<00:00, 597.51it/s]Calculating Metrics:  88%|████████▊ | 7/8 [00:00<00:00, 714.88it/s]Calculating Metrics:  88%|████████▊ | 7/8 [00:00<00:00, 619.24it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 628.29it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 582.79it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 578.53it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 573.32it/s]
_ test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_not_be_null:positive_mostly] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/8 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/8 [00:00<?, ?it/s]Calculating Metrics:  25%|██▌       | 2/8 [00:00<00:00, 3128.91it/s]Calculating Metrics:  25%|██▌       | 2/8 [00:00<00:00, 655.51it/s]Calculating Metrics:  50%|█████     | 4/8 [00:00<00:00, 953.74it/s]Calculating Metrics:  50%|█████     | 4/8 [00:00<00:00, 639.59it/s]Calculating Metrics:  62%|██████▎   | 5/8 [00:00<00:00, 738.77it/s]Calculating Metrics:  62%|██████▎   | 5/8 [00:00<00:00, 587.40it/s]Calculating Metrics:  88%|████████▊ | 7/8 [00:00<00:00, 712.11it/s]Calculating Metrics:  88%|████████▊ | 7/8 [00:00<00:00, 635.38it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 641.85it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 603.18it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 599.48it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 595.06it/s]
_ test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_not_be_null:negative_no_mostly_one_missing] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/8 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/8 [00:00<?, ?it/s]Calculating Metrics:  25%|██▌       | 2/8 [00:00<00:00, 2793.41it/s]Calculating Metrics:  25%|██▌       | 2/8 [00:00<00:00, 632.34it/s]Calculating Metrics:  50%|█████     | 4/8 [00:00<00:00, 956.24it/s]Calculating Metrics:  50%|█████     | 4/8 [00:00<00:00, 596.57it/s]Calculating Metrics:  62%|██████▎   | 5/8 [00:00<00:00, 674.59it/s]Calculating Metrics:  62%|██████▎   | 5/8 [00:00<00:00, 526.04it/s]Calculating Metrics:  88%|████████▊ | 7/8 [00:00<00:00, 641.32it/s]Calculating Metrics:  88%|████████▊ | 7/8 [00:00<00:00, 582.46it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 608.64it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 573.97it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 570.69it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 566.62it/s]
_ test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_not_be_null:negative_mostly] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 3081.78it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 494.09it/s]Calculating Metrics:  40%|████      | 4/10 [00:00<00:00, 687.06it/s]Calculating Metrics:  40%|████      | 4/10 [00:00<00:00, 448.79it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 529.10it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 408.26it/s]Calculating Metrics:  90%|█████████ | 9/10 [00:00<00:00, 582.30it/s]Calculating Metrics:  90%|█████████ | 9/10 [00:00<00:00, 531.57it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 553.81it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 520.83it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 518.35it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 515.63it/s]
_ test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_not_be_null:positive_case_with_mostly_and_no_unexpected_values] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 3159.55it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 499.00it/s]Calculating Metrics:  40%|████      | 4/10 [00:00<00:00, 808.74it/s]Calculating Metrics:  40%|████      | 4/10 [00:00<00:00, 490.62it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 574.18it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 431.66it/s]Calculating Metrics:  90%|█████████ | 9/10 [00:00<00:00, 616.27it/s]Calculating Metrics:  90%|█████████ | 9/10 [00:00<00:00, 562.22it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 588.38it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 553.32it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 550.87it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 547.95it/s]
_ test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_not_be_null:negative_case_with_75percent_null_values_no_mostly] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 3105.74it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 479.62it/s]Calculating Metrics:  40%|████      | 4/10 [00:00<00:00, 781.10it/s]Calculating Metrics:  40%|████      | 4/10 [00:00<00:00, 479.34it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 561.23it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 430.37it/s]Calculating Metrics:  90%|█████████ | 9/10 [00:00<00:00, 617.29it/s]Calculating Metrics:  90%|█████████ | 9/10 [00:00<00:00, 562.75it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 577.41it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 542.89it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 540.40it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 537.55it/s]
_ test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_value_z_scores_to_be_less_than:basic_successful_test] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/21 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/21 [00:00<?, ?it/s]Calculating Metrics:  10%|▉         | 2/21 [00:00<00:00, 2975.74it/s]Calculating Metrics:  10%|▉         | 2/21 [00:00<00:00, 182.41it/s]Calculating Metrics:  19%|█▉        | 4/21 [00:00<00:00, 328.57it/s]Calculating Metrics:  19%|█▉        | 4/21 [00:00<00:00, 183.77it/s]Calculating Metrics:  33%|███▎      | 7/21 [00:00<00:00, 290.55it/s]Calculating Metrics:  33%|███▎      | 7/21 [00:00<00:00, 212.35it/s]Calculating Metrics:  48%|████▊     | 10/21 [00:00<00:00, 282.93it/s]Calculating Metrics:  48%|████▊     | 10/21 [00:00<00:00, 231.29it/s]Calculating Metrics:  57%|█████▋    | 12/21 [00:00<00:00, 263.08it/s]Calculating Metrics:  57%|█████▋    | 12/21 [00:00<00:00, 212.59it/s]Calculating Metrics:  62%|██████▏   | 13/21 [00:00<00:00, 225.10it/s]Calculating Metrics:  62%|██████▏   | 13/21 [00:00<00:00, 195.46it/s]Calculating Metrics:  67%|██████▋   | 14/21 [00:00<00:00, 203.65it/s]Calculating Metrics:  67%|██████▋   | 14/21 [00:00<00:00, 183.04it/s]Calculating Metrics:  71%|███████▏  | 15/21 [00:00<00:00, 192.24it/s]Calculating Metrics:  71%|███████▏  | 15/21 [00:00<00:00, 176.39it/s]Calculating Metrics:  76%|███████▌  | 16/21 [00:00<00:00, 186.11it/s]Calculating Metrics:  76%|███████▌  | 16/21 [00:00<00:00, 173.73it/s]Calculating Metrics:  95%|█████████▌| 20/21 [00:00<00:00, 208.49it/s]Calculating Metrics:  95%|█████████▌| 20/21 [00:00<00:00, 200.87it/s]Calculating Metrics: 100%|██████████| 21/21 [00:00<00:00, 206.96it/s]Calculating Metrics: 100%|██████████| 21/21 [00:00<00:00, 206.96it/s]Calculating Metrics: 100%|██████████| 21/21 [00:00<00:00, 206.96it/s]Calculating Metrics: 100%|██████████| 21/21 [00:00<00:00, 206.96it/s]Calculating Metrics: 100%|██████████| 21/21 [00:00<00:00, 198.53it/s]
_ test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_value_z_scores_to_be_less_than:basic_unsuccessful_test] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/21 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/21 [00:00<?, ?it/s]Calculating Metrics:  10%|▉         | 2/21 [00:00<00:00, 3301.30it/s]Calculating Metrics:  10%|▉         | 2/21 [00:00<00:00, 189.09it/s]Calculating Metrics:  19%|█▉        | 4/21 [00:00<00:00, 346.29it/s]Calculating Metrics:  19%|█▉        | 4/21 [00:00<00:00, 190.36it/s]Calculating Metrics:  33%|███▎      | 7/21 [00:00<00:00, 313.05it/s]Calculating Metrics:  33%|███▎      | 7/21 [00:00<00:00, 224.07it/s]Calculating Metrics:  48%|████▊     | 10/21 [00:00<00:00, 299.57it/s]Calculating Metrics:  48%|████▊     | 10/21 [00:00<00:00, 238.65it/s]Calculating Metrics:  57%|█████▋    | 12/21 [00:00<00:00, 273.46it/s]Calculating Metrics:  57%|█████▋    | 12/21 [00:00<00:00, 233.99it/s]Calculating Metrics:  62%|██████▏   | 13/21 [00:00<00:00, 248.98it/s]Calculating Metrics:  62%|██████▏   | 13/21 [00:00<00:00, 221.18it/s]Calculating Metrics:  67%|██████▋   | 14/21 [00:00<00:00, 230.20it/s]Calculating Metrics:  67%|██████▋   | 14/21 [00:00<00:00, 206.41it/s]Calculating Metrics:  71%|███████▏  | 15/21 [00:00<00:00, 217.46it/s]Calculating Metrics:  71%|███████▏  | 15/21 [00:00<00:00, 200.03it/s]Calculating Metrics:  76%|███████▌  | 16/21 [00:00<00:00, 211.00it/s]Calculating Metrics:  76%|███████▌  | 16/21 [00:00<00:00, 197.28it/s]Calculating Metrics:  95%|█████████▌| 20/21 [00:00<00:00, 232.66it/s]Calculating Metrics:  95%|█████████▌| 20/21 [00:00<00:00, 220.82it/s]Calculating Metrics: 100%|██████████| 21/21 [00:00<00:00, 227.38it/s]Calculating Metrics: 100%|██████████| 21/21 [00:00<00:00, 220.10it/s]Calculating Metrics: 100%|██████████| 21/21 [00:00<00:00, 219.90it/s]Calculating Metrics: 100%|██████████| 21/21 [00:00<00:00, 219.67it/s]
_ test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_value_z_scores_to_be_less_than:basic_test_with_outlier] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/21 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/21 [00:00<?, ?it/s]Calculating Metrics:  10%|▉         | 2/21 [00:00<00:00, 3294.82it/s]Calculating Metrics:  10%|▉         | 2/21 [00:00<00:00, 194.55it/s]Calculating Metrics:  19%|█▉        | 4/21 [00:00<00:00, 356.02it/s]Calculating Metrics:  19%|█▉        | 4/21 [00:00<00:00, 194.05it/s]Calculating Metrics:  33%|███▎      | 7/21 [00:00<00:00, 318.31it/s]Calculating Metrics:  33%|███▎      | 7/21 [00:00<00:00, 228.26it/s]Calculating Metrics:  48%|████▊     | 10/21 [00:00<00:00, 301.37it/s]Calculating Metrics:  48%|████▊     | 10/21 [00:00<00:00, 244.24it/s]Calculating Metrics:  57%|█████▋    | 12/21 [00:00<00:00, 280.05it/s]Calculating Metrics:  57%|█████▋    | 12/21 [00:00<00:00, 241.16it/s]Calculating Metrics:  62%|██████▏   | 13/21 [00:00<00:00, 256.39it/s]Calculating Metrics:  62%|██████▏   | 13/21 [00:00<00:00, 227.51it/s]Calculating Metrics:  67%|██████▋   | 14/21 [00:00<00:00, 237.01it/s]Calculating Metrics:  67%|██████▋   | 14/21 [00:00<00:00, 214.95it/s]Calculating Metrics:  71%|███████▏  | 15/21 [00:00<00:00, 226.01it/s]Calculating Metrics:  71%|███████▏  | 15/21 [00:00<00:00, 207.95it/s]Calculating Metrics:  76%|███████▌  | 16/21 [00:00<00:00, 219.39it/s]Calculating Metrics:  76%|███████▌  | 16/21 [00:00<00:00, 204.84it/s]Calculating Metrics:  95%|█████████▌| 20/21 [00:00<00:00, 244.91it/s]Calculating Metrics:  95%|█████████▌| 20/21 [00:00<00:00, 234.50it/s]Calculating Metrics: 100%|██████████| 21/21 [00:00<00:00, 241.23it/s]Calculating Metrics: 100%|██████████| 21/21 [00:00<00:00, 233.13it/s]Calculating Metrics: 100%|██████████| 21/21 [00:00<00:00, 232.88it/s]Calculating Metrics: 100%|██████████| 21/21 [00:00<00:00, 232.62it/s]
_ test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_value_z_scores_to_be_less_than:basic_test_mostly_zero] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/21 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/21 [00:00<?, ?it/s]Calculating Metrics:  10%|▉         | 2/21 [00:00<00:00, 3184.74it/s]Calculating Metrics:  10%|▉         | 2/21 [00:00<00:00, 192.35it/s]Calculating Metrics:  19%|█▉        | 4/21 [00:00<00:00, 351.06it/s]Calculating Metrics:  19%|█▉        | 4/21 [00:00<00:00, 193.13it/s]Calculating Metrics:  33%|███▎      | 7/21 [00:00<00:00, 316.56it/s]Calculating Metrics:  33%|███▎      | 7/21 [00:00<00:00, 228.02it/s]Calculating Metrics:  48%|████▊     | 10/21 [00:00<00:00, 303.00it/s]Calculating Metrics:  48%|████▊     | 10/21 [00:00<00:00, 244.89it/s]Calculating Metrics:  57%|█████▋    | 12/21 [00:00<00:00, 280.27it/s]Calculating Metrics:  57%|█████▋    | 12/21 [00:00<00:00, 241.78it/s]Calculating Metrics:  62%|██████▏   | 13/21 [00:00<00:00, 257.09it/s]Calculating Metrics:  62%|██████▏   | 13/21 [00:00<00:00, 228.12it/s]Calculating Metrics:  67%|██████▋   | 14/21 [00:00<00:00, 237.76it/s]Calculating Metrics:  67%|██████▋   | 14/21 [00:00<00:00, 215.86it/s]Calculating Metrics:  71%|███████▏  | 15/21 [00:00<00:00, 227.42it/s]Calculating Metrics:  71%|███████▏  | 15/21 [00:00<00:00, 210.21it/s]Calculating Metrics:  76%|███████▌  | 16/21 [00:00<00:00, 222.06it/s]Calculating Metrics:  76%|███████▌  | 16/21 [00:00<00:00, 207.63it/s]Calculating Metrics:  95%|█████████▌| 20/21 [00:00<00:00, 248.08it/s]Calculating Metrics:  95%|█████████▌| 20/21 [00:00<00:00, 237.72it/s]Calculating Metrics: 100%|██████████| 21/21 [00:00<00:00, 244.39it/s]Calculating Metrics: 100%|██████████| 21/21 [00:00<00:00, 235.94it/s]Calculating Metrics: 100%|██████████| 21/21 [00:00<00:00, 235.64it/s]Calculating Metrics: 100%|██████████| 21/21 [00:00<00:00, 235.35it/s]
_ test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_value_z_scores_to_be_less_than:basic_test_with_missing] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/21 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/21 [00:00<?, ?it/s]Calculating Metrics:  10%|▉         | 2/21 [00:00<00:00, 3204.20it/s]Calculating Metrics:  10%|▉         | 2/21 [00:00<00:00, 197.86it/s]Calculating Metrics:  19%|█▉        | 4/21 [00:00<00:00, 362.01it/s]Calculating Metrics:  19%|█▉        | 4/21 [00:00<00:00, 195.54it/s]Calculating Metrics:  33%|███▎      | 7/21 [00:00<00:00, 320.73it/s]Calculating Metrics:  33%|███▎      | 7/21 [00:00<00:00, 230.33it/s]Calculating Metrics:  48%|████▊     | 10/21 [00:00<00:00, 305.73it/s]Calculating Metrics:  48%|████▊     | 10/21 [00:00<00:00, 244.07it/s]Calculating Metrics:  57%|█████▋    | 12/21 [00:00<00:00, 279.13it/s]Calculating Metrics:  57%|█████▋    | 12/21 [00:00<00:00, 241.18it/s]Calculating Metrics:  62%|██████▏   | 13/21 [00:00<00:00, 256.55it/s]Calculating Metrics:  62%|██████▏   | 13/21 [00:00<00:00, 227.31it/s]Calculating Metrics:  67%|██████▋   | 14/21 [00:00<00:00, 234.86it/s]Calculating Metrics:  67%|██████▋   | 14/21 [00:00<00:00, 213.72it/s]Calculating Metrics:  71%|███████▏  | 15/21 [00:00<00:00, 225.16it/s]Calculating Metrics:  71%|███████▏  | 15/21 [00:00<00:00, 208.07it/s]Calculating Metrics:  76%|███████▌  | 16/21 [00:00<00:00, 219.75it/s]Calculating Metrics:  76%|███████▌  | 16/21 [00:00<00:00, 205.96it/s]Calculating Metrics:  95%|█████████▌| 20/21 [00:00<00:00, 246.03it/s]Calculating Metrics:  95%|█████████▌| 20/21 [00:00<00:00, 235.26it/s]Calculating Metrics: 100%|██████████| 21/21 [00:00<00:00, 241.37it/s]Calculating Metrics: 100%|██████████| 21/21 [00:00<00:00, 233.11it/s]Calculating Metrics: 100%|██████████| 21/21 [00:00<00:00, 232.82it/s]Calculating Metrics: 100%|██████████| 21/21 [00:00<00:00, 232.55it/s]
_ test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_between:basic_positive_test0] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 3058.19it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 286.54it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 465.12it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 270.47it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 373.86it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 283.45it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 438.44it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 394.93it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 435.44it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 406.66it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 405.44it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 404.13it/s]
_ test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_between:another_basic_positive_test0] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 3105.74it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 286.95it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 487.27it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 283.23it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 394.48it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 293.90it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 463.02it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 414.55it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 459.05it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 427.58it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 426.25it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 424.82it/s]
_ test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_between:basic_positive_test_with_timestamps0] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 3144.16it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 288.56it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 488.29it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 285.84it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 392.16it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 294.62it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 444.53it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 396.45it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 431.17it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 403.43it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 402.25it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 400.99it/s]
_ test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_between:missing_min_value0] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 3105.74it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 235.85it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 398.33it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 230.52it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 323.42it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 250.22it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 395.51it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 357.60it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 396.75it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 371.17it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 370.14it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 369.08it/s]
_ test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_between:null_min_value0] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 3081.78it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 274.34it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 471.69it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 271.44it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 376.36it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 281.64it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 446.75it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 397.14it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 436.27it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 406.11it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 404.93it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 403.62it/s]
_ test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_between:missing_max_value0] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 3124.25it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 275.97it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 475.67it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 266.31it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 371.31it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 273.56it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 424.89it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 381.20it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 419.85it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 392.01it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 391.00it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 389.83it/s]
_ test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_between:null_max_value0] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 2876.75it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 265.82it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 451.61it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 262.33it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 367.82it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 276.89it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 438.89it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 391.69it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 431.83it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 403.00it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 401.99it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 400.79it/s]
_ test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_between:basic_negative_test0] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 3178.71it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 298.24it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 499.37it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 289.00it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 401.92it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 300.87it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 472.79it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 417.03it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 460.21it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 427.45it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 426.12it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 424.68it/s]
_ test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_between:another_negative_test0] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 2877.74it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 293.82it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 505.19it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 279.22it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 385.16it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 287.90it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 452.49it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 405.76it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 449.18it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 419.74it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 418.60it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 417.27it/s]
_ test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_between:basic_negative_test_with_timestamps0] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 2979.97it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 274.73it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 469.65it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 278.98it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 380.03it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 288.92it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 456.53it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 410.08it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 447.30it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 418.29it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 417.17it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 415.84it/s]
_ test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_between:positive_test_with_result_format__boolean_only0] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 2933.08it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 430.58it/s]Calculating Metrics:  40%|████      | 4/10 [00:00<00:00, 705.61it/s]Calculating Metrics:  40%|████      | 4/10 [00:00<00:00, 437.65it/s]Calculating Metrics:  60%|██████    | 6/10 [00:00<00:00, 587.51it/s]Calculating Metrics:  60%|██████    | 6/10 [00:00<00:00, 457.99it/s]Calculating Metrics:  80%|████████  | 8/10 [00:00<00:00, 560.11it/s]Calculating Metrics:  80%|████████  | 8/10 [00:00<00:00, 487.90it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 533.71it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 500.48it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 498.46it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 496.03it/s]
_ test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_between:another_positive_test_with_result_format__boolean_only0] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 3294.82it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 443.16it/s]Calculating Metrics:  40%|████      | 4/10 [00:00<00:00, 721.23it/s]Calculating Metrics:  40%|████      | 4/10 [00:00<00:00, 438.94it/s]Calculating Metrics:  60%|██████    | 6/10 [00:00<00:00, 583.95it/s]Calculating Metrics:  60%|██████    | 6/10 [00:00<00:00, 454.10it/s]Calculating Metrics:  80%|████████  | 8/10 [00:00<00:00, 556.91it/s]Calculating Metrics:  80%|████████  | 8/10 [00:00<00:00, 489.18it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 550.27it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 514.06it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 511.93it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 509.38it/s]
_ test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_between:negative_test_with_result_format__boolean_only0] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 3198.10it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 440.69it/s]Calculating Metrics:  40%|████      | 4/10 [00:00<00:00, 637.53it/s]Calculating Metrics:  40%|████      | 4/10 [00:00<00:00, 412.58it/s]Calculating Metrics:  60%|██████    | 6/10 [00:00<00:00, 557.77it/s]Calculating Metrics:  60%|██████    | 6/10 [00:00<00:00, 429.14it/s]Calculating Metrics:  80%|████████  | 8/10 [00:00<00:00, 518.50it/s]Calculating Metrics:  80%|████████  | 8/10 [00:00<00:00, 436.70it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 487.42it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 457.35it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 455.35it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 453.24it/s]
_ test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_between:another_negative_test_with_result_format__boolean_only0] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 3179.91it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 449.33it/s]Calculating Metrics:  40%|████      | 4/10 [00:00<00:00, 737.72it/s]Calculating Metrics:  40%|████      | 4/10 [00:00<00:00, 451.11it/s]Calculating Metrics:  60%|██████    | 6/10 [00:00<00:00, 600.54it/s]Calculating Metrics:  60%|██████    | 6/10 [00:00<00:00, 462.44it/s]Calculating Metrics:  80%|████████  | 8/10 [00:00<00:00, 566.93it/s]Calculating Metrics:  80%|████████  | 8/10 [00:00<00:00, 490.83it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 550.82it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 515.31it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 513.17it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 510.60it/s]
_ test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_between:positive_test_with_mostly0] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 3140.62it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 291.04it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 510.21it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 290.76it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 402.04it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 299.12it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 466.67it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 410.55it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 449.33it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 419.61it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 418.46it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 417.15it/s]
_ test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_between:2nd_positive_test_with_mostly0] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 3220.20it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 274.08it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 465.00it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 269.82it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 374.18it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 282.91it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 447.52it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 401.52it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 446.00it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 415.77it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 414.46it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 413.12it/s]
_ test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_between:3rd_positive_test_with_mostly0] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 2709.50it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 288.68it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 492.42it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 286.14it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 398.51it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 288.24it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 454.41it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 406.73it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 451.18it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 421.04it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 419.83it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 418.45it/s]
_ test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_between:negative_test_with_mostly0] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 3194.44it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 292.44it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 501.13it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 290.55it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 404.45it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 301.82it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 468.88it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 417.81it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 450.83it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 417.28it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 415.70it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 414.25it/s]
_ test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_between:error:_improperly_mixed_types0] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 3090.87it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 254.42it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 431.73it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 260.48it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 363.51it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 276.28it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 430.29it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 388.86it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 428.69it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 400.78it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 399.56it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 398.30it/s]
_ test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_between:error:_improperly_mixed_types_again0] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 3120.76it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 287.76it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 490.25it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 286.66it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 399.33it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 295.36it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 464.98it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 415.56it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 454.11it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 424.20it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 423.05it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 421.71it/s]
_ test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_between:error:_improperly_mixed_types_once_more0] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 3170.30it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 289.69it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 495.80it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 289.60it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 403.18it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 300.40it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 442.37it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 393.83it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 434.52it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 405.02it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 403.73it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 402.41it/s]
_ test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_between:negative_test_to_verify_that_the_denominator_for_mostly_works_with_missing_values0] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 2942.34it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 263.83it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 449.30it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 266.44it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 371.01it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 282.74it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 437.17it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 393.79it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 433.75it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 404.37it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 403.14it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 401.78it/s]
_ test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_between:positive_test_to_verify_that_the_denominator_for_mostly_works_with_missing_values0] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 2962.08it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 288.15it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 486.85it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 285.43it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 397.80it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 298.40it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 471.05it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 419.77it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 456.56it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 425.28it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 423.91it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 422.48it/s]
_ test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_between:error_on_string-to-int_comparisons0] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 3159.55it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 288.72it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 493.83it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 287.27it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 399.79it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 293.46it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 447.23it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 398.26it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 435.15it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 407.51it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 406.43it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 405.17it/s]
_ test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_between:test_min_value_is_greater_than_max_value0] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 3144.16it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 278.59it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 468.94it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 275.18it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 183.49it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 138.28it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 110.78it/s]Calculating Metrics:  38%|███▊      | 5/13 [00:00<00:00, 135.66it/s]Calculating Metrics:  38%|███▊      | 5/13 [00:00<00:00, 117.92it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 138.80it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 123.57it/s]Calculating Metrics:  54%|█████▍    | 7/13 [00:00<00:00, 139.64it/s]Calculating Metrics:  54%|█████▍    | 7/13 [00:00<00:00, 126.32it/s]Calculating Metrics:  54%|█████▍    | 7/13 [00:00<00:00, 125.45it/s]Calculating Metrics:  54%|█████▍    | 7/13 [00:00<00:00, 125.19it/s]
_ test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_between:test_strict_min_failure0] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 2979.97it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 267.09it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 455.68it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 262.66it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 364.85it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 277.02it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 427.90it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 385.87it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 425.07it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 397.57it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 396.37it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 395.11it/s]
_ test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_between:test_strict_min_success0] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 2962.08it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 282.41it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 474.48it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 280.31it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 391.21it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 291.54it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 460.96it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 413.33it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 459.49it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 428.96it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 427.82it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 426.45it/s]
_ test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_between:test_strict_max_failure0] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 3144.16it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 289.97it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 493.53it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 286.45it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 398.70it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 297.44it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 455.90it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 406.08it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 442.57it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 411.71it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 410.19it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 408.80it/s]
_ test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_between:test_strict_max_success0] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 3109.19it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 295.68it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 501.80it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 280.95it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 388.87it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 287.73it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 453.73it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 407.42it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 453.43it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 423.99it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 422.87it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 421.54it/s]
_ test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_between:basic_positive_test1] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 2797.14it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 280.40it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 482.87it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 282.67it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 394.37it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 294.40it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 453.89it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 407.11it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 441.25it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 411.76it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 410.11it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 408.64it/s]
_ test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_between:another_basic_positive_test1] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 3209.11it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 290.49it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 492.38it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 277.95it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 382.73it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 280.77it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 441.29it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 396.94it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 442.04it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 412.41it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 411.16it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 409.74it/s]
_ test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_between:basic_positive_test_with_timestamps1] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 2751.27it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 257.30it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 440.43it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 267.65it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 369.14it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 280.01it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 432.66it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 389.97it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 427.67it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 400.58it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 399.45it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 398.20it/s]
_ test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_between:missing_min_value1] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 2877.74it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 273.53it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 468.17it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 267.29it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 371.89it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 279.62it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 431.97it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 384.82it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 417.31it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 389.40it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 388.33it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 387.18it/s]
_ test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_between:null_min_value1] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 2809.31it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 268.96it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 455.22it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 254.67it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 356.57it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 270.71it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 428.58it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 384.00it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 428.30it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 399.91it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 398.92it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 397.72it/s]
_ test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_between:missing_max_value1] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 3148.88it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 275.74it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 475.96it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 270.82it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 378.86it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 284.14it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 441.29it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 392.76it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 433.42it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 404.53it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 403.51it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 402.32it/s]
_ test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_between:null_max_value1] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 3144.16it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 275.14it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 447.21it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 262.34it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 366.29it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 272.23it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 422.93it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 378.55it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 418.45it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 389.78it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 388.69it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 387.51it/s]
_ test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_between:basic_negative_test1] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 3053.73it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 272.82it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 452.44it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 272.22it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 379.58it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 285.61it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 445.47it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 400.86it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 440.13it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 412.27it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 411.18it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 409.91it/s]
_ test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_between:another_negative_test1] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 3114.97it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 284.94it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 486.72it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 282.28it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 393.11it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 294.92it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 463.43it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 415.48it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 461.79it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 431.22it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 430.02it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 428.62it/s]
_ test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_between:basic_negative_test_with_timestamps1] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 3114.97it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 286.86it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 490.13it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 284.05it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 387.72it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 289.31it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 448.41it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 400.04it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 438.20it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 410.25it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 409.19it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 407.95it/s]
_ test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_between:positive_test_with_result_format__boolean_only1] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 3119.60it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 458.92it/s]Calculating Metrics:  40%|████      | 4/10 [00:00<00:00, 756.41it/s]Calculating Metrics:  40%|████      | 4/10 [00:00<00:00, 457.13it/s]Calculating Metrics:  60%|██████    | 6/10 [00:00<00:00, 610.12it/s]Calculating Metrics:  60%|██████    | 6/10 [00:00<00:00, 466.96it/s]Calculating Metrics:  80%|████████  | 8/10 [00:00<00:00, 568.95it/s]Calculating Metrics:  80%|████████  | 8/10 [00:00<00:00, 496.00it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 553.49it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 516.31it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 513.72it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 508.57it/s]
_ test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_between:another_positive_test_with_result_format__boolean_only1] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 3200.54it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 451.36it/s]Calculating Metrics:  40%|████      | 4/10 [00:00<00:00, 747.95it/s]Calculating Metrics:  40%|████      | 4/10 [00:00<00:00, 450.96it/s]Calculating Metrics:  60%|██████    | 6/10 [00:00<00:00, 599.59it/s]Calculating Metrics:  60%|██████    | 6/10 [00:00<00:00, 462.11it/s]Calculating Metrics:  80%|████████  | 8/10 [00:00<00:00, 556.57it/s]Calculating Metrics:  80%|████████  | 8/10 [00:00<00:00, 488.31it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 550.57it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 514.91it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 512.66it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 510.08it/s]
_ test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_between:negative_test_with_result_format__boolean_only1] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 3200.54it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 455.88it/s]Calculating Metrics:  40%|████      | 4/10 [00:00<00:00, 755.42it/s]Calculating Metrics:  40%|████      | 4/10 [00:00<00:00, 457.05it/s]Calculating Metrics:  60%|██████    | 6/10 [00:00<00:00, 607.72it/s]Calculating Metrics:  60%|██████    | 6/10 [00:00<00:00, 469.37it/s]Calculating Metrics:  80%|████████  | 8/10 [00:00<00:00, 575.03it/s]Calculating Metrics:  80%|████████  | 8/10 [00:00<00:00, 502.92it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 565.19it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 527.68it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 525.46it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 522.60it/s]
_ test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_between:another_negative_test_with_result_format__boolean_only1] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 2975.74it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 447.42it/s]Calculating Metrics:  40%|████      | 4/10 [00:00<00:00, 740.19it/s]Calculating Metrics:  40%|████      | 4/10 [00:00<00:00, 450.46it/s]Calculating Metrics:  60%|██████    | 6/10 [00:00<00:00, 603.74it/s]Calculating Metrics:  60%|██████    | 6/10 [00:00<00:00, 469.37it/s]Calculating Metrics:  80%|████████  | 8/10 [00:00<00:00, 576.58it/s]Calculating Metrics:  80%|████████  | 8/10 [00:00<00:00, 504.38it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 561.86it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 524.52it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 522.30it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 519.67it/s]
_ test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_between:positive_test_with_mostly1] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 3030.57it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 287.53it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 492.93it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 283.57it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 394.42it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 297.50it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 466.07it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 414.08it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 459.06it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 428.90it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 427.70it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 426.33it/s]
_ test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_between:2nd_positive_test_with_mostly1] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 2907.66it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 274.99it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 472.76it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 271.92it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 379.10it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 288.14it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 450.54it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 401.17it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 442.13it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 412.33it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 411.04it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 409.67it/s]
_ test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_between:3rd_positive_test_with_mostly1] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 2954.78it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 282.93it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 479.73it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 273.15it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 377.45it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 286.67it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 450.97it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 402.59it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 445.75it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 416.63it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 415.51it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 414.19it/s]
_ test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_between:negative_test_with_mostly1] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 2881.69it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 288.57it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 491.58it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 286.41it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 396.80it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 295.69it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 464.80it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 416.20it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 461.60it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 428.42it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 427.05it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 425.63it/s]
_ test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_between:error:_improperly_mixed_types1] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 3135.93it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 266.53it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 459.88it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 276.19it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 385.33it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 289.35it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 444.20it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 397.26it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 431.93it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 402.75it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 401.53it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 400.26it/s]
_ test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_between:error:_improperly_mixed_types_again1] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 3120.76it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 279.26it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 469.92it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 269.18it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 369.51it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 278.80it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 441.10it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 394.68it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 437.98it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 409.04it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 407.74it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 406.44it/s]
_ test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_between:error:_improperly_mixed_types_once_more1] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 2989.53it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 287.15it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 473.37it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 279.62it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 389.67it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 291.45it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 459.23it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 410.20it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 454.82it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 423.87it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 422.52it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 421.11it/s]
_ test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_between:negative_test_to_verify_that_the_denominator_for_mostly_works_with_missing_values1] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 3072.75it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 290.67it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 495.68it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 282.71it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 390.42it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 291.94it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 441.56it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 392.55it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 424.82it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 396.73it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 395.35it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 394.04it/s]
_ test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_between:positive_test_to_verify_that_the_denominator_for_mostly_works_with_missing_values1] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 3125.41it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 287.11it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 477.67it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 270.16it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 371.80it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 283.90it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 449.59it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 401.61it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 439.70it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 411.52it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 410.40it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 409.13it/s]
_ test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_between:error_on_string-to-int_comparisons1] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 3173.90it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 287.10it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 487.44it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 283.28it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 394.37it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 295.16it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 455.28it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 408.42it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 446.33it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 417.17it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 416.04it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 414.72it/s]
_ test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_between:test_min_value_is_greater_than_max_value1] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 3034.95it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 294.20it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 498.57it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 285.39it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 184.87it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 136.39it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 109.13it/s]Calculating Metrics:  38%|███▊      | 5/13 [00:00<00:00, 133.66it/s]Calculating Metrics:  38%|███▊      | 5/13 [00:00<00:00, 116.02it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 136.53it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 121.99it/s]Calculating Metrics:  54%|█████▍    | 7/13 [00:00<00:00, 138.01it/s]Calculating Metrics:  54%|█████▍    | 7/13 [00:00<00:00, 125.45it/s]Calculating Metrics:  54%|█████▍    | 7/13 [00:00<00:00, 124.66it/s]Calculating Metrics:  54%|█████▍    | 7/13 [00:00<00:00, 124.42it/s]
_ test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_between:test_strict_min_failure1] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 3111.50it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 286.62it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 503.73it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 287.13it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 398.44it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 293.87it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 447.32it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 402.08it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 434.99it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 406.33it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 405.10it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 403.81it/s]
_ test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_between:test_strict_min_success1] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 3148.88it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 289.14it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 492.06it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 280.01it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 382.85it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 284.87it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 448.30it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 402.36it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 446.50it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 416.67it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 415.43it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 414.05it/s]
_ test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_between:test_strict_max_failure1] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 2989.53it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 277.33it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 473.04it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 277.28it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 386.00it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 289.38it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 447.08it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 401.54it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 441.76it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 411.22it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 409.95it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 408.60it/s]
_ test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_between:test_strict_max_success1] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 3148.88it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 284.69it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 485.66it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 283.10it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 392.54it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 294.32it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 463.86it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 415.82it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 459.87it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 426.28it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 424.75it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 423.32it/s]
_ test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_not_be_in_set:basic_positive_test_case_single_value_not_in_column] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 3104.59it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 331.45it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 544.64it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 314.42it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 433.93it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 320.34it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 474.87it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 424.87it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 464.63it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 437.05it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 435.84it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 434.40it/s]
_ test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_not_be_in_set:negative_test_case_include_one_existing_column_value] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 3081.78it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 316.71it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 525.72it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 305.18it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 413.74it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 316.77it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 497.49it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 445.85it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 492.37it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 459.00it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 457.44it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 455.68it/s]
_ test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_not_be_in_set:positive_empty_values_set] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 3139.45it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 301.89it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 503.65it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 306.28it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 427.08it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 325.63it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 496.63it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 447.01it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 490.77it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 460.48it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 459.13it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 457.55it/s]
_ test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_not_be_in_set:basic_negative_strings_set_all_character_values] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 3114.97it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 326.95it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 519.13it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 303.37it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 416.98it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 317.26it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 485.48it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 437.58it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 477.08it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 447.41it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 446.09it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 444.58it/s]
_ test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_not_be_in_set:positive_test_strings_set_extra_value_in_column] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 3160.74it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 315.97it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 527.72it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 313.97it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 435.13it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 325.63it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 499.28it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 447.56it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 491.03it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 459.66it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 458.02it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 456.40it/s]
_ test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_not_be_in_set:negative_test_float_set_two_out_of_three_column_values_included_mostly] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 3206.65it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 305.82it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 520.16it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 288.04it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 392.24it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 286.67it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 439.86it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 397.17it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 438.01it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 411.40it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 410.30it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 409.01it/s]
_ test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_not_be_in_set:positive_test_float_set_two_out_of_three_column_values_included_mostly] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 3081.78it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 303.44it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 510.01it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 298.48it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 413.85it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 307.66it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 479.26it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 427.11it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 465.15it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 433.75it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 432.48it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 431.02it/s]
_ test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_not_be_in_set:positive_test_values_set_is_null] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 3119.60it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 292.31it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 493.69it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 283.67it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 394.08it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 295.54it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 455.84it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 408.92it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 447.15it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 417.64it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 416.32it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 414.94it/s]
_ test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_of_type:basic_sqlalchemy_int_positive_test] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/1 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/1 [00:00<?, ?it/s]Calculating Metrics: 100%|██████████| 1/1 [00:00<00:00, 1981.25it/s]Calculating Metrics: 100%|██████████| 1/1 [00:00<00:00, 1548.28it/s]Calculating Metrics: 100%|██████████| 1/1 [00:00<00:00, 1351.69it/s]Calculating Metrics: 100%|██████████| 1/1 [00:00<00:00, 1201.81it/s]
_ test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_of_type:negative_sqlite_integer_is_not_varchar] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/1 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/1 [00:00<?, ?it/s]Calculating Metrics: 100%|██████████| 1/1 [00:00<00:00, 2457.12it/s]Calculating Metrics: 100%|██████████| 1/1 [00:00<00:00, 1824.40it/s]Calculating Metrics: 100%|██████████| 1/1 [00:00<00:00, 1595.40it/s]Calculating Metrics: 100%|██████████| 1/1 [00:00<00:00, 1392.99it/s]
_ test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_of_type:positive_test_sql_non_postgres_floats] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/1 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/1 [00:00<?, ?it/s]Calculating Metrics: 100%|██████████| 1/1 [00:00<00:00, 2445.66it/s]Calculating Metrics: 100%|██████████| 1/1 [00:00<00:00, 1822.03it/s]Calculating Metrics: 100%|██████████| 1/1 [00:00<00:00, 1592.37it/s]Calculating Metrics: 100%|██████████| 1/1 [00:00<00:00, 1391.15it/s]
_ test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_of_type:positive_test_sql_varchar] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/1 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/1 [00:00<?, ?it/s]Calculating Metrics: 100%|██████████| 1/1 [00:00<00:00, 2357.68it/s]Calculating Metrics: 100%|██████████| 1/1 [00:00<00:00, 1633.93it/s]Calculating Metrics: 100%|██████████| 1/1 [00:00<00:00, 1434.93it/s]Calculating Metrics: 100%|██████████| 1/1 [00:00<00:00, 1264.49it/s]
_ test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_of_type:negative_test_sqlalchemy_floats_are_not_boolean] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/1 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/1 [00:00<?, ?it/s]Calculating Metrics: 100%|██████████| 1/1 [00:00<00:00, 2434.30it/s]Calculating Metrics: 100%|██████████| 1/1 [00:00<00:00, 1831.57it/s]Calculating Metrics: 100%|██████████| 1/1 [00:00<00:00, 1608.25it/s]Calculating Metrics: 100%|██████████| 1/1 [00:00<00:00, 1408.90it/s]
_ test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_in_set:basic_positive_test_case_number_set] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 2981.03it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 314.12it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 527.68it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 307.78it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 425.14it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 320.25it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 485.35it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 437.25it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 477.77it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 448.44it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 447.19it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 445.72it/s]
_ test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_in_set:vacuously_true_empty_value_set] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 3164.32it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 300.30it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 500.44it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 289.27it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 402.55it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 304.65it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 468.42it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 418.56it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 458.46it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 426.01it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 424.34it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 422.82it/s]
_ test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_in_set:negative_test_case_exclude_existing_column_value] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 2916.76it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 325.63it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 542.09it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 311.44it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 429.19it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 315.36it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 486.56it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 436.88it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 475.29it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 446.19it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 444.84it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 443.34it/s]
_ test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_in_set:negative_empty_values_set] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 3278.08it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 331.57it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 546.52it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 314.56it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 434.56it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 328.71it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 513.18it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 460.21it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 502.71it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 470.83it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 469.45it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 467.83it/s]
_ test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_in_set:basic_positive_strings_set] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 3068.25it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 311.29it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 520.71it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 307.30it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 425.03it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 321.48it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 492.24it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 443.53it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 483.96it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 452.27it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 450.73it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 448.50it/s]
_ test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_in_set:negative_test_strings_set_extra_value_in_column] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 3144.16it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 320.05it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 537.47it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 315.25it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 436.16it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 328.37it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 506.68it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 454.98it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 502.59it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 470.42it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 468.99it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 467.34it/s]
_ test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_in_set:negative_test_numbers_set_no_matching_values] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 3236.35it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 324.94it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 513.35it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 305.74it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 424.54it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 323.80it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 506.35it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 452.14it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 494.26it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 460.35it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 458.20it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 455.39it/s]
_ test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_in_set:positive_test_float_set] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 3231.36it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 299.18it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 507.55it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 281.10it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 388.45it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 289.74it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 444.25it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 400.63it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 439.86it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 411.43it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 410.17it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 408.83it/s]
_ test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_in_set:negative_test_float_set] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 3100.00it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 283.28it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 478.35it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 285.40it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 398.09it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 301.48it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 472.51it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 423.09it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 463.46it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 432.88it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 431.62it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 430.19it/s]
_ test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_in_set:test_empty_column_should_be_vacuously_true] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 3049.29it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 305.24it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 500.32it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 298.51it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 414.22it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 317.51it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 484.01it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 433.97it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 474.61it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 445.74it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 444.50it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 443.01it/s]
_ test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_not_match_like_pattern_list:basic_positive_test] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 3039.35it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 272.63it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 462.42it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 277.24it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 384.89it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 291.91it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 453.18it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 408.56it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 449.39it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 419.77it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 418.44it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 417.07it/s]
_ test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_not_match_like_pattern_list:positive_test_with_multiple_like_patternes] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 3120.76it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 288.40it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 465.01it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 274.57it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 377.74it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 283.66it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 433.52it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 388.60it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 425.08it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 389.56it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 388.11it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 386.76it/s]
_ test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_not_match_like_pattern_list:negative_test_with_more_string-ish_strings] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 3209.11it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 288.44it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 489.53it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 288.47it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 393.13it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 294.84it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 448.59it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 404.54it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 444.25it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 416.36it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 415.24it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 413.59it/s]
_ test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_not_match_like_pattern_list:basic_negative_test] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 3034.95it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 287.31it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 474.89it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 280.03it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 384.73it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 291.44it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 446.81it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 402.76it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 440.83it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 412.98it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 411.91it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 410.47it/s]
_ test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_not_match_like_pattern:negative_test_insufficient_mostly_and_one_non_matching_value] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 2985.27it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 338.41it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 562.77it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 335.80it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 464.37it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 353.11it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 535.10it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 480.67it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 518.99it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 485.53it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 483.99it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 482.18it/s]
_ test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_not_match_like_pattern:positive_test_exact_mostly_w_one_non_matching_value] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 3042.66it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 335.91it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 525.26it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 322.16it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 445.43it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 334.58it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 521.67it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 468.90it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 508.06it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 475.16it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 473.50it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 471.73it/s]
_ test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_not_match_like_pattern:positive_test_sufficient_mostly_w_one_non_matching_value] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 3020.74it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 320.92it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 532.91it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 310.92it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 425.71it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 324.38it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 503.02it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 447.41it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 492.72it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 464.34it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 462.96it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 461.36it/s]
_ test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_not_match_like_pattern:negative_test_one_missing_value_and_insufficent_mostly] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 3322.22it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 347.33it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 555.17it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 330.55it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 455.23it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 345.06it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 525.89it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 474.24it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 518.69it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 487.20it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 485.75it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 483.99it/s]
_ test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_not_match_like_pattern:positive_test_one_missing_value_no_exceptions] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 2828.26it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 328.89it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 537.70it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 321.72it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 443.98it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 337.96it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 527.60it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 475.24it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 524.05it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 490.76it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 488.98it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 487.05it/s]
_ test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_not_match_like_pattern:positive_test_all_missing_values] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 3007.75it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 347.83it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 579.54it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 342.02it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 470.51it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 353.15it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 534.91it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 480.54it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 523.45it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 489.53it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 487.90it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 486.01it/s]
_ test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_not_match_like_pattern:positive_test_all_missing_values_mostly] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 3043.76it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 328.57it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 541.34it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 325.78it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 449.94it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 339.52it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 524.44it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 472.81it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 516.74it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 485.49it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 483.99it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 482.23it/s]
_ test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_not_match_like_pattern:negative_test_match_characters_not_at_the_beginning_of_string_exact_mostly] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 3048.19it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 336.58it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 559.28it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 332.00it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 458.23it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 344.77it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 537.74it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 485.09it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 533.70it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 499.31it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 497.47it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 495.56it/s]
_ test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_match_like_pattern:negative_test_insufficient_mostly_and_one_non_matching_value] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 3020.74it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 343.22it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 565.77it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 323.55it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 439.72it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 337.66it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 514.14it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 463.00it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 504.70it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 471.70it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 470.18it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 468.47it/s]
_ test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_match_like_pattern:positive_test_exact_mostly_w_one_non_matching_value] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 3025.10it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 340.43it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 556.70it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 326.79it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 451.02it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 337.80it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 523.51it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 468.88it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 515.77it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 482.21it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 480.64it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 478.84it/s]
_ test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_match_like_pattern:positive_test_column_name_has_space] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 3155.98it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 338.41it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 561.56it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 329.95it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 455.86it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 343.60it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 526.62it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 475.58it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 520.07it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 487.51it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 485.97it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 484.12it/s]
_ test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_match_like_pattern:positive_test_sufficient_mostly_w_one_non_matching_value] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 3007.75it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 313.72it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 524.30it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 322.87it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 447.76it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 341.84it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 532.74it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 478.49it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 524.11it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 490.77it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 489.14it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 487.29it/s]
_ test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_match_like_pattern:negative_test_one_missing_value_and_insufficent_mostly] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 3148.88it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 339.26it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 529.94it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 324.65it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 450.14it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 344.31it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 522.89it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 468.62it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 508.66it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 476.38it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 474.61it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 472.79it/s]
_ test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_match_like_pattern:positive_test_one_missing_value_and_exact_mostly] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 3057.07it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 335.96it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 555.79it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 324.65it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 438.98it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 319.79it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 492.94it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 445.05it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 486.64it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 457.41it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 455.88it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 454.21it/s]
_ test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_match_like_pattern:positive_test_one_missing_value_and_sufficent_mostly] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 3175.10it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 339.56it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 559.76it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 324.30it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 445.63it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 341.65it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 534.09it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 480.16it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 526.42it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 492.82it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 491.27it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 489.44it/s]
_ test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_match_like_pattern:positive_test_all_missing_values] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 3052.62it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 342.52it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 581.13it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 342.61it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 472.77it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 356.14it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 535.62it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 481.14it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 523.78it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 491.03it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 489.47it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 487.66it/s]
_ test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_match_like_pattern:positive_test_all_missing_values_mostly] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 2914.74it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 337.37it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 557.18it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 335.04it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 464.07it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 350.71it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 546.39it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 491.73it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 534.27it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 500.87it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 499.30it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 497.44it/s]
_ test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_match_like_pattern:positive_test_match_characters_not_at_the_beginning_of_string] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 3225.15it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 355.30it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 593.46it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 337.90it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 462.82it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 352.01it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 549.09it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 493.20it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 532.37it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 497.60it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 495.71it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 493.77it/s]
_ test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_value_lengths_to_be_between:positive_test_exact_min_and_max] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/14 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/14 [00:00<?, ?it/s]Calculating Metrics:  14%|█▍        | 2/14 [00:00<00:00, 3033.85it/s]Calculating Metrics:  14%|█▍        | 2/14 [00:00<00:00, 263.08it/s]Calculating Metrics:  29%|██▊       | 4/14 [00:00<00:00, 455.06it/s]Calculating Metrics:  29%|██▊       | 4/14 [00:00<00:00, 253.70it/s]Calculating Metrics:  43%|████▎     | 6/14 [00:00<00:00, 349.44it/s]Calculating Metrics:  43%|████▎     | 6/14 [00:00<00:00, 254.14it/s]Calculating Metrics:  57%|█████▋    | 8/14 [00:00<00:00, 319.79it/s]Calculating Metrics:  57%|█████▋    | 8/14 [00:00<00:00, 266.16it/s]Calculating Metrics:  93%|█████████▎| 13/14 [00:00<00:00, 369.79it/s]Calculating Metrics:  93%|█████████▎| 13/14 [00:00<00:00, 339.71it/s]Calculating Metrics: 100%|██████████| 14/14 [00:00<00:00, 349.00it/s]Calculating Metrics: 100%|██████████| 14/14 [00:00<00:00, 330.28it/s]Calculating Metrics: 100%|██████████| 14/14 [00:00<00:00, 329.57it/s]Calculating Metrics: 100%|██████████| 14/14 [00:00<00:00, 328.81it/s]
_ test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_value_lengths_to_be_between:negative_test_matching_min_max_too_small] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/14 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/14 [00:00<?, ?it/s]Calculating Metrics:  14%|█▍        | 2/14 [00:00<00:00, 3130.08it/s]Calculating Metrics:  14%|█▍        | 2/14 [00:00<00:00, 257.60it/s]Calculating Metrics:  29%|██▊       | 4/14 [00:00<00:00, 451.27it/s]Calculating Metrics:  29%|██▊       | 4/14 [00:00<00:00, 251.57it/s]Calculating Metrics:  43%|████▎     | 6/14 [00:00<00:00, 345.77it/s]Calculating Metrics:  43%|████▎     | 6/14 [00:00<00:00, 253.57it/s]Calculating Metrics:  57%|█████▋    | 8/14 [00:00<00:00, 319.54it/s]Calculating Metrics:  57%|█████▋    | 8/14 [00:00<00:00, 262.95it/s]Calculating Metrics:  93%|█████████▎| 13/14 [00:00<00:00, 365.75it/s]Calculating Metrics:  93%|█████████▎| 13/14 [00:00<00:00, 339.19it/s]Calculating Metrics: 100%|██████████| 14/14 [00:00<00:00, 350.33it/s]Calculating Metrics: 100%|██████████| 14/14 [00:00<00:00, 330.54it/s]Calculating Metrics: 100%|██████████| 14/14 [00:00<00:00, 329.84it/s]Calculating Metrics: 100%|██████████| 14/14 [00:00<00:00, 329.01it/s]
_ test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_value_lengths_to_be_between:negative_test_matching_max_min_too_large] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/14 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/14 [00:00<?, ?it/s]Calculating Metrics:  14%|█▍        | 2/14 [00:00<00:00, 2892.62it/s]Calculating Metrics:  14%|█▍        | 2/14 [00:00<00:00, 248.47it/s]Calculating Metrics:  29%|██▊       | 4/14 [00:00<00:00, 437.15it/s]Calculating Metrics:  29%|██▊       | 4/14 [00:00<00:00, 248.72it/s]Calculating Metrics:  43%|████▎     | 6/14 [00:00<00:00, 343.78it/s]Calculating Metrics:  43%|████▎     | 6/14 [00:00<00:00, 254.27it/s]Calculating Metrics:  57%|█████▋    | 8/14 [00:00<00:00, 320.41it/s]Calculating Metrics:  57%|█████▋    | 8/14 [00:00<00:00, 263.81it/s]Calculating Metrics:  93%|█████████▎| 13/14 [00:00<00:00, 378.32it/s]Calculating Metrics:  93%|█████████▎| 13/14 [00:00<00:00, 348.47it/s]Calculating Metrics: 100%|██████████| 14/14 [00:00<00:00, 360.92it/s]Calculating Metrics: 100%|██████████| 14/14 [00:00<00:00, 338.00it/s]Calculating Metrics: 100%|██████████| 14/14 [00:00<00:00, 336.93it/s]Calculating Metrics: 100%|██████████| 14/14 [00:00<00:00, 335.98it/s]
_ test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_value_lengths_to_be_between:positive_test_with_null_min_value] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/14 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/14 [00:00<?, ?it/s]Calculating Metrics:  14%|█▍        | 2/14 [00:00<00:00, 2985.27it/s]Calculating Metrics:  14%|█▍        | 2/14 [00:00<00:00, 226.01it/s]Calculating Metrics:  29%|██▊       | 4/14 [00:00<00:00, 401.89it/s]Calculating Metrics:  29%|██▊       | 4/14 [00:00<00:00, 223.59it/s]Calculating Metrics:  43%|████▎     | 6/14 [00:00<00:00, 308.58it/s]Calculating Metrics:  43%|████▎     | 6/14 [00:00<00:00, 230.81it/s]Calculating Metrics:  57%|█████▋    | 8/14 [00:00<00:00, 293.23it/s]Calculating Metrics:  57%|█████▋    | 8/14 [00:00<00:00, 242.36it/s]Calculating Metrics:  93%|█████████▎| 13/14 [00:00<00:00, 342.70it/s]Calculating Metrics:  93%|█████████▎| 13/14 [00:00<00:00, 317.79it/s]Calculating Metrics: 100%|██████████| 14/14 [00:00<00:00, 329.79it/s]Calculating Metrics: 100%|██████████| 14/14 [00:00<00:00, 310.43it/s]Calculating Metrics: 100%|██████████| 14/14 [00:00<00:00, 309.71it/s]Calculating Metrics: 100%|██████████| 14/14 [00:00<00:00, 308.98it/s]
_ test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_value_lengths_to_be_between:positive_test_with_null_max_value] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/14 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/14 [00:00<?, ?it/s]Calculating Metrics:  14%|█▍        | 2/14 [00:00<00:00, 3058.19it/s]Calculating Metrics:  14%|█▍        | 2/14 [00:00<00:00, 236.10it/s]Calculating Metrics:  29%|██▊       | 4/14 [00:00<00:00, 414.89it/s]Calculating Metrics:  29%|██▊       | 4/14 [00:00<00:00, 223.32it/s]Calculating Metrics:  43%|████▎     | 6/14 [00:00<00:00, 308.29it/s]Calculating Metrics:  43%|████▎     | 6/14 [00:00<00:00, 224.74it/s]Calculating Metrics:  57%|█████▋    | 8/14 [00:00<00:00, 284.28it/s]Calculating Metrics:  57%|█████▋    | 8/14 [00:00<00:00, 233.67it/s]Calculating Metrics:  93%|█████████▎| 13/14 [00:00<00:00, 330.52it/s]Calculating Metrics:  93%|█████████▎| 13/14 [00:00<00:00, 305.36it/s]Calculating Metrics: 100%|██████████| 14/14 [00:00<00:00, 316.16it/s]Calculating Metrics: 100%|██████████| 14/14 [00:00<00:00, 297.86it/s]Calculating Metrics: 100%|██████████| 14/14 [00:00<00:00, 297.15it/s]Calculating Metrics: 100%|██████████| 14/14 [00:00<00:00, 296.43it/s]
_ test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_value_lengths_to_be_between:negative_test_with_max_lt_min] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/14 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/14 [00:00<?, ?it/s]Calculating Metrics:  14%|█▍        | 2/14 [00:00<00:00, 3017.48it/s]Calculating Metrics:  14%|█▍        | 2/14 [00:00<00:00, 258.76it/s]Calculating Metrics:  29%|██▊       | 4/14 [00:00<00:00, 453.51it/s]Calculating Metrics:  29%|██▊       | 4/14 [00:00<00:00, 254.50it/s]Calculating Metrics:  43%|████▎     | 6/14 [00:00<00:00, 348.48it/s]Calculating Metrics:  43%|████▎     | 6/14 [00:00<00:00, 247.23it/s]Calculating Metrics:  57%|█████▋    | 8/14 [00:00<00:00, 310.93it/s]Calculating Metrics:  57%|█████▋    | 8/14 [00:00<00:00, 257.43it/s]Calculating Metrics:  93%|█████████▎| 13/14 [00:00<00:00, 366.19it/s]Calculating Metrics:  93%|█████████▎| 13/14 [00:00<00:00, 338.87it/s]Calculating Metrics: 100%|██████████| 14/14 [00:00<00:00, 352.38it/s]Calculating Metrics: 100%|██████████| 14/14 [00:00<00:00, 333.20it/s]Calculating Metrics: 100%|██████████| 14/14 [00:00<00:00, 332.53it/s]Calculating Metrics: 100%|██████████| 14/14 [00:00<00:00, 331.61it/s]
_ test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_value_lengths_to_be_between:positive_test_with_missing_value_in_column] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/14 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/14 [00:00<?, ?it/s]Calculating Metrics:  14%|█▍        | 2/14 [00:00<00:00, 2743.17it/s]Calculating Metrics:  14%|█▍        | 2/14 [00:00<00:00, 245.00it/s]Calculating Metrics:  29%|██▊       | 4/14 [00:00<00:00, 432.66it/s]Calculating Metrics:  29%|██▊       | 4/14 [00:00<00:00, 245.99it/s]Calculating Metrics:  43%|████▎     | 6/14 [00:00<00:00, 340.04it/s]Calculating Metrics:  43%|████▎     | 6/14 [00:00<00:00, 253.07it/s]Calculating Metrics:  57%|█████▋    | 8/14 [00:00<00:00, 316.87it/s]Calculating Metrics:  57%|█████▋    | 8/14 [00:00<00:00, 252.82it/s]Calculating Metrics:  93%|█████████▎| 13/14 [00:00<00:00, 350.29it/s]Calculating Metrics:  93%|█████████▎| 13/14 [00:00<00:00, 324.08it/s]Calculating Metrics: 100%|██████████| 14/14 [00:00<00:00, 332.71it/s]Calculating Metrics: 100%|██████████| 14/14 [00:00<00:00, 315.09it/s]Calculating Metrics: 100%|██████████| 14/14 [00:00<00:00, 314.34it/s]Calculating Metrics: 100%|██████████| 14/14 [00:00<00:00, 313.59it/s]
_ test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_value_lengths_to_be_between:test_error_handling_values_are_integers] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/14 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/14 [00:00<?, ?it/s]Calculating Metrics:  14%|█▍        | 2/14 [00:00<00:00, 3020.74it/s]Calculating Metrics:  14%|█▍        | 2/14 [00:00<00:00, 257.26it/s]Calculating Metrics:  29%|██▊       | 4/14 [00:00<00:00, 453.00it/s]Calculating Metrics:  29%|██▊       | 4/14 [00:00<00:00, 251.81it/s]Calculating Metrics:  43%|████▎     | 6/14 [00:00<00:00, 347.24it/s]Calculating Metrics:  43%|████▎     | 6/14 [00:00<00:00, 254.29it/s]Calculating Metrics:  57%|█████▋    | 8/14 [00:00<00:00, 321.66it/s]Calculating Metrics:  57%|█████▋    | 8/14 [00:00<00:00, 265.57it/s]Calculating Metrics:  93%|█████████▎| 13/14 [00:00<00:00, 369.28it/s]Calculating Metrics:  93%|█████████▎| 13/14 [00:00<00:00, 340.58it/s]Calculating Metrics: 100%|██████████| 14/14 [00:00<00:00, 347.12it/s]Calculating Metrics: 100%|██████████| 14/14 [00:00<00:00, 325.57it/s]Calculating Metrics: 100%|██████████| 14/14 [00:00<00:00, 324.37it/s]Calculating Metrics: 100%|██████████| 14/14 [00:00<00:00, 323.27it/s]
_ test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_match_like_pattern_list:basic_negative_test] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 2785.06it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 298.33it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 502.51it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 298.86it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 408.58it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 310.97it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 470.63it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 424.15it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 458.32it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 421.29it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 419.23it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 417.40it/s]
_ test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_match_like_pattern_list:negative_test_with_more_string-ish_strings] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 2971.52it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 293.38it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 495.05it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 291.82it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 402.98it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 304.37it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 464.39it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 416.76it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 445.16it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 411.85it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 410.07it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 408.45it/s]
_ test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_match_like_pattern_list:basic_positive_test] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 2785.99it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 268.14it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 452.85it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 258.23it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 349.15it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 269.28it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 409.27it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 373.07it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 407.91it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 385.21it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 384.16it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 383.00it/s]
_ test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_match_like_pattern_list:positive_test_with_multiple_like_patternes] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 1981.72it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 290.14it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 484.85it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 287.89it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 392.00it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 295.65it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 457.55it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 411.28it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 442.63it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 416.84it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 415.73it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 414.41it/s]
_ test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_match_like_pattern_list:positive_test_with_match_on__any] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 2940.28it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 308.73it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 514.73it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 304.00it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 416.29it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 314.50it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 479.03it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 424.79it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 457.86it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 415.22it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 413.29it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 411.25it/s]
_ test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_match_like_pattern_list:positive_test_column_name_has_space_and_match_on__any] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/13 [00:00<?, ?it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 2983.15it/s]Calculating Metrics:  15%|█▌        | 2/13 [00:00<00:00, 312.54it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 518.92it/s]Calculating Metrics:  31%|███       | 4/13 [00:00<00:00, 300.73it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 410.87it/s]Calculating Metrics:  46%|████▌     | 6/13 [00:00<00:00, 297.69it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 453.32it/s]Calculating Metrics:  85%|████████▍ | 11/13 [00:00<00:00, 409.27it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 445.75it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 416.62it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 415.11it/s]Calculating Metrics: 100%|██████████| 13/13 [00:00<00:00, 413.60it/s]
_ test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_unique:basic_positive_test_case] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/12 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/12 [00:00<?, ?it/s]Calculating Metrics:  17%|█▋        | 2/12 [00:00<00:00, 2317.94it/s]Calculating Metrics:  17%|█▋        | 2/12 [00:00<00:00, 307.88it/s]Calculating Metrics:  33%|███▎      | 4/12 [00:00<00:00, 453.01it/s]Calculating Metrics:  33%|███▎      | 4/12 [00:00<00:00, 310.13it/s]Calculating Metrics:  50%|█████     | 6/12 [00:00<00:00, 423.23it/s]Calculating Metrics:  50%|█████     | 6/12 [00:00<00:00, 332.39it/s]Calculating Metrics:  92%|█████████▏| 11/12 [00:00<00:00, 415.38it/s]Calculating Metrics:  92%|█████████▏| 11/12 [00:00<00:00, 384.91it/s]Calculating Metrics: 100%|██████████| 12/12 [00:00<00:00, 398.93it/s]Calculating Metrics: 100%|██████████| 12/12 [00:00<00:00, 377.83it/s]Calculating Metrics: 100%|██████████| 12/12 [00:00<00:00, 376.59it/s]Calculating Metrics: 100%|██████████| 12/12 [00:00<00:00, 375.17it/s]
_ test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_unique:basic_negative_case_all_non_unique_character_values] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/12 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/12 [00:00<?, ?it/s]Calculating Metrics:  17%|█▋        | 2/12 [00:00<00:00, 2070.24it/s]Calculating Metrics:  17%|█▋        | 2/12 [00:00<00:00, 361.45it/s]Calculating Metrics:  33%|███▎      | 4/12 [00:00<00:00, 601.68it/s]Calculating Metrics:  33%|███▎      | 4/12 [00:00<00:00, 334.64it/s]Calculating Metrics:  50%|█████     | 6/12 [00:00<00:00, 448.80it/s]Calculating Metrics:  50%|█████     | 6/12 [00:00<00:00, 350.22it/s]Calculating Metrics:  92%|█████████▏| 11/12 [00:00<00:00, 492.32it/s]Calculating Metrics:  92%|█████████▏| 11/12 [00:00<00:00, 457.83it/s]Calculating Metrics: 100%|██████████| 12/12 [00:00<00:00, 474.44it/s]Calculating Metrics: 100%|██████████| 12/12 [00:00<00:00, 450.35it/s]Calculating Metrics: 100%|██████████| 12/12 [00:00<00:00, 448.88it/s]Calculating Metrics: 100%|██████████| 12/12 [00:00<00:00, 447.24it/s]
_ test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_unique:positive_case_using_mostly] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/12 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/12 [00:00<?, ?it/s]Calculating Metrics:  17%|█▋        | 2/12 [00:00<00:00, 3077.26it/s]Calculating Metrics:  17%|█▋        | 2/12 [00:00<00:00, 399.51it/s]Calculating Metrics:  33%|███▎      | 4/12 [00:00<00:00, 660.08it/s]Calculating Metrics:  33%|███▎      | 4/12 [00:00<00:00, 400.01it/s]Calculating Metrics:  50%|█████     | 6/12 [00:00<00:00, 540.11it/s]Calculating Metrics:  50%|█████     | 6/12 [00:00<00:00, 414.54it/s]Calculating Metrics:  92%|█████████▏| 11/12 [00:00<00:00, 559.63it/s]Calculating Metrics:  92%|█████████▏| 11/12 [00:00<00:00, 515.37it/s]Calculating Metrics: 100%|██████████| 12/12 [00:00<00:00, 526.90it/s]Calculating Metrics: 100%|██████████| 12/12 [00:00<00:00, 490.98it/s]Calculating Metrics: 100%|██████████| 12/12 [00:00<00:00, 488.76it/s]Calculating Metrics: 100%|██████████| 12/12 [00:00<00:00, 486.68it/s]
_ test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_unique:negative_case_using_mostly] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/12 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/12 [00:00<?, ?it/s]Calculating Metrics:  17%|█▋        | 2/12 [00:00<00:00, 2758.50it/s]Calculating Metrics:  17%|█▋        | 2/12 [00:00<00:00, 383.36it/s]Calculating Metrics:  33%|███▎      | 4/12 [00:00<00:00, 604.67it/s]Calculating Metrics:  33%|███▎      | 4/12 [00:00<00:00, 368.80it/s]Calculating Metrics:  50%|█████     | 6/12 [00:00<00:00, 495.42it/s]Calculating Metrics:  50%|█████     | 6/12 [00:00<00:00, 382.78it/s]Calculating Metrics:  92%|█████████▏| 11/12 [00:00<00:00, 557.13it/s]Calculating Metrics:  92%|█████████▏| 11/12 [00:00<00:00, 505.05it/s]Calculating Metrics: 100%|██████████| 12/12 [00:00<00:00, 520.56it/s]Calculating Metrics: 100%|██████████| 12/12 [00:00<00:00, 490.39it/s]Calculating Metrics: 100%|██████████| 12/12 [00:00<00:00, 488.52it/s]Calculating Metrics: 100%|██████████| 12/12 [00:00<00:00, 486.48it/s]
_ test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_unique:positive_case_multiple_null_values] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/12 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/12 [00:00<?, ?it/s]Calculating Metrics:  17%|█▋        | 2/12 [00:00<00:00, 2759.41it/s]Calculating Metrics:  17%|█▋        | 2/12 [00:00<00:00, 374.83it/s]Calculating Metrics:  33%|███▎      | 4/12 [00:00<00:00, 622.69it/s]Calculating Metrics:  33%|███▎      | 4/12 [00:00<00:00, 373.08it/s]Calculating Metrics:  50%|█████     | 6/12 [00:00<00:00, 504.55it/s]Calculating Metrics:  50%|█████     | 6/12 [00:00<00:00, 393.24it/s]Calculating Metrics:  92%|█████████▏| 11/12 [00:00<00:00, 511.68it/s]Calculating Metrics:  92%|█████████▏| 11/12 [00:00<00:00, 472.41it/s]Calculating Metrics: 100%|██████████| 12/12 [00:00<00:00, 486.36it/s]Calculating Metrics: 100%|██████████| 12/12 [00:00<00:00, 459.70it/s]Calculating Metrics: 100%|██████████| 12/12 [00:00<00:00, 458.00it/s]Calculating Metrics: 100%|██████████| 12/12 [00:00<00:00, 456.19it/s]
_ test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_unique:negative_case_non_unique_numeric_values] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/12 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/12 [00:00<?, ?it/s]Calculating Metrics:  17%|█▋        | 2/12 [00:00<00:00, 2773.09it/s]Calculating Metrics:  17%|█▋        | 2/12 [00:00<00:00, 370.57it/s]Calculating Metrics:  33%|███▎      | 4/12 [00:00<00:00, 621.49it/s]Calculating Metrics:  33%|███▎      | 4/12 [00:00<00:00, 366.16it/s]Calculating Metrics:  50%|█████     | 6/12 [00:00<00:00, 492.97it/s]Calculating Metrics:  50%|█████     | 6/12 [00:00<00:00, 379.94it/s]Calculating Metrics:  92%|█████████▏| 11/12 [00:00<00:00, 531.04it/s]Calculating Metrics:  92%|█████████▏| 11/12 [00:00<00:00, 487.05it/s]Calculating Metrics: 100%|██████████| 12/12 [00:00<00:00, 505.07it/s]Calculating Metrics: 100%|██████████| 12/12 [00:00<00:00, 476.24it/s]Calculating Metrics: 100%|██████████| 12/12 [00:00<00:00, 474.51it/s]Calculating Metrics: 100%|██████████| 12/12 [00:00<00:00, 472.51it/s]
_ test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_unique:positive_case_all_null_values] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/12 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/12 [00:00<?, ?it/s]Calculating Metrics:  17%|█▋        | 2/12 [00:00<00:00, 2915.75it/s]Calculating Metrics:  17%|█▋        | 2/12 [00:00<00:00, 406.09it/s]Calculating Metrics:  33%|███▎      | 4/12 [00:00<00:00, 656.28it/s]Calculating Metrics:  33%|███▎      | 4/12 [00:00<00:00, 389.07it/s]Calculating Metrics:  50%|█████     | 6/12 [00:00<00:00, 519.12it/s]Calculating Metrics:  50%|█████     | 6/12 [00:00<00:00, 379.56it/s]Calculating Metrics:  92%|█████████▏| 11/12 [00:00<00:00, 499.12it/s]Calculating Metrics:  92%|█████████▏| 11/12 [00:00<00:00, 462.22it/s]Calculating Metrics: 100%|██████████| 12/12 [00:00<00:00, 477.56it/s]Calculating Metrics: 100%|██████████| 12/12 [00:00<00:00, 451.98it/s]Calculating Metrics: 100%|██████████| 12/12 [00:00<00:00, 450.48it/s]Calculating Metrics: 100%|██████████| 12/12 [00:00<00:00, 448.77it/s]
_ test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_unique:negative_multiple_duplicate_values] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/12 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/12 [00:00<?, ?it/s]Calculating Metrics:  17%|█▋        | 2/12 [00:00<00:00, 2687.79it/s]Calculating Metrics:  17%|█▋        | 2/12 [00:00<00:00, 359.46it/s]Calculating Metrics:  33%|███▎      | 4/12 [00:00<00:00, 567.37it/s]Calculating Metrics:  33%|███▎      | 4/12 [00:00<00:00, 354.23it/s]Calculating Metrics:  50%|█████     | 6/12 [00:00<00:00, 476.75it/s]Calculating Metrics:  50%|█████     | 6/12 [00:00<00:00, 372.44it/s]Calculating Metrics:  92%|█████████▏| 11/12 [00:00<00:00, 507.08it/s]Calculating Metrics:  92%|█████████▏| 11/12 [00:00<00:00, 470.21it/s]Calculating Metrics: 100%|██████████| 12/12 [00:00<00:00, 486.94it/s]Calculating Metrics: 100%|██████████| 12/12 [00:00<00:00, 459.98it/s]Calculating Metrics: 100%|██████████| 12/12 [00:00<00:00, 458.40it/s]Calculating Metrics: 100%|██████████| 12/12 [00:00<00:00, 456.57it/s]
_ test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_null:basic_positive_case_basic_result_format] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/8 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/8 [00:00<?, ?it/s]Calculating Metrics:  25%|██▌       | 2/8 [00:00<00:00, 2028.68it/s]Calculating Metrics:  25%|██▌       | 2/8 [00:00<00:00, 606.77it/s]Calculating Metrics:  50%|█████     | 4/8 [00:00<00:00, 895.26it/s]Calculating Metrics:  50%|█████     | 4/8 [00:00<00:00, 561.32it/s]Calculating Metrics:  62%|██████▎   | 5/8 [00:00<00:00, 634.35it/s]Calculating Metrics:  62%|██████▎   | 5/8 [00:00<00:00, 512.45it/s]Calculating Metrics:  88%|████████▊ | 7/8 [00:00<00:00, 599.66it/s]Calculating Metrics:  88%|████████▊ | 7/8 [00:00<00:00, 544.24it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 560.92it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 530.43it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 527.35it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 523.94it/s]
_ test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_null:negative_no_missing_values] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 2963.13it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 496.54it/s]Calculating Metrics:  40%|████      | 4/10 [00:00<00:00, 804.16it/s]Calculating Metrics:  40%|████      | 4/10 [00:00<00:00, 489.47it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 567.41it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 425.71it/s]Calculating Metrics:  90%|█████████ | 9/10 [00:00<00:00, 601.98it/s]Calculating Metrics:  90%|█████████ | 9/10 [00:00<00:00, 549.55it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 567.57it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 532.25it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 529.69it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 526.84it/s]
_ test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_null:positive_mostly] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 3134.76it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 507.88it/s]Calculating Metrics:  40%|████      | 4/10 [00:00<00:00, 819.52it/s]Calculating Metrics:  40%|████      | 4/10 [00:00<00:00, 501.58it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 586.17it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 435.08it/s]Calculating Metrics:  90%|█████████ | 9/10 [00:00<00:00, 624.32it/s]Calculating Metrics:  90%|█████████ | 9/10 [00:00<00:00, 569.44it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 589.52it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 554.17it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 551.64it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 548.73it/s]
_ test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_null:negative_mostly] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 2919.81it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 430.30it/s]Calculating Metrics:  40%|████      | 4/10 [00:00<00:00, 710.24it/s]Calculating Metrics:  40%|████      | 4/10 [00:00<00:00, 454.77it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 531.29it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 412.48it/s]Calculating Metrics:  90%|█████████ | 9/10 [00:00<00:00, 615.47it/s]Calculating Metrics:  90%|█████████ | 9/10 [00:00<00:00, 559.88it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 575.71it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 539.23it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 536.66it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 533.68it/s]
_ test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_null:positive_case_with_mostly_and_no_unexpected_values] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 3006.67it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 474.25it/s]Calculating Metrics:  40%|████      | 4/10 [00:00<00:00, 765.10it/s]Calculating Metrics:  40%|████      | 4/10 [00:00<00:00, 462.31it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 539.71it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 409.77it/s]Calculating Metrics:  90%|█████████ | 9/10 [00:00<00:00, 596.34it/s]Calculating Metrics:  90%|█████████ | 9/10 [00:00<00:00, 542.06it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 533.50it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 503.75it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 501.60it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 499.15it/s]
_ test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_null:negative_case_with_75percent_non_null_values_no_mostly] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 2778.60it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 449.36it/s]Calculating Metrics:  40%|████      | 4/10 [00:00<00:00, 721.79it/s]Calculating Metrics:  40%|████      | 4/10 [00:00<00:00, 458.31it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 538.21it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 412.89it/s]Calculating Metrics:  90%|█████████ | 9/10 [00:00<00:00, 584.57it/s]Calculating Metrics:  90%|█████████ | 9/10 [00:00<00:00, 535.65it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 556.64it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 524.80it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 522.58it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 519.91it/s]
_ test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_in_type_list:positive_test_sqlalchemy_integer_column] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/1 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/1 [00:00<?, ?it/s]Calculating Metrics: 100%|██████████| 1/1 [00:00<00:00, 2032.12it/s]Calculating Metrics: 100%|██████████| 1/1 [00:00<00:00, 1574.44it/s]Calculating Metrics: 100%|██████████| 1/1 [00:00<00:00, 1402.78it/s]Calculating Metrics: 100%|██████████| 1/1 [00:00<00:00, 1243.49it/s]
_ test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_in_type_list:negative_test_sqlalchemy_float_values_are_not_text] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/1 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/1 [00:00<?, ?it/s]Calculating Metrics: 100%|██████████| 1/1 [00:00<00:00, 2293.22it/s]Calculating Metrics: 100%|██████████| 1/1 [00:00<00:00, 1738.93it/s]Calculating Metrics: 100%|██████████| 1/1 [00:00<00:00, 1533.57it/s]Calculating Metrics: 100%|██████████| 1/1 [00:00<00:00, 1347.78it/s]
_ test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_in_type_list:positive_test_float_values] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/1 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/1 [00:00<?, ?it/s]Calculating Metrics: 100%|██████████| 1/1 [00:00<00:00, 2495.12it/s]Calculating Metrics: 100%|██████████| 1/1 [00:00<00:00, 1863.31it/s]Calculating Metrics: 100%|██████████| 1/1 [00:00<00:00, 1629.49it/s]Calculating Metrics: 100%|██████████| 1/1 [00:00<00:00, 1422.76it/s]
_ test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_in_type_list:positive_test_text_values] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/1 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/1 [00:00<?, ?it/s]Calculating Metrics: 100%|██████████| 1/1 [00:00<00:00, 2252.58it/s]Calculating Metrics: 100%|██████████| 1/1 [00:00<00:00, 1700.16it/s]Calculating Metrics: 100%|██████████| 1/1 [00:00<00:00, 1503.33it/s]Calculating Metrics: 100%|██████████| 1/1 [00:00<00:00, 1321.04it/s]
_ test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_in_type_list:positive_test_boolean_values] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/1 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/1 [00:00<?, ?it/s]Calculating Metrics: 100%|██████████| 1/1 [00:00<00:00, 2100.30it/s]Calculating Metrics: 100%|██████████| 1/1 [00:00<00:00, 1622.55it/s]Calculating Metrics: 100%|██████████| 1/1 [00:00<00:00, 1442.33it/s]Calculating Metrics: 100%|██████████| 1/1 [00:00<00:00, 1273.70it/s]
_ test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_in_type_list:positive_test_text_and_integer_values] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/1 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/1 [00:00<?, ?it/s]Calculating Metrics: 100%|██████████| 1/1 [00:00<00:00, 1687.17it/s]Calculating Metrics: 100%|██████████| 1/1 [00:00<00:00, 1266.01it/s]Calculating Metrics: 100%|██████████| 1/1 [00:00<00:00, 1144.42it/s]Calculating Metrics: 100%|██████████| 1/1 [00:00<00:00, 1031.05it/s]
_ test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_in_type_list:positive_test_placeholder_expectation] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/1 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/1 [00:00<?, ?it/s]Calculating Metrics: 100%|██████████| 1/1 [00:00<00:00, 1969.16it/s]Calculating Metrics: 100%|██████████| 1/1 [00:00<00:00, 1522.43it/s]Calculating Metrics: 100%|██████████| 1/1 [00:00<00:00, 1358.70it/s]Calculating Metrics: 100%|██████████| 1/1 [00:00<00:00, 1200.43it/s]
_ test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_stdev_to_be_between:basic_positive_test] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 3603.35it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 1504.95it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 1878.61it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 1355.04it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1340.46it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1195.13it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1163.47it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1127.73it/s]
_ test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_stdev_to_be_between:vacuously_true] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 3477.86it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 1370.69it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 1692.84it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 1224.50it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1253.90it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1116.69it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1086.61it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1053.45it/s]
_ test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_stdev_to_be_between:positive_test__exact_min_and_max] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 3383.87it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 1341.32it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 1578.19it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 1142.97it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1088.72it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 980.89it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 957.39it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 931.08it/s]
_ test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_stdev_to_be_between:basic_negative_test] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 3418.34it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 1373.83it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 1684.46it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 1213.63it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1121.70it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 976.33it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 950.39it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 923.35it/s]
_ test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_stdev_to_be_between:positive_zero_stdev_exact_min_and_max] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 2658.83it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 1179.17it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 1421.16it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 1028.35it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1007.28it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 894.02it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 868.75it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 833.15it/s]
_ test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_stdev_to_be_between:positive_test_null_max_exact_min] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 3220.20it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 1285.41it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 1590.76it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 1168.33it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1182.41it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1059.03it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1032.00it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1001.57it/s]
_ test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_stdev_to_be_between:positive_test_null_min_exact_max] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 3549.98it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 1388.84it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 1711.26it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 1187.18it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1185.84it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1059.57it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1032.19it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1001.21it/s]
_ test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_stdev_to_be_between:negative_test_null_max] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 3577.23it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 1403.72it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 1730.32it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 1247.93it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1278.36it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1137.75it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1107.77it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1074.15it/s]
_ test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_stdev_to_be_between:negative_test_null_min] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 3231.36it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 1321.67it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 1623.39it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 1179.72it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1206.21it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1058.50it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1021.75it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 989.81it/s]
_ test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_stdev_to_be_between:positive_test_missing_value_in_column_complete_result_format] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 3477.86it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 1395.54it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 1725.10it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 1245.83it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1244.88it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1109.90it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1081.08it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1048.51it/s]
_ test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_distinct_values_to_be_in_set:basic_positive_test] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 3440.77it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 1171.59it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 1407.80it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 1002.94it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 540.68it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 507.80it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 501.50it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 494.19it/s]
_ test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_distinct_values_to_be_in_set:vacuously_true_universal_set] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 3591.01it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 1257.10it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 1576.41it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 1100.58it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1064.68it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 918.09it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 883.57it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 854.89it/s]
_ test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_distinct_values_to_be_in_set:positive_test_with_null_values_in_column] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 3252.66it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 1162.34it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 1455.01it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 1021.92it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 810.22it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 739.41it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 725.97it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 710.87it/s]
_ test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_distinct_values_to_be_in_set:positive_test_with_duplicate_values_in_column] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 3394.82it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 1216.45it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 1532.07it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 1079.06it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1028.02it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 913.00it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 892.26it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 869.74it/s]
_ test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_distinct_values_to_be_in_set:positive_test_duplicate_and_null_values] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 3442.19it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 1228.38it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 1542.40it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 1082.68it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1024.63it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 913.05it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 892.69it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 869.78it/s]
_ test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_distinct_values_to_be_in_set:basic_negative_test_no_set_intersection] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 3355.44it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 1088.86it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 1256.28it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 772.43it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 735.29it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 676.61it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 665.47it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 652.43it/s]
_ test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_distinct_values_to_be_in_set:negative_test_some_set_intersection_and_extra] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 3054.85it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 1074.77it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 1360.02it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 985.66it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 918.70it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 825.28it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 808.11it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 786.67it/s]
_ test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_distinct_values_to_be_in_set:negative_test_with_null_values_in_column] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 3453.52it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 1234.53it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 1509.65it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 1062.75it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1036.27it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 925.89it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 905.36it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 882.41it/s]
_ test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_distinct_values_to_be_in_set:negative_test_with_duplicate_values_in_column] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 3527.59it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 1191.90it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 1471.34it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 1041.29it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1023.81it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 915.94it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 884.55it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 860.72it/s]
_ test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_distinct_values_to_be_in_set:negative_test_duplicate_and_null_values] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 2586.68it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 854.67it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 1116.50it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 843.13it/s] Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 839.66it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 761.74it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 747.12it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 730.71it/s]
_ test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_most_common_value_to_be_in_set:negative_test_tie_for_most_common_with_missing_values_and_complete_result_format] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 3258.98it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 1345.19it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 1667.72it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 1211.18it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 660.86it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 613.99it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 603.87it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 593.13it/s]
_ test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_most_common_value_to_be_in_set:negative_test__tie_for_most_common__full_value_set__ties_okay__false] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 3241.35it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 1301.16it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 1610.30it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 1168.22it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1193.00it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1064.75it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1037.10it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1006.07it/s]
_ test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_most_common_value_to_be_in_set:positive_test__tie_for_most_common__ties_okay__true] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 3026.19it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 1266.59it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 1577.99it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 1159.61it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1190.89it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1064.95it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1038.13it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1007.82it/s]
_ test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_most_common_value_to_be_in_set:positive_test__tie_for_most_common_but_test_for_last_value__ties_okay__true] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 3148.88it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 894.78it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 1041.29it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 813.27it/s] Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 852.72it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 781.43it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 765.84it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 748.21it/s]
_ test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_most_common_value_to_be_in_set:negative_test__tie_for_most_common__value_set_does_not_match__ties_okay__false] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 3546.98it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 1372.03it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 1631.60it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 1184.94it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1185.59it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1056.57it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1028.90it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 998.82it/s]
_ test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_most_common_value_to_be_in_set:negative_test__tie_for_most_common__value_set_does_not_match__ties_okay__true] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 3095.43it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 1117.44it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 1413.18it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 1063.46it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1091.98it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 983.54it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 960.40it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 934.35it/s]
_ test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_most_common_value_to_be_in_set:positive_test_string_values_value_set_contains_more_than_actual_mode_list] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 3642.47it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 1405.36it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 1743.03it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 1261.57it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1041.67it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 949.42it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 929.33it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 907.22it/s]
_ test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_most_common_value_to_be_in_set:negative_test_string_values_value_set_contains_more_than_actual_mode_list] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 3766.78it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 1463.47it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 1788.11it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 1287.65it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1264.68it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1123.35it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1089.43it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1055.50it/s]
_ test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_quantile_values_to_be_between:basic_positive_test_extremes] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 3130.08it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 1174.55it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 1485.24it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 1045.70it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 662.16it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 612.40it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 602.87it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 592.08it/s]
_ test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_quantile_values_to_be_between:basic_positive_test_normal_quantiles] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 3527.59it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 1189.87it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 1504.59it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 1044.23it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 864.89it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 778.96it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 763.23it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 746.02it/s]
_ test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_quantile_values_to_be_between:basic_positive_test_uneven_spacing] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 3711.77it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 1183.33it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 1441.51it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 990.39it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 858.92it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 769.67it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 754.27it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 737.33it/s]
_ test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_quantile_values_to_be_between:basic_negative_test_normal_quantiles_wrong_distribution] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 2910.69it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 1062.12it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 1358.11it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 906.03it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 796.34it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 712.74it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 698.44it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 683.86it/s]
_ test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_max_to_be_between:basic_positive_test_case] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 2894.62it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 1241.47it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 1554.41it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 1144.11it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1188.69it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1061.31it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1034.67it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1004.80it/s]
_ test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_max_to_be_between:basic_negative_test_case] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 3489.44it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 1339.61it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 1656.30it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 1197.46it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1207.69it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1073.47it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1044.85it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1013.18it/s]
_ test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_max_to_be_between:test_case_with_result_format_summary_also_verifies_that_max_value_is_inclusive] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 2988.46it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 1271.39it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 1588.15it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 1163.15it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1125.16it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 999.24it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 973.72it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 928.30it/s]
_ test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_max_to_be_between:test_case_with_only_a_lower_bound_and_a_missing_value] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 3236.35it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 1200.43it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 1483.13it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 1100.58it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1038.71it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 921.93it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 897.51it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 873.04it/s]
_ test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_max_to_be_between:negative_test_case_with_only_a_lower_bound] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 3514.29it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 1379.25it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 1703.62it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 1225.93it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1288.28it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1142.16it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1111.37it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1077.26it/s]
_ test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_max_to_be_between:datetime_except_sqlite] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 3490.89it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 1379.48it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 1703.62it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 1227.12it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1233.44it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1100.43it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1072.44it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1040.38it/s]
_ test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_max_to_be_between:datetime_with_evaluation_parameter_except_sqlite] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 3514.29it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 1391.84it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 1714.29it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 1228.56it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1257.85it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1120.20it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1088.72it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1057.10it/s]
_ test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_max_to_be_between:test_empty_column_should_be_false_no_observed_value_with_which_to_compare] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 3731.59it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 1584.85it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 1966.08it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 1432.64it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1358.26it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1206.65it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1175.45it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1142.86it/s]
_ test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_mean_to_be_between:simple_mean_positive_range] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 3669.56it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 1554.02it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 1907.37it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 1331.10it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1342.82it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1196.24it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1164.84it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1129.63it/s]
_ test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_mean_to_be_between:simple_mean_negative_range] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 3703.58it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 1392.76it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 1689.21it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 1217.62it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1240.73it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1100.15it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1070.11it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1036.59it/s]
_ test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_mean_to_be_between:simple_mean_positive_exact_match0] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 3753.29it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 1459.14it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 1795.51it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 1283.18it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1320.21it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1173.81it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1141.93it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1106.82it/s]
_ test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_mean_to_be_between:simple_mean_negative_exact_match] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 2634.61it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 1189.03it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 142.46it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 137.77it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 177.68it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 95.12it/s] Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 94.53it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 94.12it/s]
_ test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_mean_to_be_between:simple_mean_positive_exact_match1] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 3025.10it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 1273.70it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 1582.96it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 1140.58it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1179.58it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1053.18it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1026.63it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 995.44it/s]
_ test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_mean_to_be_between:simple_mean_negative_range_match] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 3462.08it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 1361.57it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 1677.05it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 1200.09it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1258.42it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1121.47it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1092.05it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1058.57it/s]
_ test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_mean_to_be_between:simple_mean_includes_null] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 3723.31it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 1426.39it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 1753.23it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 1219.39it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1211.70it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1076.43it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1046.81it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1012.87it/s]
_ test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_mean_to_be_between:vacuously_true_missing_min_and_max] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 3412.78it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 1361.57it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 1679.96it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 1208.73it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1234.25it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1102.60it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1074.22it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1042.52it/s]
_ test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_mean_to_be_between:type_mismatch_null_observed_value] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 3663.15it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 1439.86it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 1765.53it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 1270.23it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 201.30it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 178.98it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 162.78it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 160.53it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 159.58it/s]
_ test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_mean_to_be_between:coerced_types_true_false_values] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 3885.41it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 1532.73it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 1864.69it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 1338.18it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1376.99it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1225.51it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1193.34it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1153.15it/s]
_ test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_mean_to_be_between:coerced_types_true_false_and_null] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 3128.91it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 1358.70it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 1702.46it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 1230.48it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1260.12it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1136.36it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1108.94it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1077.88it/s]
_ test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_mean_to_be_between:test_empty_column_should_be_false_no_observed_value_with_which_to_compare] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 3284.50it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 1326.26it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 1628.86it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 1119.87it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1149.44it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1031.24it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1005.53it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 969.50it/s]
_ test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_distinct_values_to_equal_set:basic_positive_test] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 3231.36it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 1193.43it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 1498.68it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 1067.34it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1014.22it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 907.66it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 887.92it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 864.89it/s]
_ test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_distinct_values_to_equal_set:positive_test_with_null_values_in_column] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 3718.35it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 1265.82it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 1586.75it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 1106.68it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1072.16it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 957.44it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 936.12it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 912.65it/s]
_ test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_distinct_values_to_equal_set:positive_test_with_duplicate_values_in_column] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 3429.52it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 1225.51it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 1538.44it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 1001.66it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 967.99it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 864.67it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 846.18it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 826.75it/s]
_ test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_distinct_values_to_equal_set:positive_test_duplicate_and_null_values] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 2395.38it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 1182.83it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 1541.83it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 1136.87it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1119.53it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1011.65it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 989.16it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 965.32it/s]
_ test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_distinct_values_to_equal_set:basic_negative_test_set_contained] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 3738.24it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 1368.01it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 1587.35it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 1133.80it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1111.74it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1008.85it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 989.40it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 966.65it/s]
_ test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_distinct_values_to_equal_set:negative_test_some_set_intersection] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 4375.90it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 1561.25it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 1965.77it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 1378.04it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1343.14it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1196.15it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1169.31it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1138.60it/s]
_ test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_distinct_values_to_equal_set:negative_test_with_null_values_in_column] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 4445.47it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 1516.10it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 1905.92it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 1335.06it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1286.60it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1147.71it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1122.37it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1092.91it/s]
_ test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_distinct_values_to_equal_set:negative_test_with_duplicate_values_in_column] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 4424.37it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 1570.02it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 1968.54it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 1370.54it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1323.23it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1180.25it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1153.39it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1122.97it/s]
_ test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_distinct_values_to_equal_set:negative_test_duplicate_and_null_values] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 4366.79it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 1450.31it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 1813.62it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 1143.69it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1107.04it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 998.94it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 978.43it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 954.44it/s]
_ test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_sum_to_be_between:basic_positive_test_case] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 2989.53it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 1344.97it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 1692.16it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 1285.81it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1342.28it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1202.67it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1169.55it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1136.36it/s]
_ test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_sum_to_be_between:basic_negative_test_case] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 4366.79it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 1724.28it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 2133.78it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 1536.94it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1620.05it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1440.35it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1401.49it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1357.71it/s]
_ test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_sum_to_be_between:test_case_with_result_format_summary] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 4586.45it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 1736.05it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 2138.50it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 1540.14it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1611.64it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1430.04it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1391.38it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1346.70it/s]
_ test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_sum_to_be_between:test_case_with_only_a_lower_bound_and_a_missing_value] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 4452.55it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 1546.57it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 1916.96it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 1412.38it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1442.46it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1289.86it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1257.10it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1219.54it/s]
_ test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_sum_to_be_between:negative_test_case_with_only_a_lower_bound] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 4192.21it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 1662.43it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 2049.33it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 1449.31it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1519.26it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1347.68it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1311.95it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1271.10it/s]
_ test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_sum_to_be_between:negative_test_case_with_only_a_max] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 3771.86it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 1418.43it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 1696.72it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 1145.88it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1205.52it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1087.80it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1063.80it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1036.46it/s]
_ test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_sum_to_be_between:raise_valueerror_with_both_max_and_min_are_missing] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 3184.74it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 1508.20it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 1897.59it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 1411.75it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1506.57it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1342.71it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1308.06it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1269.08it/s]
_ test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_unique_value_count_to_be_between:basic_positive_test] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 4328.49it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 1733.18it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 2141.41it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 1540.70it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1372.14it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1217.68it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1184.83it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1149.05it/s]
_ test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_unique_value_count_to_be_between:positive_test_with_null_values_in_column] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 4264.67it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 1706.74it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 2109.46it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 1524.28it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1331.00it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1166.14it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1131.84it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1094.62it/s]
_ test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_unique_value_count_to_be_between:null_min_duplicate_values_in_column] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 4262.50it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 1627.59it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 2008.13it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 1413.65it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1457.75it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1287.88it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1251.94it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1197.26it/s]
_ test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_unique_value_count_to_be_between:null_max_duplicate_and_null_values] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 3760.02it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 1507.12it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 1796.28it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 1224.50it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1263.06it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1141.54it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1115.43it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1081.35it/s]
_ test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_unique_value_count_to_be_between:vacuously_true_null_min_and_max] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 4148.67it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 1692.28it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 2087.76it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 1515.28it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1586.20it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1410.44it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1372.71it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1330.26it/s]
_ test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_median_to_be_between:positive_test_min_equal_max] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 3773.55it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 1278.95it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 1606.81it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 1196.66it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1087.24it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 983.08it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 960.67it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 935.71it/s]
_ test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_median_to_be_between:positive_test_null_min] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 3788.89it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 1444.07it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 1720.39it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 1236.16it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1028.58it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 895.45it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 873.72it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 851.77it/s]
_ test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_median_to_be_between:positive_test_null_max] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 3661.55it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 1474.79it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 1842.57it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 1351.84it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1246.08it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1113.80it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1085.69it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1057.57it/s]
_ test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_median_to_be_between:negative_test_min_equal_max] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 4209.04it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 1612.57it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 1996.02it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 1453.33it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1356.28it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1222.12it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1192.16it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1160.01it/s]
_ test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_median_to_be_between:negative_test_null_min] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 4130.28it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 1700.51it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 2105.22it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 1518.21it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1420.96it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1276.32it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1245.34it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1210.30it/s]
_ test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_median_to_be_between:negative_test_null_max] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 4750.06it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 1808.28it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 2217.64it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 1588.15it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1509.42it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1347.68it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1313.18it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1275.54it/s]
_ test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_median_to_be_between:positive_test_missing_value_in_column_exact_min_and_max] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 4576.44it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 1758.99it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 2159.79it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 1554.41it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1446.19it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1297.94it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1265.82it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1230.72it/s]
_ test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_median_to_be_between:negative_test_missing_value_in_column_complete_result_format] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 4387.35it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 1736.41it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 2147.62it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 1547.33it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1457.75it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1305.52it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1273.61it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1236.89it/s]
_ test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_median_to_be_between:vacuously_true_both_min_and_max_null] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 4245.25it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 1702.23it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 2106.98it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 1528.35it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1433.22it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1269.46it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1234.16it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1197.60it/s]
_ test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_median_to_be_between:test_empty_column_should_be_false_no_observed_value_with_which_to_compare] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 3914.42it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 1610.10it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 1984.37it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 1439.53it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1404.54it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1256.63it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1225.15it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1188.02it/s]
_ test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_min_to_be_between:basic_positive_test_case] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 4142.52it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 1661.11it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 2043.68it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 1474.96it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1511.19it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1342.82it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1306.74it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1268.69it/s]
_ test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_min_to_be_between:negative_test_case_with_max_value_none] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 4609.13it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 1734.62it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 2135.59it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 1519.86it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1588.60it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1406.07it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1367.56it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1319.27it/s]
_ test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_min_to_be_between:test_case_with_result_format_summary_also_verifies_that_max_value_is_inclusive] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 4175.51it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 1601.49it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 1988.45it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 1439.03it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1518.03it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1349.63it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1314.21it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1274.38it/s]
_ test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_min_to_be_between:test_case_with_only_a_lower_bound_and_a_missing_value] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 1551.43it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 989.11it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 1316.89it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 1056.41it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1156.81it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1052.66it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1030.35it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1005.04it/s]
_ test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_min_to_be_between:negative_test_case_with_only_a_lower_bound] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 3945.72it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 1537.22it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 1905.06it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 1379.40it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1446.69it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1277.19it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1243.03it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1205.52it/s]
_ test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_min_to_be_between:test_on_a_series_with_mostly_nulls] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 4108.04it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 1589.96it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 1949.63it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 1404.03it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1426.03it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1269.17it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1233.44it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1198.03it/s]
_ test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_min_to_be_between:negative_test_case_with_only_a_max] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 4150.72it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 1607.94it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 1979.07it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 1419.87it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1468.47it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1304.30it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1268.69it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1230.45it/s]
_ test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_min_to_be_between:raise_valueerror_with_both_max_and_min_are_missing] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 3899.86it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 1640.64it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 2039.37it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 1442.33it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1506.03it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1346.38it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1312.26it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1272.25it/s]
_ test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_proportion_of_unique_values_to_be_between:basic_positive_test] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/7 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/7 [00:00<?, ?it/s]Calculating Metrics:  29%|██▊       | 2/7 [00:00<00:00, 4150.72it/s]Calculating Metrics:  29%|██▊       | 2/7 [00:00<00:00, 741.57it/s]Calculating Metrics:  43%|████▎     | 3/7 [00:00<00:00, 1012.55it/s]Calculating Metrics:  43%|████▎     | 3/7 [00:00<00:00, 608.28it/s] Calculating Metrics:  71%|███████▏  | 5/7 [00:00<00:00, 826.30it/s]Calculating Metrics:  71%|███████▏  | 5/7 [00:00<00:00, 666.59it/s]Calculating Metrics:  86%|████████▌ | 6/7 [00:00<00:00, 752.45it/s]Calculating Metrics:  86%|████████▌ | 6/7 [00:00<00:00, 661.39it/s]Calculating Metrics: 100%|██████████| 7/7 [00:00<00:00, 729.41it/s]Calculating Metrics: 100%|██████████| 7/7 [00:00<00:00, 678.83it/s]Calculating Metrics: 100%|██████████| 7/7 [00:00<00:00, 673.86it/s]Calculating Metrics: 100%|██████████| 7/7 [00:00<00:00, 667.25it/s]
_ test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_proportion_of_unique_values_to_be_between:positive_test_with_null_values_in_column] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/7 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/7 [00:00<?, ?it/s]Calculating Metrics:  29%|██▊       | 2/7 [00:00<00:00, 3200.54it/s]Calculating Metrics:  29%|██▊       | 2/7 [00:00<00:00, 720.98it/s]Calculating Metrics:  43%|████▎     | 3/7 [00:00<00:00, 990.70it/s]Calculating Metrics:  43%|████▎     | 3/7 [00:00<00:00, 602.02it/s]Calculating Metrics:  71%|███████▏  | 5/7 [00:00<00:00, 827.80it/s]Calculating Metrics:  71%|███████▏  | 5/7 [00:00<00:00, 666.76it/s]Calculating Metrics:  86%|████████▌ | 6/7 [00:00<00:00, 752.45it/s]Calculating Metrics:  86%|████████▌ | 6/7 [00:00<00:00, 661.51it/s]Calculating Metrics: 100%|██████████| 7/7 [00:00<00:00, 728.92it/s]Calculating Metrics: 100%|██████████| 7/7 [00:00<00:00, 678.61it/s]Calculating Metrics: 100%|██████████| 7/7 [00:00<00:00, 673.64it/s]Calculating Metrics: 100%|██████████| 7/7 [00:00<00:00, 667.61it/s]
_ test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_proportion_of_unique_values_to_be_between:duplicate_values_in_column] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/7 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/7 [00:00<?, ?it/s]Calculating Metrics:  29%|██▊       | 2/7 [00:00<00:00, 4290.85it/s]Calculating Metrics:  29%|██▊       | 2/7 [00:00<00:00, 752.48it/s]Calculating Metrics:  43%|████▎     | 3/7 [00:00<00:00, 1030.88it/s]Calculating Metrics:  43%|████▎     | 3/7 [00:00<00:00, 610.76it/s] Calculating Metrics:  71%|███████▏  | 5/7 [00:00<00:00, 834.89it/s]Calculating Metrics:  71%|███████▏  | 5/7 [00:00<00:00, 654.19it/s]Calculating Metrics:  86%|████████▌ | 6/7 [00:00<00:00, 735.13it/s]Calculating Metrics:  86%|████████▌ | 6/7 [00:00<00:00, 646.62it/s]Calculating Metrics: 100%|██████████| 7/7 [00:00<00:00, 712.40it/s]Calculating Metrics: 100%|██████████| 7/7 [00:00<00:00, 664.08it/s]Calculating Metrics: 100%|██████████| 7/7 [00:00<00:00, 659.32it/s]Calculating Metrics: 100%|██████████| 7/7 [00:00<00:00, 653.35it/s]
_ test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_proportion_of_unique_values_to_be_between:null_max_duplicate_and_null_values] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/7 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/7 [00:00<?, ?it/s]Calculating Metrics:  29%|██▊       | 2/7 [00:00<00:00, 4116.10it/s]Calculating Metrics:  29%|██▊       | 2/7 [00:00<00:00, 760.53it/s]Calculating Metrics:  43%|████▎     | 3/7 [00:00<00:00, 1039.22it/s]Calculating Metrics:  43%|████▎     | 3/7 [00:00<00:00, 609.02it/s] Calculating Metrics:  71%|███████▏  | 5/7 [00:00<00:00, 831.11it/s]Calculating Metrics:  71%|███████▏  | 5/7 [00:00<00:00, 664.37it/s]Calculating Metrics:  86%|████████▌ | 6/7 [00:00<00:00, 748.60it/s]Calculating Metrics:  86%|████████▌ | 6/7 [00:00<00:00, 656.09it/s]Calculating Metrics: 100%|██████████| 7/7 [00:00<00:00, 720.62it/s]Calculating Metrics: 100%|██████████| 7/7 [00:00<00:00, 667.43it/s]Calculating Metrics: 100%|██████████| 7/7 [00:00<00:00, 661.44it/s]Calculating Metrics: 100%|██████████| 7/7 [00:00<00:00, 655.43it/s]
_ test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_proportion_of_unique_values_to_be_between:vacuously_true_null_min_and_max] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/7 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/7 [00:00<?, ?it/s]Calculating Metrics:  29%|██▊       | 2/7 [00:00<00:00, 3597.17it/s]Calculating Metrics:  29%|██▊       | 2/7 [00:00<00:00, 679.13it/s]Calculating Metrics:  43%|████▎     | 3/7 [00:00<00:00, 929.73it/s]Calculating Metrics:  43%|████▎     | 3/7 [00:00<00:00, 568.74it/s]Calculating Metrics:  71%|███████▏  | 5/7 [00:00<00:00, 788.17it/s]Calculating Metrics:  71%|███████▏  | 5/7 [00:00<00:00, 639.73it/s]Calculating Metrics:  86%|████████▌ | 6/7 [00:00<00:00, 722.89it/s]Calculating Metrics:  86%|████████▌ | 6/7 [00:00<00:00, 638.58it/s]Calculating Metrics: 100%|██████████| 7/7 [00:00<00:00, 705.08it/s]Calculating Metrics: 100%|██████████| 7/7 [00:00<00:00, 657.40it/s]Calculating Metrics: 100%|██████████| 7/7 [00:00<00:00, 652.74it/s]Calculating Metrics: 100%|██████████| 7/7 [00:00<00:00, 647.07it/s]
_ test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_distinct_values_to_contain_set:basic_positive_test] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 4058.35it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 1492.63it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 1881.98it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 1318.69it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1258.23it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1125.23it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1098.63it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1070.66it/s]
_ test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_distinct_values_to_contain_set:positive_test_with_null_values_in_column] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 3962.50it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 1462.96it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 1842.84it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 1300.96it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1251.56it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1118.85it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1094.40it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1065.83it/s]
_ test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_distinct_values_to_contain_set:positive_test_with_duplicate_values_in_column] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 4512.43it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 1539.48it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 1870.23it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 1305.96it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1270.62it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1122.30it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1097.34it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1065.56it/s]
_ test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_distinct_values_to_contain_set:positive_test_duplicate_and_null_values] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 4433.73it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 1510.37it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 1876.93it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 1317.44it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1217.50it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1083.66it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1059.84it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1033.27it/s]
_ test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_distinct_values_to_contain_set:basic_negative_test_no_set_intersection] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 4473.92it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 1545.72it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 1914.62it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 1332.09it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1296.64it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1149.12it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1117.66it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1088.44it/s]
_ test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_distinct_values_to_contain_set:negative_test_some_set_intersection] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 4025.24it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 1474.01it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 1862.20it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 1314.55it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1276.71it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1140.92it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1113.58it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1084.01it/s]
_ test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_distinct_values_to_contain_set:negative_test_with_null_values_in_column] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 4273.36it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 1500.38it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 1863.31it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 1289.23it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1250.44it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1108.65it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1083.73it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1055.70it/s]
_ test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_distinct_values_to_contain_set:negative_test_with_duplicate_values_in_column] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 4514.86it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 1507.39it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 1884.23it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 1265.25it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1226.67it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1094.40it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1070.66it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1042.78it/s]
_ test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_distinct_values_to_contain_set:negative_test_duplicate_and_null_values] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 4412.73it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 1559.80it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 1961.79it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 1367.41it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1336.30it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1189.70it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1162.10it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1130.84it/s]
_ test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_distinct_values_to_contain_set:positive_test_string_values] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 4464.40it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 1471.69it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 1808.41it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 1279.92it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1152.76it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1038.45it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1017.60it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 990.10it/s]
_ test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_distinct_values_to_contain_set:negative_test_string_values] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 4230.26it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 1517.48it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 1901.03it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 1323.96it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1184.16it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1062.99it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1040.32it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1013.42it/s]
_ test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_distinct_values_to_contain_set:basic_positive_test_case_date_set] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 3876.44it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 1402.78it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 1784.81it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 1266.91it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1151.10it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1030.16it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1007.82it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 979.92it/s]
_ test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_distinct_values_to_contain_set:basic_negative_test_case_date_set] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 3460.65it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 1413.41it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 1805.04it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 1288.70it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1190.21it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1071.48it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1049.36it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1024.13it/s]
_ test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_distinct_values_to_contain_set:basic_positive_test_case_datetime_set] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 4473.92it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 1543.44it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 1934.34it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 1344.90it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 838.61it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 777.91it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 766.33it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 752.78it/s]
_ test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_distinct_values_to_contain_set:basic_negative_test_case_datetime_set] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 4116.10it/s]Calculating Metrics:  50%|█████     | 2/4 [00:00<00:00, 1497.16it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 1862.20it/s]Calculating Metrics:  75%|███████▌  | 3/4 [00:00<00:00, 1272.42it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1121.10it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 1014.71it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 994.32it/s]Calculating Metrics: 100%|██████████| 4/4 [00:00<00:00, 970.90it/s]
_ test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_stdev_to_be_between:basic_positive_test] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/11 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/11 [00:00<?, ?it/s]Calculating Metrics:  18%|█▊        | 2/11 [00:00<00:00, 3114.97it/s]Calculating Metrics:  18%|█▊        | 2/11 [00:00<00:00, 437.91it/s]Calculating Metrics:  36%|███▋      | 4/11 [00:00<00:00, 696.38it/s]Calculating Metrics:  36%|███▋      | 4/11 [00:00<00:00, 412.88it/s]Calculating Metrics:  55%|█████▍    | 6/11 [00:00<00:00, 557.78it/s]Calculating Metrics:  55%|█████▍    | 6/11 [00:00<00:00, 417.16it/s]Calculating Metrics:  73%|███████▎  | 8/11 [00:00<00:00, 491.82it/s]Calculating Metrics:  73%|███████▎  | 8/11 [00:00<00:00, 426.06it/s]Calculating Metrics:  82%|████████▏ | 9/11 [00:00<00:00, 448.86it/s]Calculating Metrics:  82%|████████▏ | 9/11 [00:00<00:00, 405.94it/s]Calculating Metrics:  91%|█████████ | 10/11 [00:00<00:00, 433.48it/s]Calculating Metrics:  91%|█████████ | 10/11 [00:00<00:00, 402.09it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 413.91it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 394.52it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 393.33it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 391.96it/s]
_ test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_stdev_to_be_between:vacuously_true] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/11 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/11 [00:00<?, ?it/s]Calculating Metrics:  18%|█▊        | 2/11 [00:00<00:00, 2937.19it/s]Calculating Metrics:  18%|█▊        | 2/11 [00:00<00:00, 420.08it/s]Calculating Metrics:  36%|███▋      | 4/11 [00:00<00:00, 688.24it/s]Calculating Metrics:  36%|███▋      | 4/11 [00:00<00:00, 409.76it/s]Calculating Metrics:  55%|█████▍    | 6/11 [00:00<00:00, 551.06it/s]Calculating Metrics:  55%|█████▍    | 6/11 [00:00<00:00, 425.41it/s]Calculating Metrics:  73%|███████▎  | 8/11 [00:00<00:00, 503.77it/s]Calculating Metrics:  73%|███████▎  | 8/11 [00:00<00:00, 435.75it/s]Calculating Metrics:  82%|████████▏ | 9/11 [00:00<00:00, 465.19it/s]Calculating Metrics:  82%|████████▏ | 9/11 [00:00<00:00, 419.23it/s]Calculating Metrics:  91%|█████████ | 10/11 [00:00<00:00, 447.45it/s]Calculating Metrics:  91%|█████████ | 10/11 [00:00<00:00, 413.74it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 428.95it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 408.39it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 407.13it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 405.64it/s]
_ test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_stdev_to_be_between:positive_test__exact_min_and_max] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/11 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/11 [00:00<?, ?it/s]Calculating Metrics:  18%|█▊        | 2/11 [00:00<00:00, 3138.27it/s]Calculating Metrics:  18%|█▊        | 2/11 [00:00<00:00, 420.52it/s]Calculating Metrics:  36%|███▋      | 4/11 [00:00<00:00, 689.43it/s]Calculating Metrics:  36%|███▋      | 4/11 [00:00<00:00, 412.24it/s]Calculating Metrics:  55%|█████▍    | 6/11 [00:00<00:00, 555.95it/s]Calculating Metrics:  55%|█████▍    | 6/11 [00:00<00:00, 431.31it/s]Calculating Metrics:  73%|███████▎  | 8/11 [00:00<00:00, 511.28it/s]Calculating Metrics:  73%|███████▎  | 8/11 [00:00<00:00, 441.79it/s]Calculating Metrics:  82%|████████▏ | 9/11 [00:00<00:00, 471.52it/s]Calculating Metrics:  82%|████████▏ | 9/11 [00:00<00:00, 422.69it/s]Calculating Metrics:  91%|█████████ | 10/11 [00:00<00:00, 450.47it/s]Calculating Metrics:  91%|█████████ | 10/11 [00:00<00:00, 416.56it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 431.40it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 410.24it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 408.95it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 407.44it/s]
_ test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_stdev_to_be_between:basic_negative_test] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/11 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/11 [00:00<?, ?it/s]Calculating Metrics:  18%|█▊        | 2/11 [00:00<00:00, 2341.88it/s]Calculating Metrics:  18%|█▊        | 2/11 [00:00<00:00, 399.29it/s]Calculating Metrics:  36%|███▋      | 4/11 [00:00<00:00, 664.47it/s]Calculating Metrics:  36%|███▋      | 4/11 [00:00<00:00, 405.60it/s]Calculating Metrics:  55%|█████▍    | 6/11 [00:00<00:00, 551.37it/s]Calculating Metrics:  55%|█████▍    | 6/11 [00:00<00:00, 423.70it/s]Calculating Metrics:  73%|███████▎  | 8/11 [00:00<00:00, 508.16it/s]Calculating Metrics:  73%|███████▎  | 8/11 [00:00<00:00, 436.37it/s]Calculating Metrics:  82%|████████▏ | 9/11 [00:00<00:00, 464.35it/s]Calculating Metrics:  82%|████████▏ | 9/11 [00:00<00:00, 416.67it/s]Calculating Metrics:  91%|█████████ | 10/11 [00:00<00:00, 444.88it/s]Calculating Metrics:  91%|█████████ | 10/11 [00:00<00:00, 414.03it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 431.38it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 411.49it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 410.27it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 408.77it/s]
_ test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_stdev_to_be_between:positive_zero_stdev_exact_min_and_max] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/11 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/11 [00:00<?, ?it/s]Calculating Metrics:  18%|█▊        | 2/11 [00:00<00:00, 3200.54it/s]Calculating Metrics:  18%|█▊        | 2/11 [00:00<00:00, 438.05it/s]Calculating Metrics:  36%|███▋      | 4/11 [00:00<00:00, 724.25it/s]Calculating Metrics:  36%|███▋      | 4/11 [00:00<00:00, 430.15it/s]Calculating Metrics:  55%|█████▍    | 6/11 [00:00<00:00, 584.64it/s]Calculating Metrics:  55%|█████▍    | 6/11 [00:00<00:00, 448.40it/s]Calculating Metrics:  73%|███████▎  | 8/11 [00:00<00:00, 525.49it/s]Calculating Metrics:  73%|███████▎  | 8/11 [00:00<00:00, 448.63it/s]Calculating Metrics:  82%|████████▏ | 9/11 [00:00<00:00, 472.86it/s]Calculating Metrics:  82%|████████▏ | 9/11 [00:00<00:00, 419.87it/s]Calculating Metrics:  91%|█████████ | 10/11 [00:00<00:00, 447.53it/s]Calculating Metrics:  91%|█████████ | 10/11 [00:00<00:00, 415.73it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 427.38it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 407.59it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 406.39it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 404.93it/s]
_ test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_stdev_to_be_between:positive_test_null_max_exact_min] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/11 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/11 [00:00<?, ?it/s]Calculating Metrics:  18%|█▊        | 2/11 [00:00<00:00, 3200.54it/s]Calculating Metrics:  18%|█▊        | 2/11 [00:00<00:00, 426.90it/s]Calculating Metrics:  36%|███▋      | 4/11 [00:00<00:00, 712.38it/s]Calculating Metrics:  36%|███▋      | 4/11 [00:00<00:00, 423.55it/s]Calculating Metrics:  55%|█████▍    | 6/11 [00:00<00:00, 576.42it/s]Calculating Metrics:  55%|█████▍    | 6/11 [00:00<00:00, 443.62it/s]Calculating Metrics:  73%|███████▎  | 8/11 [00:00<00:00, 520.09it/s]Calculating Metrics:  73%|███████▎  | 8/11 [00:00<00:00, 445.68it/s]Calculating Metrics:  82%|████████▏ | 9/11 [00:00<00:00, 470.47it/s]Calculating Metrics:  82%|████████▏ | 9/11 [00:00<00:00, 422.24it/s]Calculating Metrics:  91%|█████████ | 10/11 [00:00<00:00, 450.55it/s]Calculating Metrics:  91%|█████████ | 10/11 [00:00<00:00, 418.78it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 431.54it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 411.52it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 410.33it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 408.82it/s]
_ test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_stdev_to_be_between:positive_test_null_min_exact_max] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/11 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/11 [00:00<?, ?it/s]Calculating Metrics:  18%|█▊        | 2/11 [00:00<00:00, 3332.78it/s]Calculating Metrics:  18%|█▊        | 2/11 [00:00<00:00, 436.11it/s]Calculating Metrics:  36%|███▋      | 4/11 [00:00<00:00, 719.03it/s]Calculating Metrics:  36%|███▋      | 4/11 [00:00<00:00, 431.82it/s]Calculating Metrics:  55%|█████▍    | 6/11 [00:00<00:00, 585.88it/s]Calculating Metrics:  55%|█████▍    | 6/11 [00:00<00:00, 449.50it/s]Calculating Metrics:  73%|███████▎  | 8/11 [00:00<00:00, 518.70it/s]Calculating Metrics:  73%|███████▎  | 8/11 [00:00<00:00, 444.59it/s]Calculating Metrics:  82%|████████▏ | 9/11 [00:00<00:00, 472.51it/s]Calculating Metrics:  82%|████████▏ | 9/11 [00:00<00:00, 422.71it/s]Calculating Metrics:  91%|█████████ | 10/11 [00:00<00:00, 449.94it/s]Calculating Metrics:  91%|█████████ | 10/11 [00:00<00:00, 415.70it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 430.80it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 410.40it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 409.13it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 407.65it/s]
_ test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_stdev_to_be_between:negative_test_null_max] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/11 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/11 [00:00<?, ?it/s]Calculating Metrics:  18%|█▊        | 2/11 [00:00<00:00, 2953.74it/s]Calculating Metrics:  18%|█▊        | 2/11 [00:00<00:00, 409.66it/s]Calculating Metrics:  36%|███▋      | 4/11 [00:00<00:00, 682.94it/s]Calculating Metrics:  36%|███▋      | 4/11 [00:00<00:00, 422.21it/s]Calculating Metrics:  55%|█████▍    | 6/11 [00:00<00:00, 575.97it/s]Calculating Metrics:  55%|█████▍    | 6/11 [00:00<00:00, 441.27it/s]Calculating Metrics:  73%|███████▎  | 8/11 [00:00<00:00, 520.26it/s]Calculating Metrics:  73%|███████▎  | 8/11 [00:00<00:00, 447.40it/s]Calculating Metrics:  82%|████████▏ | 9/11 [00:00<00:00, 476.39it/s]Calculating Metrics:  82%|████████▏ | 9/11 [00:00<00:00, 427.64it/s]Calculating Metrics:  91%|█████████ | 10/11 [00:00<00:00, 455.29it/s]Calculating Metrics:  91%|█████████ | 10/11 [00:00<00:00, 418.83it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 426.61it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 404.71it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 403.22it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 401.47it/s]
_ test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_stdev_to_be_between:negative_test_null_min] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/11 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/11 [00:00<?, ?it/s]Calculating Metrics:  18%|█▊        | 2/11 [00:00<00:00, 3240.10it/s]Calculating Metrics:  18%|█▊        | 2/11 [00:00<00:00, 427.60it/s]Calculating Metrics:  36%|███▋      | 4/11 [00:00<00:00, 710.84it/s]Calculating Metrics:  36%|███▋      | 4/11 [00:00<00:00, 428.16it/s]Calculating Metrics:  55%|█████▍    | 6/11 [00:00<00:00, 581.33it/s]Calculating Metrics:  55%|█████▍    | 6/11 [00:00<00:00, 442.83it/s]Calculating Metrics:  73%|███████▎  | 8/11 [00:00<00:00, 522.77it/s]Calculating Metrics:  73%|███████▎  | 8/11 [00:00<00:00, 449.66it/s]Calculating Metrics:  82%|████████▏ | 9/11 [00:00<00:00, 476.59it/s]Calculating Metrics:  82%|████████▏ | 9/11 [00:00<00:00, 427.45it/s]Calculating Metrics:  91%|█████████ | 10/11 [00:00<00:00, 455.10it/s]Calculating Metrics:  91%|█████████ | 10/11 [00:00<00:00, 421.92it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 437.08it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 415.84it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 414.42it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 412.88it/s]
_ test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_stdev_to_be_between:positive_test_missing_value_in_column_complete_result_format] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/11 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/11 [00:00<?, ?it/s]Calculating Metrics:  18%|█▊        | 2/11 [00:00<00:00, 2981.03it/s]Calculating Metrics:  18%|█▊        | 2/11 [00:00<00:00, 391.24it/s]Calculating Metrics:  36%|███▋      | 4/11 [00:00<00:00, 646.10it/s]Calculating Metrics:  36%|███▋      | 4/11 [00:00<00:00, 389.67it/s]Calculating Metrics:  55%|█████▍    | 6/11 [00:00<00:00, 529.39it/s]Calculating Metrics:  55%|█████▍    | 6/11 [00:00<00:00, 401.37it/s]Calculating Metrics:  73%|███████▎  | 8/11 [00:00<00:00, 464.55it/s]Calculating Metrics:  73%|███████▎  | 8/11 [00:00<00:00, 400.54it/s]Calculating Metrics:  82%|████████▏ | 9/11 [00:00<00:00, 420.36it/s]Calculating Metrics:  82%|████████▏ | 9/11 [00:00<00:00, 378.43it/s]Calculating Metrics:  91%|█████████ | 10/11 [00:00<00:00, 402.16it/s]Calculating Metrics:  91%|█████████ | 10/11 [00:00<00:00, 375.42it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 387.39it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 371.06it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 370.01it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 368.60it/s]
_ test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_distinct_values_to_be_in_set:basic_positive_test] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/5 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/5 [00:00<?, ?it/s]Calculating Metrics:  40%|████      | 2/5 [00:00<00:00, 3038.25it/s]Calculating Metrics:  40%|████      | 2/5 [00:00<00:00, 1186.84it/s]Calculating Metrics:  80%|████████  | 4/5 [00:00<00:00, 1405.83it/s]Calculating Metrics:  80%|████████  | 4/5 [00:00<00:00, 1119.15it/s]Calculating Metrics: 100%|██████████| 5/5 [00:00<00:00, 1026.00it/s]Calculating Metrics: 100%|██████████| 5/5 [00:00<00:00, 947.27it/s]Calculating Metrics: 100%|██████████| 5/5 [00:00<00:00, 933.15it/s]Calculating Metrics: 100%|██████████| 5/5 [00:00<00:00, 917.07it/s]
_ test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_distinct_values_to_be_in_set:vacuously_true_universal_set] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/5 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/5 [00:00<?, ?it/s]Calculating Metrics:  40%|████      | 2/5 [00:00<00:00, 3379.78it/s]Calculating Metrics:  40%|████      | 2/5 [00:00<00:00, 1167.03it/s]Calculating Metrics:  80%|████████  | 4/5 [00:00<00:00, 1481.04it/s]Calculating Metrics:  80%|████████  | 4/5 [00:00<00:00, 1094.76it/s]Calculating Metrics: 100%|██████████| 5/5 [00:00<00:00, 1029.28it/s]Calculating Metrics: 100%|██████████| 5/5 [00:00<00:00, 940.76it/s]Calculating Metrics: 100%|██████████| 5/5 [00:00<00:00, 925.45it/s]Calculating Metrics: 100%|██████████| 5/5 [00:00<00:00, 908.13it/s]
_ test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_distinct_values_to_be_in_set:positive_test_with_null_values_in_column] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/5 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/5 [00:00<?, ?it/s]Calculating Metrics:  40%|████      | 2/5 [00:00<00:00, 3407.23it/s]Calculating Metrics:  40%|████      | 2/5 [00:00<00:00, 1264.11it/s]Calculating Metrics:  80%|████████  | 4/5 [00:00<00:00, 1428.70it/s]Calculating Metrics:  80%|████████  | 4/5 [00:00<00:00, 1122.30it/s]Calculating Metrics: 100%|██████████| 5/5 [00:00<00:00, 994.05it/s]Calculating Metrics: 100%|██████████| 5/5 [00:00<00:00, 915.95it/s]Calculating Metrics: 100%|██████████| 5/5 [00:00<00:00, 902.23it/s]Calculating Metrics: 100%|██████████| 5/5 [00:00<00:00, 886.71it/s]
_ test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_distinct_values_to_be_in_set:positive_test_with_duplicate_values_in_column] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/5 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/5 [00:00<?, ?it/s]Calculating Metrics:  40%|████      | 2/5 [00:00<00:00, 3396.20it/s]Calculating Metrics:  40%|████      | 2/5 [00:00<00:00, 1286.20it/s]Calculating Metrics:  80%|████████  | 4/5 [00:00<00:00, 1636.80it/s]Calculating Metrics:  80%|████████  | 4/5 [00:00<00:00, 1265.06it/s]Calculating Metrics: 100%|██████████| 5/5 [00:00<00:00, 1122.61it/s]Calculating Metrics: 100%|██████████| 5/5 [00:00<00:00, 1027.56it/s]Calculating Metrics: 100%|██████████| 5/5 [00:00<00:00, 1010.29it/s]Calculating Metrics: 100%|██████████| 5/5 [00:00<00:00, 985.23it/s]
_ test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_distinct_values_to_be_in_set:positive_test_duplicate_and_null_values] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/5 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/5 [00:00<?, ?it/s]Calculating Metrics:  40%|████      | 2/5 [00:00<00:00, 3396.20it/s]Calculating Metrics:  40%|████      | 2/5 [00:00<00:00, 1245.52it/s]Calculating Metrics:  80%|████████  | 4/5 [00:00<00:00, 1586.20it/s]Calculating Metrics:  80%|████████  | 4/5 [00:00<00:00, 1226.32it/s]Calculating Metrics: 100%|██████████| 5/5 [00:00<00:00, 1124.90it/s]Calculating Metrics: 100%|██████████| 5/5 [00:00<00:00, 1029.28it/s]Calculating Metrics: 100%|██████████| 5/5 [00:00<00:00, 1012.58it/s]Calculating Metrics: 100%|██████████| 5/5 [00:00<00:00, 994.05it/s]
_ test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_distinct_values_to_be_in_set:basic_negative_test_no_set_intersection] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/5 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/5 [00:00<?, ?it/s]Calculating Metrics:  40%|████      | 2/5 [00:00<00:00, 3430.92it/s]Calculating Metrics:  40%|████      | 2/5 [00:00<00:00, 1277.97it/s]Calculating Metrics:  80%|████████  | 4/5 [00:00<00:00, 1637.92it/s]Calculating Metrics:  80%|████████  | 4/5 [00:00<00:00, 1266.97it/s]Calculating Metrics: 100%|██████████| 5/5 [00:00<00:00, 1154.69it/s]Calculating Metrics: 100%|██████████| 5/5 [00:00<00:00, 1030.95it/s]Calculating Metrics: 100%|██████████| 5/5 [00:00<00:00, 1011.36it/s]Calculating Metrics: 100%|██████████| 5/5 [00:00<00:00, 991.47it/s]
_ test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_distinct_values_to_be_in_set:negative_test_some_set_intersection_and_extra] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/5 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/5 [00:00<?, ?it/s]Calculating Metrics:  40%|████      | 2/5 [00:00<00:00, 3418.34it/s]Calculating Metrics:  40%|████      | 2/5 [00:00<00:00, 1273.70it/s]Calculating Metrics:  80%|████████  | 4/5 [00:00<00:00, 1590.41it/s]Calculating Metrics:  80%|████████  | 4/5 [00:00<00:00, 1230.00it/s]Calculating Metrics: 100%|██████████| 5/5 [00:00<00:00, 1148.05it/s]Calculating Metrics: 100%|██████████| 5/5 [00:00<00:00, 1040.77it/s]Calculating Metrics: 100%|██████████| 5/5 [00:00<00:00, 1021.01it/s]Calculating Metrics: 100%|██████████| 5/5 [00:00<00:00, 999.79it/s]
_ test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_distinct_values_to_be_in_set:negative_test_with_null_values_in_column] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/5 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/5 [00:00<?, ?it/s]Calculating Metrics:  40%|████      | 2/5 [00:00<00:00, 3278.08it/s]Calculating Metrics:  40%|████      | 2/5 [00:00<00:00, 1258.61it/s]Calculating Metrics:  80%|████████  | 4/5 [00:00<00:00, 1599.35it/s]Calculating Metrics:  80%|████████  | 4/5 [00:00<00:00, 1246.54it/s]Calculating Metrics: 100%|██████████| 5/5 [00:00<00:00, 1184.76it/s]Calculating Metrics: 100%|██████████| 5/5 [00:00<00:00, 1078.95it/s]Calculating Metrics: 100%|██████████| 5/5 [00:00<00:00, 1060.45it/s]Calculating Metrics: 100%|██████████| 5/5 [00:00<00:00, 1039.53it/s]
_ test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_distinct_values_to_be_in_set:negative_test_with_duplicate_values_in_column] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/5 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/5 [00:00<?, ?it/s]Calculating Metrics:  40%|████      | 2/5 [00:00<00:00, 3283.21it/s]Calculating Metrics:  40%|████      | 2/5 [00:00<00:00, 1216.45it/s]Calculating Metrics:  80%|████████  | 4/5 [00:00<00:00, 1543.73it/s]Calculating Metrics:  80%|████████  | 4/5 [00:00<00:00, 1187.26it/s]Calculating Metrics: 100%|██████████| 5/5 [00:00<00:00, 1132.74it/s]Calculating Metrics: 100%|██████████| 5/5 [00:00<00:00, 1032.62it/s]Calculating Metrics: 100%|██████████| 5/5 [00:00<00:00, 1015.23it/s]Calculating Metrics: 100%|██████████| 5/5 [00:00<00:00, 995.61it/s]
_ test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_distinct_values_to_be_in_set:negative_test_duplicate_and_null_values] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/5 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/5 [00:00<?, ?it/s]Calculating Metrics:  40%|████      | 2/5 [00:00<00:00, 2946.47it/s]Calculating Metrics:  40%|████      | 2/5 [00:00<00:00, 1081.84it/s]Calculating Metrics:  80%|████████  | 4/5 [00:00<00:00, 1373.72it/s]Calculating Metrics:  80%|████████  | 4/5 [00:00<00:00, 1060.51it/s]Calculating Metrics: 100%|██████████| 5/5 [00:00<00:00, 1013.41it/s]Calculating Metrics: 100%|██████████| 5/5 [00:00<00:00, 922.88it/s]Calculating Metrics: 100%|██████████| 5/5 [00:00<00:00, 907.94it/s]Calculating Metrics: 100%|██████████| 5/5 [00:00<00:00, 890.96it/s]
_ test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_most_common_value_to_be_in_set:negative_test_tie_for_most_common_with_missing_values_and_complete_result_format] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/6 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/6 [00:00<?, ?it/s]Calculating Metrics:  33%|███▎      | 2/6 [00:00<00:00, 3184.74it/s]Calculating Metrics:  33%|███▎      | 2/6 [00:00<00:00, 951.09it/s]Calculating Metrics:  67%|██████▋   | 4/6 [00:00<00:00, 1223.99it/s]Calculating Metrics:  67%|██████▋   | 4/6 [00:00<00:00, 884.36it/s] Calculating Metrics:  83%|████████▎ | 5/6 [00:00<00:00, 826.04it/s]Calculating Metrics:  83%|████████▎ | 5/6 [00:00<00:00, 721.49it/s]Calculating Metrics: 100%|██████████| 6/6 [00:00<00:00, 770.63it/s]Calculating Metrics: 100%|██████████| 6/6 [00:00<00:00, 719.58it/s]Calculating Metrics: 100%|██████████| 6/6 [00:00<00:00, 712.51it/s]Calculating Metrics: 100%|██████████| 6/6 [00:00<00:00, 703.15it/s]
_ test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_most_common_value_to_be_in_set:negative_test__tie_for_most_common__full_value_set__ties_okay__false] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/6 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/6 [00:00<?, ?it/s]Calculating Metrics:  33%|███▎      | 2/6 [00:00<00:00, 3261.51it/s]Calculating Metrics:  33%|███▎      | 2/6 [00:00<00:00, 927.64it/s]Calculating Metrics:  67%|██████▋   | 4/6 [00:00<00:00, 1225.51it/s]Calculating Metrics:  67%|██████▋   | 4/6 [00:00<00:00, 866.55it/s] Calculating Metrics:  83%|████████▎ | 5/6 [00:00<00:00, 861.93it/s]Calculating Metrics:  83%|████████▎ | 5/6 [00:00<00:00, 722.13it/s]Calculating Metrics: 100%|██████████| 6/6 [00:00<00:00, 753.38it/s]Calculating Metrics: 100%|██████████| 6/6 [00:00<00:00, 694.44it/s]Calculating Metrics: 100%|██████████| 6/6 [00:00<00:00, 686.65it/s]Calculating Metrics: 100%|██████████| 6/6 [00:00<00:00, 677.58it/s]
_ test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_most_common_value_to_be_in_set:positive_test__tie_for_most_common__ties_okay__true] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/6 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/6 [00:00<?, ?it/s]Calculating Metrics:  33%|███▎      | 2/6 [00:00<00:00, 3210.34it/s]Calculating Metrics:  33%|███▎      | 2/6 [00:00<00:00, 856.16it/s]Calculating Metrics:  67%|██████▋   | 4/6 [00:00<00:00, 1208.12it/s]Calculating Metrics:  67%|██████▋   | 4/6 [00:00<00:00, 878.39it/s] Calculating Metrics:  83%|████████▎ | 5/6 [00:00<00:00, 896.87it/s]Calculating Metrics:  83%|████████▎ | 5/6 [00:00<00:00, 773.89it/s]Calculating Metrics: 100%|██████████| 6/6 [00:00<00:00, 828.64it/s]Calculating Metrics: 100%|██████████| 6/6 [00:00<00:00, 770.63it/s]Calculating Metrics: 100%|██████████| 6/6 [00:00<00:00, 762.88it/s]Calculating Metrics: 100%|██████████| 6/6 [00:00<00:00, 753.76it/s]
_ test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_most_common_value_to_be_in_set:positive_test__tie_for_most_common_but_test_for_last_value__ties_okay__true] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/6 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/6 [00:00<?, ?it/s]Calculating Metrics:  33%|███▎      | 2/6 [00:00<00:00, 3165.51it/s]Calculating Metrics:  33%|███▎      | 2/6 [00:00<00:00, 942.12it/s]Calculating Metrics:  67%|██████▋   | 4/6 [00:00<00:00, 1305.11it/s]Calculating Metrics:  67%|██████▋   | 4/6 [00:00<00:00, 935.71it/s] Calculating Metrics:  83%|████████▎ | 5/6 [00:00<00:00, 937.23it/s]Calculating Metrics:  83%|████████▎ | 5/6 [00:00<00:00, 803.63it/s]Calculating Metrics: 100%|██████████| 6/6 [00:00<00:00, 850.94it/s]Calculating Metrics: 100%|██████████| 6/6 [00:00<00:00, 788.77it/s]Calculating Metrics: 100%|██████████| 6/6 [00:00<00:00, 780.46it/s]Calculating Metrics: 100%|██████████| 6/6 [00:00<00:00, 770.73it/s]
_ test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_most_common_value_to_be_in_set:negative_test__tie_for_most_common__value_set_does_not_match__ties_okay__false] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/6 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/6 [00:00<?, ?it/s]Calculating Metrics:  33%|███▎      | 2/6 [00:00<00:00, 3377.06it/s]Calculating Metrics:  33%|███▎      | 2/6 [00:00<00:00, 941.59it/s]Calculating Metrics:  67%|██████▋   | 4/6 [00:00<00:00, 1269.37it/s]Calculating Metrics:  67%|██████▋   | 4/6 [00:00<00:00, 892.22it/s] Calculating Metrics:  83%|████████▎ | 5/6 [00:00<00:00, 901.69it/s]Calculating Metrics:  83%|████████▎ | 5/6 [00:00<00:00, 772.43it/s]Calculating Metrics: 100%|██████████| 6/6 [00:00<00:00, 822.60it/s]Calculating Metrics: 100%|██████████| 6/6 [00:00<00:00, 760.53it/s]Calculating Metrics: 100%|██████████| 6/6 [00:00<00:00, 752.07it/s]Calculating Metrics: 100%|██████████| 6/6 [00:00<00:00, 742.84it/s]
_ test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_most_common_value_to_be_in_set:negative_test__tie_for_most_common__value_set_does_not_match__ties_okay__true] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/6 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/6 [00:00<?, ?it/s]Calculating Metrics:  33%|███▎      | 2/6 [00:00<00:00, 3296.11it/s]Calculating Metrics:  33%|███▎      | 2/6 [00:00<00:00, 959.79it/s]Calculating Metrics:  67%|██████▋   | 4/6 [00:00<00:00, 1319.38it/s]Calculating Metrics:  67%|██████▋   | 4/6 [00:00<00:00, 943.92it/s] Calculating Metrics:  83%|████████▎ | 5/6 [00:00<00:00, 939.16it/s]Calculating Metrics:  83%|████████▎ | 5/6 [00:00<00:00, 806.35it/s]Calculating Metrics: 100%|██████████| 6/6 [00:00<00:00, 855.46it/s]Calculating Metrics: 100%|██████████| 6/6 [00:00<00:00, 790.63it/s]Calculating Metrics: 100%|██████████| 6/6 [00:00<00:00, 781.98it/s]Calculating Metrics: 100%|██████████| 6/6 [00:00<00:00, 771.82it/s]
_ test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_most_common_value_to_be_in_set:positive_test_string_values_value_set_contains_more_than_actual_mode_list] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/6 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/6 [00:00<?, ?it/s]Calculating Metrics:  33%|███▎      | 2/6 [00:00<00:00, 2614.90it/s]Calculating Metrics:  33%|███▎      | 2/6 [00:00<00:00, 751.67it/s]Calculating Metrics:  67%|██████▋   | 4/6 [00:00<00:00, 986.66it/s]Calculating Metrics:  67%|██████▋   | 4/6 [00:00<00:00, 712.14it/s]Calculating Metrics:  83%|████████▎ | 5/6 [00:00<00:00, 700.78it/s]Calculating Metrics:  83%|████████▎ | 5/6 [00:00<00:00, 609.18it/s]Calculating Metrics: 100%|██████████| 6/6 [00:00<00:00, 653.39it/s]Calculating Metrics: 100%|██████████| 6/6 [00:00<00:00, 609.45it/s]Calculating Metrics: 100%|██████████| 6/6 [00:00<00:00, 603.40it/s]Calculating Metrics: 100%|██████████| 6/6 [00:00<00:00, 596.49it/s]
_ test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_most_common_value_to_be_in_set:negative_test_string_values_value_set_contains_more_than_actual_mode_list] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/6 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/6 [00:00<?, ?it/s]Calculating Metrics:  33%|███▎      | 2/6 [00:00<00:00, 2933.08it/s]Calculating Metrics:  33%|███▎      | 2/6 [00:00<00:00, 791.75it/s]Calculating Metrics:  67%|██████▋   | 4/6 [00:00<00:00, 1084.64it/s]Calculating Metrics:  67%|██████▋   | 4/6 [00:00<00:00, 750.05it/s] Calculating Metrics:  83%|████████▎ | 5/6 [00:00<00:00, 759.65it/s]Calculating Metrics:  83%|████████▎ | 5/6 [00:00<00:00, 654.30it/s]Calculating Metrics: 100%|██████████| 6/6 [00:00<00:00, 696.94it/s]Calculating Metrics: 100%|██████████| 6/6 [00:00<00:00, 647.19it/s]Calculating Metrics: 100%|██████████| 6/6 [00:00<00:00, 640.35it/s]Calculating Metrics: 100%|██████████| 6/6 [00:00<00:00, 632.05it/s]
_ test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_quantile_values_to_be_between:basic_positive_test_extremes] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/5 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/5 [00:00<?, ?it/s]Calculating Metrics:  40%|████      | 2/5 [00:00<00:00, 2832.08it/s]Calculating Metrics:  40%|████      | 2/5 [00:00<00:00, 1161.54it/s]Calculating Metrics:  80%|████████  | 4/5 [00:00<00:00, 1427.00it/s]Calculating Metrics:  80%|████████  | 4/5 [00:00<00:00, 1131.84it/s]Calculating Metrics: 100%|██████████| 5/5 [00:00<00:00, 947.87it/s]Calculating Metrics: 100%|██████████| 5/5 [00:00<00:00, 874.58it/s]Calculating Metrics: 100%|██████████| 5/5 [00:00<00:00, 861.93it/s]Calculating Metrics: 100%|██████████| 5/5 [00:00<00:00, 847.75it/s]
_ test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_quantile_values_to_be_between:basic_positive_test_normal_quantiles] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/5 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/5 [00:00<?, ?it/s]Calculating Metrics:  40%|████      | 2/5 [00:00<00:00, 2824.45it/s]Calculating Metrics:  40%|████      | 2/5 [00:00<00:00, 1106.09it/s]Calculating Metrics:  80%|████████  | 4/5 [00:00<00:00, 1427.97it/s]Calculating Metrics:  80%|████████  | 4/5 [00:00<00:00, 1106.75it/s]Calculating Metrics: 100%|██████████| 5/5 [00:00<00:00, 729.47it/s]Calculating Metrics: 100%|██████████| 5/5 [00:00<00:00, 684.36it/s]Calculating Metrics: 100%|██████████| 5/5 [00:00<00:00, 675.48it/s]Calculating Metrics: 100%|██████████| 5/5 [00:00<00:00, 666.48it/s]
_ test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_quantile_values_to_be_between:basic_positive_test_uneven_spacing] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/5 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/5 [00:00<?, ?it/s]Calculating Metrics:  40%|████      | 2/5 [00:00<00:00, 3159.55it/s]Calculating Metrics:  40%|████      | 2/5 [00:00<00:00, 1092.98it/s]Calculating Metrics:  80%|████████  | 4/5 [00:00<00:00, 1402.19it/s]Calculating Metrics:  80%|████████  | 4/5 [00:00<00:00, 1062.72it/s]Calculating Metrics: 100%|██████████| 5/5 [00:00<00:00, 630.68it/s]Calculating Metrics: 100%|██████████| 5/5 [00:00<00:00, 594.75it/s]Calculating Metrics: 100%|██████████| 5/5 [00:00<00:00, 588.58it/s]Calculating Metrics: 100%|██████████| 5/5 [00:00<00:00, 581.81it/s]
_ test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_quantile_values_to_be_between:basic_negative_test_normal_quantiles_wrong_distribution] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/5 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/5 [00:00<?, ?it/s]Calculating Metrics:  40%|████      | 2/5 [00:00<00:00, 3508.41it/s]Calculating Metrics:  40%|████      | 2/5 [00:00<00:00, 1236.89it/s]Calculating Metrics:  80%|████████  | 4/5 [00:00<00:00, 1578.44it/s]Calculating Metrics:  80%|████████  | 4/5 [00:00<00:00, 1200.52it/s]Calculating Metrics: 100%|██████████| 5/5 [00:00<00:00, 762.66it/s]Calculating Metrics: 100%|██████████| 5/5 [00:00<00:00, 714.07it/s]Calculating Metrics: 100%|██████████| 5/5 [00:00<00:00, 705.61it/s]Calculating Metrics: 100%|██████████| 5/5 [00:00<00:00, 695.69it/s]
_ test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_max_to_be_between:basic_positive_test_case] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/6 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/6 [00:00<?, ?it/s]Calculating Metrics:  33%|███▎      | 2/6 [00:00<00:00, 3034.95it/s]Calculating Metrics:  33%|███▎      | 2/6 [00:00<00:00, 976.21it/s]Calculating Metrics:  67%|██████▋   | 4/6 [00:00<00:00, 1208.91it/s]Calculating Metrics:  67%|██████▋   | 4/6 [00:00<00:00, 898.28it/s] Calculating Metrics:  83%|████████▎ | 5/6 [00:00<00:00, 1005.83it/s]Calculating Metrics:  83%|████████▎ | 5/6 [00:00<00:00, 863.13it/s]alculating Metrics: 100%|██████████| 6/6 [00:00<00:00, 836.35it/s]Calculating Metrics: 100%|██████████| 6/6 [00:00<00:00, 780.24it/s]Calculating Metrics: 100%|██████████| 6/6 [00:00<00:00, 771.91it/s]Calculating Metrics: 100%|██████████| 6/6 [00:00<00:00, 762.49it/s]
_ test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_max_to_be_between:basic_negative_test_case] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/6 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/6 [00:00<?, ?it/s]Calculating Metrics:  33%|███▎      | 2/6 [00:00<00:00, 3440.77it/s]Calculating Metrics:  33%|███▎      | 2/6 [00:00<00:00, 1050.94it/s]Calculating Metrics:  67%|██████▋   | 4/6 [00:00<00:00, 1427.00it/s]Calculating Metrics:  67%|██████▋   | 4/6 [00:00<00:00, 1027.20it/s]Calculating Metrics:  83%|████████▎ | 5/6 [00:00<00:00, 1142.86it/s]Calculating Metrics:  83%|████████▎ | 5/6 [00:00<00:00, 965.58it/s]Calculating Metrics: 100%|██████████| 6/6 [00:00<00:00, 984.54it/s]Calculating Metrics: 100%|██████████| 6/6 [00:00<00:00, 909.89it/s]Calculating Metrics: 100%|██████████| 6/6 [00:00<00:00, 899.16it/s]Calculating Metrics: 100%|██████████| 6/6 [00:00<00:00, 886.50it/s]
_ test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_max_to_be_between:test_case_with_result_format_summary_also_verifies_that_max_value_is_inclusive] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/6 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/6 [00:00<?, ?it/s]Calculating Metrics:  33%|███▎      | 2/6 [00:00<00:00, 3496.71it/s]Calculating Metrics:  33%|███▎      | 2/6 [00:00<00:00, 1064.95it/s]Calculating Metrics:  67%|██████▋   | 4/6 [00:00<00:00, 1441.34it/s]Calculating Metrics:  67%|██████▋   | 4/6 [00:00<00:00, 1036.78it/s]Calculating Metrics:  83%|████████▎ | 5/6 [00:00<00:00, 1153.36it/s]Calculating Metrics:  83%|████████▎ | 5/6 [00:00<00:00, 974.65it/s]Calculating Metrics: 100%|██████████| 6/6 [00:00<00:00, 1002.30it/s]Calculating Metrics: 100%|██████████| 6/6 [00:00<00:00, 926.47it/s] Calculating Metrics: 100%|██████████| 6/6 [00:00<00:00, 915.59it/s]Calculating Metrics: 100%|██████████| 6/6 [00:00<00:00, 902.94it/s]
_ test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_max_to_be_between:test_case_with_only_a_lower_bound_and_a_missing_value] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/6 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/6 [00:00<?, ?it/s]Calculating Metrics:  33%|███▎      | 2/6 [00:00<00:00, 3089.73it/s]Calculating Metrics:  33%|███▎      | 2/6 [00:00<00:00, 956.40it/s]Calculating Metrics:  67%|██████▋   | 4/6 [00:00<00:00, 1293.94it/s]Calculating Metrics:  67%|██████▋   | 4/6 [00:00<00:00, 934.30it/s] Calculating Metrics:  83%|████████▎ | 5/6 [00:00<00:00, 1035.37it/s]Calculating Metrics:  83%|████████▎ | 5/6 [00:00<00:00, 881.97it/s]alculating Metrics: 100%|██████████| 6/6 [00:00<00:00, 884.16it/s]Calculating Metrics: 100%|██████████| 6/6 [00:00<00:00, 820.67it/s]Calculating Metrics: 100%|██████████| 6/6 [00:00<00:00, 811.46it/s]Calculating Metrics: 100%|██████████| 6/6 [00:00<00:00, 800.82it/s]
_ test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_max_to_be_between:negative_test_case_with_only_a_lower_bound] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/6 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/6 [00:00<?, ?it/s]Calculating Metrics:  33%|███▎      | 2/6 [00:00<00:00, 3242.60it/s]Calculating Metrics:  33%|███▎      | 2/6 [00:00<00:00, 982.85it/s]Calculating Metrics:  67%|██████▋   | 4/6 [00:00<00:00, 1271.10it/s]Calculating Metrics:  67%|██████▋   | 4/6 [00:00<00:00, 916.19it/s] Calculating Metrics:  83%|████████▎ | 5/6 [00:00<00:00, 1025.85it/s]Calculating Metrics:  83%|████████▎ | 5/6 [00:00<00:00, 866.27it/s]alculating Metrics: 100%|██████████| 6/6 [00:00<00:00, 893.80it/s]Calculating Metrics: 100%|██████████| 6/6 [00:00<00:00, 829.19it/s]Calculating Metrics: 100%|██████████| 6/6 [00:00<00:00, 819.79it/s]Calculating Metrics: 100%|██████████| 6/6 [00:00<00:00, 808.96it/s]
_ test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_max_to_be_between:datetime_sqlite] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/6 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/6 [00:00<?, ?it/s]Calculating Metrics:  33%|███▎      | 2/6 [00:00<00:00, 3245.11it/s]Calculating Metrics:  33%|███▎      | 2/6 [00:00<00:00, 995.92it/s]Calculating Metrics:  67%|██████▋   | 4/6 [00:00<00:00, 1346.70it/s]Calculating Metrics:  67%|██████▋   | 4/6 [00:00<00:00, 965.65it/s] Calculating Metrics:  83%|████████▎ | 5/6 [00:00<00:00, 1068.78it/s]Calculating Metrics:  83%|████████▎ | 5/6 [00:00<00:00, 905.27it/s]alculating Metrics: 100%|██████████| 6/6 [00:00<00:00, 903.59it/s]Calculating Metrics: 100%|██████████| 6/6 [00:00<00:00, 837.49it/s]Calculating Metrics: 100%|██████████| 6/6 [00:00<00:00, 827.66it/s]Calculating Metrics: 100%|██████████| 6/6 [00:00<00:00, 816.30it/s]
_ test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_max_to_be_between:datetime_with_evaluation_parameter_sqlite] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/6 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/6 [00:00<?, ?it/s]Calculating Metrics:  33%|███▎      | 2/6 [00:00<00:00, 3159.55it/s]Calculating Metrics:  33%|███▎      | 2/6 [00:00<00:00, 1006.55it/s]Calculating Metrics:  67%|██████▋   | 4/6 [00:00<00:00, 1091.70it/s]Calculating Metrics:  67%|██████▋   | 4/6 [00:00<00:00, 835.06it/s]Calculating Metrics:  83%|████████▎ | 5/6 [00:00<00:00, 942.33it/s]Calculating Metrics:  83%|████████▎ | 5/6 [00:00<00:00, 815.12it/s]Calculating Metrics: 100%|██████████| 6/6 [00:00<00:00, 849.02it/s]Calculating Metrics: 100%|██████████| 6/6 [00:00<00:00, 791.55it/s]Calculating Metrics: 100%|██████████| 6/6 [00:00<00:00, 783.18it/s]Calculating Metrics: 100%|██████████| 6/6 [00:00<00:00, 773.31it/s]
_ test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_max_to_be_between:test_empty_column_should_be_false_no_observed_value_with_which_to_compare] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/6 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/6 [00:00<?, ?it/s]Calculating Metrics:  33%|███▎      | 2/6 [00:00<00:00, 3303.90it/s]Calculating Metrics:  33%|███▎      | 2/6 [00:00<00:00, 997.46it/s]Calculating Metrics:  67%|██████▋   | 4/6 [00:00<00:00, 1268.12it/s]Calculating Metrics:  67%|██████▋   | 4/6 [00:00<00:00, 929.33it/s] Calculating Metrics:  83%|████████▎ | 5/6 [00:00<00:00, 1032.42it/s]Calculating Metrics:  83%|████████▎ | 5/6 [00:00<00:00, 866.95it/s]alculating Metrics: 100%|██████████| 6/6 [00:00<00:00, 867.91it/s]Calculating Metrics: 100%|██████████| 6/6 [00:00<00:00, 803.30it/s]Calculating Metrics: 100%|██████████| 6/6 [00:00<00:00, 794.28it/s]Calculating Metrics: 100%|██████████| 6/6 [00:00<00:00, 783.49it/s]
_ test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_mean_to_be_between:simple_mean_positive_range] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/6 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/6 [00:00<?, ?it/s]Calculating Metrics:  33%|███▎      | 2/6 [00:00<00:00, 3334.10it/s]Calculating Metrics:  33%|███▎      | 2/6 [00:00<00:00, 1006.55it/s]Calculating Metrics:  67%|██████▋   | 4/6 [00:00<00:00, 1302.17it/s]Calculating Metrics:  67%|██████▋   | 4/6 [00:00<00:00, 946.10it/s]Calculating Metrics:  83%|████████▎ | 5/6 [00:00<00:00, 1040.82it/s]Calculating Metrics:  83%|████████▎ | 5/6 [00:00<00:00, 888.13it/s] Calculating Metrics: 100%|██████████| 6/6 [00:00<00:00, 844.12it/s]Calculating Metrics: 100%|██████████| 6/6 [00:00<00:00, 787.39it/s]Calculating Metrics: 100%|██████████| 6/6 [00:00<00:00, 778.62it/s]Calculating Metrics: 100%|██████████| 6/6 [00:00<00:00, 768.45it/s]
_ test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_mean_to_be_between:simple_mean_negative_range] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/6 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/6 [00:00<?, ?it/s]Calculating Metrics:  33%|███▎      | 2/6 [00:00<00:00, 3165.51it/s]Calculating Metrics:  33%|███▎      | 2/6 [00:00<00:00, 1031.43it/s]Calculating Metrics:  67%|██████▋   | 4/6 [00:00<00:00, 1404.07it/s]Calculating Metrics:  67%|██████▋   | 4/6 [00:00<00:00, 1021.20it/s]Calculating Metrics:  83%|████████▎ | 5/6 [00:00<00:00, 1117.35it/s]Calculating Metrics:  83%|████████▎ | 5/6 [00:00<00:00, 948.25it/s]Calculating Metrics: 100%|██████████| 6/6 [00:00<00:00, 961.41it/s]Calculating Metrics: 100%|██████████| 6/6 [00:00<00:00, 891.80it/s]Calculating Metrics: 100%|██████████| 6/6 [00:00<00:00, 881.59it/s]Calculating Metrics: 100%|██████████| 6/6 [00:00<00:00, 869.68it/s]
_ test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_mean_to_be_between:simple_mean_positive_exact_match0] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/6 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/6 [00:00<?, ?it/s]Calculating Metrics:  33%|███▎      | 2/6 [00:00<00:00, 3539.50it/s]Calculating Metrics:  33%|███▎      | 2/6 [00:00<00:00, 1072.99it/s]Calculating Metrics:  67%|██████▋   | 4/6 [00:00<00:00, 1454.59it/s]Calculating Metrics:  67%|██████▋   | 4/6 [00:00<00:00, 1043.36it/s]Calculating Metrics:  83%|████████▎ | 5/6 [00:00<00:00, 1140.81it/s]Calculating Metrics:  83%|████████▎ | 5/6 [00:00<00:00, 965.85it/s]Calculating Metrics: 100%|██████████| 6/6 [00:00<00:00, 945.80it/s]Calculating Metrics: 100%|██████████| 6/6 [00:00<00:00, 876.83it/s]Calculating Metrics: 100%|██████████| 6/6 [00:00<00:00, 866.68it/s]Calculating Metrics: 100%|██████████| 6/6 [00:00<00:00, 854.96it/s]
_ test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_mean_to_be_between:simple_mean_negative_exact_match] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/6 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/6 [00:00<?, ?it/s]Calculating Metrics:  33%|███▎      | 2/6 [00:00<00:00, 3453.52it/s]Calculating Metrics:  33%|███▎      | 2/6 [00:00<00:00, 1063.19it/s]Calculating Metrics:  67%|██████▋   | 4/6 [00:00<00:00, 1447.06it/s]Calculating Metrics:  67%|██████▋   | 4/6 [00:00<00:00, 1010.61it/s]Calculating Metrics:  83%|████████▎ | 5/6 [00:00<00:00, 1099.08it/s]Calculating Metrics:  83%|████████▎ | 5/6 [00:00<00:00, 929.18it/s]Calculating Metrics: 100%|██████████| 6/6 [00:00<00:00, 907.30it/s]Calculating Metrics: 100%|██████████| 6/6 [00:00<00:00, 839.39it/s]Calculating Metrics: 100%|██████████| 6/6 [00:00<00:00, 825.27it/s]Calculating Metrics: 100%|██████████| 6/6 [00:00<00:00, 812.22it/s]
_ test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_mean_to_be_between:simple_mean_positive_exact_match1] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/6 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/6 [00:00<?, ?it/s]Calculating Metrics:  33%|███▎      | 2/6 [00:00<00:00, 3081.78it/s]Calculating Metrics:  33%|███▎      | 2/6 [00:00<00:00, 1001.98it/s]Calculating Metrics:  67%|██████▋   | 4/6 [00:00<00:00, 1331.53it/s]Calculating Metrics:  67%|██████▋   | 4/6 [00:00<00:00, 978.26it/s]Calculating Metrics:  83%|████████▎ | 5/6 [00:00<00:00, 1071.83it/s]Calculating Metrics:  83%|████████▎ | 5/6 [00:00<00:00, 915.23it/s] Calculating Metrics: 100%|██████████| 6/6 [00:00<00:00, 902.65it/s]Calculating Metrics: 100%|██████████| 6/6 [00:00<00:00, 840.91it/s]Calculating Metrics: 100%|██████████| 6/6 [00:00<00:00, 831.74it/s]Calculating Metrics: 100%|██████████| 6/6 [00:00<00:00, 820.80it/s]
_ test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_mean_to_be_between:simple_mean_negative_range_match] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/6 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/6 [00:00<?, ?it/s]Calculating Metrics:  33%|███▎      | 2/6 [00:00<00:00, 3298.71it/s]Calculating Metrics:  33%|███▎      | 2/6 [00:00<00:00, 937.59it/s]Calculating Metrics:  67%|██████▋   | 4/6 [00:00<00:00, 1251.47it/s]Calculating Metrics:  67%|██████▋   | 4/6 [00:00<00:00, 889.85it/s] Calculating Metrics:  83%|████████▎ | 5/6 [00:00<00:00, 969.74it/s]Calculating Metrics:  83%|████████▎ | 5/6 [00:00<00:00, 812.72it/s]Calculating Metrics: 100%|██████████| 6/6 [00:00<00:00, 816.20it/s]Calculating Metrics: 100%|██████████| 6/6 [00:00<00:00, 754.51it/s]Calculating Metrics: 100%|██████████| 6/6 [00:00<00:00, 745.70it/s]Calculating Metrics: 100%|██████████| 6/6 [00:00<00:00, 735.28it/s]
_ test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_mean_to_be_between:simple_mean_includes_null] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/6 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/6 [00:00<?, ?it/s]Calculating Metrics:  33%|███▎      | 2/6 [00:00<00:00, 3466.37it/s]Calculating Metrics:  33%|███▎      | 2/6 [00:00<00:00, 1066.17it/s]Calculating Metrics:  67%|██████▋   | 4/6 [00:00<00:00, 1437.88it/s]Calculating Metrics:  67%|██████▋   | 4/6 [00:00<00:00, 1037.30it/s]Calculating Metrics:  83%|████████▎ | 5/6 [00:00<00:00, 1136.11it/s]Calculating Metrics:  83%|████████▎ | 5/6 [00:00<00:00, 961.38it/s]Calculating Metrics: 100%|██████████| 6/6 [00:00<00:00, 905.51it/s]Calculating Metrics: 100%|██████████| 6/6 [00:00<00:00, 838.83it/s]Calculating Metrics: 100%|██████████| 6/6 [00:00<00:00, 828.72it/s]Calculating Metrics: 100%|██████████| 6/6 [00:00<00:00, 815.56it/s]
_ test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_mean_to_be_between:vacuously_true_missing_min_and_max] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/6 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/6 [00:00<?, ?it/s]Calculating Metrics:  33%|███▎      | 2/6 [00:00<00:00, 3184.74it/s]Calculating Metrics:  33%|███▎      | 2/6 [00:00<00:00, 1008.61it/s]Calculating Metrics:  67%|██████▋   | 4/6 [00:00<00:00, 1357.71it/s]Calculating Metrics:  67%|██████▋   | 4/6 [00:00<00:00, 994.03it/s]Calculating Metrics:  83%|████████▎ | 5/6 [00:00<00:00, 1091.70it/s]Calculating Metrics:  83%|████████▎ | 5/6 [00:00<00:00, 931.07it/s] Calculating Metrics: 100%|██████████| 6/6 [00:00<00:00, 946.05it/s]Calculating Metrics: 100%|██████████| 6/6 [00:00<00:00, 875.24it/s]Calculating Metrics: 100%|██████████| 6/6 [00:00<00:00, 865.04it/s]Calculating Metrics: 100%|██████████| 6/6 [00:00<00:00, 852.99it/s]
_ test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_mean_to_be_between:coerced_types_true_false_values] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/6 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/6 [00:00<?, ?it/s]Calculating Metrics:  33%|███▎      | 2/6 [00:00<00:00, 3165.51it/s]Calculating Metrics:  33%|███▎      | 2/6 [00:00<00:00, 962.00it/s]Calculating Metrics:  67%|██████▋   | 4/6 [00:00<00:00, 1221.40it/s]Calculating Metrics:  67%|██████▋   | 4/6 [00:00<00:00, 910.52it/s] Calculating Metrics:  83%|████████▎ | 5/6 [00:00<00:00, 1007.86it/s]Calculating Metrics:  83%|████████▎ | 5/6 [00:00<00:00, 866.56it/s]alculating Metrics: 100%|██████████| 6/6 [00:00<00:00, 838.22it/s]Calculating Metrics: 100%|██████████| 6/6 [00:00<00:00, 783.71it/s]Calculating Metrics: 100%|██████████| 6/6 [00:00<00:00, 775.69it/s]Calculating Metrics: 100%|██████████| 6/6 [00:00<00:00, 766.11it/s]
_ test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_mean_to_be_between:coerced_types_true_false_and_null] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/6 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/6 [00:00<?, ?it/s]Calculating Metrics:  33%|███▎      | 2/6 [00:00<00:00, 3504.01it/s]Calculating Metrics:  33%|███▎      | 2/6 [00:00<00:00, 1025.75it/s]Calculating Metrics:  67%|██████▋   | 4/6 [00:00<00:00, 1392.42it/s]Calculating Metrics:  67%|██████▋   | 4/6 [00:00<00:00, 1009.40it/s]Calculating Metrics:  83%|████████▎ | 5/6 [00:00<00:00, 1106.21it/s]Calculating Metrics:  83%|████████▎ | 5/6 [00:00<00:00, 939.88it/s]Calculating Metrics: 100%|██████████| 6/6 [00:00<00:00, 930.24it/s]Calculating Metrics: 100%|██████████| 6/6 [00:00<00:00, 860.49it/s]Calculating Metrics: 100%|██████████| 6/6 [00:00<00:00, 850.11it/s]Calculating Metrics: 100%|██████████| 6/6 [00:00<00:00, 838.58it/s]
_ test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_mean_to_be_between:test_empty_column_should_be_false_no_observed_value_with_which_to_compare] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/6 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/6 [00:00<?, ?it/s]Calculating Metrics:  33%|███▎      | 2/6 [00:00<00:00, 3322.22it/s]Calculating Metrics:  33%|███▎      | 2/6 [00:00<00:00, 1036.91it/s]Calculating Metrics:  67%|██████▋   | 4/6 [00:00<00:00, 1289.17it/s]Calculating Metrics:  67%|██████▋   | 4/6 [00:00<00:00, 944.08it/s]Calculating Metrics:  83%|████████▎ | 5/6 [00:00<00:00, 1037.58it/s]Calculating Metrics:  83%|████████▎ | 5/6 [00:00<00:00, 882.79it/s] Calculating Metrics: 100%|██████████| 6/6 [00:00<00:00, 846.62it/s]Calculating Metrics: 100%|██████████| 6/6 [00:00<00:00, 784.84it/s]Calculating Metrics: 100%|██████████| 6/6 [00:00<00:00, 775.50it/s]Calculating Metrics: 100%|██████████| 6/6 [00:00<00:00, 765.78it/s]
_ test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_distinct_values_to_equal_set:basic_positive_test] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/5 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/5 [00:00<?, ?it/s]Calculating Metrics:  40%|████      | 2/5 [00:00<00:00, 3629.86it/s]Calculating Metrics:  40%|████      | 2/5 [00:00<00:00, 1323.75it/s]Calculating Metrics:  80%|████████  | 4/5 [00:00<00:00, 1566.94it/s]Calculating Metrics:  80%|████████  | 4/5 [00:00<00:00, 1225.15it/s]Calculating Metrics: 100%|██████████| 5/5 [00:00<00:00, 1087.00it/s]Calculating Metrics: 100%|██████████| 5/5 [00:00<00:00, 986.01it/s]Calculating Metrics: 100%|██████████| 5/5 [00:00<00:00, 967.72it/s]Calculating Metrics: 100%|██████████| 5/5 [00:00<00:00, 948.77it/s]
_ test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_distinct_values_to_equal_set:positive_test_with_null_values_in_column] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/5 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/5 [00:00<?, ?it/s]Calculating Metrics:  40%|████      | 2/5 [00:00<00:00, 3514.29it/s]Calculating Metrics:  40%|████      | 2/5 [00:00<00:00, 1278.75it/s]Calculating Metrics:  80%|████████  | 4/5 [00:00<00:00, 1592.37it/s]Calculating Metrics:  80%|████████  | 4/5 [00:00<00:00, 1218.39it/s]Calculating Metrics: 100%|██████████| 5/5 [00:00<00:00, 1100.35it/s]Calculating Metrics: 100%|██████████| 5/5 [00:00<00:00, 1004.82it/s]Calculating Metrics: 100%|██████████| 5/5 [00:00<00:00, 988.29it/s]Calculating Metrics: 100%|██████████| 5/5 [00:00<00:00, 969.38it/s]
_ test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_distinct_values_to_equal_set:positive_test_with_duplicate_values_in_column] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/5 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/5 [00:00<?, ?it/s]Calculating Metrics:  40%|████      | 2/5 [00:00<00:00, 2555.17it/s]Calculating Metrics:  40%|████      | 2/5 [00:00<00:00, 1128.26it/s]Calculating Metrics:  80%|████████  | 4/5 [00:00<00:00, 1478.82it/s]Calculating Metrics:  80%|████████  | 4/5 [00:00<00:00, 1160.49it/s]Calculating Metrics: 100%|██████████| 5/5 [00:00<00:00, 1023.00it/s]Calculating Metrics: 100%|██████████| 5/5 [00:00<00:00, 928.35it/s]Calculating Metrics: 100%|██████████| 5/5 [00:00<00:00, 913.27it/s]Calculating Metrics: 100%|██████████| 5/5 [00:00<00:00, 895.61it/s]
_ test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_distinct_values_to_equal_set:positive_test_duplicate_and_null_values] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/5 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/5 [00:00<?, ?it/s]Calculating Metrics:  40%|████      | 2/5 [00:00<00:00, 3344.74it/s]Calculating Metrics:  40%|████      | 2/5 [00:00<00:00, 1277.19it/s]Calculating Metrics:  80%|████████  | 4/5 [00:00<00:00, 1609.02it/s]Calculating Metrics:  80%|████████  | 4/5 [00:00<00:00, 1249.61it/s]Calculating Metrics: 100%|██████████| 5/5 [00:00<00:00, 1126.11it/s]Calculating Metrics: 100%|██████████| 5/5 [00:00<00:00, 1031.76it/s]Calculating Metrics: 100%|██████████| 5/5 [00:00<00:00, 1015.03it/s]Calculating Metrics: 100%|██████████| 5/5 [00:00<00:00, 995.80it/s]
_ test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_distinct_values_to_equal_set:basic_negative_test_set_contained] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/5 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/5 [00:00<?, ?it/s]Calculating Metrics:  40%|████      | 2/5 [00:00<00:00, 3591.01it/s]Calculating Metrics:  40%|████      | 2/5 [00:00<00:00, 1312.16it/s]Calculating Metrics:  80%|████████  | 4/5 [00:00<00:00, 1666.06it/s]Calculating Metrics:  80%|████████  | 4/5 [00:00<00:00, 1286.10it/s]Calculating Metrics: 100%|██████████| 5/5 [00:00<00:00, 1215.32it/s]Calculating Metrics: 100%|██████████| 5/5 [00:00<00:00, 1105.22it/s]Calculating Metrics: 100%|██████████| 5/5 [00:00<00:00, 1085.99it/s]Calculating Metrics: 100%|██████████| 5/5 [00:00<00:00, 1063.79it/s]
_ test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_distinct_values_to_equal_set:negative_test_some_set_intersection] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/5 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/5 [00:00<?, ?it/s]Calculating Metrics:  40%|████      | 2/5 [00:00<00:00, 3464.94it/s]Calculating Metrics:  40%|████      | 2/5 [00:00<00:00, 1299.35it/s]Calculating Metrics:  80%|████████  | 4/5 [00:00<00:00, 1644.02it/s]Calculating Metrics:  80%|████████  | 4/5 [00:00<00:00, 1272.93it/s]Calculating Metrics: 100%|██████████| 5/5 [00:00<00:00, 1200.71it/s]Calculating Metrics: 100%|██████████| 5/5 [00:00<00:00, 1095.29it/s]Calculating Metrics: 100%|██████████| 5/5 [00:00<00:00, 1076.62it/s]Calculating Metrics: 100%|██████████| 5/5 [00:00<00:00, 1055.70it/s]
_ test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_distinct_values_to_equal_set:negative_test_with_null_values_in_column] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/5 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/5 [00:00<?, ?it/s]Calculating Metrics:  40%|████      | 2/5 [00:00<00:00, 3407.23it/s]Calculating Metrics:  40%|████      | 2/5 [00:00<00:00, 1021.88it/s]Calculating Metrics:  80%|████████  | 4/5 [00:00<00:00, 1301.67it/s]Calculating Metrics:  80%|████████  | 4/5 [00:00<00:00, 1046.81it/s]Calculating Metrics: 100%|██████████| 5/5 [00:00<00:00, 1008.63it/s]Calculating Metrics: 100%|██████████| 5/5 [00:00<00:00, 929.88it/s]Calculating Metrics: 100%|██████████| 5/5 [00:00<00:00, 915.23it/s]Calculating Metrics: 100%|██████████| 5/5 [00:00<00:00, 899.10it/s]
_ test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_distinct_values_to_equal_set:negative_test_with_duplicate_values_in_column] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/5 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/5 [00:00<?, ?it/s]Calculating Metrics:  40%|████      | 2/5 [00:00<00:00, 3311.73it/s]Calculating Metrics:  40%|████      | 2/5 [00:00<00:00, 1242.39it/s]Calculating Metrics:  80%|████████  | 4/5 [00:00<00:00, 1431.14it/s]Calculating Metrics:  80%|████████  | 4/5 [00:00<00:00, 1126.82it/s]Calculating Metrics: 100%|██████████| 5/5 [00:00<00:00, 1072.49it/s]Calculating Metrics: 100%|██████████| 5/5 [00:00<00:00, 985.64it/s]Calculating Metrics: 100%|██████████| 5/5 [00:00<00:00, 970.36it/s]Calculating Metrics: 100%|██████████| 5/5 [00:00<00:00, 952.77it/s]
_ test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_distinct_values_to_equal_set:negative_test_duplicate_and_null_values] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/5 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/5 [00:00<?, ?it/s]Calculating Metrics:  40%|████      | 2/5 [00:00<00:00, 3540.99it/s]Calculating Metrics:  40%|████      | 2/5 [00:00<00:00, 1272.54it/s]Calculating Metrics:  80%|████████  | 4/5 [00:00<00:00, 1613.66it/s]Calculating Metrics:  80%|████████  | 4/5 [00:00<00:00, 1237.26it/s]Calculating Metrics: 100%|██████████| 5/5 [00:00<00:00, 1176.85it/s]Calculating Metrics: 100%|██████████| 5/5 [00:00<00:00, 1065.90it/s]Calculating Metrics: 100%|██████████| 5/5 [00:00<00:00, 1047.16it/s]Calculating Metrics: 100%|██████████| 5/5 [00:00<00:00, 1025.30it/s]
_ test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_sum_to_be_between:basic_positive_test_case] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/6 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/6 [00:00<?, ?it/s]Calculating Metrics:  33%|███▎      | 2/6 [00:00<00:00, 3195.66it/s]Calculating Metrics:  33%|███▎      | 2/6 [00:00<00:00, 1029.91it/s]Calculating Metrics:  67%|██████▋   | 4/6 [00:00<00:00, 1314.93it/s]Calculating Metrics:  67%|██████▋   | 4/6 [00:00<00:00, 934.77it/s]Calculating Metrics:  83%|████████▎ | 5/6 [00:00<00:00, 1027.31it/s]Calculating Metrics:  83%|████████▎ | 5/6 [00:00<00:00, 877.65it/s] Calculating Metrics: 100%|██████████| 6/6 [00:00<00:00, 882.61it/s]Calculating Metrics: 100%|██████████| 6/6 [00:00<00:00, 820.13it/s]Calculating Metrics: 100%|██████████| 6/6 [00:00<00:00, 810.94it/s]Calculating Metrics: 100%|██████████| 6/6 [00:00<00:00, 800.44it/s]
_ test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_sum_to_be_between:basic_negative_test_case] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/6 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/6 [00:00<?, ?it/s]Calculating Metrics:  33%|███▎      | 2/6 [00:00<00:00, 2762.14it/s]Calculating Metrics:  33%|███▎      | 2/6 [00:00<00:00, 898.04it/s]Calculating Metrics:  67%|██████▋   | 4/6 [00:00<00:00, 1166.46it/s]Calculating Metrics:  67%|██████▋   | 4/6 [00:00<00:00, 878.16it/s] Calculating Metrics:  83%|████████▎ | 5/6 [00:00<00:00, 985.04it/s]Calculating Metrics:  83%|████████▎ | 5/6 [00:00<00:00, 846.17it/s]Calculating Metrics: 100%|██████████| 6/6 [00:00<00:00, 878.08it/s]Calculating Metrics: 100%|██████████| 6/6 [00:00<00:00, 816.33it/s]Calculating Metrics: 100%|██████████| 6/6 [00:00<00:00, 806.78it/s]Calculating Metrics: 100%|██████████| 6/6 [00:00<00:00, 796.26it/s]
_ test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_sum_to_be_between:test_case_with_result_format_summary] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/6 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/6 [00:00<?, ?it/s]Calculating Metrics:  33%|███▎      | 2/6 [00:00<00:00, 3159.55it/s]Calculating Metrics:  33%|███▎      | 2/6 [00:00<00:00, 1004.98it/s]Calculating Metrics:  67%|██████▋   | 4/6 [00:00<00:00, 1368.01it/s]Calculating Metrics:  67%|██████▋   | 4/6 [00:00<00:00, 988.64it/s]Calculating Metrics:  83%|████████▎ | 5/6 [00:00<00:00, 1098.62it/s]Calculating Metrics:  83%|████████▎ | 5/6 [00:00<00:00, 929.01it/s] Calculating Metrics: 100%|██████████| 6/6 [00:00<00:00, 950.55it/s]Calculating Metrics: 100%|██████████| 6/6 [00:00<00:00, 874.75it/s]Calculating Metrics: 100%|██████████| 6/6 [00:00<00:00, 864.54it/s]Calculating Metrics: 100%|██████████| 6/6 [00:00<00:00, 850.83it/s]
_ test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_sum_to_be_between:test_case_with_only_a_lower_bound_and_a_missing_value] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/6 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/6 [00:00<?, ?it/s]Calculating Metrics:  33%|███▎      | 2/6 [00:00<00:00, 3372.98it/s]Calculating Metrics:  33%|███▎      | 2/6 [00:00<00:00, 1050.41it/s]Calculating Metrics:  67%|██████▋   | 4/6 [00:00<00:00, 1422.52it/s]Calculating Metrics:  67%|██████▋   | 4/6 [00:00<00:00, 1028.27it/s]Calculating Metrics:  83%|████████▎ | 5/6 [00:00<00:00, 1143.61it/s]Calculating Metrics:  83%|████████▎ | 5/6 [00:00<00:00, 967.32it/s]Calculating Metrics: 100%|██████████| 6/6 [00:00<00:00, 975.46it/s]Calculating Metrics: 100%|██████████| 6/6 [00:00<00:00, 902.65it/s]Calculating Metrics: 100%|██████████| 6/6 [00:00<00:00, 892.22it/s]Calculating Metrics: 100%|██████████| 6/6 [00:00<00:00, 879.89it/s]
_ test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_sum_to_be_between:negative_test_case_with_only_a_lower_bound] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/6 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/6 [00:00<?, ?it/s]Calculating Metrics:  33%|███▎      | 2/6 [00:00<00:00, 3148.88it/s]Calculating Metrics:  33%|███▎      | 2/6 [00:00<00:00, 1020.26it/s]Calculating Metrics:  67%|██████▋   | 4/6 [00:00<00:00, 1334.60it/s]Calculating Metrics:  67%|██████▋   | 4/6 [00:00<00:00, 950.77it/s]Calculating Metrics:  83%|████████▎ | 5/6 [00:00<00:00, 1055.76it/s]Calculating Metrics:  83%|████████▎ | 5/6 [00:00<00:00, 900.72it/s] Calculating Metrics: 100%|██████████| 6/6 [00:00<00:00, 919.80it/s]Calculating Metrics: 100%|██████████| 6/6 [00:00<00:00, 851.89it/s]Calculating Metrics: 100%|██████████| 6/6 [00:00<00:00, 841.61it/s]Calculating Metrics: 100%|██████████| 6/6 [00:00<00:00, 829.95it/s]
_ test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_sum_to_be_between:negative_test_case_with_only_a_max] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/6 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/6 [00:00<?, ?it/s]Calculating Metrics:  33%|███▎      | 2/6 [00:00<00:00, 3283.21it/s]Calculating Metrics:  33%|███▎      | 2/6 [00:00<00:00, 960.56it/s]Calculating Metrics:  67%|██████▋   | 4/6 [00:00<00:00, 1317.41it/s]Calculating Metrics:  67%|██████▋   | 4/6 [00:00<00:00, 947.12it/s] Calculating Metrics:  83%|████████▎ | 5/6 [00:00<00:00, 1039.64it/s]Calculating Metrics:  83%|████████▎ | 5/6 [00:00<00:00, 878.87it/s]alculating Metrics: 100%|██████████| 6/6 [00:00<00:00, 863.80it/s]Calculating Metrics: 100%|██████████| 6/6 [00:00<00:00, 806.55it/s]Calculating Metrics: 100%|██████████| 6/6 [00:00<00:00, 798.05it/s]Calculating Metrics: 100%|██████████| 6/6 [00:00<00:00, 788.21it/s]
_ test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_sum_to_be_between:raise_valueerror_with_both_max_and_min_are_missing] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/6 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/6 [00:00<?, ?it/s]Calculating Metrics:  33%|███▎      | 2/6 [00:00<00:00, 3485.09it/s]Calculating Metrics:  33%|███▎      | 2/6 [00:00<00:00, 1052.66it/s]Calculating Metrics:  67%|██████▋   | 4/6 [00:00<00:00, 1370.46it/s]Calculating Metrics:  67%|██████▋   | 4/6 [00:00<00:00, 998.05it/s]Calculating Metrics:  83%|████████▎ | 5/6 [00:00<00:00, 1109.19it/s]Calculating Metrics:  83%|████████▎ | 5/6 [00:00<00:00, 939.16it/s] Calculating Metrics: 100%|██████████| 6/6 [00:00<00:00, 959.10it/s]Calculating Metrics: 100%|██████████| 6/6 [00:00<00:00, 885.87it/s]Calculating Metrics: 100%|██████████| 6/6 [00:00<00:00, 875.42it/s]Calculating Metrics: 100%|██████████| 6/6 [00:00<00:00, 861.22it/s]
_ test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_unique_value_count_to_be_between:basic_positive_test] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/6 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/6 [00:00<?, ?it/s]Calculating Metrics:  33%|███▎      | 2/6 [00:00<00:00, 2828.26it/s]Calculating Metrics:  33%|███▎      | 2/6 [00:00<00:00, 830.14it/s]Calculating Metrics:  67%|██████▋   | 4/6 [00:00<00:00, 1051.99it/s]Calculating Metrics:  67%|██████▋   | 4/6 [00:00<00:00, 771.44it/s] Calculating Metrics:  83%|████████▎ | 5/6 [00:00<00:00, 856.75it/s]Calculating Metrics:  83%|████████▎ | 5/6 [00:00<00:00, 731.71it/s]Calculating Metrics: 100%|██████████| 6/6 [00:00<00:00, 716.06it/s]Calculating Metrics: 100%|██████████| 6/6 [00:00<00:00, 664.29it/s]Calculating Metrics: 100%|██████████| 6/6 [00:00<00:00, 656.08it/s]Calculating Metrics: 100%|██████████| 6/6 [00:00<00:00, 647.44it/s]
_ test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_unique_value_count_to_be_between:positive_test_with_null_values_in_column] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/6 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/6 [00:00<?, ?it/s]Calculating Metrics:  33%|███▎      | 2/6 [00:00<00:00, 2967.32it/s]Calculating Metrics:  33%|███▎      | 2/6 [00:00<00:00, 886.09it/s]Calculating Metrics:  67%|██████▋   | 4/6 [00:00<00:00, 1116.92it/s]Calculating Metrics:  67%|██████▋   | 4/6 [00:00<00:00, 800.13it/s] Calculating Metrics:  83%|████████▎ | 5/6 [00:00<00:00, 891.72it/s]Calculating Metrics:  83%|████████▎ | 5/6 [00:00<00:00, 745.15it/s]Calculating Metrics: 100%|██████████| 6/6 [00:00<00:00, 744.31it/s]Calculating Metrics: 100%|██████████| 6/6 [00:00<00:00, 691.16it/s]Calculating Metrics: 100%|██████████| 6/6 [00:00<00:00, 683.37it/s]Calculating Metrics: 100%|██████████| 6/6 [00:00<00:00, 674.45it/s]
_ test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_unique_value_count_to_be_between:null_min_duplicate_values_in_column] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/6 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/6 [00:00<?, ?it/s]Calculating Metrics:  33%|███▎      | 2/6 [00:00<00:00, 3447.85it/s]Calculating Metrics:  33%|███▎      | 2/6 [00:00<00:00, 998.52it/s]Calculating Metrics:  67%|██████▋   | 4/6 [00:00<00:00, 1361.90it/s]Calculating Metrics:  67%|██████▋   | 4/6 [00:00<00:00, 974.17it/s] Calculating Metrics:  83%|████████▎ | 5/6 [00:00<00:00, 1086.21it/s]Calculating Metrics:  83%|████████▎ | 5/6 [00:00<00:00, 927.82it/s]alculating Metrics: 100%|██████████| 6/6 [00:00<00:00, 931.10it/s]Calculating Metrics: 100%|██████████| 6/6 [00:00<00:00, 864.66it/s]Calculating Metrics: 100%|██████████| 6/6 [00:00<00:00, 854.44it/s]Calculating Metrics: 100%|██████████| 6/6 [00:00<00:00, 843.05it/s]
_ test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_unique_value_count_to_be_between:null_max_duplicate_and_null_values] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/6 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/6 [00:00<?, ?it/s]Calculating Metrics:  33%|███▎      | 2/6 [00:00<00:00, 3184.74it/s]Calculating Metrics:  33%|███▎      | 2/6 [00:00<00:00, 1004.50it/s]Calculating Metrics:  67%|██████▋   | 4/6 [00:00<00:00, 1354.09it/s]Calculating Metrics:  67%|██████▋   | 4/6 [00:00<00:00, 980.15it/s]Calculating Metrics:  83%|████████▎ | 5/6 [00:00<00:00, 1087.40it/s]Calculating Metrics:  83%|████████▎ | 5/6 [00:00<00:00, 922.84it/s] Calculating Metrics: 100%|██████████| 6/6 [00:00<00:00, 910.45it/s]Calculating Metrics: 100%|██████████| 6/6 [00:00<00:00, 843.53it/s]Calculating Metrics: 100%|██████████| 6/6 [00:00<00:00, 833.80it/s]Calculating Metrics: 100%|██████████| 6/6 [00:00<00:00, 822.25it/s]
_ test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_unique_value_count_to_be_between:vacuously_true_null_min_and_max] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/6 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/6 [00:00<?, ?it/s]Calculating Metrics:  33%|███▎      | 2/6 [00:00<00:00, 3435.14it/s]Calculating Metrics:  33%|███▎      | 2/6 [00:00<00:00, 1050.94it/s]Calculating Metrics:  67%|██████▋   | 4/6 [00:00<00:00, 1429.55it/s]Calculating Metrics:  67%|██████▋   | 4/6 [00:00<00:00, 1031.43it/s]Calculating Metrics:  83%|████████▎ | 5/6 [00:00<00:00, 1146.30it/s]Calculating Metrics:  83%|████████▎ | 5/6 [00:00<00:00, 970.10it/s]Calculating Metrics: 100%|██████████| 6/6 [00:00<00:00, 960.60it/s]Calculating Metrics: 100%|██████████| 6/6 [00:00<00:00, 888.75it/s]Calculating Metrics: 100%|██████████| 6/6 [00:00<00:00, 878.20it/s]Calculating Metrics: 100%|██████████| 6/6 [00:00<00:00, 866.17it/s]
_ test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_median_to_be_between:positive_test_min_equal_max] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:  22%|██▏       | 2/9 [00:00<00:00, 3350.08it/s]Calculating Metrics:  22%|██▏       | 2/9 [00:00<00:00, 632.86it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 925.69it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 610.84it/s]Calculating Metrics:  56%|█████▌    | 5/9 [00:00<00:00, 704.00it/s]Calculating Metrics:  56%|█████▌    | 5/9 [00:00<00:00, 552.29it/s]Calculating Metrics:  67%|██████▋   | 6/9 [00:00<00:00, 621.30it/s]Calculating Metrics:  67%|██████▋   | 6/9 [00:00<00:00, 531.01it/s]Calculating Metrics:  78%|███████▊  | 7/9 [00:00<00:00, 545.72it/s]Calculating Metrics:  78%|███████▊  | 7/9 [00:00<00:00, 494.87it/s]Calculating Metrics:  89%|████████▉ | 8/9 [00:00<00:00, 553.82it/s]Calculating Metrics:  89%|████████▉ | 8/9 [00:00<00:00, 510.33it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 507.78it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 484.41it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 481.85it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 479.13it/s]
_ test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_median_to_be_between:positive_test_null_min] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:  22%|██▏       | 2/9 [00:00<00:00, 3396.20it/s]Calculating Metrics:  22%|██▏       | 2/9 [00:00<00:00, 666.50it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 1025.69it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 661.48it/s] Calculating Metrics:  56%|█████▌    | 5/9 [00:00<00:00, 761.96it/s]Calculating Metrics:  56%|█████▌    | 5/9 [00:00<00:00, 590.88it/s]Calculating Metrics:  67%|██████▋   | 6/9 [00:00<00:00, 663.57it/s]Calculating Metrics:  67%|██████▋   | 6/9 [00:00<00:00, 530.88it/s]Calculating Metrics:  78%|███████▊  | 7/9 [00:00<00:00, 557.54it/s]Calculating Metrics:  78%|███████▊  | 7/9 [00:00<00:00, 506.66it/s]Calculating Metrics:  89%|████████▉ | 8/9 [00:00<00:00, 568.34it/s]Calculating Metrics:  89%|████████▉ | 8/9 [00:00<00:00, 527.64it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 564.41it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 537.47it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 534.96it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 531.94it/s]
_ test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_median_to_be_between:positive_test_null_max] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:  22%|██▏       | 2/9 [00:00<00:00, 3158.36it/s]Calculating Metrics:  22%|██▏       | 2/9 [00:00<00:00, 618.81it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 938.53it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 603.21it/s]Calculating Metrics:  56%|█████▌    | 5/9 [00:00<00:00, 692.02it/s]Calculating Metrics:  56%|█████▌    | 5/9 [00:00<00:00, 546.45it/s]Calculating Metrics:  67%|██████▋   | 6/9 [00:00<00:00, 616.13it/s]Calculating Metrics:  67%|██████▋   | 6/9 [00:00<00:00, 532.71it/s]Calculating Metrics:  78%|███████▊  | 7/9 [00:00<00:00, 567.53it/s]Calculating Metrics:  78%|███████▊  | 7/9 [00:00<00:00, 515.61it/s]Calculating Metrics:  89%|████████▉ | 8/9 [00:00<00:00, 578.37it/s]Calculating Metrics:  89%|████████▉ | 8/9 [00:00<00:00, 536.26it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 571.93it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 543.54it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 540.90it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 537.72it/s]
_ test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_median_to_be_between:negative_test_min_equal_max] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:  22%|██▏       | 2/9 [00:00<00:00, 3189.58it/s]Calculating Metrics:  22%|██▏       | 2/9 [00:00<00:00, 627.56it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 25.88it/s] Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 25.88it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 25.88it/s]Calculating Metrics:  56%|█████▌    | 5/9 [00:00<00:00, 25.88it/s]Calculating Metrics:  56%|█████▌    | 5/9 [00:00<00:00, 25.88it/s]Calculating Metrics:  67%|██████▋   | 6/9 [00:00<00:00, 25.88it/s]Calculating Metrics:  67%|██████▋   | 6/9 [00:00<00:00, 25.88it/s]Calculating Metrics:  78%|███████▊  | 7/9 [00:00<00:00, 25.88it/s]Calculating Metrics:  78%|███████▊  | 7/9 [00:00<00:00, 25.88it/s]Calculating Metrics:  89%|████████▉ | 8/9 [00:00<00:00, 25.88it/s]Calculating Metrics:  89%|████████▉ | 8/9 [00:00<00:00, 25.88it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 25.88it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 25.88it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 25.88it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 49.48it/s]
_ test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_median_to_be_between:negative_test_null_min] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:  22%|██▏       | 2/9 [00:00<00:00, 2593.88it/s]Calculating Metrics:  22%|██▏       | 2/9 [00:00<00:00, 500.36it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 771.01it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 497.52it/s]Calculating Metrics:  56%|█████▌    | 5/9 [00:00<00:00, 573.71it/s]Calculating Metrics:  56%|█████▌    | 5/9 [00:00<00:00, 446.87it/s]Calculating Metrics:  67%|██████▋   | 6/9 [00:00<00:00, 503.44it/s]Calculating Metrics:  67%|██████▋   | 6/9 [00:00<00:00, 431.53it/s]Calculating Metrics:  78%|███████▊  | 7/9 [00:00<00:00, 460.13it/s]Calculating Metrics:  78%|███████▊  | 7/9 [00:00<00:00, 417.84it/s]Calculating Metrics:  89%|████████▉ | 8/9 [00:00<00:00, 468.79it/s]Calculating Metrics:  89%|████████▉ | 8/9 [00:00<00:00, 432.99it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 462.44it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 438.06it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 435.50it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 432.86it/s]
_ test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_median_to_be_between:negative_test_null_max] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:  22%|██▏       | 2/9 [00:00<00:00, 2717.40it/s]Calculating Metrics:  22%|██▏       | 2/9 [00:00<00:00, 518.55it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 802.55it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 524.58it/s]Calculating Metrics:  56%|█████▌    | 5/9 [00:00<00:00, 605.76it/s]Calculating Metrics:  56%|█████▌    | 5/9 [00:00<00:00, 471.07it/s]Calculating Metrics:  67%|██████▋   | 6/9 [00:00<00:00, 528.36it/s]Calculating Metrics:  67%|██████▋   | 6/9 [00:00<00:00, 452.86it/s]Calculating Metrics:  78%|███████▊  | 7/9 [00:00<00:00, 473.90it/s]Calculating Metrics:  78%|███████▊  | 7/9 [00:00<00:00, 429.56it/s]Calculating Metrics:  89%|████████▉ | 8/9 [00:00<00:00, 481.67it/s]Calculating Metrics:  89%|████████▉ | 8/9 [00:00<00:00, 446.06it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 476.85it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 453.38it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 451.19it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 448.59it/s]
_ test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_median_to_be_between:positive_test_missing_value_in_column_exact_min_and_max] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:  22%|██▏       | 2/9 [00:00<00:00, 2890.63it/s]Calculating Metrics:  22%|██▏       | 2/9 [00:00<00:00, 643.89it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 995.21it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 637.75it/s]Calculating Metrics:  56%|█████▌    | 5/9 [00:00<00:00, 732.17it/s]Calculating Metrics:  56%|█████▌    | 5/9 [00:00<00:00, 564.72it/s]Calculating Metrics:  67%|██████▋   | 6/9 [00:00<00:00, 631.50it/s]Calculating Metrics:  67%|██████▋   | 6/9 [00:00<00:00, 541.71it/s]Calculating Metrics:  78%|███████▊  | 7/9 [00:00<00:00, 562.47it/s]Calculating Metrics:  78%|███████▊  | 7/9 [00:00<00:00, 509.31it/s]Calculating Metrics:  89%|████████▉ | 8/9 [00:00<00:00, 566.73it/s]Calculating Metrics:  89%|████████▉ | 8/9 [00:00<00:00, 522.74it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 546.97it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 520.50it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 517.96it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 514.87it/s]
_ test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_median_to_be_between:negative_test_missing_value_in_column_complete_result_format] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:  22%|██▏       | 2/9 [00:00<00:00, 3396.20it/s]Calculating Metrics:  22%|██▏       | 2/9 [00:00<00:00, 666.93it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 1024.13it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 657.47it/s] Calculating Metrics:  56%|█████▌    | 5/9 [00:00<00:00, 757.81it/s]Calculating Metrics:  56%|█████▌    | 5/9 [00:00<00:00, 588.51it/s]Calculating Metrics:  67%|██████▋   | 6/9 [00:00<00:00, 657.31it/s]Calculating Metrics:  67%|██████▋   | 6/9 [00:00<00:00, 560.85it/s]Calculating Metrics:  78%|███████▊  | 7/9 [00:00<00:00, 586.60it/s]Calculating Metrics:  78%|███████▊  | 7/9 [00:00<00:00, 531.15it/s]Calculating Metrics:  89%|████████▉ | 8/9 [00:00<00:00, 595.46it/s]Calculating Metrics:  89%|████████▉ | 8/9 [00:00<00:00, 550.73it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 588.43it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 559.17it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 556.49it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 553.20it/s]
_ test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_median_to_be_between:vacuously_true_both_min_and_max_null] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:  22%|██▏       | 2/9 [00:00<00:00, 3294.82it/s]Calculating Metrics:  22%|██▏       | 2/9 [00:00<00:00, 630.53it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 958.10it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 620.16it/s]Calculating Metrics:  56%|█████▌    | 5/9 [00:00<00:00, 713.17it/s]Calculating Metrics:  56%|█████▌    | 5/9 [00:00<00:00, 560.92it/s]Calculating Metrics:  67%|██████▋   | 6/9 [00:00<00:00, 631.51it/s]Calculating Metrics:  67%|██████▋   | 6/9 [00:00<00:00, 542.75it/s]Calculating Metrics:  78%|███████▊  | 7/9 [00:00<00:00, 576.23it/s]Calculating Metrics:  78%|███████▊  | 7/9 [00:00<00:00, 522.66it/s]Calculating Metrics:  89%|████████▉ | 8/9 [00:00<00:00, 585.91it/s]Calculating Metrics:  89%|████████▉ | 8/9 [00:00<00:00, 542.60it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 579.26it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 551.06it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 548.42it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 545.19it/s]
_ test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_median_to_be_between:test_empty_column_should_be_false_no_observed_value_with_which_to_compare] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:  22%|██▏       | 2/9 [00:00<00:00, 3306.51it/s]Calculating Metrics:  22%|██▏       | 2/9 [00:00<00:00, 649.37it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 954.44it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 629.73it/s]Calculating Metrics:  56%|█████▌    | 5/9 [00:00<00:00, 729.09it/s]Calculating Metrics:  56%|█████▌    | 5/9 [00:00<00:00, 571.70it/s]Calculating Metrics:  67%|██████▋   | 6/9 [00:00<00:00, 644.47it/s]Calculating Metrics:  67%|██████▋   | 6/9 [00:00<00:00, 553.10it/s]Calculating Metrics:  78%|███████▊  | 7/9 [00:00<00:00, 570.73it/s]Calculating Metrics:  78%|███████▊  | 7/9 [00:00<00:00, 518.10it/s]Calculating Metrics:  89%|████████▉ | 8/9 [00:00<00:00, 580.93it/s]Calculating Metrics:  89%|████████▉ | 8/9 [00:00<00:00, 534.73it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 576.70it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 544.24it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 540.90it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 537.51it/s]
_ test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_min_to_be_between:basic_positive_test_case] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/6 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/6 [00:00<?, ?it/s]Calculating Metrics:  33%|███▎      | 2/6 [00:00<00:00, 3179.91it/s]Calculating Metrics:  33%|███▎      | 2/6 [00:00<00:00, 980.44it/s]Calculating Metrics:  67%|██████▋   | 4/6 [00:00<00:00, 1228.47it/s]Calculating Metrics:  67%|██████▋   | 4/6 [00:00<00:00, 914.29it/s] Calculating Metrics:  83%|████████▎ | 5/6 [00:00<00:00, 1025.00it/s]Calculating Metrics:  83%|████████▎ | 5/6 [00:00<00:00, 879.53it/s]alculating Metrics: 100%|██████████| 6/6 [00:00<00:00, 873.36it/s]Calculating Metrics: 100%|██████████| 6/6 [00:00<00:00, 813.66it/s]Calculating Metrics: 100%|██████████| 6/6 [00:00<00:00, 805.05it/s]Calculating Metrics: 100%|██████████| 6/6 [00:00<00:00, 794.83it/s]
_ test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_min_to_be_between:negative_test_case_with_max_value_none] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/6 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/6 [00:00<?, ?it/s]Calculating Metrics:  33%|███▎      | 2/6 [00:00<00:00, 3267.86it/s]Calculating Metrics:  33%|███▎      | 2/6 [00:00<00:00, 1019.89it/s]Calculating Metrics:  67%|██████▋   | 4/6 [00:00<00:00, 1382.09it/s]Calculating Metrics:  67%|██████▋   | 4/6 [00:00<00:00, 1001.21it/s]Calculating Metrics:  83%|████████▎ | 5/6 [00:00<00:00, 1115.03it/s]Calculating Metrics:  83%|████████▎ | 5/6 [00:00<00:00, 945.86it/s]Calculating Metrics: 100%|██████████| 6/6 [00:00<00:00, 972.93it/s]Calculating Metrics: 100%|██████████| 6/6 [00:00<00:00, 900.90it/s]Calculating Metrics: 100%|██████████| 6/6 [00:00<00:00, 890.48it/s]Calculating Metrics: 100%|██████████| 6/6 [00:00<00:00, 878.33it/s]
_ test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_min_to_be_between:test_case_with_result_format_summary_also_verifies_that_max_value_is_inclusive] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/6 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/6 [00:00<?, ?it/s]Calculating Metrics:  33%|███▎      | 2/6 [00:00<00:00, 3559.02it/s]Calculating Metrics:  33%|███▎      | 2/6 [00:00<00:00, 1048.18it/s]Calculating Metrics:  67%|██████▋   | 4/6 [00:00<00:00, 1345.41it/s]Calculating Metrics:  67%|██████▋   | 4/6 [00:00<00:00, 966.65it/s]Calculating Metrics:  83%|████████▎ | 5/6 [00:00<00:00, 1080.12it/s]Calculating Metrics:  83%|████████▎ | 5/6 [00:00<00:00, 921.34it/s] Calculating Metrics: 100%|██████████| 6/6 [00:00<00:00, 919.97it/s]Calculating Metrics: 100%|██████████| 6/6 [00:00<00:00, 851.55it/s]Calculating Metrics: 100%|██████████| 6/6 [00:00<00:00, 841.41it/s]Calculating Metrics: 100%|██████████| 6/6 [00:00<00:00, 829.87it/s]
_ test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_min_to_be_between:test_case_with_only_a_lower_bound_and_a_missing_value] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/6 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/6 [00:00<?, ?it/s]Calculating Metrics:  33%|███▎      | 2/6 [00:00<00:00, 3377.06it/s]Calculating Metrics:  33%|███▎      | 2/6 [00:00<00:00, 1043.75it/s]Calculating Metrics:  67%|██████▋   | 4/6 [00:00<00:00, 1417.35it/s]Calculating Metrics:  67%|██████▋   | 4/6 [00:00<00:00, 985.68it/s]Calculating Metrics:  83%|████████▎ | 5/6 [00:00<00:00, 1092.67it/s]Calculating Metrics:  83%|████████▎ | 5/6 [00:00<00:00, 923.86it/s] Calculating Metrics: 100%|██████████| 6/6 [00:00<00:00, 926.03it/s]Calculating Metrics: 100%|██████████| 6/6 [00:00<00:00, 849.82it/s]Calculating Metrics: 100%|██████████| 6/6 [00:00<00:00, 838.41it/s]Calculating Metrics: 100%|██████████| 6/6 [00:00<00:00, 826.76it/s]
_ test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_min_to_be_between:negative_test_case_with_only_a_lower_bound] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/6 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/6 [00:00<?, ?it/s]Calculating Metrics:  33%|███▎      | 2/6 [00:00<00:00, 3533.53it/s]Calculating Metrics:  33%|███▎      | 2/6 [00:00<00:00, 1050.94it/s]Calculating Metrics:  67%|██████▋   | 4/6 [00:00<00:00, 1422.04it/s]Calculating Metrics:  67%|██████▋   | 4/6 [00:00<00:00, 1026.19it/s]Calculating Metrics:  83%|████████▎ | 5/6 [00:00<00:00, 1136.61it/s]Calculating Metrics:  83%|████████▎ | 5/6 [00:00<00:00, 954.77it/s]Calculating Metrics: 100%|██████████| 6/6 [00:00<00:00, 971.69it/s]Calculating Metrics: 100%|██████████| 6/6 [00:00<00:00, 891.55it/s]Calculating Metrics: 100%|██████████| 6/6 [00:00<00:00, 880.17it/s]Calculating Metrics: 100%|██████████| 6/6 [00:00<00:00, 866.95it/s]
_ test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_min_to_be_between:test_on_a_series_with_mostly_nulls] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/6 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/6 [00:00<?, ?it/s]Calculating Metrics:  33%|███▎      | 2/6 [00:00<00:00, 3631.43it/s]Calculating Metrics:  33%|███▎      | 2/6 [00:00<00:00, 1074.22it/s]Calculating Metrics:  67%|██████▋   | 4/6 [00:00<00:00, 1432.73it/s]Calculating Metrics:  67%|██████▋   | 4/6 [00:00<00:00, 1004.32it/s]Calculating Metrics:  83%|████████▎ | 5/6 [00:00<00:00, 1118.84it/s]Calculating Metrics:  83%|████████▎ | 5/6 [00:00<00:00, 948.64it/s]Calculating Metrics: 100%|██████████| 6/6 [00:00<00:00, 953.94it/s]Calculating Metrics: 100%|██████████| 6/6 [00:00<00:00, 884.59it/s]Calculating Metrics: 100%|██████████| 6/6 [00:00<00:00, 874.69it/s]Calculating Metrics: 100%|██████████| 6/6 [00:00<00:00, 863.20it/s]
_ test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_min_to_be_between:negative_test_case_with_only_a_max] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/6 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/6 [00:00<?, ?it/s]Calculating Metrics:  33%|███▎      | 2/6 [00:00<00:00, 2403.61it/s]Calculating Metrics:  33%|███▎      | 2/6 [00:00<00:00, 729.95it/s]Calculating Metrics:  67%|██████▋   | 4/6 [00:00<00:00, 1013.42it/s]Calculating Metrics:  67%|██████▋   | 4/6 [00:00<00:00, 792.05it/s] Calculating Metrics:  83%|████████▎ | 5/6 [00:00<00:00, 902.16it/s]Calculating Metrics:  83%|████████▎ | 5/6 [00:00<00:00, 788.02it/s]Calculating Metrics: 100%|██████████| 6/6 [00:00<00:00, 815.67it/s]Calculating Metrics: 100%|██████████| 6/6 [00:00<00:00, 763.16it/s]Calculating Metrics: 100%|██████████| 6/6 [00:00<00:00, 755.55it/s]Calculating Metrics: 100%|██████████| 6/6 [00:00<00:00, 746.63it/s]
_ test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_min_to_be_between:raise_valueerror_with_both_max_and_min_are_missing] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/6 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/6 [00:00<?, ?it/s]Calculating Metrics:  33%|███▎      | 2/6 [00:00<00:00, 2923.88it/s]Calculating Metrics:  33%|███▎      | 2/6 [00:00<00:00, 869.20it/s]Calculating Metrics:  67%|██████▋   | 4/6 [00:00<00:00, 1186.59it/s]Calculating Metrics:  67%|██████▋   | 4/6 [00:00<00:00, 850.73it/s] Calculating Metrics:  83%|████████▎ | 5/6 [00:00<00:00, 944.11it/s]Calculating Metrics:  83%|████████▎ | 5/6 [00:00<00:00, 799.62it/s]Calculating Metrics: 100%|██████████| 6/6 [00:00<00:00, 826.36it/s]Calculating Metrics: 100%|██████████| 6/6 [00:00<00:00, 767.58it/s]Calculating Metrics: 100%|██████████| 6/6 [00:00<00:00, 759.10it/s]Calculating Metrics: 100%|██████████| 6/6 [00:00<00:00, 749.27it/s]
_ test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_proportion_of_unique_values_to_be_between:basic_positive_test] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 3090.87it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 502.49it/s]Calculating Metrics:  40%|████      | 4/10 [00:00<00:00, 152.43it/s]Calculating Metrics:  40%|████      | 4/10 [00:00<00:00, 129.69it/s]Calculating Metrics:  60%|██████    | 6/10 [00:00<00:00, 187.38it/s]Calculating Metrics:  60%|██████    | 6/10 [00:00<00:00, 170.40it/s]Calculating Metrics:  80%|████████  | 8/10 [00:00<00:00, 214.10it/s]Calculating Metrics:  80%|████████  | 8/10 [00:00<00:00, 201.95it/s]Calculating Metrics:  90%|█████████ | 9/10 [00:00<00:00, 218.66it/s]Calculating Metrics:  90%|█████████ | 9/10 [00:00<00:00, 209.50it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 229.23it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 222.68it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 222.21it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 221.62it/s]
_ test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_proportion_of_unique_values_to_be_between:positive_test_with_null_values_in_column] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 2857.16it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 434.13it/s]Calculating Metrics:  40%|████      | 4/10 [00:00<00:00, 702.36it/s]Calculating Metrics:  40%|████      | 4/10 [00:00<00:00, 426.54it/s]Calculating Metrics:  60%|██████    | 6/10 [00:00<00:00, 568.72it/s]Calculating Metrics:  60%|██████    | 6/10 [00:00<00:00, 454.96it/s]Calculating Metrics:  80%|████████  | 8/10 [00:00<00:00, 539.45it/s]Calculating Metrics:  80%|████████  | 8/10 [00:00<00:00, 481.55it/s]Calculating Metrics:  90%|█████████ | 9/10 [00:00<00:00, 500.33it/s]Calculating Metrics:  90%|█████████ | 9/10 [00:00<00:00, 463.56it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 501.29it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 476.62it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 474.88it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 472.75it/s]
_ test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_proportion_of_unique_values_to_be_between:duplicate_values_in_column] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 2968.37it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 488.05it/s]Calculating Metrics:  40%|████      | 4/10 [00:00<00:00, 791.94it/s]Calculating Metrics:  40%|████      | 4/10 [00:00<00:00, 490.79it/s]Calculating Metrics:  60%|██████    | 6/10 [00:00<00:00, 657.31it/s]Calculating Metrics:  60%|██████    | 6/10 [00:00<00:00, 520.83it/s]Calculating Metrics:  80%|████████  | 8/10 [00:00<00:00, 609.16it/s]Calculating Metrics:  80%|████████  | 8/10 [00:00<00:00, 537.17it/s]Calculating Metrics:  90%|█████████ | 9/10 [00:00<00:00, 554.67it/s]Calculating Metrics:  90%|█████████ | 9/10 [00:00<00:00, 509.75it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 549.82it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 520.67it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 518.62it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 516.08it/s]
_ test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_proportion_of_unique_values_to_be_between:null_max_duplicate_and_null_values] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 3423.92it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 522.56it/s]Calculating Metrics:  40%|████      | 4/10 [00:00<00:00, 835.44it/s]Calculating Metrics:  40%|████      | 4/10 [00:00<00:00, 517.80it/s]Calculating Metrics:  60%|██████    | 6/10 [00:00<00:00, 694.04it/s]Calculating Metrics:  60%|██████    | 6/10 [00:00<00:00, 539.32it/s]Calculating Metrics:  80%|████████  | 8/10 [00:00<00:00, 617.75it/s]Calculating Metrics:  80%|████████  | 8/10 [00:00<00:00, 530.25it/s]Calculating Metrics:  90%|█████████ | 9/10 [00:00<00:00, 541.78it/s]Calculating Metrics:  90%|█████████ | 9/10 [00:00<00:00, 499.16it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 538.55it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 510.33it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 508.36it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 505.87it/s]
_ test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_proportion_of_unique_values_to_be_between:vacuously_true_null_min_and_max] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 3040.45it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 504.94it/s]Calculating Metrics:  40%|████      | 4/10 [00:00<00:00, 814.67it/s]Calculating Metrics:  40%|████      | 4/10 [00:00<00:00, 508.79it/s]Calculating Metrics:  60%|██████    | 6/10 [00:00<00:00, 680.73it/s]Calculating Metrics:  60%|██████    | 6/10 [00:00<00:00, 534.57it/s]Calculating Metrics:  80%|████████  | 8/10 [00:00<00:00, 632.12it/s]Calculating Metrics:  80%|████████  | 8/10 [00:00<00:00, 551.12it/s]Calculating Metrics:  90%|█████████ | 9/10 [00:00<00:00, 568.65it/s]Calculating Metrics:  90%|█████████ | 9/10 [00:00<00:00, 518.41it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 557.48it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 527.09it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 524.88it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 522.11it/s]
_ test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_distinct_values_to_contain_set:basic_positive_test] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/5 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/5 [00:00<?, ?it/s]Calculating Metrics:  40%|████      | 2/5 [00:00<00:00, 2972.58it/s]Calculating Metrics:  40%|████      | 2/5 [00:00<00:00, 1181.33it/s]Calculating Metrics:  80%|████████  | 4/5 [00:00<00:00, 1434.69it/s]Calculating Metrics:  80%|████████  | 4/5 [00:00<00:00, 1141.85it/s]Calculating Metrics: 100%|██████████| 5/5 [00:00<00:00, 1034.15it/s]Calculating Metrics: 100%|██████████| 5/5 [00:00<00:00, 954.03it/s]Calculating Metrics: 100%|██████████| 5/5 [00:00<00:00, 939.84it/s]Calculating Metrics: 100%|██████████| 5/5 [00:00<00:00, 923.16it/s]
_ test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_distinct_values_to_contain_set:positive_test_with_null_values_in_column] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/5 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/5 [00:00<?, ?it/s]Calculating Metrics:  40%|████      | 2/5 [00:00<00:00, 2544.32it/s]Calculating Metrics:  40%|████      | 2/5 [00:00<00:00, 1026.63it/s]Calculating Metrics:  80%|████████  | 4/5 [00:00<00:00, 1299.55it/s]Calculating Metrics:  80%|████████  | 4/5 [00:00<00:00, 1004.98it/s]Calculating Metrics: 100%|██████████| 5/5 [00:00<00:00, 918.59it/s]Calculating Metrics: 100%|██████████| 5/5 [00:00<00:00, 842.03it/s]Calculating Metrics: 100%|██████████| 5/5 [00:00<00:00, 828.62it/s]Calculating Metrics: 100%|██████████| 5/5 [00:00<00:00, 813.29it/s]
_ test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_distinct_values_to_contain_set:positive_test_with_duplicate_values_in_column] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/5 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/5 [00:00<?, ?it/s]Calculating Metrics:  40%|████      | 2/5 [00:00<00:00, 3114.97it/s]Calculating Metrics:  40%|████      | 2/5 [00:00<00:00, 1172.25it/s]Calculating Metrics:  80%|████████  | 4/5 [00:00<00:00, 1461.94it/s]Calculating Metrics:  80%|████████  | 4/5 [00:00<00:00, 1144.81it/s]Calculating Metrics: 100%|██████████| 5/5 [00:00<00:00, 1044.71it/s]Calculating Metrics: 100%|██████████| 5/5 [00:00<00:00, 963.41it/s]Calculating Metrics: 100%|██████████| 5/5 [00:00<00:00, 949.11it/s]Calculating Metrics: 100%|██████████| 5/5 [00:00<00:00, 932.65it/s]
_ test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_distinct_values_to_contain_set:positive_test_duplicate_and_null_values] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/5 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/5 [00:00<?, ?it/s]Calculating Metrics:  40%|████      | 2/5 [00:00<00:00, 3447.85it/s]Calculating Metrics:  40%|████      | 2/5 [00:00<00:00, 1297.74it/s]Calculating Metrics:  80%|████████  | 4/5 [00:00<00:00, 1586.05it/s]Calculating Metrics:  80%|████████  | 4/5 [00:00<00:00, 1213.98it/s]Calculating Metrics: 100%|██████████| 5/5 [00:00<00:00, 1066.76it/s]Calculating Metrics: 100%|██████████| 5/5 [00:00<00:00, 979.61it/s]Calculating Metrics: 100%|██████████| 5/5 [00:00<00:00, 964.70it/s]Calculating Metrics: 100%|██████████| 5/5 [00:00<00:00, 947.31it/s]
_ test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_distinct_values_to_contain_set:basic_negative_test_no_set_intersection] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/5 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/5 [00:00<?, ?it/s]Calculating Metrics:  40%|████      | 2/5 [00:00<00:00, 3327.49it/s]Calculating Metrics:  40%|████      | 2/5 [00:00<00:00, 1273.90it/s]Calculating Metrics:  80%|████████  | 4/5 [00:00<00:00, 1503.74it/s]Calculating Metrics:  80%|████████  | 4/5 [00:00<00:00, 1178.59it/s]Calculating Metrics: 100%|██████████| 5/5 [00:00<00:00, 1107.90it/s]Calculating Metrics: 100%|██████████| 5/5 [00:00<00:00, 996.41it/s]Calculating Metrics: 100%|██████████| 5/5 [00:00<00:00, 977.88it/s]Calculating Metrics: 100%|██████████| 5/5 [00:00<00:00, 959.49it/s]
_ test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_distinct_values_to_contain_set:negative_test_some_set_intersection] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/5 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/5 [00:00<?, ?it/s]Calculating Metrics:  40%|████      | 2/5 [00:00<00:00, 3316.97it/s]Calculating Metrics:  40%|████      | 2/5 [00:00<00:00, 1257.10it/s]Calculating Metrics:  80%|████████  | 4/5 [00:00<00:00, 1605.78it/s]Calculating Metrics:  80%|████████  | 4/5 [00:00<00:00, 1245.34it/s]Calculating Metrics: 100%|██████████| 5/5 [00:00<00:00, 1186.84it/s]Calculating Metrics: 100%|██████████| 5/5 [00:00<00:00, 1074.58it/s]Calculating Metrics: 100%|██████████| 5/5 [00:00<00:00, 1055.54it/s]Calculating Metrics: 100%|██████████| 5/5 [00:00<00:00, 1034.56it/s]
_ test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_distinct_values_to_contain_set:negative_test_with_null_values_in_column] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/5 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/5 [00:00<?, ?it/s]Calculating Metrics:  40%|████      | 2/5 [00:00<00:00, 3396.20it/s]Calculating Metrics:  40%|████      | 2/5 [00:00<00:00, 1282.86it/s]Calculating Metrics:  80%|████████  | 4/5 [00:00<00:00, 1627.43it/s]Calculating Metrics:  80%|████████  | 4/5 [00:00<00:00, 1258.32it/s]Calculating Metrics: 100%|██████████| 5/5 [00:00<00:00, 1195.91it/s]Calculating Metrics: 100%|██████████| 5/5 [00:00<00:00, 1088.64it/s]Calculating Metrics: 100%|██████████| 5/5 [00:00<00:00, 1069.76it/s]Calculating Metrics: 100%|██████████| 5/5 [00:00<00:00, 1048.00it/s]
_ test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_distinct_values_to_contain_set:negative_test_with_duplicate_values_in_column] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/5 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/5 [00:00<?, ?it/s]Calculating Metrics:  40%|████      | 2/5 [00:00<00:00, 3145.33it/s]Calculating Metrics:  40%|████      | 2/5 [00:00<00:00, 1155.46it/s]Calculating Metrics:  80%|████████  | 4/5 [00:00<00:00, 1402.07it/s]Calculating Metrics:  80%|████████  | 4/5 [00:00<00:00, 1073.26it/s]Calculating Metrics: 100%|██████████| 5/5 [00:00<00:00, 909.95it/s]Calculating Metrics: 100%|██████████| 5/5 [00:00<00:00, 787.04it/s]Calculating Metrics: 100%|██████████| 5/5 [00:00<00:00, 766.64it/s]Calculating Metrics: 100%|██████████| 5/5 [00:00<00:00, 745.71it/s]
_ test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_distinct_values_to_contain_set:negative_test_duplicate_and_null_values] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/5 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/5 [00:00<?, ?it/s]Calculating Metrics:  40%|████      | 2/5 [00:00<00:00, 2789.69it/s]Calculating Metrics:  40%|████      | 2/5 [00:00<00:00, 1040.00it/s]Calculating Metrics:  80%|████████  | 4/5 [00:00<00:00, 1262.68it/s]Calculating Metrics:  80%|████████  | 4/5 [00:00<00:00, 967.15it/s]Calculating Metrics: 100%|██████████| 5/5 [00:00<00:00, 928.35it/s]Calculating Metrics: 100%|██████████| 5/5 [00:00<00:00, 848.29it/s]Calculating Metrics: 100%|██████████| 5/5 [00:00<00:00, 833.89it/s]Calculating Metrics: 100%|██████████| 5/5 [00:00<00:00, 817.92it/s]
_ test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_distinct_values_to_contain_set:positive_test_string_values] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/5 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/5 [00:00<?, ?it/s]Calculating Metrics:  40%|████      | 2/5 [00:00<00:00, 2962.08it/s]Calculating Metrics:  40%|████      | 2/5 [00:00<00:00, 1077.53it/s]Calculating Metrics:  80%|████████  | 4/5 [00:00<00:00, 1367.45it/s]Calculating Metrics:  80%|████████  | 4/5 [00:00<00:00, 1052.85it/s]Calculating Metrics: 100%|██████████| 5/5 [00:00<00:00, 922.84it/s]Calculating Metrics: 100%|██████████| 5/5 [00:00<00:00, 829.04it/s]Calculating Metrics: 100%|██████████| 5/5 [00:00<00:00, 813.39it/s]Calculating Metrics: 100%|██████████| 5/5 [00:00<00:00, 791.02it/s]
_ test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_distinct_values_to_contain_set:negative_test_string_values] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/5 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/5 [00:00<?, ?it/s]Calculating Metrics:  40%|████      | 2/5 [00:00<00:00, 1355.84it/s]Calculating Metrics:  40%|████      | 2/5 [00:00<00:00, 754.98it/s]Calculating Metrics:  80%|████████  | 4/5 [00:00<00:00, 1079.27it/s]Calculating Metrics:  80%|████████  | 4/5 [00:00<00:00, 875.82it/s] Calculating Metrics: 100%|██████████| 5/5 [00:00<00:00, 848.74it/s]Calculating Metrics: 100%|██████████| 5/5 [00:00<00:00, 782.23it/s]Calculating Metrics: 100%|██████████| 5/5 [00:00<00:00, 770.42it/s]Calculating Metrics: 100%|██████████| 5/5 [00:00<00:00, 756.88it/s]
_ test_case_runner_v3_api[pandas/column_distributional_expectations/expect_column_kl_divergence_to_be_less_than:discrete_baseline_categorical_fixed_partition] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/7 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/7 [00:00<?, ?it/s]Calculating Metrics:  29%|██▊       | 2/7 [00:00<00:00, 4405.78it/s]Calculating Metrics:  29%|██▊       | 2/7 [00:00<00:00, 1186.17it/s]Calculating Metrics:  43%|████▎     | 3/7 [00:00<00:00, 1575.62it/s]Calculating Metrics:  43%|████▎     | 3/7 [00:00<00:00, 1063.11it/s]Calculating Metrics:  71%|███████▏  | 5/7 [00:00<00:00, 1244.38it/s]Calculating Metrics:  71%|███████▏  | 5/7 [00:00<00:00, 1073.65it/s]Calculating Metrics:  86%|████████▌ | 6/7 [00:00<00:00, 1203.88it/s]Calculating Metrics:  86%|████████▌ | 6/7 [00:00<00:00, 1106.97it/s]Calculating Metrics: 100%|██████████| 7/7 [00:00<00:00, 1247.99it/s]Calculating Metrics: 100%|██████████| 7/7 [00:00<00:00, 1168.84it/s]Calculating Metrics: 100%|██████████| 7/7 [00:00<00:00, 1153.37it/s]Calculating Metrics: 100%|██████████| 7/7 [00:00<00:00, 1135.61it/s]
_ test_case_runner_v3_api[pandas/column_distributional_expectations/expect_column_kl_divergence_to_be_less_than:discrete_categorical_fixed_alternate_partition] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/7 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/7 [00:00<?, ?it/s]Calculating Metrics:  29%|██▊       | 2/7 [00:00<00:00, 5272.54it/s]Calculating Metrics:  29%|██▊       | 2/7 [00:00<00:00, 1443.82it/s]Calculating Metrics:  43%|████▎     | 3/7 [00:00<00:00, 1905.64it/s]Calculating Metrics:  43%|████▎     | 3/7 [00:00<00:00, 1213.98it/s]Calculating Metrics:  71%|███████▏  | 5/7 [00:00<00:00, 1405.97it/s]Calculating Metrics:  71%|███████▏  | 5/7 [00:00<00:00, 1196.05it/s]Calculating Metrics:  86%|████████▌ | 6/7 [00:00<00:00, 1332.09it/s]Calculating Metrics:  86%|████████▌ | 6/7 [00:00<00:00, 1215.51it/s]Calculating Metrics: 100%|██████████| 7/7 [00:00<00:00, 1366.10it/s]Calculating Metrics: 100%|██████████| 7/7 [00:00<00:00, 1273.87it/s]Calculating Metrics: 100%|██████████| 7/7 [00:00<00:00, 1256.91it/s]Calculating Metrics: 100%|██████████| 7/7 [00:00<00:00, 1236.89it/s]
_ test_case_runner_v3_api[pandas/column_distributional_expectations/expect_column_kl_divergence_to_be_less_than:internal_holdout_with_categorical_partition] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/7 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/7 [00:00<?, ?it/s]Calculating Metrics:  29%|██▊       | 2/7 [00:00<00:00, 4394.24it/s]Calculating Metrics:  29%|██▊       | 2/7 [00:00<00:00, 1255.40it/s]Calculating Metrics:  43%|████▎     | 3/7 [00:00<00:00, 1657.39it/s]Calculating Metrics:  43%|████▎     | 3/7 [00:00<00:00, 1052.26it/s]Calculating Metrics:  71%|███████▏  | 5/7 [00:00<00:00, 1198.51it/s]Calculating Metrics:  71%|███████▏  | 5/7 [00:00<00:00, 1001.84it/s]Calculating Metrics:  86%|████████▌ | 6/7 [00:00<00:00, 1111.12it/s]Calculating Metrics:  86%|████████▌ | 6/7 [00:00<00:00, 1007.92it/s]Calculating Metrics: 100%|██████████| 7/7 [00:00<00:00, 1131.24it/s]Calculating Metrics: 100%|██████████| 7/7 [00:00<00:00, 1053.28it/s]Calculating Metrics: 100%|██████████| 7/7 [00:00<00:00, 1038.86it/s]Calculating Metrics: 100%|██████████| 7/7 [00:00<00:00, 1021.44it/s]
_ test_case_runner_v3_api[pandas/column_distributional_expectations/expect_column_kl_divergence_to_be_less_than:holdout_0_1] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/7 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/7 [00:00<?, ?it/s]Calculating Metrics:  29%|██▊       | 2/7 [00:00<00:00, 3623.59it/s]Calculating Metrics:  29%|██▊       | 2/7 [00:00<00:00, 1079.75it/s]Calculating Metrics:  43%|████▎     | 3/7 [00:00<00:00, 1401.22it/s]Calculating Metrics:  43%|████▎     | 3/7 [00:00<00:00, 905.51it/s]Calculating Metrics:  71%|███████▏  | 5/7 [00:00<00:00, 1067.20it/s]Calculating Metrics:  71%|███████▏  | 5/7 [00:00<00:00, 911.73it/s] Calculating Metrics:  86%|████████▌ | 6/7 [00:00<00:00, 1015.20it/s]Calculating Metrics:  86%|████████▌ | 6/7 [00:00<00:00, 927.19it/s]alculating Metrics: 100%|██████████| 7/7 [00:00<00:00, 1043.36it/s]Calculating Metrics: 100%|██████████| 7/7 [00:00<00:00, 976.98it/s] Clculating Metrics: 100%|██████████| 7/7 [00:00<00:00, 964.56it/s]Calculating Metrics: 100%|██████████| 7/7 [00:00<00:00, 949.52it/s]
_ test_case_runner_v3_api[pandas/column_distributional_expectations/expect_column_kl_divergence_to_be_less_than:holdout_0_05] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/7 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/7 [00:00<?, ?it/s]Calculating Metrics:  29%|██▊       | 2/7 [00:00<00:00, 4346.43it/s]Calculating Metrics:  29%|██▊       | 2/7 [00:00<00:00, 1083.38it/s]Calculating Metrics:  43%|████▎     | 3/7 [00:00<00:00, 1429.23it/s]Calculating Metrics:  43%|████▎     | 3/7 [00:00<00:00, 915.72it/s]Calculating Metrics:  71%|███████▏  | 5/7 [00:00<00:00, 1073.65it/s]Calculating Metrics:  71%|███████▏  | 5/7 [00:00<00:00, 913.39it/s] Calculating Metrics:  86%|████████▌ | 6/7 [00:00<00:00, 1019.02it/s]Calculating Metrics:  86%|████████▌ | 6/7 [00:00<00:00, 931.83it/s]alculating Metrics: 100%|██████████| 7/7 [00:00<00:00, 1029.53it/s]Calculating Metrics: 100%|██████████| 7/7 [00:00<00:00, 950.94it/s] Clculating Metrics: 100%|██████████| 7/7 [00:00<00:00, 935.93it/s]Calculating Metrics: 100%|██████████| 7/7 [00:00<00:00, 920.44it/s]
_ test_case_runner_v3_api[pandas/column_distributional_expectations/expect_column_kl_divergence_to_be_less_than:missing_vals_no_holdout] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/7 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/7 [00:00<?, ?it/s]Calculating Metrics:  29%|██▊       | 2/7 [00:00<00:00, 4209.04it/s]Calculating Metrics:  29%|██▊       | 2/7 [00:00<00:00, 1140.22it/s]Calculating Metrics:  43%|████▎     | 3/7 [00:00<00:00, 1506.03it/s]Calculating Metrics:  43%|████▎     | 3/7 [00:00<00:00, 959.36it/s]Calculating Metrics:  71%|███████▏  | 5/7 [00:00<00:00, 1135.38it/s]Calculating Metrics:  71%|███████▏  | 5/7 [00:00<00:00, 961.91it/s] Calculating Metrics:  86%|████████▌ | 6/7 [00:00<00:00, 1046.74it/s]Calculating Metrics:  86%|████████▌ | 6/7 [00:00<00:00, 953.76it/s]alculating Metrics: 100%|██████████| 7/7 [00:00<00:00, 1070.33it/s]Calculating Metrics: 100%|██████████| 7/7 [00:00<00:00, 998.58it/s] Clculating Metrics: 100%|██████████| 7/7 [00:00<00:00, 985.37it/s]Calculating Metrics: 100%|██████████| 7/7 [00:00<00:00, 969.81it/s]
_ test_case_runner_v3_api[pandas/column_distributional_expectations/expect_column_kl_divergence_to_be_less_than:norm_0_1_auto_explicit_infinite_endpoints] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 3105.74it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 1339.61it/s]Calculating Metrics:  56%|█████▌    | 5/9 [00:00<00:00, 1540.78it/s]Calculating Metrics:  56%|█████▌    | 5/9 [00:00<00:00, 1026.66it/s]Calculating Metrics:  78%|███████▊  | 7/9 [00:00<00:00, 991.80it/s]Calculating Metrics:  78%|███████▊  | 7/9 [00:00<00:00, 868.90it/s]Calculating Metrics:  89%|████████▉ | 8/9 [00:00<00:00, 944.05it/s]Calculating Metrics:  89%|████████▉ | 8/9 [00:00<00:00, 868.61it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 951.76it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 890.91it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 882.33it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 872.08it/s]
_ test_case_runner_v3_api[pandas/column_distributional_expectations/expect_column_kl_divergence_to_be_less_than:norm_0_1_auto_inf_partition_test_tail_weights] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 4525.82it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 1845.27it/s]Calculating Metrics:  56%|█████▌    | 5/9 [00:00<00:00, 2117.91it/s]Calculating Metrics:  56%|█████▌    | 5/9 [00:00<00:00, 1392.07it/s]Calculating Metrics:  78%|███████▊  | 7/9 [00:00<00:00, 1515.52it/s]Calculating Metrics:  78%|███████▊  | 7/9 [00:00<00:00, 1278.14it/s]Calculating Metrics:  89%|████████▉ | 8/9 [00:00<00:00, 1373.66it/s]Calculating Metrics:  89%|████████▉ | 8/9 [00:00<00:00, 1247.47it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 1361.39it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 1262.12it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 1247.77it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 1231.53it/s]
_ test_case_runner_v3_api[pandas/column_distributional_expectations/expect_column_kl_divergence_to_be_less_than:norm_0_1_auto_inf_partition_should_fail_with_no_holdout] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 4352.07it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 1745.08it/s]Calculating Metrics:  56%|█████▌    | 5/9 [00:00<00:00, 1995.96it/s]Calculating Metrics:  56%|█████▌    | 5/9 [00:00<00:00, 1347.35it/s]Calculating Metrics:  78%|███████▊  | 7/9 [00:00<00:00, 1460.12it/s]Calculating Metrics:  78%|███████▊  | 7/9 [00:00<00:00, 1251.76it/s]Calculating Metrics:  89%|████████▉ | 8/9 [00:00<00:00, 1329.52it/s]Calculating Metrics:  89%|████████▉ | 8/9 [00:00<00:00, 1205.69it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 1316.53it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 1223.11it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 1210.13it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 1194.73it/s]
_ test_case_runner_v3_api[pandas/column_distributional_expectations/expect_column_kl_divergence_to_be_less_than:norm_0_1_auto_inf_partition_null_threshold_should_always_succeed] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 4430.21it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 1723.39it/s]Calculating Metrics:  56%|█████▌    | 5/9 [00:00<00:00, 1973.79it/s]Calculating Metrics:  56%|█████▌    | 5/9 [00:00<00:00, 1297.66it/s]Calculating Metrics:  78%|███████▊  | 7/9 [00:00<00:00, 1402.78it/s]Calculating Metrics:  78%|███████▊  | 7/9 [00:00<00:00, 1187.80it/s]Calculating Metrics:  89%|████████▉ | 8/9 [00:00<00:00, 1274.86it/s]Calculating Metrics:  89%|████████▉ | 8/9 [00:00<00:00, 1155.38it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 1258.71it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 1167.32it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 1155.00it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 1140.38it/s]
_ test_case_runner_v3_api[pandas/column_distributional_expectations/expect_column_kl_divergence_to_be_less_than:norm_0_1_auto_inf_partition_null_threshold_and_partition_object_supports_profiling] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/8 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/8 [00:00<?, ?it/s]Calculating Metrics:  25%|██▌       | 2/8 [00:00<00:00, 5275.85it/s]Calculating Metrics:  25%|██▌       | 2/8 [00:00<00:00, 988.52it/s]Calculating Metrics:  38%|███▊      | 3/8 [00:00<00:00, 1354.31it/s]Calculating Metrics:  38%|███▊      | 3/8 [00:00<00:00, 807.32it/s] Calculating Metrics:  62%|██████▎   | 5/8 [00:00<00:00, 984.99it/s]Calculating Metrics:  62%|██████▎   | 5/8 [00:00<00:00, 802.03it/s]Calculating Metrics:  75%|███████▌  | 6/8 [00:00<00:00, 914.22it/s]Calculating Metrics:  75%|███████▌  | 6/8 [00:00<00:00, 796.89it/s]Calculating Metrics:  88%|████████▊ | 7/8 [00:00<00:00, 906.71it/s]Calculating Metrics:  88%|████████▊ | 7/8 [00:00<00:00, 778.29it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 809.81it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 762.48it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 752.86it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 745.22it/s]
Calculating Metrics:   0%|          | 0/11 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/11 [00:00<?, ?it/s]Calculating Metrics:  36%|███▋      | 4/11 [00:00<00:00, 3770.16it/s]Calculating Metrics:  36%|███▋      | 4/11 [00:00<00:00, 1227.75it/s]Calculating Metrics:  45%|████▌     | 5/11 [00:00<00:00, 1440.15it/s]Calculating Metrics:  45%|████▌     | 5/11 [00:00<00:00, 891.57it/s]Calculating Metrics:  73%|███████▎  | 8/11 [00:00<00:00, 1045.21it/s]Calculating Metrics:  73%|███████▎  | 8/11 [00:00<00:00, 883.01it/s] Calculating Metrics:  82%|████████▏ | 9/11 [00:00<00:00, 957.85it/s]Calculating Metrics:  82%|████████▏ | 9/11 [00:00<00:00, 851.23it/s]Calculating Metrics:  91%|█████████ | 10/11 [00:00<00:00, 928.60it/s]Calculating Metrics:  91%|█████████ | 10/11 [00:00<00:00, 843.18it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 888.54it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 831.39it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 825.89it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 819.80it/s]
Calculating Metrics:   0%|          | 0/8 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/8 [00:00<?, ?it/s]Calculating Metrics:  25%|██▌       | 2/8 [00:00<00:00, 5540.69it/s]Calculating Metrics:  25%|██▌       | 2/8 [00:00<00:00, 968.89it/s]Calculating Metrics:  38%|███▊      | 3/8 [00:00<00:00, 1325.07it/s]Calculating Metrics:  38%|███▊      | 3/8 [00:00<00:00, 789.24it/s] Calculating Metrics:  62%|██████▎   | 5/8 [00:00<00:00, 966.74it/s]Calculating Metrics:  62%|██████▎   | 5/8 [00:00<00:00, 788.61it/s]Calculating Metrics:  75%|███████▌  | 6/8 [00:00<00:00, 899.68it/s]Calculating Metrics:  75%|███████▌  | 6/8 [00:00<00:00, 781.45it/s]Calculating Metrics:  88%|████████▊ | 7/8 [00:00<00:00, 887.98it/s]Calculating Metrics:  88%|████████▊ | 7/8 [00:00<00:00, 797.44it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 854.87it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 802.47it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 795.92it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 788.33it/s]
_ test_case_runner_v3_api[pandas/column_distributional_expectations/expect_column_kl_divergence_to_be_less_than:norm_0_1_auto_inf_partition_tail_weights_should_fail_with_no_internal_holdout] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 4107.03it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 1755.12it/s]Calculating Metrics:  56%|█████▌    | 5/9 [00:00<00:00, 2018.63it/s]Calculating Metrics:  56%|█████▌    | 5/9 [00:00<00:00, 1363.56it/s]Calculating Metrics:  78%|███████▊  | 7/9 [00:00<00:00, 1471.83it/s]Calculating Metrics:  78%|███████▊  | 7/9 [00:00<00:00, 1232.22it/s]Calculating Metrics:  89%|████████▉ | 8/9 [00:00<00:00, 1321.93it/s]Calculating Metrics:  89%|████████▉ | 8/9 [00:00<00:00, 1201.94it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 1312.54it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 1218.72it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 1205.95it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 1189.99it/s]
_ test_case_runner_v3_api[pandas/column_distributional_expectations/expect_column_kl_divergence_to_be_less_than:positive_norm_0_1_auto_partition] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 4439.59it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 1845.07it/s]Calculating Metrics:  56%|█████▌    | 5/9 [00:00<00:00, 2117.69it/s]Calculating Metrics:  56%|█████▌    | 5/9 [00:00<00:00, 1431.89it/s]Calculating Metrics:  78%|███████▊  | 7/9 [00:00<00:00, 1568.47it/s]Calculating Metrics:  78%|███████▊  | 7/9 [00:00<00:00, 1337.41it/s]Calculating Metrics:  89%|████████▉ | 8/9 [00:00<00:00, 1439.12it/s]Calculating Metrics:  89%|████████▉ | 8/9 [00:00<00:00, 1302.93it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 1421.10it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 1313.87it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 1299.08it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 1282.05it/s]
_ test_case_runner_v3_api[pandas/column_distributional_expectations/expect_column_kl_divergence_to_be_less_than:positive_norm_0_1_uniform_partition] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 4433.73it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 1916.74it/s]Calculating Metrics:  56%|█████▌    | 5/9 [00:00<00:00, 2189.09it/s]Calculating Metrics:  56%|█████▌    | 5/9 [00:00<00:00, 1500.54it/s]Calculating Metrics:  78%|███████▊  | 7/9 [00:00<00:00, 1633.93it/s]Calculating Metrics:  78%|███████▊  | 7/9 [00:00<00:00, 1393.06it/s]Calculating Metrics:  89%|████████▉ | 8/9 [00:00<00:00, 1494.83it/s]Calculating Metrics:  89%|████████▉ | 8/9 [00:00<00:00, 1356.89it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 1477.62it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 1351.16it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 1331.76it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 1312.91it/s]
_ test_case_runner_v3_api[pandas/column_distributional_expectations/expect_column_kl_divergence_to_be_less_than:positive_norm_0_1_ntile_partition] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 4565.23it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 1973.33it/s]Calculating Metrics:  56%|█████▌    | 5/9 [00:00<00:00, 2253.31it/s]Calculating Metrics:  56%|█████▌    | 5/9 [00:00<00:00, 1537.95it/s]Calculating Metrics:  78%|███████▊  | 7/9 [00:00<00:00, 1670.66it/s]Calculating Metrics:  78%|███████▊  | 7/9 [00:00<00:00, 1424.49it/s]Calculating Metrics:  89%|████████▉ | 8/9 [00:00<00:00, 1527.91it/s]Calculating Metrics:  89%|████████▉ | 8/9 [00:00<00:00, 1384.54it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 1507.30it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 1397.74it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 1381.42it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 1361.98it/s]
_ test_case_runner_v3_api[pandas/column_distributional_expectations/expect_column_kl_divergence_to_be_less_than:positive_norm_0_1_kde_partition] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 4673.32it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 1989.94it/s]Calculating Metrics:  56%|█████▌    | 5/9 [00:00<00:00, 2272.84it/s]Calculating Metrics:  56%|█████▌    | 5/9 [00:00<00:00, 1546.12it/s]Calculating Metrics:  78%|███████▊  | 7/9 [00:00<00:00, 1644.83it/s]Calculating Metrics:  78%|███████▊  | 7/9 [00:00<00:00, 1399.50it/s]Calculating Metrics:  89%|████████▉ | 8/9 [00:00<00:00, 1485.89it/s]Calculating Metrics:  89%|████████▉ | 8/9 [00:00<00:00, 1345.68it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 1465.34it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 1360.56it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 1344.71it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 1326.05it/s]
_ test_case_runner_v3_api[pandas/column_distributional_expectations/expect_column_kl_divergence_to_be_less_than:negative_norm_1_1_column_norm_0_1_auto_partition] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 4405.78it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 1793.01it/s]Calculating Metrics:  56%|█████▌    | 5/9 [00:00<00:00, 2058.45it/s]Calculating Metrics:  56%|█████▌    | 5/9 [00:00<00:00, 1378.16it/s]Calculating Metrics:  78%|███████▊  | 7/9 [00:00<00:00, 1474.94it/s]Calculating Metrics:  78%|███████▊  | 7/9 [00:00<00:00, 1264.00it/s]Calculating Metrics:  89%|████████▉ | 8/9 [00:00<00:00, 1363.34it/s]Calculating Metrics:  89%|████████▉ | 8/9 [00:00<00:00, 1239.18it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 1352.95it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 1254.69it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 1241.20it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 1225.13it/s]
_ test_case_runner_v3_api[pandas/column_distributional_expectations/expect_column_kl_divergence_to_be_less_than:negative_norm_1_1_column_norm_0_1_uniform_partition] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 4562.75it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 1788.23it/s]Calculating Metrics:  56%|█████▌    | 5/9 [00:00<00:00, 2033.50it/s]Calculating Metrics:  56%|█████▌    | 5/9 [00:00<00:00, 1394.01it/s]Calculating Metrics:  78%|███████▊  | 7/9 [00:00<00:00, 1502.49it/s]Calculating Metrics:  78%|███████▊  | 7/9 [00:00<00:00, 1291.58it/s]Calculating Metrics:  89%|████████▉ | 8/9 [00:00<00:00, 1388.90it/s]Calculating Metrics:  89%|████████▉ | 8/9 [00:00<00:00, 1270.09it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 1383.35it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 1287.03it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 1272.50it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 1255.28it/s]
_ test_case_runner_v3_api[pandas/column_distributional_expectations/expect_column_kl_divergence_to_be_less_than:negative_norm_1_1_column_norm_0_1_ntile_partition] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 3794.89it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 1752.74it/s]Calculating Metrics:  56%|█████▌    | 5/9 [00:00<00:00, 2002.25it/s]Calculating Metrics:  56%|█████▌    | 5/9 [00:00<00:00, 1401.37it/s]Calculating Metrics:  78%|███████▊  | 7/9 [00:00<00:00, 1540.08it/s]Calculating Metrics:  78%|███████▊  | 7/9 [00:00<00:00, 1324.02it/s]Calculating Metrics:  89%|████████▉ | 8/9 [00:00<00:00, 1425.06it/s]Calculating Metrics:  89%|████████▉ | 8/9 [00:00<00:00, 1298.50it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 1415.77it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 1298.90it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 1282.97it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 1265.63it/s]
_ test_case_runner_v3_api[pandas/column_distributional_expectations/expect_column_kl_divergence_to_be_less_than:negative_norm_1_1_column_norm_0_1_kde_partition] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 4618.01it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 1902.83it/s]Calculating Metrics:  56%|█████▌    | 5/9 [00:00<00:00, 2165.36it/s]Calculating Metrics:  56%|█████▌    | 5/9 [00:00<00:00, 1490.30it/s]Calculating Metrics:  78%|███████▊  | 7/9 [00:00<00:00, 1626.33it/s]Calculating Metrics:  78%|███████▊  | 7/9 [00:00<00:00, 1389.70it/s]Calculating Metrics:  89%|████████▉ | 8/9 [00:00<00:00, 1492.30it/s]Calculating Metrics:  89%|████████▉ | 8/9 [00:00<00:00, 1355.68it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 1476.81it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 1370.29it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 1354.41it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 1335.53it/s]
_ test_case_runner_v3_api[pandas/column_distributional_expectations/expect_column_kl_divergence_to_be_less_than:positive_bimodal_auto_partition] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 4560.27it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 1957.90it/s]Calculating Metrics:  56%|█████▌    | 5/9 [00:00<00:00, 2236.01it/s]Calculating Metrics:  56%|█████▌    | 5/9 [00:00<00:00, 1449.71it/s]Calculating Metrics:  78%|███████▊  | 7/9 [00:00<00:00, 1405.60it/s]Calculating Metrics:  78%|███████▊  | 7/9 [00:00<00:00, 1120.53it/s]Calculating Metrics:  89%|████████▉ | 8/9 [00:00<00:00, 1196.15it/s]Calculating Metrics:  89%|████████▉ | 8/9 [00:00<00:00, 1102.10it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 1204.84it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 1130.91it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 1119.54it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 1105.48it/s]
_ test_case_runner_v3_api[pandas/column_distributional_expectations/expect_column_kl_divergence_to_be_less_than:negative_bimodal_column_norm_0_1_auto_partition] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 4596.50it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 1809.84it/s]Calculating Metrics:  56%|█████▌    | 5/9 [00:00<00:00, 2009.54it/s]Calculating Metrics:  56%|█████▌    | 5/9 [00:00<00:00, 1362.05it/s]Calculating Metrics:  78%|███████▊  | 7/9 [00:00<00:00, 1493.17it/s]Calculating Metrics:  78%|███████▊  | 7/9 [00:00<00:00, 1280.15it/s]Calculating Metrics:  89%|████████▉ | 8/9 [00:00<00:00, 1380.73it/s]Calculating Metrics:  89%|████████▉ | 8/9 [00:00<00:00, 1253.53it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 1369.25it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 1269.38it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 1255.95it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 1239.66it/s]
_ test_case_runner_v3_api[pandas/column_distributional_expectations/expect_column_kl_divergence_to_be_less_than:negative_bimodal_column_norm_0_1_uniform_partition] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 4650.00it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 1947.22it/s]Calculating Metrics:  56%|█████▌    | 5/9 [00:00<00:00, 2215.22it/s]Calculating Metrics:  56%|█████▌    | 5/9 [00:00<00:00, 1512.88it/s]Calculating Metrics:  78%|███████▊  | 7/9 [00:00<00:00, 1639.68it/s]Calculating Metrics:  78%|███████▊  | 7/9 [00:00<00:00, 1397.17it/s]Calculating Metrics:  89%|████████▉ | 8/9 [00:00<00:00, 1498.37it/s]Calculating Metrics:  89%|████████▉ | 8/9 [00:00<00:00, 1360.52it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 1481.74it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 1374.03it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 1357.87it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 1339.27it/s]
_ test_case_runner_v3_api[pandas/column_distributional_expectations/expect_column_kl_divergence_to_be_less_than:manual_partition_lower_inf_bound] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 4490.69it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 2017.22it/s]Calculating Metrics:  56%|█████▌    | 5/9 [00:00<00:00, 2303.30it/s]Calculating Metrics:  56%|█████▌    | 5/9 [00:00<00:00, 1581.44it/s]Calculating Metrics:  78%|███████▊  | 7/9 [00:00<00:00, 1660.73it/s]Calculating Metrics:  78%|███████▊  | 7/9 [00:00<00:00, 1410.46it/s]Calculating Metrics:  89%|████████▉ | 8/9 [00:00<00:00, 1493.10it/s]Calculating Metrics:  89%|████████▉ | 8/9 [00:00<00:00, 1297.44it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 1406.75it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 1313.14it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 1298.14it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 1280.05it/s]
_ test_case_runner_v3_api[pandas/column_distributional_expectations/expect_column_kl_divergence_to_be_less_than:manual_partition_lower_inf_bound_tail_weights] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 4609.13it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 2031.39it/s]Calculating Metrics:  56%|█████▌    | 5/9 [00:00<00:00, 2312.70it/s]Calculating Metrics:  56%|█████▌    | 5/9 [00:00<00:00, 1589.23it/s]Calculating Metrics:  78%|███████▊  | 7/9 [00:00<00:00, 1629.04it/s]Calculating Metrics:  78%|███████▊  | 7/9 [00:00<00:00, 1398.37it/s]Calculating Metrics:  89%|████████▉ | 8/9 [00:00<00:00, 1484.84it/s]Calculating Metrics:  89%|████████▉ | 8/9 [00:00<00:00, 1351.09it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 1470.31it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 1372.13it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 1356.06it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 1336.52it/s]
_ test_case_runner_v3_api[pandas/column_distributional_expectations/expect_column_kl_divergence_to_be_less_than:manual_partition_upper_inf_bound] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 4591.47it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 1956.75it/s]Calculating Metrics:  56%|█████▌    | 5/9 [00:00<00:00, 2217.10it/s]Calculating Metrics:  56%|█████▌    | 5/9 [00:00<00:00, 1533.12it/s]Calculating Metrics:  78%|███████▊  | 7/9 [00:00<00:00, 1657.55it/s]Calculating Metrics:  78%|███████▊  | 7/9 [00:00<00:00, 1401.64it/s]Calculating Metrics:  89%|████████▉ | 8/9 [00:00<00:00, 1475.18it/s]Calculating Metrics:  89%|████████▉ | 8/9 [00:00<00:00, 1341.37it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 1460.75it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 1362.77it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 1347.30it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 1328.81it/s]
_ test_case_runner_v3_api[pandas/column_distributional_expectations/expect_column_kl_divergence_to_be_less_than:manual_partition_upper_inf_bound_tail_weights] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 5020.11it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 2117.53it/s]Calculating Metrics:  56%|█████▌    | 5/9 [00:00<00:00, 2404.99it/s]Calculating Metrics:  56%|█████▌    | 5/9 [00:00<00:00, 1649.09it/s]Calculating Metrics:  78%|███████▊  | 7/9 [00:00<00:00, 1749.08it/s]Calculating Metrics:  78%|███████▊  | 7/9 [00:00<00:00, 1487.12it/s]Calculating Metrics:  89%|████████▉ | 8/9 [00:00<00:00, 1588.60it/s]Calculating Metrics:  89%|████████▉ | 8/9 [00:00<00:00, 1444.32it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 1570.38it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 1459.40it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 1441.40it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 1419.55it/s]
_ test_case_runner_v3_api[pandas/column_distributional_expectations/expect_column_kl_divergence_to_be_less_than:manual_partition_lower_and_upper_inf_bounds0] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 3880.02it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 1716.16it/s]Calculating Metrics:  56%|█████▌    | 5/9 [00:00<00:00, 1963.99it/s]Calculating Metrics:  56%|█████▌    | 5/9 [00:00<00:00, 1410.13it/s]Calculating Metrics:  78%|███████▊  | 7/9 [00:00<00:00, 1536.46it/s]Calculating Metrics:  78%|███████▊  | 7/9 [00:00<00:00, 1328.51it/s]Calculating Metrics:  89%|████████▉ | 8/9 [00:00<00:00, 1430.16it/s]Calculating Metrics:  89%|████████▉ | 8/9 [00:00<00:00, 1310.46it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 1428.58it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 1333.36it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 1318.32it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 1300.20it/s]
_ test_case_runner_v3_api[pandas/column_distributional_expectations/expect_column_kl_divergence_to_be_less_than:manual_partition_lower_and_upper_inf_bounds1] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 4723.32it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 2056.79it/s]Calculating Metrics:  56%|█████▌    | 5/9 [00:00<00:00, 2341.09it/s]Calculating Metrics:  56%|█████▌    | 5/9 [00:00<00:00, 1607.75it/s]Calculating Metrics:  78%|███████▊  | 7/9 [00:00<00:00, 1753.58it/s]Calculating Metrics:  78%|███████▊  | 7/9 [00:00<00:00, 1493.85it/s]Calculating Metrics:  89%|████████▉ | 8/9 [00:00<00:00, 1595.85it/s]Calculating Metrics:  89%|████████▉ | 8/9 [00:00<00:00, 1450.12it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 1577.33it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 1465.34it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 1447.42it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 1426.58it/s]
_ test_case_runner_v3_api[pandas/column_distributional_expectations/expect_column_kl_divergence_to_be_less_than:tail_weight_holdout_is_not_defined_for_partitions_already_extending_to_inifinity] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 4762.20it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 2059.06it/s]Calculating Metrics:  56%|█████▌    | 5/9 [00:00<00:00, 2342.14it/s]Calculating Metrics:  56%|█████▌    | 5/9 [00:00<00:00, 1606.77it/s]Calculating Metrics:  78%|███████▊  | 7/9 [00:00<00:00, 1761.04it/s]Calculating Metrics:  78%|███████▊  | 7/9 [00:00<00:00, 1498.65it/s]Calculating Metrics:  89%|████████▉ | 8/9 [00:00<00:00, 1602.33it/s]Calculating Metrics:  89%|████████▉ | 8/9 [00:00<00:00, 1431.20it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 1551.21it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 1440.52it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 1422.76it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 1402.10it/s]
_ test_case_runner_v3_api[pandas/column_distributional_expectations/expect_column_kl_divergence_to_be_less_than:tail_weight_holdout_is_not_defined_for_partitions_already_extending_to_inifinity_tail_weight] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 3617.34it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 1781.02it/s]Calculating Metrics:  56%|█████▌    | 5/9 [00:00<00:00, 2044.21it/s]Calculating Metrics:  56%|█████▌    | 5/9 [00:00<00:00, 1448.11it/s]Calculating Metrics:  78%|███████▊  | 7/9 [00:00<00:00, 1533.09it/s]Calculating Metrics:  78%|███████▊  | 7/9 [00:00<00:00, 1319.02it/s]Calculating Metrics:  89%|████████▉ | 8/9 [00:00<00:00, 1412.16it/s]Calculating Metrics:  89%|████████▉ | 8/9 [00:00<00:00, 1292.84it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 1406.91it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 1316.16it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 1301.73it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 1283.88it/s]
_ test_case_runner_v3_api[pandas/column_distributional_expectations/expect_column_kl_divergence_to_be_less_than:too_big_tail_weight] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 4519.72it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 1973.33it/s]Calculating Metrics:  56%|█████▌    | 5/9 [00:00<00:00, 2254.27it/s]Calculating Metrics:  56%|█████▌    | 5/9 [00:00<00:00, 1523.54it/s]Calculating Metrics:  78%|███████▊  | 7/9 [00:00<00:00, 1676.67it/s]Calculating Metrics:  78%|███████▊  | 7/9 [00:00<00:00, 1413.04it/s]Calculating Metrics:  89%|████████▉ | 8/9 [00:00<00:00, 1509.76it/s]Calculating Metrics:  89%|████████▉ | 8/9 [00:00<00:00, 1374.34it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 1495.77it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 1394.07it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 1377.84it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 1358.31it/s]
_ test_case_runner_v3_api[pandas/column_distributional_expectations/expect_column_kl_divergence_to_be_less_than:too_big_internal_weight] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 4319.57it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 1956.98it/s]Calculating Metrics:  56%|█████▌    | 5/9 [00:00<00:00, 2236.25it/s]Calculating Metrics:  56%|█████▌    | 5/9 [00:00<00:00, 1525.31it/s]Calculating Metrics:  78%|███████▊  | 7/9 [00:00<00:00, 1669.04it/s]Calculating Metrics:  78%|███████▊  | 7/9 [00:00<00:00, 1432.62it/s]Calculating Metrics:  89%|████████▉ | 8/9 [00:00<00:00, 1535.18it/s]Calculating Metrics:  89%|████████▉ | 8/9 [00:00<00:00, 1401.25it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 1525.94it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 1421.74it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 1404.97it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 1385.02it/s]
_ test_case_runner_v3_api[pandas/column_distributional_expectations/expect_column_kl_divergence_to_be_less_than:manual_partition_undefined_infinite_kl_divergence0] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/7 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/7 [00:00<?, ?it/s]Calculating Metrics:  29%|██▊       | 2/7 [00:00<00:00, 4299.65it/s]Calculating Metrics:  29%|██▊       | 2/7 [00:00<00:00, 1180.66it/s]Calculating Metrics:  43%|████▎     | 3/7 [00:00<00:00, 1566.60it/s]Calculating Metrics:  43%|████▎     | 3/7 [00:00<00:00, 1059.26it/s]Calculating Metrics:  71%|███████▏  | 5/7 [00:00<00:00, 1245.93it/s]Calculating Metrics:  71%|███████▏  | 5/7 [00:00<00:00, 1073.87it/s]Calculating Metrics:  86%|████████▌ | 6/7 [00:00<00:00, 1201.69it/s]Calculating Metrics:  86%|████████▌ | 6/7 [00:00<00:00, 1092.31it/s]Calculating Metrics: 100%|██████████| 7/7 [00:00<00:00, 1224.21it/s]Calculating Metrics: 100%|██████████| 7/7 [00:00<00:00, 1145.09it/s]Calculating Metrics: 100%|██████████| 7/7 [00:00<00:00, 1129.97it/s]Calculating Metrics: 100%|██████████| 7/7 [00:00<00:00, 1111.45it/s]
_ test_case_runner_v3_api[pandas/column_distributional_expectations/expect_column_kl_divergence_to_be_less_than:manual_partition_undefined_infinite_kl_divergence1] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 3976.59it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 1773.12it/s]Calculating Metrics:  56%|█████▌    | 5/9 [00:00<00:00, 2010.69it/s]Calculating Metrics:  56%|█████▌    | 5/9 [00:00<00:00, 1372.12it/s]Calculating Metrics:  78%|███████▊  | 7/9 [00:00<00:00, 1482.76it/s]Calculating Metrics:  78%|███████▊  | 7/9 [00:00<00:00, 1264.71it/s]Calculating Metrics:  89%|████████▉ | 8/9 [00:00<00:00, 1356.61it/s]Calculating Metrics:  89%|████████▉ | 8/9 [00:00<00:00, 1235.16it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 1346.73it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 1256.28it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 1241.90it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 1225.33it/s]
_ test_case_runner_v3_api[pandas/column_distributional_expectations/expect_column_kl_divergence_to_be_less_than:manual_partition_undefined_infinite_kl_divergence_bins_do_not_cover_all_data] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 4098.00it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 1911.93it/s]Calculating Metrics:  56%|█████▌    | 5/9 [00:00<00:00, 2188.18it/s]Calculating Metrics:  56%|█████▌    | 5/9 [00:00<00:00, 1527.65it/s]Calculating Metrics:  78%|███████▊  | 7/9 [00:00<00:00, 1683.98it/s]Calculating Metrics:  78%|███████▊  | 7/9 [00:00<00:00, 1436.48it/s]Calculating Metrics:  89%|████████▉ | 8/9 [00:00<00:00, 1537.29it/s]Calculating Metrics:  89%|████████▉ | 8/9 [00:00<00:00, 1394.96it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 1517.72it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 1412.38it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 1395.78it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 1374.88it/s]
_ test_case_runner_v3_api[pandas/column_distributional_expectations/expect_column_kl_divergence_to_be_less_than:manual_partition_return_partitions_should_have_tail_weights] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 4128.25it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 1800.13it/s]Calculating Metrics:  56%|█████▌    | 5/9 [00:00<00:00, 2035.87it/s]Calculating Metrics:  56%|█████▌    | 5/9 [00:00<00:00, 1435.23it/s]Calculating Metrics:  78%|███████▊  | 7/9 [00:00<00:00, 1568.13it/s]Calculating Metrics:  78%|███████▊  | 7/9 [00:00<00:00, 1304.78it/s]Calculating Metrics:  89%|████████▉ | 8/9 [00:00<00:00, 1391.32it/s]Calculating Metrics:  89%|████████▉ | 8/9 [00:00<00:00, 1226.85it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 1314.46it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 1196.66it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 1181.27it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 1165.05it/s]
_ test_case_runner_v3_api[pandas/column_distributional_expectations/expect_column_kl_divergence_to_be_less_than:manual_partition_tail_weights_return_partitions_should_have_tail_weights] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 4641.00it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 2022.08it/s]Calculating Metrics:  56%|█████▌    | 5/9 [00:00<00:00, 2302.03it/s]Calculating Metrics:  56%|█████▌    | 5/9 [00:00<00:00, 1584.79it/s]Calculating Metrics:  78%|███████▊  | 7/9 [00:00<00:00, 1665.44it/s]Calculating Metrics:  78%|███████▊  | 7/9 [00:00<00:00, 1429.76it/s]Calculating Metrics:  89%|████████▉ | 8/9 [00:00<00:00, 1533.43it/s]Calculating Metrics:  89%|████████▉ | 8/9 [00:00<00:00, 1401.02it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 1525.94it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 1422.01it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 1405.81it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 1385.68it/s]
_ test_case_runner_v3_api[pandas/column_distributional_expectations/expect_column_kl_divergence_to_be_less_than:manual_partition_bounded_endpoints_empty_tail_weights_in_return] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 4973.97it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 2126.39it/s]Calculating Metrics:  56%|█████▌    | 5/9 [00:00<00:00, 2403.89it/s]Calculating Metrics:  56%|█████▌    | 5/9 [00:00<00:00, 1596.86it/s]Calculating Metrics:  78%|███████▊  | 7/9 [00:00<00:00, 1728.29it/s]Calculating Metrics:  78%|███████▊  | 7/9 [00:00<00:00, 1473.98it/s]Calculating Metrics:  89%|████████▉ | 8/9 [00:00<00:00, 1576.95it/s]Calculating Metrics:  89%|████████▉ | 8/9 [00:00<00:00, 1435.18it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 1561.09it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 1450.87it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 1433.79it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 1412.86it/s]
_ test_case_runner_v3_api[pandas/column_distributional_expectations/expect_column_kl_divergence_to_be_less_than:manual_partition_bounded_endpoints_observed_with_tail_weight_infinite_kl_divergence] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 4188.02it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 1925.76it/s]Calculating Metrics:  56%|█████▌    | 5/9 [00:00<00:00, 2152.25it/s]Calculating Metrics:  56%|█████▌    | 5/9 [00:00<00:00, 1388.47it/s]Calculating Metrics:  78%|███████▊  | 7/9 [00:00<00:00, 1513.80it/s]Calculating Metrics:  78%|███████▊  | 7/9 [00:00<00:00, 1307.22it/s]Calculating Metrics:  89%|████████▉ | 8/9 [00:00<00:00, 1397.11it/s]Calculating Metrics:  89%|████████▉ | 8/9 [00:00<00:00, 1280.41it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 1395.31it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 1306.46it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 1292.19it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 1274.65it/s]
_ test_case_runner_v3_api[pandas/column_distributional_expectations/expect_column_kl_divergence_to_be_less_than:manual_partition_infinite_endpoints_non_zero_tail_hold_out] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 4566.47it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 2022.08it/s]Calculating Metrics:  56%|█████▌    | 5/9 [00:00<00:00, 2302.03it/s]Calculating Metrics:  56%|█████▌    | 5/9 [00:00<00:00, 1589.84it/s]Calculating Metrics:  78%|███████▊  | 7/9 [00:00<00:00, 1734.82it/s]Calculating Metrics:  78%|███████▊  | 7/9 [00:00<00:00, 1478.35it/s]Calculating Metrics:  89%|████████▉ | 8/9 [00:00<00:00, 1582.01it/s]Calculating Metrics:  89%|████████▉ | 8/9 [00:00<00:00, 1435.48it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 1557.87it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 1445.76it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 1427.93it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 1406.70it/s]
_ test_case_runner_v3_api[pandas/column_distributional_expectations/expect_column_kl_divergence_to_be_less_than:manual_partition_upper_infinite_endpoint_non_zero_tail_hold_out] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 4865.78it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 2086.72it/s]Calculating Metrics:  56%|█████▌    | 5/9 [00:00<00:00, 2372.08it/s]Calculating Metrics:  56%|█████▌    | 5/9 [00:00<00:00, 1621.80it/s]Calculating Metrics:  78%|███████▊  | 7/9 [00:00<00:00, 1774.88it/s]Calculating Metrics:  78%|███████▊  | 7/9 [00:00<00:00, 1509.29it/s]Calculating Metrics:  89%|████████▉ | 8/9 [00:00<00:00, 1613.19it/s]Calculating Metrics:  89%|████████▉ | 8/9 [00:00<00:00, 1463.85it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 1591.16it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 1476.17it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 1457.99it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 1436.30it/s]
_ test_case_runner_v3_api[pandas/column_distributional_expectations/expect_column_kl_divergence_to_be_less_than:manual_partition_lower_infinite_endpoint_non_zero_tail_hold_out] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 4149.69it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 1953.34it/s]Calculating Metrics:  56%|█████▌    | 5/9 [00:00<00:00, 2234.34it/s]Calculating Metrics:  56%|█████▌    | 5/9 [00:00<00:00, 1559.68it/s]Calculating Metrics:  78%|███████▊  | 7/9 [00:00<00:00, 1720.79it/s]Calculating Metrics:  78%|███████▊  | 7/9 [00:00<00:00, 1444.18it/s]Calculating Metrics:  89%|████████▉ | 8/9 [00:00<00:00, 1543.23it/s]Calculating Metrics:  89%|████████▉ | 8/9 [00:00<00:00, 1405.95it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 1530.09it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 1418.91it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 1401.22it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 1376.18it/s]
_ test_case_runner_v3_api[pandas/column_distributional_expectations/expect_column_kl_divergence_to_be_less_than:manual_partition_bounded_endpoints_non_zero_tail_hold_out] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 4566.47it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 1997.76it/s]Calculating Metrics:  56%|█████▌    | 5/9 [00:00<00:00, 2274.82it/s]Calculating Metrics:  56%|█████▌    | 5/9 [00:00<00:00, 1576.69it/s]Calculating Metrics:  78%|███████▊  | 7/9 [00:00<00:00, 1719.38it/s]Calculating Metrics:  78%|███████▊  | 7/9 [00:00<00:00, 1471.17it/s]Calculating Metrics:  89%|████████▉ | 8/9 [00:00<00:00, 1575.10it/s]Calculating Metrics:  89%|████████▉ | 8/9 [00:00<00:00, 1435.48it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 1562.19it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 1452.99it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 1434.93it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 1413.92it/s]
_ test_case_runner_v3_api[sqlite/column_distributional_expectations/expect_column_kl_divergence_to_be_less_than:discrete_baseline_categorical_fixed_partition] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:  22%|██▏       | 2/9 [00:00<00:00, 3482.20it/s]Calculating Metrics:  22%|██▏       | 2/9 [00:00<00:00, 1023.50it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 1370.69it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 965.43it/s]Calculating Metrics:  67%|██████▋   | 6/9 [00:00<00:00, 1035.72it/s]Calculating Metrics:  67%|██████▋   | 6/9 [00:00<00:00, 890.57it/s] Calculating Metrics:  78%|███████▊  | 7/9 [00:00<00:00, 975.06it/s]Calculating Metrics:  78%|███████▊  | 7/9 [00:00<00:00, 883.49it/s]Calculating Metrics:  89%|████████▉ | 8/9 [00:00<00:00, 876.30it/s]Calculating Metrics:  89%|████████▉ | 8/9 [00:00<00:00, 824.64it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 908.16it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 866.04it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 858.94it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 850.71it/s]
_ test_case_runner_v3_api[sqlite/column_distributional_expectations/expect_column_kl_divergence_to_be_less_than:discrete_categorical_fixed_alternate_partition] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:  22%|██▏       | 2/9 [00:00<00:00, 3571.14it/s]Calculating Metrics:  22%|██▏       | 2/9 [00:00<00:00, 994.97it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 1438.38it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 999.24it/s] Calculating Metrics:  67%|██████▋   | 6/9 [00:00<00:00, 1112.74it/s]Calculating Metrics:  67%|██████▋   | 6/9 [00:00<00:00, 950.44it/s]alculating Metrics:  78%|███████▊  | 7/9 [00:00<00:00, 1039.33it/s]Calculating Metrics:  78%|███████▊  | 7/9 [00:00<00:00, 939.49it/s] Clculating Metrics:  89%|████████▉ | 8/9 [00:00<00:00, 941.09it/s]Calculating Metrics:  89%|████████▉ | 8/9 [00:00<00:00, 883.97it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 973.73it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 926.12it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 919.02it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 910.29it/s]
_ test_case_runner_v3_api[sqlite/column_distributional_expectations/expect_column_kl_divergence_to_be_less_than:internal_holdout_with_categorical_partition] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:  22%|██▏       | 2/9 [00:00<00:00, 3975.64it/s]Calculating Metrics:  22%|██▏       | 2/9 [00:00<00:00, 1070.66it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 1525.62it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 1035.18it/s]Calculating Metrics:  67%|██████▋   | 6/9 [00:00<00:00, 1166.00it/s]Calculating Metrics:  67%|██████▋   | 6/9 [00:00<00:00, 988.95it/s]Calculating Metrics:  78%|███████▊  | 7/9 [00:00<00:00, 1078.27it/s]Calculating Metrics:  78%|███████▊  | 7/9 [00:00<00:00, 970.74it/s] Calculating Metrics:  89%|████████▉ | 8/9 [00:00<00:00, 990.60it/s]Calculating Metrics:  89%|████████▉ | 8/9 [00:00<00:00, 926.89it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 1017.07it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 960.82it/s]alculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 952.48it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 942.80it/s]
_ test_case_runner_v3_api[sqlite/column_distributional_expectations/expect_column_kl_divergence_to_be_less_than:holdout_0_1] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:  22%|██▏       | 2/9 [00:00<00:00, 3110.35it/s]Calculating Metrics:  22%|██▏       | 2/9 [00:00<00:00, 897.66it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 1209.95it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 866.95it/s] Calculating Metrics:  67%|██████▋   | 6/9 [00:00<00:00, 942.82it/s]Calculating Metrics:  67%|██████▋   | 6/9 [00:00<00:00, 806.13it/s]Calculating Metrics:  78%|███████▊  | 7/9 [00:00<00:00, 879.84it/s]Calculating Metrics:  78%|███████▊  | 7/9 [00:00<00:00, 792.57it/s]Calculating Metrics:  89%|████████▉ | 8/9 [00:00<00:00, 792.16it/s]Calculating Metrics:  89%|████████▉ | 8/9 [00:00<00:00, 742.75it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 817.44it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 776.93it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 770.82it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 763.36it/s]
_ test_case_runner_v3_api[sqlite/column_distributional_expectations/expect_column_kl_divergence_to_be_less_than:holdout_0_05] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:  22%|██▏       | 2/9 [00:00<00:00, 3355.44it/s]Calculating Metrics:  22%|██▏       | 2/9 [00:00<00:00, 884.13it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 1259.83it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 852.89it/s] Calculating Metrics:  67%|██████▋   | 6/9 [00:00<00:00, 996.82it/s]Calculating Metrics:  67%|██████▋   | 6/9 [00:00<00:00, 841.75it/s]Calculating Metrics:  78%|███████▊  | 7/9 [00:00<00:00, 917.53it/s]Calculating Metrics:  78%|███████▊  | 7/9 [00:00<00:00, 824.10it/s]Calculating Metrics:  89%|████████▉ | 8/9 [00:00<00:00, 845.67it/s]Calculating Metrics:  89%|████████▉ | 8/9 [00:00<00:00, 790.35it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 869.07it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 822.97it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 816.40it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 808.48it/s]
_ test_case_runner_v3_api[sqlite/column_distributional_expectations/expect_column_kl_divergence_to_be_less_than:missing_vals_no_holdout] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:  22%|██▏       | 2/9 [00:00<00:00, 3058.19it/s]Calculating Metrics:  22%|██▏       | 2/9 [00:00<00:00, 840.04it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 1216.53it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 813.68it/s] Calculating Metrics:  67%|██████▋   | 6/9 [00:00<00:00, 926.82it/s]Calculating Metrics:  67%|██████▋   | 6/9 [00:00<00:00, 776.60it/s]Calculating Metrics:  78%|███████▊  | 7/9 [00:00<00:00, 844.29it/s]Calculating Metrics:  78%|███████▊  | 7/9 [00:00<00:00, 761.12it/s]Calculating Metrics:  89%|████████▉ | 8/9 [00:00<00:00, 767.31it/s]Calculating Metrics:  89%|████████▉ | 8/9 [00:00<00:00, 721.24it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 794.58it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 756.76it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 750.99it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 744.05it/s]
_ test_case_runner_v3_api[sqlite/column_distributional_expectations/expect_column_kl_divergence_to_be_less_than:norm_0_1_auto_explicit_infinite_endpoints] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/11 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/11 [00:00<?, ?it/s]Calculating Metrics:  36%|███▋      | 4/11 [00:00<00:00, 1666.06it/s]Calculating Metrics:  36%|███▋      | 4/11 [00:00<00:00, 886.93it/s]Calculating Metrics:  55%|█████▍    | 6/11 [00:00<00:00, 1078.97it/s]Calculating Metrics:  55%|█████▍    | 6/11 [00:00<00:00, 817.13it/s] Calculating Metrics:  73%|███████▎  | 8/11 [00:00<00:00, 384.77it/s]Calculating Metrics:  73%|███████▎  | 8/11 [00:00<00:00, 364.39it/s]Calculating Metrics:  82%|████████▏ | 9/11 [00:00<00:00, 401.74it/s]Calculating Metrics:  82%|████████▏ | 9/11 [00:00<00:00, 386.85it/s]Calculating Metrics:  91%|█████████ | 10/11 [00:00<00:00, 408.90it/s]Calculating Metrics:  91%|█████████ | 10/11 [00:00<00:00, 397.56it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 433.89it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 423.73it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 422.46it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 420.94it/s]
_ test_case_runner_v3_api[sqlite/column_distributional_expectations/expect_column_kl_divergence_to_be_less_than:norm_0_1_auto_inf_partition_test_tail_weights] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/11 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/11 [00:00<?, ?it/s]Calculating Metrics:  36%|███▋      | 4/11 [00:00<00:00, 2775.85it/s]Calculating Metrics:  36%|███▋      | 4/11 [00:00<00:00, 1271.48it/s]Calculating Metrics:  55%|█████▍    | 6/11 [00:00<00:00, 1554.02it/s]Calculating Metrics:  55%|█████▍    | 6/11 [00:00<00:00, 1127.60it/s]Calculating Metrics:  73%|███████▎  | 8/11 [00:00<00:00, 488.85it/s]Calculating Metrics:  73%|███████▎  | 8/11 [00:00<00:00, 457.69it/s]Calculating Metrics:  82%|████████▏ | 9/11 [00:00<00:00, 502.91it/s]Calculating Metrics:  82%|████████▏ | 9/11 [00:00<00:00, 479.51it/s]Calculating Metrics:  91%|█████████ | 10/11 [00:00<00:00, 505.66it/s]Calculating Metrics:  91%|█████████ | 10/11 [00:00<00:00, 488.16it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 531.76it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 516.31it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 514.42it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 512.10it/s]
_ test_case_runner_v3_api[sqlite/column_distributional_expectations/expect_column_kl_divergence_to_be_less_than:norm_0_1_auto_inf_partition_should_fail_with_no_holdout] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/11 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/11 [00:00<?, ?it/s]Calculating Metrics:  36%|███▋      | 4/11 [00:00<00:00, 2704.26it/s]Calculating Metrics:  36%|███▋      | 4/11 [00:00<00:00, 1286.10it/s]Calculating Metrics:  55%|█████▍    | 6/11 [00:00<00:00, 1574.34it/s]Calculating Metrics:  55%|█████▍    | 6/11 [00:00<00:00, 1140.48it/s]Calculating Metrics:  73%|███████▎  | 8/11 [00:00<00:00, 604.54it/s]Calculating Metrics:  73%|███████▎  | 8/11 [00:00<00:00, 559.17it/s]Calculating Metrics:  82%|████████▏ | 9/11 [00:00<00:00, 611.79it/s]Calculating Metrics:  82%|████████▏ | 9/11 [00:00<00:00, 578.26it/s]Calculating Metrics:  91%|█████████ | 10/11 [00:00<00:00, 608.86it/s]Calculating Metrics:  91%|█████████ | 10/11 [00:00<00:00, 584.35it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 635.50it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 613.88it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 611.21it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 608.01it/s]
_ test_case_runner_v3_api[sqlite/column_distributional_expectations/expect_column_kl_divergence_to_be_less_than:norm_0_1_auto_inf_partition_null_threshold_should_always_succeed] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/11 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/11 [00:00<?, ?it/s]Calculating Metrics:  36%|███▋      | 4/11 [00:00<00:00, 2814.50it/s]Calculating Metrics:  36%|███▋      | 4/11 [00:00<00:00, 1254.28it/s]Calculating Metrics:  55%|█████▍    | 6/11 [00:00<00:00, 1526.40it/s]Calculating Metrics:  55%|█████▍    | 6/11 [00:00<00:00, 1093.69it/s]Calculating Metrics:  73%|███████▎  | 8/11 [00:00<00:00, 580.14it/s]Calculating Metrics:  73%|███████▎  | 8/11 [00:00<00:00, 536.55it/s]Calculating Metrics:  82%|████████▏ | 9/11 [00:00<00:00, 586.70it/s]Calculating Metrics:  82%|████████▏ | 9/11 [00:00<00:00, 555.49it/s]Calculating Metrics:  91%|█████████ | 10/11 [00:00<00:00, 581.83it/s]Calculating Metrics:  91%|█████████ | 10/11 [00:00<00:00, 555.61it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 603.33it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 582.90it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 580.23it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 577.19it/s]
_ test_case_runner_v3_api[sqlite/column_distributional_expectations/expect_column_kl_divergence_to_be_less_than:norm_0_1_auto_inf_partition_tail_weights_should_fail_with_no_internal_holdout] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/11 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/11 [00:00<?, ?it/s]Calculating Metrics:  36%|███▋      | 4/11 [00:00<00:00, 2749.01it/s]Calculating Metrics:  36%|███▋      | 4/11 [00:00<00:00, 1248.03it/s]Calculating Metrics:  55%|█████▍    | 6/11 [00:00<00:00, 1520.59it/s]Calculating Metrics:  55%|█████▍    | 6/11 [00:00<00:00, 1108.43it/s]Calculating Metrics:  73%|███████▎  | 8/11 [00:00<00:00, 607.35it/s]Calculating Metrics:  73%|███████▎  | 8/11 [00:00<00:00, 559.68it/s]Calculating Metrics:  82%|████████▏ | 9/11 [00:00<00:00, 611.92it/s]Calculating Metrics:  82%|████████▏ | 9/11 [00:00<00:00, 577.67it/s]Calculating Metrics:  91%|█████████ | 10/11 [00:00<00:00, 604.60it/s]Calculating Metrics:  91%|█████████ | 10/11 [00:00<00:00, 579.85it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 630.49it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 608.82it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 606.19it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 603.01it/s]
_ test_case_runner_v3_api[sqlite/column_distributional_expectations/expect_column_kl_divergence_to_be_less_than:positive_norm_0_1_auto_partition] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/11 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/11 [00:00<?, ?it/s]Calculating Metrics:  36%|███▋      | 4/11 [00:00<00:00, 2599.51it/s]Calculating Metrics:  36%|███▋      | 4/11 [00:00<00:00, 1248.12it/s]Calculating Metrics:  55%|█████▍    | 6/11 [00:00<00:00, 1511.01it/s]Calculating Metrics:  55%|█████▍    | 6/11 [00:00<00:00, 1094.07it/s]Calculating Metrics:  73%|███████▎  | 8/11 [00:00<00:00, 620.06it/s]Calculating Metrics:  73%|███████▎  | 8/11 [00:00<00:00, 569.76it/s]Calculating Metrics:  82%|████████▏ | 9/11 [00:00<00:00, 622.19it/s]Calculating Metrics:  82%|████████▏ | 9/11 [00:00<00:00, 586.78it/s]Calculating Metrics:  91%|█████████ | 10/11 [00:00<00:00, 615.57it/s]Calculating Metrics:  91%|█████████ | 10/11 [00:00<00:00, 589.97it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 641.36it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 618.85it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 616.11it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 612.81it/s]
_ test_case_runner_v3_api[sqlite/column_distributional_expectations/expect_column_kl_divergence_to_be_less_than:positive_norm_0_1_uniform_partition] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/11 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/11 [00:00<?, ?it/s]Calculating Metrics:  36%|███▋      | 4/11 [00:00<00:00, 2695.57it/s]Calculating Metrics:  36%|███▋      | 4/11 [00:00<00:00, 1347.68it/s]Calculating Metrics:  55%|█████▍    | 6/11 [00:00<00:00, 1638.83it/s]Calculating Metrics:  55%|█████▍    | 6/11 [00:00<00:00, 1203.59it/s]Calculating Metrics:  73%|███████▎  | 8/11 [00:00<00:00, 832.39it/s]Calculating Metrics:  73%|███████▎  | 8/11 [00:00<00:00, 750.88it/s]Calculating Metrics:  82%|████████▏ | 9/11 [00:00<00:00, 813.38it/s]Calculating Metrics:  82%|████████▏ | 9/11 [00:00<00:00, 758.07it/s]Calculating Metrics:  91%|█████████ | 10/11 [00:00<00:00, 786.58it/s]Calculating Metrics:  91%|█████████ | 10/11 [00:00<00:00, 748.38it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 811.38it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 777.55it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 773.12it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 767.88it/s]
_ test_case_runner_v3_api[sqlite/column_distributional_expectations/expect_column_kl_divergence_to_be_less_than:positive_norm_0_1_ntile_partition] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/11 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/11 [00:00<?, ?it/s]Calculating Metrics:  36%|███▋      | 4/11 [00:00<00:00, 2736.01it/s]Calculating Metrics:  36%|███▋      | 4/11 [00:00<00:00, 1348.11it/s]Calculating Metrics:  55%|█████▍    | 6/11 [00:00<00:00, 1619.42it/s]Calculating Metrics:  55%|█████▍    | 6/11 [00:00<00:00, 1110.24it/s]Calculating Metrics:  73%|███████▎  | 8/11 [00:00<00:00, 885.04it/s]Calculating Metrics:  73%|███████▎  | 8/11 [00:00<00:00, 792.54it/s]Calculating Metrics:  82%|████████▏ | 9/11 [00:00<00:00, 856.87it/s]Calculating Metrics:  82%|████████▏ | 9/11 [00:00<00:00, 794.91it/s]Calculating Metrics:  91%|█████████ | 10/11 [00:00<00:00, 819.20it/s]Calculating Metrics:  91%|█████████ | 10/11 [00:00<00:00, 777.30it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 841.95it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 806.15it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 801.51it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 795.90it/s]
_ test_case_runner_v3_api[sqlite/column_distributional_expectations/expect_column_kl_divergence_to_be_less_than:positive_norm_0_1_kde_partition] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/11 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/11 [00:00<?, ?it/s]Calculating Metrics:  36%|███▋      | 4/11 [00:00<00:00, 2669.83it/s]Calculating Metrics:  36%|███▋      | 4/11 [00:00<00:00, 1318.34it/s]Calculating Metrics:  55%|█████▍    | 6/11 [00:00<00:00, 1601.69it/s]Calculating Metrics:  55%|█████▍    | 6/11 [00:00<00:00, 1174.38it/s]Calculating Metrics:  73%|███████▎  | 8/11 [00:00<00:00, 947.20it/s]Calculating Metrics:  73%|███████▎  | 8/11 [00:00<00:00, 830.29it/s]Calculating Metrics:  82%|████████▏ | 9/11 [00:00<00:00, 891.96it/s]Calculating Metrics:  82%|████████▏ | 9/11 [00:00<00:00, 819.43it/s]Calculating Metrics:  91%|█████████ | 10/11 [00:00<00:00, 825.62it/s]Calculating Metrics:  91%|█████████ | 10/11 [00:00<00:00, 772.19it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 824.71it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 754.54it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 745.45it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 735.39it/s]
_ test_case_runner_v3_api[sqlite/column_distributional_expectations/expect_column_kl_divergence_to_be_less_than:negative_norm_1_1_column_norm_0_1_auto_partition] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/11 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/11 [00:00<?, ?it/s]Calculating Metrics:  36%|███▋      | 4/11 [00:00<00:00, 1652.11it/s]Calculating Metrics:  36%|███▋      | 4/11 [00:00<00:00, 863.03it/s]Calculating Metrics:  55%|█████▍    | 6/11 [00:00<00:00, 1083.19it/s]Calculating Metrics:  55%|█████▍    | 6/11 [00:00<00:00, 802.02it/s] Calculating Metrics:  73%|███████▎  | 8/11 [00:00<00:00, 396.11it/s]Calculating Metrics:  73%|███████▎  | 8/11 [00:00<00:00, 374.76it/s]Calculating Metrics:  82%|████████▏ | 9/11 [00:00<00:00, 413.35it/s]Calculating Metrics:  82%|████████▏ | 9/11 [00:00<00:00, 397.42it/s]Calculating Metrics:  91%|█████████ | 10/11 [00:00<00:00, 420.45it/s]Calculating Metrics:  91%|█████████ | 10/11 [00:00<00:00, 408.38it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 445.57it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 434.75it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 433.41it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 431.78it/s]
_ test_case_runner_v3_api[sqlite/column_distributional_expectations/expect_column_kl_divergence_to_be_less_than:negative_norm_1_1_column_norm_0_1_uniform_partition] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/11 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/11 [00:00<?, ?it/s]Calculating Metrics:  36%|███▋      | 4/11 [00:00<00:00, 2533.56it/s]Calculating Metrics:  36%|███▋      | 4/11 [00:00<00:00, 1280.31it/s]Calculating Metrics:  55%|█████▍    | 6/11 [00:00<00:00, 1550.38it/s]Calculating Metrics:  55%|█████▍    | 6/11 [00:00<00:00, 1142.19it/s]Calculating Metrics:  73%|███████▎  | 8/11 [00:00<00:00, 796.73it/s]Calculating Metrics:  73%|███████▎  | 8/11 [00:00<00:00, 717.68it/s]Calculating Metrics:  82%|████████▏ | 9/11 [00:00<00:00, 777.54it/s]Calculating Metrics:  82%|████████▏ | 9/11 [00:00<00:00, 725.92it/s]Calculating Metrics:  91%|█████████ | 10/11 [00:00<00:00, 707.86it/s]Calculating Metrics:  91%|█████████ | 10/11 [00:00<00:00, 675.49it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 732.94it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 705.81it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 702.30it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 697.97it/s]
_ test_case_runner_v3_api[sqlite/column_distributional_expectations/expect_column_kl_divergence_to_be_less_than:negative_norm_1_1_column_norm_0_1_ntile_partition] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/11 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/11 [00:00<?, ?it/s]Calculating Metrics:  36%|███▋      | 4/11 [00:00<00:00, 2812.61it/s]Calculating Metrics:  36%|███▋      | 4/11 [00:00<00:00, 1317.41it/s]Calculating Metrics:  55%|█████▍    | 6/11 [00:00<00:00, 1598.64it/s]Calculating Metrics:  55%|█████▍    | 6/11 [00:00<00:00, 1179.22it/s]Calculating Metrics:  73%|███████▎  | 8/11 [00:00<00:00, 950.87it/s]Calculating Metrics:  73%|███████▎  | 8/11 [00:00<00:00, 847.36it/s]Calculating Metrics:  82%|████████▏ | 9/11 [00:00<00:00, 911.36it/s]Calculating Metrics:  82%|████████▏ | 9/11 [00:00<00:00, 834.78it/s]Calculating Metrics:  91%|█████████ | 10/11 [00:00<00:00, 857.40it/s]Calculating Metrics:  91%|█████████ | 10/11 [00:00<00:00, 811.87it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 879.01it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 840.39it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 835.47it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 829.55it/s]
_ test_case_runner_v3_api[sqlite/column_distributional_expectations/expect_column_kl_divergence_to_be_less_than:negative_norm_1_1_column_norm_0_1_kde_partition] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/11 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/11 [00:00<?, ?it/s]Calculating Metrics:  36%|███▋      | 4/11 [00:00<00:00, 2594.28it/s]Calculating Metrics:  36%|███▋      | 4/11 [00:00<00:00, 1275.16it/s]Calculating Metrics:  55%|█████▍    | 6/11 [00:00<00:00, 1544.01it/s]Calculating Metrics:  55%|█████▍    | 6/11 [00:00<00:00, 1137.49it/s]Calculating Metrics:  73%|███████▎  | 8/11 [00:00<00:00, 916.71it/s]Calculating Metrics:  73%|███████▎  | 8/11 [00:00<00:00, 817.09it/s]Calculating Metrics:  82%|████████▏ | 9/11 [00:00<00:00, 881.94it/s]Calculating Metrics:  82%|████████▏ | 9/11 [00:00<00:00, 808.27it/s]Calculating Metrics:  91%|█████████ | 10/11 [00:00<00:00, 829.27it/s]Calculating Metrics:  91%|█████████ | 10/11 [00:00<00:00, 781.44it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 844.99it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 809.02it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 804.28it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 798.50it/s]
_ test_case_runner_v3_api[sqlite/column_distributional_expectations/expect_column_kl_divergence_to_be_less_than:positive_bimodal_auto_partition] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/11 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/11 [00:00<?, ?it/s]Calculating Metrics:  36%|███▋      | 4/11 [00:00<00:00, 1660.29it/s]Calculating Metrics:  36%|███▋      | 4/11 [00:00<00:00, 861.34it/s]Calculating Metrics:  55%|█████▍    | 6/11 [00:00<00:00, 1088.72it/s]Calculating Metrics:  55%|█████▍    | 6/11 [00:00<00:00, 844.35it/s] Calculating Metrics:  73%|███████▎  | 8/11 [00:00<00:00, 646.67it/s]Calculating Metrics:  73%|███████▎  | 8/11 [00:00<00:00, 591.18it/s]Calculating Metrics:  82%|████████▏ | 9/11 [00:00<00:00, 644.19it/s]Calculating Metrics:  82%|████████▏ | 9/11 [00:00<00:00, 608.02it/s]Calculating Metrics:  91%|█████████ | 10/11 [00:00<00:00, 625.89it/s]Calculating Metrics:  91%|█████████ | 10/11 [00:00<00:00, 600.78it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 652.98it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 631.27it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 628.14it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 624.57it/s]
_ test_case_runner_v3_api[sqlite/column_distributional_expectations/expect_column_kl_divergence_to_be_less_than:negative_bimodal_column_norm_0_1_auto_partition] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/11 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/11 [00:00<?, ?it/s]Calculating Metrics:  36%|███▋      | 4/11 [00:00<00:00, 2328.23it/s]Calculating Metrics:  36%|███▋      | 4/11 [00:00<00:00, 1098.34it/s]Calculating Metrics:  55%|█████▍    | 6/11 [00:00<00:00, 1316.07it/s]Calculating Metrics:  55%|█████▍    | 6/11 [00:00<00:00, 920.11it/s]Calculating Metrics:  73%|███████▎  | 8/11 [00:00<00:00, 406.28it/s]Calculating Metrics:  73%|███████▎  | 8/11 [00:00<00:00, 383.41it/s]Calculating Metrics:  82%|████████▏ | 9/11 [00:00<00:00, 422.51it/s]Calculating Metrics:  82%|████████▏ | 9/11 [00:00<00:00, 403.88it/s]Calculating Metrics:  91%|█████████ | 10/11 [00:00<00:00, 430.05it/s]Calculating Metrics:  91%|█████████ | 10/11 [00:00<00:00, 417.20it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 455.03it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 443.51it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 442.11it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 440.37it/s]
_ test_case_runner_v3_api[sqlite/column_distributional_expectations/expect_column_kl_divergence_to_be_less_than:negative_bimodal_column_norm_0_1_uniform_partition] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/11 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/11 [00:00<?, ?it/s]Calculating Metrics:  36%|███▋      | 4/11 [00:00<00:00, 2562.19it/s]Calculating Metrics:  36%|███▋      | 4/11 [00:00<00:00, 1224.70it/s]Calculating Metrics:  55%|█████▍    | 6/11 [00:00<00:00, 1460.92it/s]Calculating Metrics:  55%|█████▍    | 6/11 [00:00<00:00, 1091.32it/s]Calculating Metrics:  73%|███████▎  | 8/11 [00:00<00:00, 758.65it/s]Calculating Metrics:  73%|███████▎  | 8/11 [00:00<00:00, 688.89it/s]Calculating Metrics:  82%|████████▏ | 9/11 [00:00<00:00, 746.70it/s]Calculating Metrics:  82%|████████▏ | 9/11 [00:00<00:00, 695.30it/s]Calculating Metrics:  91%|█████████ | 10/11 [00:00<00:00, 719.27it/s]Calculating Metrics:  91%|█████████ | 10/11 [00:00<00:00, 686.77it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 745.32it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 717.32it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 713.68it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 709.27it/s]
_ test_case_runner_v3_api[sqlite/column_distributional_expectations/expect_column_kl_divergence_to_be_less_than:manual_partition_lower_inf_bound] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/11 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/11 [00:00<?, ?it/s]Calculating Metrics:  36%|███▋      | 4/11 [00:00<00:00, 1708.47it/s]Calculating Metrics:  36%|███▋      | 4/11 [00:00<00:00, 975.82it/s]Calculating Metrics:  55%|█████▍    | 6/11 [00:00<00:00, 1178.51it/s]Calculating Metrics:  55%|█████▍    | 6/11 [00:00<00:00, 899.39it/s] Calculating Metrics:  73%|███████▎  | 8/11 [00:00<00:00, 846.46it/s]Calculating Metrics:  73%|███████▎  | 8/11 [00:00<00:00, 748.43it/s]Calculating Metrics:  82%|████████▏ | 9/11 [00:00<00:00, 803.76it/s]Calculating Metrics:  82%|████████▏ | 9/11 [00:00<00:00, 740.23it/s]Calculating Metrics:  91%|█████████ | 10/11 [00:00<00:00, 733.23it/s]Calculating Metrics:  91%|█████████ | 10/11 [00:00<00:00, 694.00it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 750.63it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 718.06it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 712.99it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 707.83it/s]
_ test_case_runner_v3_api[sqlite/column_distributional_expectations/expect_column_kl_divergence_to_be_less_than:manual_partition_lower_inf_bound_tail_weights] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/11 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/11 [00:00<?, ?it/s]Calculating Metrics:  36%|███▋      | 4/11 [00:00<00:00, 2433.60it/s]Calculating Metrics:  36%|███▋      | 4/11 [00:00<00:00, 1282.07it/s]Calculating Metrics:  55%|█████▍    | 6/11 [00:00<00:00, 1557.68it/s]Calculating Metrics:  55%|█████▍    | 6/11 [00:00<00:00, 1162.82it/s]Calculating Metrics:  73%|███████▎  | 8/11 [00:00<00:00, 775.81it/s]Calculating Metrics:  73%|███████▎  | 8/11 [00:00<00:00, 703.73it/s]Calculating Metrics:  82%|████████▏ | 9/11 [00:00<00:00, 762.08it/s]Calculating Metrics:  82%|████████▏ | 9/11 [00:00<00:00, 712.76it/s]Calculating Metrics:  91%|█████████ | 10/11 [00:00<00:00, 740.86it/s]Calculating Metrics:  91%|█████████ | 10/11 [00:00<00:00, 708.22it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 768.11it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 740.29it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 736.54it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 731.68it/s]
_ test_case_runner_v3_api[sqlite/column_distributional_expectations/expect_column_kl_divergence_to_be_less_than:manual_partition_upper_inf_bound] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/11 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/11 [00:00<?, ?it/s]Calculating Metrics:  36%|███▋      | 4/11 [00:00<00:00, 1838.20it/s]Calculating Metrics:  36%|███▋      | 4/11 [00:00<00:00, 1012.20it/s]Calculating Metrics:  55%|█████▍    | 6/11 [00:00<00:00, 1254.22it/s]Calculating Metrics:  55%|█████▍    | 6/11 [00:00<00:00, 945.48it/s]Calculating Metrics:  73%|███████▎  | 8/11 [00:00<00:00, 891.95it/s]Calculating Metrics:  73%|███████▎  | 8/11 [00:00<00:00, 795.96it/s]Calculating Metrics:  82%|████████▏ | 9/11 [00:00<00:00, 853.73it/s]Calculating Metrics:  82%|████████▏ | 9/11 [00:00<00:00, 790.65it/s]Calculating Metrics:  91%|█████████ | 10/11 [00:00<00:00, 798.02it/s]Calculating Metrics:  91%|█████████ | 10/11 [00:00<00:00, 759.12it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 822.31it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 790.17it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 785.72it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 780.31it/s]
_ test_case_runner_v3_api[sqlite/column_distributional_expectations/expect_column_kl_divergence_to_be_less_than:manual_partition_upper_inf_bound_tail_weights] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/11 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/11 [00:00<?, ?it/s]Calculating Metrics:  36%|███▋      | 4/11 [00:00<00:00, 2358.34it/s]Calculating Metrics:  36%|███▋      | 4/11 [00:00<00:00, 1277.10it/s]Calculating Metrics:  55%|█████▍    | 6/11 [00:00<00:00, 1574.73it/s]Calculating Metrics:  55%|█████▍    | 6/11 [00:00<00:00, 1174.82it/s]Calculating Metrics:  73%|███████▎  | 8/11 [00:00<00:00, 1102.06it/s]Calculating Metrics:  73%|███████▎  | 8/11 [00:00<00:00, 969.56it/s]Calculating Metrics:  82%|████████▏ | 9/11 [00:00<00:00, 1040.68it/s]Calculating Metrics:  82%|████████▏ | 9/11 [00:00<00:00, 953.78it/s] Calculating Metrics:  91%|█████████ | 10/11 [00:00<00:00, 973.13it/s]Calculating Metrics:  91%|█████████ | 10/11 [00:00<00:00, 914.81it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 987.70it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 939.51it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 933.24it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 925.75it/s]
_ test_case_runner_v3_api[sqlite/column_distributional_expectations/expect_column_kl_divergence_to_be_less_than:manual_partition_lower_and_upper_inf_bounds0] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/11 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/11 [00:00<?, ?it/s]Calculating Metrics:  36%|███▋      | 4/11 [00:00<00:00, 2149.00it/s]Calculating Metrics:  36%|███▋      | 4/11 [00:00<00:00, 1153.71it/s]Calculating Metrics:  55%|█████▍    | 6/11 [00:00<00:00, 1373.01it/s]Calculating Metrics:  55%|█████▍    | 6/11 [00:00<00:00, 1040.73it/s]Calculating Metrics:  73%|███████▎  | 8/11 [00:00<00:00, 944.61it/s]Calculating Metrics:  73%|███████▎  | 8/11 [00:00<00:00, 844.50it/s]Calculating Metrics:  82%|████████▏ | 9/11 [00:00<00:00, 911.39it/s]Calculating Metrics:  82%|████████▏ | 9/11 [00:00<00:00, 844.42it/s]Calculating Metrics:  91%|█████████ | 10/11 [00:00<00:00, 860.72it/s]Calculating Metrics:  91%|█████████ | 10/11 [00:00<00:00, 817.05it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 884.67it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 847.63it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 842.65it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 836.61it/s]
_ test_case_runner_v3_api[sqlite/column_distributional_expectations/expect_column_kl_divergence_to_be_less_than:manual_partition_lower_and_upper_inf_bounds1] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/11 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/11 [00:00<?, ?it/s]Calculating Metrics:  36%|███▋      | 4/11 [00:00<00:00, 1916.74it/s]Calculating Metrics:  36%|███▋      | 4/11 [00:00<00:00, 1104.71it/s]Calculating Metrics:  55%|█████▍    | 6/11 [00:00<00:00, 1371.51it/s]Calculating Metrics:  55%|█████▍    | 6/11 [00:00<00:00, 1059.35it/s]Calculating Metrics:  73%|███████▎  | 8/11 [00:00<00:00, 1028.05it/s]Calculating Metrics:  73%|███████▎  | 8/11 [00:00<00:00, 906.93it/s]Calculating Metrics:  82%|████████▏ | 9/11 [00:00<00:00, 973.18it/s]Calculating Metrics:  82%|████████▏ | 9/11 [00:00<00:00, 897.13it/s]Calculating Metrics:  91%|█████████ | 10/11 [00:00<00:00, 919.90it/s]Calculating Metrics:  91%|█████████ | 10/11 [00:00<00:00, 870.33it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 941.23it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 899.43it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 893.87it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 887.03it/s]
_ test_case_runner_v3_api[sqlite/column_distributional_expectations/expect_column_kl_divergence_to_be_less_than:tail_weight_holdout_is_not_defined_for_partitions_already_extending_to_inifinity] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/11 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/11 [00:00<?, ?it/s]Calculating Metrics:  36%|███▋      | 4/11 [00:00<00:00, 2976.27it/s]Calculating Metrics:  36%|███▋      | 4/11 [00:00<00:00, 1384.60it/s]Calculating Metrics:  55%|█████▍    | 6/11 [00:00<00:00, 1638.61it/s]Calculating Metrics:  55%|█████▍    | 6/11 [00:00<00:00, 1171.21it/s]Calculating Metrics:  73%|███████▎  | 8/11 [00:00<00:00, 1100.90it/s]Calculating Metrics:  73%|███████▎  | 8/11 [00:00<00:00, 957.08it/s]Calculating Metrics:  82%|████████▏ | 9/11 [00:00<00:00, 1022.50it/s]Calculating Metrics:  82%|████████▏ | 9/11 [00:00<00:00, 928.51it/s] Calculating Metrics:  91%|█████████ | 10/11 [00:00<00:00, 946.54it/s]Calculating Metrics:  91%|█████████ | 10/11 [00:00<00:00, 887.16it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 957.70it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 914.32it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 908.36it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 901.14it/s]
_ test_case_runner_v3_api[sqlite/column_distributional_expectations/expect_column_kl_divergence_to_be_less_than:tail_weight_holdout_is_not_defined_for_partitions_already_extending_to_inifinity_tail_weight] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/11 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/11 [00:00<?, ?it/s]Calculating Metrics:  36%|███▋      | 4/11 [00:00<00:00, 3110.92it/s]Calculating Metrics:  36%|███▋      | 4/11 [00:00<00:00, 1370.91it/s]Calculating Metrics:  55%|█████▍    | 6/11 [00:00<00:00, 1594.49it/s]Calculating Metrics:  55%|█████▍    | 6/11 [00:00<00:00, 1184.16it/s]Calculating Metrics:  73%|███████▎  | 8/11 [00:00<00:00, 1228.56it/s]Calculating Metrics:  73%|███████▎  | 8/11 [00:00<00:00, 1066.81it/s]Calculating Metrics:  82%|████████▏ | 9/11 [00:00<00:00, 1139.10it/s]Calculating Metrics:  82%|████████▏ | 9/11 [00:00<00:00, 1037.14it/s]Calculating Metrics:  91%|█████████ | 10/11 [00:00<00:00, 1051.86it/s]Calculating Metrics:  91%|█████████ | 10/11 [00:00<00:00, 988.52it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 1066.73it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 1014.12it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 1006.97it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 993.59it/s]
_ test_case_runner_v3_api[sqlite/column_distributional_expectations/expect_column_kl_divergence_to_be_less_than:too_big_tail_weight] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/11 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/11 [00:00<?, ?it/s]Calculating Metrics:  36%|███▋      | 4/11 [00:00<00:00, 2888.64it/s]Calculating Metrics:  36%|███▋      | 4/11 [00:00<00:00, 1393.23it/s]Calculating Metrics:  55%|█████▍    | 6/11 [00:00<00:00, 1676.05it/s]Calculating Metrics:  55%|█████▍    | 6/11 [00:00<00:00, 1225.75it/s]Calculating Metrics:  73%|███████▎  | 8/11 [00:00<00:00, 1221.40it/s]Calculating Metrics:  73%|███████▎  | 8/11 [00:00<00:00, 1060.44it/s]Calculating Metrics:  82%|████████▏ | 9/11 [00:00<00:00, 1132.68it/s]Calculating Metrics:  82%|████████▏ | 9/11 [00:00<00:00, 1029.87it/s]Calculating Metrics:  91%|█████████ | 10/11 [00:00<00:00, 1048.44it/s]Calculating Metrics:  91%|█████████ | 10/11 [00:00<00:00, 985.13it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 1063.02it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 1010.21it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 1003.12it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 994.60it/s]
_ test_case_runner_v3_api[sqlite/column_distributional_expectations/expect_column_kl_divergence_to_be_less_than:too_big_internal_weight] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/11 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/11 [00:00<?, ?it/s]Calculating Metrics:  36%|███▋      | 4/11 [00:00<00:00, 3093.71it/s]Calculating Metrics:  36%|███▋      | 4/11 [00:00<00:00, 1464.11it/s]Calculating Metrics:  55%|█████▍    | 6/11 [00:00<00:00, 1752.86it/s]Calculating Metrics:  55%|█████▍    | 6/11 [00:00<00:00, 1280.64it/s]Calculating Metrics:  73%|███████▎  | 8/11 [00:00<00:00, 1329.31it/s]Calculating Metrics:  73%|███████▎  | 8/11 [00:00<00:00, 1136.71it/s]Calculating Metrics:  82%|████████▏ | 9/11 [00:00<00:00, 1209.04it/s]Calculating Metrics:  82%|████████▏ | 9/11 [00:00<00:00, 1095.28it/s]Calculating Metrics:  91%|█████████ | 10/11 [00:00<00:00, 1101.30it/s]Calculating Metrics:  91%|█████████ | 10/11 [00:00<00:00, 1031.25it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 1111.77it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 1055.24it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 1047.60it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 1036.17it/s]
_ test_case_runner_v3_api[sqlite/column_distributional_expectations/expect_column_kl_divergence_to_be_less_than:manual_partition_undefined_infinite_kl_divergence0] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:  22%|██▏       | 2/9 [00:00<00:00, 3738.24it/s]Calculating Metrics:  22%|██▏       | 2/9 [00:00<00:00, 1055.97it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 1409.97it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 960.62it/s]Calculating Metrics:  67%|██████▋   | 6/9 [00:00<00:00, 1040.77it/s]Calculating Metrics:  67%|██████▋   | 6/9 [00:00<00:00, 893.39it/s] Calculating Metrics:  78%|███████▊  | 7/9 [00:00<00:00, 980.40it/s]Calculating Metrics:  78%|███████▊  | 7/9 [00:00<00:00, 889.43it/s]Calculating Metrics:  89%|████████▉ | 8/9 [00:00<00:00, 901.81it/s]Calculating Metrics:  89%|████████▉ | 8/9 [00:00<00:00, 848.17it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 934.56it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 890.20it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 883.65it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 875.48it/s]
_ test_case_runner_v3_api[sqlite/column_distributional_expectations/expect_column_kl_divergence_to_be_less_than:manual_partition_undefined_infinite_kl_divergence1] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/11 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/11 [00:00<?, ?it/s]Calculating Metrics:  36%|███▋      | 4/11 [00:00<00:00, 2187.10it/s]Calculating Metrics:  36%|███▋      | 4/11 [00:00<00:00, 1191.56it/s]Calculating Metrics:  55%|█████▍    | 6/11 [00:00<00:00, 1426.23it/s]Calculating Metrics:  55%|█████▍    | 6/11 [00:00<00:00, 1094.12it/s]Calculating Metrics:  73%|███████▎  | 8/11 [00:00<00:00, 926.02it/s]Calculating Metrics:  73%|███████▎  | 8/11 [00:00<00:00, 829.96it/s]Calculating Metrics:  82%|████████▏ | 9/11 [00:00<00:00, 896.60it/s]Calculating Metrics:  82%|████████▏ | 9/11 [00:00<00:00, 832.81it/s]Calculating Metrics:  91%|█████████ | 10/11 [00:00<00:00, 850.05it/s]Calculating Metrics:  91%|█████████ | 10/11 [00:00<00:00, 807.44it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 874.26it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 838.10it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 833.33it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 827.50it/s]
_ test_case_runner_v3_api[sqlite/column_distributional_expectations/expect_column_kl_divergence_to_be_less_than:manual_partition_undefined_infinite_kl_divergence_bins_do_not_cover_all_data] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/11 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/11 [00:00<?, ?it/s]Calculating Metrics:  36%|███▋      | 4/11 [00:00<00:00, 2467.60it/s]Calculating Metrics:  36%|███▋      | 4/11 [00:00<00:00, 1253.15it/s]Calculating Metrics:  55%|█████▍    | 6/11 [00:00<00:00, 1526.03it/s]Calculating Metrics:  55%|█████▍    | 6/11 [00:00<00:00, 1146.61it/s]Calculating Metrics:  73%|███████▎  | 8/11 [00:00<00:00, 1065.83it/s]Calculating Metrics:  73%|███████▎  | 8/11 [00:00<00:00, 939.43it/s]Calculating Metrics:  82%|████████▏ | 9/11 [00:00<00:00, 1008.76it/s]Calculating Metrics:  82%|████████▏ | 9/11 [00:00<00:00, 926.51it/s] Calculating Metrics:  91%|█████████ | 10/11 [00:00<00:00, 947.97it/s]Calculating Metrics:  91%|█████████ | 10/11 [00:00<00:00, 895.42it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 967.65it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 923.69it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 917.75it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 910.38it/s]
_ test_case_runner_v3_api[sqlite/column_distributional_expectations/expect_column_kl_divergence_to_be_less_than:manual_partition_return_partitions_should_have_tail_weights] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/11 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/11 [00:00<?, ?it/s]Calculating Metrics:  36%|███▋      | 4/11 [00:00<00:00, 1935.09it/s]Calculating Metrics:  36%|███▋      | 4/11 [00:00<00:00, 1121.32it/s]Calculating Metrics:  55%|█████▍    | 6/11 [00:00<00:00, 1403.17it/s]Calculating Metrics:  55%|█████▍    | 6/11 [00:00<00:00, 1070.61it/s]Calculating Metrics:  73%|███████▎  | 8/11 [00:00<00:00, 967.68it/s]Calculating Metrics:  73%|███████▎  | 8/11 [00:00<00:00, 862.07it/s]Calculating Metrics:  82%|████████▏ | 9/11 [00:00<00:00, 929.75it/s]Calculating Metrics:  82%|████████▏ | 9/11 [00:00<00:00, 850.10it/s]Calculating Metrics:  91%|█████████ | 10/11 [00:00<00:00, 856.54it/s]Calculating Metrics:  91%|█████████ | 10/11 [00:00<00:00, 813.13it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 879.93it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 843.09it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 837.90it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 831.81it/s]
_ test_case_runner_v3_api[sqlite/column_distributional_expectations/expect_column_kl_divergence_to_be_less_than:manual_partition_tail_weights_return_partitions_should_have_tail_weights] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/11 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/11 [00:00<?, ?it/s]Calculating Metrics:  36%|███▋      | 4/11 [00:00<00:00, 2154.24it/s]Calculating Metrics:  36%|███▋      | 4/11 [00:00<00:00, 1188.02it/s]Calculating Metrics:  55%|█████▍    | 6/11 [00:00<00:00, 1449.98it/s]Calculating Metrics:  55%|█████▍    | 6/11 [00:00<00:00, 1085.20it/s]Calculating Metrics:  73%|███████▎  | 8/11 [00:00<00:00, 909.61it/s]Calculating Metrics:  73%|███████▎  | 8/11 [00:00<00:00, 816.59it/s]Calculating Metrics:  82%|████████▏ | 9/11 [00:00<00:00, 882.54it/s]Calculating Metrics:  82%|████████▏ | 9/11 [00:00<00:00, 818.63it/s]Calculating Metrics:  91%|█████████ | 10/11 [00:00<00:00, 847.97it/s]Calculating Metrics:  91%|█████████ | 10/11 [00:00<00:00, 805.95it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 872.67it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 837.06it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 832.28it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 826.38it/s]
_ test_case_runner_v3_api[sqlite/column_distributional_expectations/expect_column_kl_divergence_to_be_less_than:manual_partition_bounded_endpoints_empty_tail_weights_in_return] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/11 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/11 [00:00<?, ?it/s]Calculating Metrics:  36%|███▋      | 4/11 [00:00<00:00, 3114.97it/s]Calculating Metrics:  36%|███▋      | 4/11 [00:00<00:00, 1406.54it/s]Calculating Metrics:  55%|█████▍    | 6/11 [00:00<00:00, 1673.15it/s]Calculating Metrics:  55%|█████▍    | 6/11 [00:00<00:00, 1231.33it/s]Calculating Metrics:  73%|███████▎  | 8/11 [00:00<00:00, 991.83it/s]Calculating Metrics:  73%|███████▎  | 8/11 [00:00<00:00, 881.64it/s]Calculating Metrics:  82%|████████▏ | 9/11 [00:00<00:00, 948.77it/s]Calculating Metrics:  82%|████████▏ | 9/11 [00:00<00:00, 875.66it/s]Calculating Metrics:  91%|█████████ | 10/11 [00:00<00:00, 902.37it/s]Calculating Metrics:  91%|█████████ | 10/11 [00:00<00:00, 854.55it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 923.91it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 883.47it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 878.02it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 871.42it/s]
_ test_case_runner_v3_api[sqlite/column_distributional_expectations/expect_column_kl_divergence_to_be_less_than:manual_partition_bounded_endpoints_observed_with_tail_weight_infinite_kl_divergence] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/11 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/11 [00:00<?, ?it/s]Calculating Metrics:  36%|███▋      | 4/11 [00:00<00:00, 2873.80it/s]Calculating Metrics:  36%|███▋      | 4/11 [00:00<00:00, 1376.42it/s]Calculating Metrics:  55%|█████▍    | 6/11 [00:00<00:00, 1640.75it/s]Calculating Metrics:  55%|█████▍    | 6/11 [00:00<00:00, 1197.18it/s]Calculating Metrics:  73%|███████▎  | 8/11 [00:00<00:00, 997.99it/s]Calculating Metrics:  73%|███████▎  | 8/11 [00:00<00:00, 885.34it/s]Calculating Metrics:  82%|████████▏ | 9/11 [00:00<00:00, 952.07it/s]Calculating Metrics:  82%|████████▏ | 9/11 [00:00<00:00, 878.92it/s]Calculating Metrics:  91%|█████████ | 10/11 [00:00<00:00, 896.64it/s]Calculating Metrics:  91%|█████████ | 10/11 [00:00<00:00, 850.06it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 919.73it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 880.85it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 875.59it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 869.09it/s]
_ test_case_runner_v3_api[sqlite/column_distributional_expectations/expect_column_kl_divergence_to_be_less_than:manual_partition_infinite_endpoints_non_zero_tail_hold_out] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/11 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/11 [00:00<?, ?it/s]Calculating Metrics:  36%|███▋      | 4/11 [00:00<00:00, 2853.27it/s]Calculating Metrics:  36%|███▋      | 4/11 [00:00<00:00, 1397.64it/s]Calculating Metrics:  55%|█████▍    | 6/11 [00:00<00:00, 1684.46it/s]Calculating Metrics:  55%|█████▍    | 6/11 [00:00<00:00, 1235.86it/s]Calculating Metrics:  73%|███████▎  | 8/11 [00:00<00:00, 1167.07it/s]Calculating Metrics:  73%|███████▎  | 8/11 [00:00<00:00, 1019.24it/s]Calculating Metrics:  82%|████████▏ | 9/11 [00:00<00:00, 1090.94it/s]Calculating Metrics:  82%|████████▏ | 9/11 [00:00<00:00, 995.48it/s]Calculating Metrics:  91%|█████████ | 10/11 [00:00<00:00, 1017.42it/s]Calculating Metrics:  91%|█████████ | 10/11 [00:00<00:00, 956.47it/s] Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 1032.78it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 982.59it/s]alculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 975.87it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 967.81it/s]
_ test_case_runner_v3_api[sqlite/column_distributional_expectations/expect_column_kl_divergence_to_be_less_than:manual_partition_upper_infinite_endpoint_non_zero_tail_hold_out] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/11 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/11 [00:00<?, ?it/s]Calculating Metrics:  36%|███▋      | 4/11 [00:00<00:00, 2358.67it/s]Calculating Metrics:  36%|███▋      | 4/11 [00:00<00:00, 1187.68it/s]Calculating Metrics:  55%|█████▍    | 6/11 [00:00<00:00, 1383.50it/s]Calculating Metrics:  55%|█████▍    | 6/11 [00:00<00:00, 943.28it/s]Calculating Metrics:  73%|███████▎  | 8/11 [00:00<00:00, 770.59it/s]Calculating Metrics:  73%|███████▎  | 8/11 [00:00<00:00, 682.90it/s]Calculating Metrics:  82%|████████▏ | 9/11 [00:00<00:00, 733.57it/s]Calculating Metrics:  82%|████████▏ | 9/11 [00:00<00:00, 676.29it/s]Calculating Metrics:  91%|█████████ | 10/11 [00:00<00:00, 690.52it/s]Calculating Metrics:  91%|█████████ | 10/11 [00:00<00:00, 654.42it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 708.14it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 678.31it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 674.11it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 669.27it/s]
_ test_case_runner_v3_api[sqlite/column_distributional_expectations/expect_column_kl_divergence_to_be_less_than:manual_partition_lower_infinite_endpoint_non_zero_tail_hold_out] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/11 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/11 [00:00<?, ?it/s]Calculating Metrics:  36%|███▋      | 4/11 [00:00<00:00, 2285.41it/s]Calculating Metrics:  36%|███▋      | 4/11 [00:00<00:00, 1067.73it/s]Calculating Metrics:  55%|█████▍    | 6/11 [00:00<00:00, 1293.67it/s]Calculating Metrics:  55%|█████▍    | 6/11 [00:00<00:00, 969.45it/s]Calculating Metrics:  73%|███████▎  | 8/11 [00:00<00:00, 852.50it/s]Calculating Metrics:  73%|███████▎  | 8/11 [00:00<00:00, 754.78it/s]Calculating Metrics:  82%|████████▏ | 9/11 [00:00<00:00, 807.67it/s]Calculating Metrics:  82%|████████▏ | 9/11 [00:00<00:00, 743.36it/s]Calculating Metrics:  91%|█████████ | 10/11 [00:00<00:00, 765.93it/s]Calculating Metrics:  91%|█████████ | 10/11 [00:00<00:00, 723.48it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 782.29it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 743.14it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 737.65it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 731.76it/s]
_ test_case_runner_v3_api[sqlite/column_distributional_expectations/expect_column_kl_divergence_to_be_less_than:manual_partition_bounded_endpoints_non_zero_tail_hold_out] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/11 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/11 [00:00<?, ?it/s]Calculating Metrics:  36%|███▋      | 4/11 [00:00<00:00, 2095.32it/s]Calculating Metrics:  36%|███▋      | 4/11 [00:00<00:00, 1132.14it/s]Calculating Metrics:  55%|█████▍    | 6/11 [00:00<00:00, 1322.15it/s]Calculating Metrics:  55%|█████▍    | 6/11 [00:00<00:00, 1011.45it/s]Calculating Metrics:  73%|███████▎  | 8/11 [00:00<00:00, 1003.90it/s]Calculating Metrics:  73%|███████▎  | 8/11 [00:00<00:00, 893.95it/s]Calculating Metrics:  82%|████████▏ | 9/11 [00:00<00:00, 963.30it/s]Calculating Metrics:  82%|████████▏ | 9/11 [00:00<00:00, 888.27it/s]Calculating Metrics:  91%|█████████ | 10/11 [00:00<00:00, 909.83it/s]Calculating Metrics:  91%|█████████ | 10/11 [00:00<00:00, 861.39it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 931.73it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 891.04it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 885.66it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 878.87it/s]
_ test_case_runner_v3_api[pandas/other_expectations/expect_table_row_count_to_equal:basic_positive_case] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/1 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/1 [00:00<?, ?it/s]Calculating Metrics: 100%|██████████| 1/1 [00:00<00:00, 4236.67it/s]Calculating Metrics: 100%|██████████| 1/1 [00:00<00:00, 2695.57it/s]Calculating Metrics: 100%|██████████| 1/1 [00:00<00:00, 2236.96it/s]Calculating Metrics: 100%|██████████| 1/1 [00:00<00:00, 1848.53it/s]
_ test_case_runner_v3_api[pandas/other_expectations/expect_table_row_count_to_equal:basic_negative_case_upper_error] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/1 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/1 [00:00<?, ?it/s]Calculating Metrics: 100%|██████████| 1/1 [00:00<00:00, 4629.47it/s]Calculating Metrics: 100%|██████████| 1/1 [00:00<00:00, 2880.70it/s]Calculating Metrics: 100%|██████████| 1/1 [00:00<00:00, 2369.66it/s]Calculating Metrics: 100%|██████████| 1/1 [00:00<00:00, 1968.23it/s]
_ test_case_runner_v3_api[pandas/other_expectations/expect_table_row_count_to_equal:basic_negative_case_lower_error] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/1 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/1 [00:00<?, ?it/s]Calculating Metrics: 100%|██████████| 1/1 [00:00<00:00, 4739.33it/s]Calculating Metrics: 100%|██████████| 1/1 [00:00<00:00, 2914.74it/s]Calculating Metrics: 100%|██████████| 1/1 [00:00<00:00, 2398.12it/s]Calculating Metrics: 100%|██████████| 1/1 [00:00<00:00, 1984.06it/s]
_ test_case_runner_v3_api[pandas/other_expectations/expect_table_row_count_to_equal:basic_negative_case_kwargs_args] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/1 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/1 [00:00<?, ?it/s]Calculating Metrics: 100%|██████████| 1/1 [00:00<00:00, 4848.91it/s]Calculating Metrics: 100%|██████████| 1/1 [00:00<00:00, 2824.45it/s]Calculating Metrics: 100%|██████████| 1/1 [00:00<00:00, 2114.06it/s]Calculating Metrics: 100%|██████████| 1/1 [00:00<00:00, 1760.09it/s]
_ test_case_runner_v3_api[pandas/other_expectations/expect_column_to_exist:basic_positive_test] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/2 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/2 [00:00<?, ?it/s]Calculating Metrics:  50%|█████     | 1/2 [00:00<00:00, 2391.28it/s]Calculating Metrics:  50%|█████     | 1/2 [00:00<00:00, 1382.89it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 1995.86it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 1694.67it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 1584.55it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 1469.37it/s]
_ test_case_runner_v3_api[pandas/other_expectations/expect_column_to_exist:basic_negative_test] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/2 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/2 [00:00<?, ?it/s]Calculating Metrics:  50%|█████     | 1/2 [00:00<00:00, 2487.72it/s]Calculating Metrics:  50%|█████     | 1/2 [00:00<00:00, 1321.04it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 1728.54it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 1430.77it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 1335.13it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 1239.09it/s]
_ test_case_runner_v3_api[pandas/other_expectations/expect_column_to_exist:positive_test_with_column_order] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/2 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/2 [00:00<?, ?it/s]Calculating Metrics:  50%|█████     | 1/2 [00:00<00:00, 2563.76it/s]Calculating Metrics:  50%|█████     | 1/2 [00:00<00:00, 1366.22it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 1935.98it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 1628.54it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 1517.48it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 1404.42it/s]
_ test_case_runner_v3_api[pandas/other_expectations/expect_column_to_exist:column_exists_but_wrong_index] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/2 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/2 [00:00<?, ?it/s]Calculating Metrics:  50%|█████     | 1/2 [00:00<00:00, 2666.44it/s]Calculating Metrics:  50%|█████     | 1/2 [00:00<00:00, 1362.67it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 1927.09it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 1610.72it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 1493.96it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 1382.21it/s]
_ test_case_runner_v3_api[pandas/other_expectations/expect_table_row_count_to_be_between:basic_positive_test] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/1 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/1 [00:00<?, ?it/s]Calculating Metrics: 100%|██████████| 1/1 [00:00<00:00, 3236.35it/s]Calculating Metrics: 100%|██████████| 1/1 [00:00<00:00, 2109.81it/s]Calculating Metrics: 100%|██████████| 1/1 [00:00<00:00, 1738.93it/s]Calculating Metrics: 100%|██████████| 1/1 [00:00<00:00, 1453.83it/s]
_ test_case_runner_v3_api[pandas/other_expectations/expect_table_row_count_to_be_between:vacuously_true] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/1 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/1 [00:00<?, ?it/s]Calculating Metrics: 100%|██████████| 1/1 [00:00<00:00, 3509.88it/s]Calculating Metrics: 100%|██████████| 1/1 [00:00<00:00, 2165.36it/s]Calculating Metrics: 100%|██████████| 1/1 [00:00<00:00, 1797.82it/s]Calculating Metrics: 100%|██████████| 1/1 [00:00<00:00, 1496.90it/s]
_ test_case_runner_v3_api[pandas/other_expectations/expect_table_row_count_to_be_between:basic_negative_test] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/1 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/1 [00:00<?, ?it/s]Calculating Metrics: 100%|██████████| 1/1 [00:00<00:00, 3666.35it/s]Calculating Metrics: 100%|██████████| 1/1 [00:00<00:00, 2299.51it/s]Calculating Metrics: 100%|██████████| 1/1 [00:00<00:00, 1894.45it/s]Calculating Metrics: 100%|██████████| 1/1 [00:00<00:00, 1565.04it/s]
_ test_case_runner_v3_api[pandas/other_expectations/expect_table_row_count_to_be_between:positive_test_with_null_min_value] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/1 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/1 [00:00<?, ?it/s]Calculating Metrics: 100%|██████████| 1/1 [00:00<00:00, 3634.58it/s]Calculating Metrics: 100%|██████████| 1/1 [00:00<00:00, 2298.25it/s]Calculating Metrics: 100%|██████████| 1/1 [00:00<00:00, 1897.02it/s]Calculating Metrics: 100%|██████████| 1/1 [00:00<00:00, 1574.44it/s]
_ test_case_runner_v3_api[pandas/other_expectations/expect_table_row_count_to_be_between:test_error_handling_for_min_greater_than_max] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/1 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/1 [00:00<?, ?it/s]Calculating Metrics: 100%|██████████| 1/1 [00:00<00:00, 3934.62it/s]Calculating Metrics: 100%|██████████| 1/1 [00:00<00:00, 2444.23it/s]Calculating Metrics: 100%|██████████| 1/1 [00:00<00:00, 2003.01it/s]Calculating Metrics: 100%|██████████| 1/1 [00:00<00:00, 1651.95it/s]
_ test_case_runner_v3_api[pandas/other_expectations/expect_table_row_count_to_be_between:positive_test_with_null_min] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/1 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/1 [00:00<?, ?it/s]Calculating Metrics: 100%|██████████| 1/1 [00:00<00:00, 3320.91it/s]Calculating Metrics: 100%|██████████| 1/1 [00:00<00:00, 2136.68it/s]Calculating Metrics: 100%|██████████| 1/1 [00:00<00:00, 1785.57it/s]Calculating Metrics: 100%|██████████| 1/1 [00:00<00:00, 1499.57it/s]
_ test_case_runner_v3_api[pandas/other_expectations/expect_table_columns_to_match_ordered_list:basic_positive_test] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/2 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/2 [00:00<?, ?it/s]Calculating Metrics:  50%|█████     | 1/2 [00:00<00:00, 2174.34it/s]Calculating Metrics:  50%|█████     | 1/2 [00:00<00:00, 1242.39it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 1837.99it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 1571.19it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 1471.69it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 1367.11it/s]
_ test_case_runner_v3_api[pandas/other_expectations/expect_table_columns_to_match_ordered_list:negative_test_column_is_missing] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/2 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/2 [00:00<?, ?it/s]Calculating Metrics:  50%|█████     | 1/2 [00:00<00:00, 2918.79it/s]Calculating Metrics:  50%|█████     | 1/2 [00:00<00:00, 1611.33it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 2317.94it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 1959.04it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 1830.37it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 1695.35it/s]
_ test_case_runner_v3_api[pandas/other_expectations/expect_table_columns_to_match_ordered_list:negative_test_column_is_misnamed] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/2 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/2 [00:00<?, ?it/s]Calculating Metrics:  50%|█████     | 1/2 [00:00<00:00, 3187.16it/s]Calculating Metrics:  50%|█████     | 1/2 [00:00<00:00, 1698.79it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 2398.12it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 2016.49it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 1874.55it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 1732.11it/s]
_ test_case_runner_v3_api[pandas/other_expectations/expect_table_columns_to_match_ordered_list:negative_test_columns_are_right_but_ordering_wrong] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/2 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/2 [00:00<?, ?it/s]Calculating Metrics:  50%|█████     | 1/2 [00:00<00:00, 2545.09it/s]Calculating Metrics:  50%|█████     | 1/2 [00:00<00:00, 1457.37it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 2132.34it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 1814.54it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 1700.51it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 1579.78it/s]
_ test_case_runner_v3_api[pandas/other_expectations/expect_table_columns_to_match_ordered_list:negative_test_extra_column] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/2 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/2 [00:00<?, ?it/s]Calculating Metrics:  50%|█████     | 1/2 [00:00<00:00, 2826.35it/s]Calculating Metrics:  50%|█████     | 1/2 [00:00<00:00, 1548.28it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 2204.63it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 1869.12it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 1748.36it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 1609.17it/s]
_ test_case_runner_v3_api[pandas/other_expectations/expect_table_columns_to_match_ordered_list:null_list_provides_vacuously_true_expectation] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/2 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/2 [00:00<?, ?it/s]Calculating Metrics:  50%|█████     | 1/2 [00:00<00:00, 3021.83it/s]Calculating Metrics:  50%|█████     | 1/2 [00:00<00:00, 1639.04it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 2339.27it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 1972.40it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 1841.22it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 1703.62it/s]
_ test_case_runner_v3_api[pandas/other_expectations/expect_table_column_count_to_be_between:basic_positive_test] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/3 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/3 [00:00<?, ?it/s]Calculating Metrics:  33%|███▎      | 1/3 [00:00<00:00, 2700.78it/s]Calculating Metrics:  33%|███▎      | 1/3 [00:00<00:00, 1268.69it/s]Calculating Metrics:  67%|██████▋   | 2/3 [00:00<00:00, 1932.41it/s]Calculating Metrics:  67%|██████▋   | 2/3 [00:00<00:00, 1482.35it/s]Calculating Metrics: 100%|██████████| 3/3 [00:00<00:00, 1879.45it/s]Calculating Metrics: 100%|██████████| 3/3 [00:00<00:00, 1672.15it/s]Calculating Metrics: 100%|██████████| 3/3 [00:00<00:00, 1604.96it/s]Calculating Metrics: 100%|██████████| 3/3 [00:00<00:00, 1530.40it/s]
_ test_case_runner_v3_api[pandas/other_expectations/expect_table_column_count_to_be_between:vacuously_true] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/3 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/3 [00:00<?, ?it/s]Calculating Metrics:  33%|███▎      | 1/3 [00:00<00:00, 3344.74it/s]Calculating Metrics:  33%|███▎      | 1/3 [00:00<00:00, 1410.80it/s]Calculating Metrics:  67%|██████▋   | 2/3 [00:00<00:00, 2099.25it/s]Calculating Metrics:  67%|██████▋   | 2/3 [00:00<00:00, 1602.71it/s]Calculating Metrics: 100%|██████████| 3/3 [00:00<00:00, 2024.28it/s]Calculating Metrics: 100%|██████████| 3/3 [00:00<00:00, 1795.51it/s]Calculating Metrics: 100%|██████████| 3/3 [00:00<00:00, 1721.33it/s]Calculating Metrics: 100%|██████████| 3/3 [00:00<00:00, 1640.32it/s]
_ test_case_runner_v3_api[pandas/other_expectations/expect_table_column_count_to_be_between:basic_negative_test] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/3 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/3 [00:00<?, ?it/s]Calculating Metrics:  33%|███▎      | 1/3 [00:00<00:00, 2457.12it/s]Calculating Metrics:  33%|███▎      | 1/3 [00:00<00:00, 1219.63it/s]Calculating Metrics:  67%|██████▋   | 2/3 [00:00<00:00, 1880.01it/s]Calculating Metrics:  67%|██████▋   | 2/3 [00:00<00:00, 1472.98it/s]Calculating Metrics: 100%|██████████| 3/3 [00:00<00:00, 1889.33it/s]Calculating Metrics: 100%|██████████| 3/3 [00:00<00:00, 1688.30it/s]Calculating Metrics: 100%|██████████| 3/3 [00:00<00:00, 1623.39it/s]Calculating Metrics: 100%|██████████| 3/3 [00:00<00:00, 1549.81it/s]
_ test_case_runner_v3_api[pandas/other_expectations/expect_table_column_count_to_be_between:positive_test_with_null_min_value] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/3 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/3 [00:00<?, ?it/s]Calculating Metrics:  33%|███▎      | 1/3 [00:00<00:00, 3134.76it/s]Calculating Metrics:  33%|███▎      | 1/3 [00:00<00:00, 1368.01it/s]Calculating Metrics:  67%|██████▋   | 2/3 [00:00<00:00, 2039.04it/s]Calculating Metrics:  67%|██████▋   | 2/3 [00:00<00:00, 1569.72it/s]Calculating Metrics: 100%|██████████| 3/3 [00:00<00:00, 1987.82it/s]Calculating Metrics: 100%|██████████| 3/3 [00:00<00:00, 1769.00it/s]Calculating Metrics: 100%|██████████| 3/3 [00:00<00:00, 1697.87it/s]Calculating Metrics: 100%|██████████| 3/3 [00:00<00:00, 1618.17it/s]
_ test_case_runner_v3_api[pandas/other_expectations/expect_table_column_count_to_be_between:test_error_handling_for_max_lt_min] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/3 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/3 [00:00<?, ?it/s]Calculating Metrics:  33%|███▎      | 1/3 [00:00<00:00, 3084.05it/s]Calculating Metrics:  33%|███▎      | 1/3 [00:00<00:00, 1353.00it/s]Calculating Metrics:  67%|██████▋   | 2/3 [00:00<00:00, 2028.19it/s]Calculating Metrics:  67%|██████▋   | 2/3 [00:00<00:00, 1558.64it/s]Calculating Metrics: 100%|██████████| 3/3 [00:00<00:00, 1970.70it/s]Calculating Metrics: 100%|██████████| 3/3 [00:00<00:00, 1637.55it/s]Calculating Metrics: 100%|██████████| 3/3 [00:00<00:00, 1567.18it/s]Calculating Metrics: 100%|██████████| 3/3 [00:00<00:00, 1492.46it/s]
_ test_case_runner_v3_api[pandas/other_expectations/expect_table_column_count_to_be_between:positive_test_with_null_min] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/3 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/3 [00:00<?, ?it/s]Calculating Metrics:  33%|███▎      | 1/3 [00:00<00:00, 2849.39it/s]Calculating Metrics:  33%|███▎      | 1/3 [00:00<00:00, 1303.79it/s]Calculating Metrics:  67%|██████▋   | 2/3 [00:00<00:00, 1838.40it/s]Calculating Metrics:  67%|██████▋   | 2/3 [00:00<00:00, 1411.51it/s]Calculating Metrics: 100%|██████████| 3/3 [00:00<00:00, 1760.59it/s]Calculating Metrics: 100%|██████████| 3/3 [00:00<00:00, 1560.19it/s]Calculating Metrics: 100%|██████████| 3/3 [00:00<00:00, 1499.39it/s]Calculating Metrics: 100%|██████████| 3/3 [00:00<00:00, 1431.34it/s]
_ test_case_runner_v3_api[pandas/other_expectations/expect_table_columns_to_match_set:basic_positive_test_exact_match_true] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/2 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/2 [00:00<?, ?it/s]Calculating Metrics:  50%|█████     | 1/2 [00:00<00:00, 2652.94it/s]Calculating Metrics:  50%|█████     | 1/2 [00:00<00:00, 1508.74it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 2190.81it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 1860.41it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 1740.74it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 1615.37it/s]
_ test_case_runner_v3_api[pandas/other_expectations/expect_table_columns_to_match_set:basic_positive_test_exact_match_false] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/2 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/2 [00:00<?, ?it/s]Calculating Metrics:  50%|█████     | 1/2 [00:00<00:00, 2702.52it/s]Calculating Metrics:  50%|█████     | 1/2 [00:00<00:00, 1503.87it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 2157.56it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 1795.12it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 1675.04it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 1551.72it/s]
_ test_case_runner_v3_api[pandas/other_expectations/expect_table_columns_to_match_set:negative_test_column_is_missing_exact_match_true] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/2 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/2 [00:00<?, ?it/s]Calculating Metrics:  50%|█████     | 1/2 [00:00<00:00, 2623.08it/s]Calculating Metrics:  50%|█████     | 1/2 [00:00<00:00, 1394.38it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 1968.23it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 1653.91it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 1540.61it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 1425.18it/s]
_ test_case_runner_v3_api[pandas/other_expectations/expect_table_columns_to_match_set:positive_test_column_is_missing_exact_match_false] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/2 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/2 [00:00<?, ?it/s]Calculating Metrics:  50%|█████     | 1/2 [00:00<00:00, 2914.74it/s]Calculating Metrics:  50%|█████     | 1/2 [00:00<00:00, 1521.88it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 2170.96it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 1829.58it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 1706.39it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 1579.78it/s]
_ test_case_runner_v3_api[pandas/other_expectations/expect_table_columns_to_match_set:negative_test_column_is_misnamed_exact_match_true] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/2 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/2 [00:00<?, ?it/s]Calculating Metrics:  50%|█████     | 1/2 [00:00<00:00, 2611.65it/s]Calculating Metrics:  50%|█████     | 1/2 [00:00<00:00, 1476.87it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 2164.24it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 1844.87it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 1728.54it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 1604.86it/s]
_ test_case_runner_v3_api[pandas/other_expectations/expect_table_columns_to_match_set:negative_test_column_is_misnamed_exact_match_false] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/2 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/2 [00:00<?, ?it/s]Calculating Metrics:  50%|█████     | 1/2 [00:00<00:00, 3182.32it/s]Calculating Metrics:  50%|█████     | 1/2 [00:00<00:00, 1691.93it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 2394.69it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 2009.73it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 1874.55it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 1734.62it/s]
_ test_case_runner_v3_api[pandas/other_expectations/expect_table_columns_to_match_set:positive_test_columns_are_right_but_ordering_wrong_exact_match_true] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/2 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/2 [00:00<?, ?it/s]Calculating Metrics:  50%|█████     | 1/2 [00:00<00:00, 3313.04it/s]Calculating Metrics:  50%|█████     | 1/2 [00:00<00:00, 1736.05it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 2438.55it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 2043.01it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 1901.32it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 1756.04it/s]
_ test_case_runner_v3_api[pandas/other_expectations/expect_table_columns_to_match_set:positive_test_columns_are_right_but_ordering_wrong_exact_match_false] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/2 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/2 [00:00<?, ?it/s]Calculating Metrics:  50%|█████     | 1/2 [00:00<00:00, 2959.99it/s]Calculating Metrics:  50%|█████     | 1/2 [00:00<00:00, 1623.81it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 2323.07it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 1960.87it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 1829.98it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 1692.28it/s]
_ test_case_runner_v3_api[pandas/other_expectations/expect_table_columns_to_match_set:negative_test_extra_column_exact_match_true] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/2 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/2 [00:00<?, ?it/s]Calculating Metrics:  50%|█████     | 1/2 [00:00<00:00, 2857.16it/s]Calculating Metrics:  50%|█████     | 1/2 [00:00<00:00, 1567.38it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 2257.43it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 1913.90it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 1776.12it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 1644.83it/s]
_ test_case_runner_v3_api[pandas/other_expectations/expect_table_columns_to_match_set:negative_test_extra_column_exact_match_false] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/2 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/2 [00:00<?, ?it/s]Calculating Metrics:  50%|█████     | 1/2 [00:00<00:00, 2809.31it/s]Calculating Metrics:  50%|█████     | 1/2 [00:00<00:00, 1569.72it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 2264.74it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 1919.59it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 1797.05it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 1665.40it/s]
_ test_case_runner_v3_api[pandas/other_expectations/expect_table_columns_to_match_set:negative_test_null_set_exact_match_true] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/2 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/2 [00:00<?, ?it/s]Calculating Metrics:  50%|█████     | 1/2 [00:00<00:00, 2087.76it/s]Calculating Metrics:  50%|█████     | 1/2 [00:00<00:00, 1309.08it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 1971.01it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 1698.10it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 1593.88it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 1482.87it/s]
_ test_case_runner_v3_api[pandas/other_expectations/expect_table_columns_to_match_set:positive_test_vacuously_true_null_set_exact_match_false] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/2 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/2 [00:00<?, ?it/s]Calculating Metrics:  50%|█████     | 1/2 [00:00<00:00, 3184.74it/s]Calculating Metrics:  50%|█████     | 1/2 [00:00<00:00, 1655.86it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 2363.65it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 1904.77it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 1760.83it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 1625.70it/s]
_ test_case_runner_v3_api[pandas/other_expectations/expect_table_column_count_to_equal:basic_positive_case] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/3 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/3 [00:00<?, ?it/s]Calculating Metrics:  33%|███▎      | 1/3 [00:00<00:00, 2968.37it/s]Calculating Metrics:  33%|███▎      | 1/3 [00:00<00:00, 1328.15it/s]Calculating Metrics:  67%|██████▋   | 2/3 [00:00<00:00, 2000.14it/s]Calculating Metrics:  67%|██████▋   | 2/3 [00:00<00:00, 1540.89it/s]Calculating Metrics: 100%|██████████| 3/3 [00:00<00:00, 1952.05it/s]Calculating Metrics: 100%|██████████| 3/3 [00:00<00:00, 1735.33it/s]Calculating Metrics: 100%|██████████| 3/3 [00:00<00:00, 1663.75it/s]Calculating Metrics: 100%|██████████| 3/3 [00:00<00:00, 1587.35it/s]
_ test_case_runner_v3_api[pandas/other_expectations/expect_table_column_count_to_equal:basic_negative_case_upper_error] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/3 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/3 [00:00<?, ?it/s]Calculating Metrics:  33%|███▎      | 1/3 [00:00<00:00, 3281.93it/s]Calculating Metrics:  33%|███▎      | 1/3 [00:00<00:00, 1392.99it/s]Calculating Metrics:  67%|██████▋   | 2/3 [00:00<00:00, 2075.36it/s]Calculating Metrics:  67%|██████▋   | 2/3 [00:00<00:00, 1586.05it/s]Calculating Metrics: 100%|██████████| 3/3 [00:00<00:00, 2003.01it/s]Calculating Metrics: 100%|██████████| 3/3 [00:00<00:00, 1776.49it/s]Calculating Metrics: 100%|██████████| 3/3 [00:00<00:00, 1703.62it/s]Calculating Metrics: 100%|██████████| 3/3 [00:00<00:00, 1623.39it/s]
_ test_case_runner_v3_api[pandas/other_expectations/expect_table_column_count_to_equal:basic_negative_case_lower_error] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/3 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/3 [00:00<?, ?it/s]Calculating Metrics:  33%|███▎      | 1/3 [00:00<00:00, 2941.31it/s]Calculating Metrics:  33%|███▎      | 1/3 [00:00<00:00, 1318.96it/s]Calculating Metrics:  67%|██████▋   | 2/3 [00:00<00:00, 1985.94it/s]Calculating Metrics:  67%|██████▋   | 2/3 [00:00<00:00, 1386.78it/s]Calculating Metrics: 100%|██████████| 3/3 [00:00<00:00, 1722.04it/s]Calculating Metrics: 100%|██████████| 3/3 [00:00<00:00, 1541.46it/s]Calculating Metrics: 100%|██████████| 3/3 [00:00<00:00, 1482.78it/s]Calculating Metrics: 100%|██████████| 3/3 [00:00<00:00, 1416.99it/s]
_ test_case_runner_v3_api[pandas/other_expectations/expect_table_column_count_to_equal:basic_negative_case_kwargs_args] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/3 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/3 [00:00<?, ?it/s]Calculating Metrics:  33%|███▎      | 1/3 [00:00<00:00, 2882.68it/s]Calculating Metrics:  33%|███▎      | 1/3 [00:00<00:00, 1319.38it/s]Calculating Metrics:  67%|██████▋   | 2/3 [00:00<00:00, 1846.90it/s]Calculating Metrics:  67%|██████▋   | 2/3 [00:00<00:00, 1412.46it/s]Calculating Metrics: 100%|██████████| 3/3 [00:00<00:00, 1805.30it/s]Calculating Metrics: 100%|██████████| 3/3 [00:00<00:00, 1613.19it/s]Calculating Metrics: 100%|██████████| 3/3 [00:00<00:00, 1550.57it/s]Calculating Metrics: 100%|██████████| 3/3 [00:00<00:00, 1481.56it/s]
_ test_case_runner_v3_api[sqlite/other_expectations/expect_table_row_count_to_equal:basic_positive_case] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/2 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/2 [00:00<?, ?it/s]Calculating Metrics:  50%|█████     | 1/2 [00:00<00:00, 3125.41it/s]Calculating Metrics:  50%|█████     | 1/2 [00:00<00:00, 1497.43it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 1019.89it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 914.89it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 876.00it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 829.16it/s]
_ test_case_runner_v3_api[sqlite/other_expectations/expect_table_row_count_to_equal:basic_negative_case_upper_error] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/2 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/2 [00:00<?, ?it/s]Calculating Metrics:  50%|█████     | 1/2 [00:00<00:00, 3163.13it/s]Calculating Metrics:  50%|█████     | 1/2 [00:00<00:00, 1517.48it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 1138.83it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 1013.73it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 967.66it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 912.40it/s]
_ test_case_runner_v3_api[sqlite/other_expectations/expect_table_row_count_to_equal:basic_negative_case_lower_error] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/2 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/2 [00:00<?, ?it/s]Calculating Metrics:  50%|█████     | 1/2 [00:00<00:00, 3366.22it/s]Calculating Metrics:  50%|█████     | 1/2 [00:00<00:00, 1559.80it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 1292.74it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 1132.52it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 1073.54it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 1012.14it/s]
_ test_case_runner_v3_api[sqlite/other_expectations/expect_table_row_count_to_equal:basic_negative_case_kwargs_args] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/2 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/2 [00:00<?, ?it/s]Calculating Metrics:  50%|█████     | 1/2 [00:00<00:00, 3597.17it/s]Calculating Metrics:  50%|█████     | 1/2 [00:00<00:00, 1618.17it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 1284.63it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 1121.77it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 1063.33it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 1003.06it/s]
_ test_case_runner_v3_api[sqlite/other_expectations/expect_column_to_exist:basic_positive_test] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/2 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/2 [00:00<?, ?it/s]Calculating Metrics:  50%|█████     | 1/2 [00:00<00:00, 1801.68it/s]Calculating Metrics:  50%|█████     | 1/2 [00:00<00:00, 1110.19it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 1662.76it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 1429.55it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 1344.33it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 1252.40it/s]
_ test_case_runner_v3_api[sqlite/other_expectations/expect_column_to_exist:basic_negative_test] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/2 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/2 [00:00<?, ?it/s]Calculating Metrics:  50%|█████     | 1/2 [00:00<00:00, 2159.79it/s]Calculating Metrics:  50%|█████     | 1/2 [00:00<00:00, 1254.65it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 1826.39it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 1550.29it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 1450.06it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 1346.70it/s]
_ test_case_runner_v3_api[sqlite/other_expectations/expect_column_to_exist:positive_test_with_column_order] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/2 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/2 [00:00<?, ?it/s]Calculating Metrics:  50%|█████     | 1/2 [00:00<00:00, 2070.24it/s]Calculating Metrics:  50%|█████     | 1/2 [00:00<00:00, 1219.63it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 1790.52it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 1529.09it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 1432.73it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 1331.53it/s]
_ test_case_runner_v3_api[sqlite/other_expectations/expect_column_to_exist:column_exists_but_wrong_index] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/2 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/2 [00:00<?, ?it/s]Calculating Metrics:  50%|█████     | 1/2 [00:00<00:00, 2299.51it/s]Calculating Metrics:  50%|█████     | 1/2 [00:00<00:00, 1210.83it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 1679.40it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 1408.67it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 1330.68it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 1251.66it/s]
_ test_case_runner_v3_api[sqlite/other_expectations/expect_table_row_count_to_be_between:basic_positive_test] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/2 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/2 [00:00<?, ?it/s]Calculating Metrics:  50%|█████     | 1/2 [00:00<00:00, 3663.15it/s]Calculating Metrics:  50%|█████     | 1/2 [00:00<00:00, 1738.93it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 994.50it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 899.29it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 863.91it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 824.11it/s]
_ test_case_runner_v3_api[sqlite/other_expectations/expect_table_row_count_to_be_between:vacuously_true] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/2 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/2 [00:00<?, ?it/s]Calculating Metrics:  50%|█████     | 1/2 [00:00<00:00, 3802.63it/s]Calculating Metrics:  50%|█████     | 1/2 [00:00<00:00, 1769.75it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 1459.65it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 1273.90it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 1206.82it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 1136.36it/s]
_ test_case_runner_v3_api[sqlite/other_expectations/expect_table_row_count_to_be_between:basic_negative_test] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/2 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/2 [00:00<?, ?it/s]Calculating Metrics:  50%|█████     | 1/2 [00:00<00:00, 3495.25it/s]Calculating Metrics:  50%|█████     | 1/2 [00:00<00:00, 1594.79it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 1321.04it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 1157.37it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 1097.12it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 1034.23it/s]
_ test_case_runner_v3_api[sqlite/other_expectations/expect_table_row_count_to_be_between:positive_test_with_null_min_value] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/2 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/2 [00:00<?, ?it/s]Calculating Metrics:  50%|█████     | 1/2 [00:00<00:00, 3721.65it/s]Calculating Metrics:  50%|█████     | 1/2 [00:00<00:00, 1669.71it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 1384.26it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 1206.30it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 1140.38it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 1072.99it/s]
_ test_case_runner_v3_api[sqlite/other_expectations/expect_table_row_count_to_be_between:test_error_handling_for_min_greater_than_max] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/2 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/2 [00:00<?, ?it/s]Calculating Metrics:  50%|█████     | 1/2 [00:00<00:00, 3246.37it/s]Calculating Metrics:  50%|█████     | 1/2 [00:00<00:00, 1448.81it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 1176.36it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 1038.32it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 986.20it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 917.39it/s]
_ test_case_runner_v3_api[sqlite/other_expectations/expect_table_row_count_to_be_between:positive_test_with_null_min] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/2 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/2 [00:00<?, ?it/s]Calculating Metrics:  50%|█████     | 1/2 [00:00<00:00, 3597.17it/s]Calculating Metrics:  50%|█████     | 1/2 [00:00<00:00, 1625.70it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 1332.16it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 1109.90it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 1049.76it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 993.44it/s]
_ test_case_runner_v3_api[sqlite/other_expectations/expect_table_columns_to_match_ordered_list:basic_positive_test] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/2 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/2 [00:00<?, ?it/s]Calculating Metrics:  50%|█████     | 1/2 [00:00<00:00, 2078.45it/s]Calculating Metrics:  50%|█████     | 1/2 [00:00<00:00, 1283.84it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 1938.22it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 1532.45it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 1427.36it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 1333.43it/s]
_ test_case_runner_v3_api[sqlite/other_expectations/expect_table_columns_to_match_ordered_list:negative_test_column_is_missing] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/2 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/2 [00:00<?, ?it/s]Calculating Metrics:  50%|█████     | 1/2 [00:00<00:00, 1655.86it/s]Calculating Metrics:  50%|█████     | 1/2 [00:00<00:00, 1124.78it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 1748.36it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 1530.21it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 1449.31it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 1360.68it/s]
_ test_case_runner_v3_api[sqlite/other_expectations/expect_table_columns_to_match_ordered_list:negative_test_column_is_misnamed] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/2 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/2 [00:00<?, ?it/s]Calculating Metrics:  50%|█████     | 1/2 [00:00<00:00, 2236.96it/s]Calculating Metrics:  50%|█████     | 1/2 [00:00<00:00, 1342.18it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 2014.07it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 1737.49it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 1633.93it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 1522.16it/s]
_ test_case_runner_v3_api[sqlite/other_expectations/expect_table_columns_to_match_ordered_list:negative_test_columns_are_right_but_ordering_wrong] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/2 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/2 [00:00<?, ?it/s]Calculating Metrics:  50%|█████     | 1/2 [00:00<00:00, 2571.61it/s]Calculating Metrics:  50%|█████     | 1/2 [00:00<00:00, 1506.57it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 2155.35it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 1835.18it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 1718.27it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 1596.31it/s]
_ test_case_runner_v3_api[sqlite/other_expectations/expect_table_columns_to_match_ordered_list:negative_test_extra_column] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/2 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/2 [00:00<?, ?it/s]Calculating Metrics:  50%|█████     | 1/2 [00:00<00:00, 2278.27it/s]Calculating Metrics:  50%|█████     | 1/2 [00:00<00:00, 1184.83it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 1728.54it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 1480.52it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 1388.84it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 1279.53it/s]
_ test_case_runner_v3_api[sqlite/other_expectations/expect_table_columns_to_match_ordered_list:null_list_provides_vacuously_true_expectation] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/2 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/2 [00:00<?, ?it/s]Calculating Metrics:  50%|█████     | 1/2 [00:00<00:00, 2142.14it/s]Calculating Metrics:  50%|█████     | 1/2 [00:00<00:00, 1239.45it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 1800.52it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 1529.09it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 1430.77it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 1328.15it/s]
_ test_case_runner_v3_api[sqlite/other_expectations/expect_table_column_count_to_be_between:basic_positive_test] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/3 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/3 [00:00<?, ?it/s]Calculating Metrics:  33%|███▎      | 1/3 [00:00<00:00, 2099.25it/s]Calculating Metrics:  33%|███▎      | 1/3 [00:00<00:00, 1081.84it/s]Calculating Metrics:  67%|██████▋   | 2/3 [00:00<00:00, 1678.73it/s]Calculating Metrics:  67%|██████▋   | 2/3 [00:00<00:00, 1323.33it/s]Calculating Metrics: 100%|██████████| 3/3 [00:00<00:00, 1686.94it/s]Calculating Metrics: 100%|██████████| 3/3 [00:00<00:00, 1472.72it/s]Calculating Metrics: 100%|██████████| 3/3 [00:00<00:00, 1402.31it/s]Calculating Metrics: 100%|██████████| 3/3 [00:00<00:00, 1337.33it/s]
_ test_case_runner_v3_api[sqlite/other_expectations/expect_table_column_count_to_be_between:vacuously_true] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/3 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/3 [00:00<?, ?it/s]Calculating Metrics:  33%|███▎      | 1/3 [00:00<00:00, 2391.28it/s]Calculating Metrics:  33%|███▎      | 1/3 [00:00<00:00, 1192.92it/s]Calculating Metrics:  67%|██████▋   | 2/3 [00:00<00:00, 1838.40it/s]Calculating Metrics:  67%|██████▋   | 2/3 [00:00<00:00, 1443.82it/s]Calculating Metrics: 100%|██████████| 3/3 [00:00<00:00, 1843.65it/s]Calculating Metrics: 100%|██████████| 3/3 [00:00<00:00, 1646.33it/s]Calculating Metrics: 100%|██████████| 3/3 [00:00<00:00, 1582.16it/s]Calculating Metrics: 100%|██████████| 3/3 [00:00<00:00, 1511.28it/s]
_ test_case_runner_v3_api[sqlite/other_expectations/expect_table_column_count_to_be_between:basic_negative_test] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/3 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/3 [00:00<?, ?it/s]Calculating Metrics:  33%|███▎      | 1/3 [00:00<00:00, 2543.54it/s]Calculating Metrics:  33%|███▎      | 1/3 [00:00<00:00, 1232.89it/s]Calculating Metrics:  67%|██████▋   | 2/3 [00:00<00:00, 1891.88it/s]Calculating Metrics:  67%|██████▋   | 2/3 [00:00<00:00, 1478.95it/s]Calculating Metrics: 100%|██████████| 3/3 [00:00<00:00, 1884.23it/s]Calculating Metrics: 100%|██████████| 3/3 [00:00<00:00, 1681.53it/s]Calculating Metrics: 100%|██████████| 3/3 [00:00<00:00, 1614.64it/s]Calculating Metrics: 100%|██████████| 3/3 [00:00<00:00, 1542.40it/s]
_ test_case_runner_v3_api[sqlite/other_expectations/expect_table_column_count_to_be_between:positive_test_with_null_min_value] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/3 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/3 [00:00<?, ?it/s]Calculating Metrics:  33%|███▎      | 1/3 [00:00<00:00, 2212.19it/s]Calculating Metrics:  33%|███▎      | 1/3 [00:00<00:00, 1154.82it/s]Calculating Metrics:  67%|██████▋   | 2/3 [00:00<00:00, 1795.12it/s]Calculating Metrics:  67%|██████▋   | 2/3 [00:00<00:00, 1421.56it/s]Calculating Metrics: 100%|██████████| 3/3 [00:00<00:00, 1823.61it/s]Calculating Metrics: 100%|██████████| 3/3 [00:00<00:00, 1634.99it/s]Calculating Metrics: 100%|██████████| 3/3 [00:00<00:00, 1573.26it/s]Calculating Metrics: 100%|██████████| 3/3 [00:00<00:00, 1502.26it/s]
_ test_case_runner_v3_api[sqlite/other_expectations/expect_table_column_count_to_be_between:test_error_handling_for_max_lt_min] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/3 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/3 [00:00<?, ?it/s]Calculating Metrics:  33%|███▎      | 1/3 [00:00<00:00, 2257.43it/s]Calculating Metrics:  33%|███▎      | 1/3 [00:00<00:00, 1161.54it/s]Calculating Metrics:  67%|██████▋   | 2/3 [00:00<00:00, 1773.12it/s]Calculating Metrics:  67%|██████▋   | 2/3 [00:00<00:00, 1400.67it/s]Calculating Metrics: 100%|██████████| 3/3 [00:00<00:00, 1788.87it/s]Calculating Metrics: 100%|██████████| 3/3 [00:00<00:00, 1604.35it/s]Calculating Metrics: 100%|██████████| 3/3 [00:00<00:00, 1534.50it/s]Calculating Metrics: 100%|██████████| 3/3 [00:00<00:00, 1463.47it/s]
_ test_case_runner_v3_api[sqlite/other_expectations/expect_table_column_count_to_be_between:positive_test_with_null_min] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/3 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/3 [00:00<?, ?it/s]Calculating Metrics:  33%|███▎      | 1/3 [00:00<00:00, 1579.78it/s]Calculating Metrics:  33%|███▎      | 1/3 [00:00<00:00, 950.44it/s]Calculating Metrics:  67%|██████▋   | 2/3 [00:00<00:00, 1538.63it/s]Calculating Metrics:  67%|██████▋   | 2/3 [00:00<00:00, 1250.17it/s]Calculating Metrics: 100%|██████████| 3/3 [00:00<00:00, 1633.08it/s]Calculating Metrics: 100%|██████████| 3/3 [00:00<00:00, 1480.17it/s]Calculating Metrics: 100%|██████████| 3/3 [00:00<00:00, 1428.74it/s]Calculating Metrics: 100%|██████████| 3/3 [00:00<00:00, 1370.54it/s]
_ test_case_runner_v3_api[sqlite/other_expectations/expect_table_columns_to_match_set:basic_positive_test_exact_match_true] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/2 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/2 [00:00<?, ?it/s]Calculating Metrics:  50%|█████     | 1/2 [00:00<00:00, 2197.12it/s]Calculating Metrics:  50%|█████     | 1/2 [00:00<00:00, 1232.89it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 1858.77it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 1614.13it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 1522.99it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 1424.21it/s]
_ test_case_runner_v3_api[sqlite/other_expectations/expect_table_columns_to_match_set:basic_positive_test_exact_match_false] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/2 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/2 [00:00<?, ?it/s]Calculating Metrics:  50%|█████     | 1/2 [00:00<00:00, 2526.69it/s]Calculating Metrics:  50%|█████     | 1/2 [00:00<00:00, 1484.18it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 2174.34it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 1855.89it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 1736.05it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 1613.19it/s]
_ test_case_runner_v3_api[sqlite/other_expectations/expect_table_columns_to_match_set:negative_test_column_is_missing_exact_match_true] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/2 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/2 [00:00<?, ?it/s]Calculating Metrics:  50%|█████     | 1/2 [00:00<00:00, 2697.30it/s]Calculating Metrics:  50%|█████     | 1/2 [00:00<00:00, 1531.89it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 2228.05it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 1891.03it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 1770.12it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 1643.54it/s]
_ test_case_runner_v3_api[sqlite/other_expectations/expect_table_columns_to_match_set:positive_test_column_is_missing_exact_match_false] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/2 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/2 [00:00<?, ?it/s]Calculating Metrics:  50%|█████     | 1/2 [00:00<00:00, 2525.17it/s]Calculating Metrics:  50%|█████     | 1/2 [00:00<00:00, 1476.87it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 1980.31it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 1665.40it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 1561.25it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 1455.60it/s]
_ test_case_runner_v3_api[sqlite/other_expectations/expect_table_columns_to_match_set:negative_test_column_is_misnamed_exact_match_true] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/2 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/2 [00:00<?, ?it/s]Calculating Metrics:  50%|█████     | 1/2 [00:00<00:00, 2487.72it/s]Calculating Metrics:  50%|█████     | 1/2 [00:00<00:00, 1457.37it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 2139.41it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 1826.79it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 1709.52it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 1589.96it/s]
_ test_case_runner_v3_api[sqlite/other_expectations/expect_table_columns_to_match_set:negative_test_column_is_misnamed_exact_match_false] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/2 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/2 [00:00<?, ?it/s]Calculating Metrics:  50%|█████     | 1/2 [00:00<00:00, 2114.06it/s]Calculating Metrics:  50%|█████     | 1/2 [00:00<00:00, 1307.04it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 1932.41it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 1668.05it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 1567.38it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 1425.42it/s]
_ test_case_runner_v3_api[sqlite/other_expectations/expect_table_columns_to_match_set:positive_test_columns_are_right_but_ordering_wrong_exact_match_true] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/2 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/2 [00:00<?, ?it/s]Calculating Metrics:  50%|█████     | 1/2 [00:00<00:00, 2165.36it/s]Calculating Metrics:  50%|█████     | 1/2 [00:00<00:00, 1295.74it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 1949.93it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 1685.14it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 1585.15it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 1480.52it/s]
_ test_case_runner_v3_api[sqlite/other_expectations/expect_table_columns_to_match_set:positive_test_columns_are_right_but_ordering_wrong_exact_match_false] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/2 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/2 [00:00<?, ?it/s]Calculating Metrics:  50%|█████     | 1/2 [00:00<00:00, 1799.36it/s]Calculating Metrics:  50%|█████     | 1/2 [00:00<00:00, 1193.60it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 1845.27it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 1607.94it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 1516.38it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 1417.71it/s]
_ test_case_runner_v3_api[sqlite/other_expectations/expect_table_columns_to_match_set:negative_test_extra_column_exact_match_true] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/2 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/2 [00:00<?, ?it/s]Calculating Metrics:  50%|█████     | 1/2 [00:00<00:00, 2474.52it/s]Calculating Metrics:  50%|█████     | 1/2 [00:00<00:00, 1315.65it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 1823.21it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 1574.73it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 1484.45it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 1377.21it/s]
_ test_case_runner_v3_api[sqlite/other_expectations/expect_table_columns_to_match_set:negative_test_extra_column_exact_match_false] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/2 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/2 [00:00<?, ?it/s]Calculating Metrics:  50%|█████     | 1/2 [00:00<00:00, 2232.20it/s]Calculating Metrics:  50%|█████     | 1/2 [00:00<00:00, 1340.46it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 1985.94it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 1697.41it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 1591.16it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 1462.96it/s]
_ test_case_runner_v3_api[sqlite/other_expectations/expect_table_columns_to_match_set:negative_test_null_set_exact_match_true] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/2 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/2 [00:00<?, ?it/s]Calculating Metrics:  50%|█████     | 1/2 [00:00<00:00, 2183.40it/s]Calculating Metrics:  50%|█████     | 1/2 [00:00<00:00, 1256.16it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 1732.83it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 1456.61it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 1354.75it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 1259.36it/s]
_ test_case_runner_v3_api[sqlite/other_expectations/expect_table_columns_to_match_set:positive_test_vacuously_true_null_set_exact_match_false] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/2 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/2 [00:00<?, ?it/s]Calculating Metrics:  50%|█████     | 1/2 [00:00<00:00, 2003.97it/s]Calculating Metrics:  50%|█████     | 1/2 [00:00<00:00, 1204.91it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 1792.05it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 1537.22it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 1444.07it/s]Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00, 1344.11it/s]
_ test_case_runner_v3_api[sqlite/other_expectations/expect_table_column_count_to_equal:basic_positive_case] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/3 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/3 [00:00<?, ?it/s]Calculating Metrics:  33%|███▎      | 1/3 [00:00<00:00, 2278.27it/s]Calculating Metrics:  33%|███▎      | 1/3 [00:00<00:00, 1169.63it/s]Calculating Metrics:  67%|██████▋   | 2/3 [00:00<00:00, 1812.97it/s]Calculating Metrics:  67%|██████▋   | 2/3 [00:00<00:00, 1427.61it/s]Calculating Metrics: 100%|██████████| 3/3 [00:00<00:00, 1831.31it/s]Calculating Metrics: 100%|██████████| 3/3 [00:00<00:00, 1640.11it/s]Calculating Metrics: 100%|██████████| 3/3 [00:00<00:00, 1577.20it/s]Calculating Metrics: 100%|██████████| 3/3 [00:00<00:00, 1506.76it/s]
_ test_case_runner_v3_api[sqlite/other_expectations/expect_table_column_count_to_equal:basic_negative_case_upper_error] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/3 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/3 [00:00<?, ?it/s]Calculating Metrics:  33%|███▎      | 1/3 [00:00<00:00, 2445.66it/s]Calculating Metrics:  33%|███▎      | 1/3 [00:00<00:00, 1216.45it/s]Calculating Metrics:  67%|██████▋   | 2/3 [00:00<00:00, 1873.29it/s]Calculating Metrics:  67%|██████▋   | 2/3 [00:00<00:00, 1466.28it/s]Calculating Metrics: 100%|██████████| 3/3 [00:00<00:00, 1874.97it/s]Calculating Metrics: 100%|██████████| 3/3 [00:00<00:00, 1675.04it/s]Calculating Metrics: 100%|██████████| 3/3 [00:00<00:00, 1610.51it/s]Calculating Metrics: 100%|██████████| 3/3 [00:00<00:00, 1538.44it/s]
_ test_case_runner_v3_api[sqlite/other_expectations/expect_table_column_count_to_equal:basic_negative_case_lower_error] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/3 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/3 [00:00<?, ?it/s]Calculating Metrics:  33%|███▎      | 1/3 [00:00<00:00, 2109.81it/s]Calculating Metrics:  33%|███▎      | 1/3 [00:00<00:00, 1100.00it/s]Calculating Metrics:  67%|██████▋   | 2/3 [00:00<00:00, 1721.45it/s]Calculating Metrics:  67%|██████▋   | 2/3 [00:00<00:00, 1368.01it/s]Calculating Metrics: 100%|██████████| 3/3 [00:00<00:00, 1752.25it/s]Calculating Metrics: 100%|██████████| 3/3 [00:00<00:00, 1568.94it/s]Calculating Metrics: 100%|██████████| 3/3 [00:00<00:00, 1509.83it/s]Calculating Metrics: 100%|██████████| 3/3 [00:00<00:00, 1442.99it/s]
_ test_case_runner_v3_api[sqlite/other_expectations/expect_table_column_count_to_equal:basic_negative_case_kwargs_args] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/3 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/3 [00:00<?, ?it/s]Calculating Metrics:  33%|███▎      | 1/3 [00:00<00:00, 2079.48it/s]Calculating Metrics:  33%|███▎      | 1/3 [00:00<00:00, 1112.25it/s]Calculating Metrics:  67%|██████▋   | 2/3 [00:00<00:00, 1738.93it/s]Calculating Metrics:  67%|██████▋   | 2/3 [00:00<00:00, 1374.51it/s]Calculating Metrics: 100%|██████████| 3/3 [00:00<00:00, 1756.41it/s]Calculating Metrics: 100%|██████████| 3/3 [00:00<00:00, 1564.84it/s]Calculating Metrics: 100%|██████████| 3/3 [00:00<00:00, 1505.13it/s]Calculating Metrics: 100%|██████████| 3/3 [00:00<00:00, 1434.11it/s]
_ test_case_runner_v3_api[pandas/multicolumn_map_expectations/expect_compound_columns_to_be_unique:mostly_default_successful_expectation] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:  22%|██▏       | 2/9 [00:00<00:00, 3853.29it/s]Calculating Metrics:  22%|██▏       | 2/9 [00:00<00:00, 505.28it/s]Calculating Metrics:  33%|███▎      | 3/9 [00:00<00:00, 711.74it/s]Calculating Metrics:  33%|███▎      | 3/9 [00:00<00:00, 402.79it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 374.47it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 295.77it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 437.85it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 413.26it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 411.48it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 409.57it/s]
_ test_case_runner_v3_api[pandas/multicolumn_map_expectations/expect_compound_columns_to_be_unique:mostly_default_fails_expectation] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:  22%|██▏       | 2/9 [00:00<00:00, 4264.67it/s]Calculating Metrics:  22%|██▏       | 2/9 [00:00<00:00, 509.70it/s]Calculating Metrics:  33%|███▎      | 3/9 [00:00<00:00, 717.71it/s]Calculating Metrics:  33%|███▎      | 3/9 [00:00<00:00, 394.13it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 398.69it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 308.24it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 458.96it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 433.78it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 432.10it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 430.09it/s]
_ test_case_runner_v3_api[pandas/multicolumn_map_expectations/expect_compound_columns_to_be_unique:mostly_set_successful_expectation] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:  22%|██▏       | 2/9 [00:00<00:00, 4106.02it/s]Calculating Metrics:  22%|██▏       | 2/9 [00:00<00:00, 491.17it/s]Calculating Metrics:  33%|███▎      | 3/9 [00:00<00:00, 691.10it/s]Calculating Metrics:  33%|███▎      | 3/9 [00:00<00:00, 380.77it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 394.28it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 308.46it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 462.11it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 434.36it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 432.44it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 430.40it/s]
_ test_case_runner_v3_api[pandas/multicolumn_map_expectations/expect_compound_columns_to_be_unique:mostly_set_fails_expectation] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:  22%|██▏       | 2/9 [00:00<00:00, 4122.17it/s]Calculating Metrics:  22%|██▏       | 2/9 [00:00<00:00, 508.25it/s]Calculating Metrics:  33%|███▎      | 3/9 [00:00<00:00, 713.60it/s]Calculating Metrics:  33%|███▎      | 3/9 [00:00<00:00, 401.16it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 402.37it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 313.11it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 460.59it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 434.05it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 432.25it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 430.20it/s]
_ test_case_runner_v3_api[pandas/multicolumn_map_expectations/expect_compound_columns_to_be_unique:basic_test_ignore_if_all_are_missing_the_default_behavior] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:  22%|██▏       | 2/9 [00:00<00:00, 3839.18it/s]Calculating Metrics:  22%|██▏       | 2/9 [00:00<00:00, 507.32it/s]Calculating Metrics:  33%|███▎      | 3/9 [00:00<00:00, 714.29it/s]Calculating Metrics:  33%|███▎      | 3/9 [00:00<00:00, 400.47it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 406.84it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 311.38it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 492.85it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 464.25it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 462.37it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 460.08it/s]
_ test_case_runner_v3_api[pandas/multicolumn_map_expectations/expect_compound_columns_to_be_unique:basic_test_ignore_if_any_are_missing] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:  22%|██▏       | 2/9 [00:00<00:00, 3929.09it/s]Calculating Metrics:  22%|██▏       | 2/9 [00:00<00:00, 502.16it/s]Calculating Metrics:  33%|███▎      | 3/9 [00:00<00:00, 704.25it/s]Calculating Metrics:  33%|███▎      | 3/9 [00:00<00:00, 392.11it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 392.61it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 306.96it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 474.46it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 447.40it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 445.59it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 443.48it/s]
_ test_case_runner_v3_api[pandas/multicolumn_map_expectations/expect_compound_columns_to_be_unique:row_condition_with_ignore_if_any_are_missing] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:  22%|██▏       | 2/9 [00:00<00:00, 631.53it/s]Calculating Metrics:  22%|██▏       | 2/9 [00:00<00:00, 365.10it/s]Calculating Metrics:  33%|███▎      | 3/9 [00:00<00:00, 527.98it/s]Calculating Metrics:  33%|███▎      | 3/9 [00:00<00:00, 381.79it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 364.24it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 290.70it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 371.50it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 359.04it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 357.81it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 356.45it/s]
_ test_case_runner_v3_api[pandas/multicolumn_map_expectations/expect_compound_columns_to_be_unique:unexpected_values_without_unexpected_index_list] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:  22%|██▏       | 2/9 [00:00<00:00, 3350.08it/s]Calculating Metrics:  22%|██▏       | 2/9 [00:00<00:00, 436.00it/s]Calculating Metrics:  33%|███▎      | 3/9 [00:00<00:00, 610.85it/s]Calculating Metrics:  33%|███▎      | 3/9 [00:00<00:00, 358.55it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 356.60it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 282.72it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 414.08it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 392.77it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 391.32it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 389.66it/s]
_ test_case_runner_v3_api[pandas/multicolumn_map_expectations/expect_compound_columns_to_be_unique:unexpected_values_exact_match_out_without_unexpected_index_list] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:  22%|██▏       | 2/9 [00:00<00:00, 3809.54it/s]Calculating Metrics:  22%|██▏       | 2/9 [00:00<00:00, 482.41it/s]Calculating Metrics:  33%|███▎      | 3/9 [00:00<00:00, 675.67it/s]Calculating Metrics:  33%|███▎      | 3/9 [00:00<00:00, 387.00it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 389.52it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 302.41it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 435.20it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 412.19it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 410.62it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 408.80it/s]
_ test_case_runner_v3_api[pandas/multicolumn_map_expectations/expect_select_column_values_to_be_unique_within_record:mostly_default_successful_expectation] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:  22%|██▏       | 2/9 [00:00<00:00, 3811.27it/s]Calculating Metrics:  22%|██▏       | 2/9 [00:00<00:00, 494.20it/s]Calculating Metrics:  33%|███▎      | 3/9 [00:00<00:00, 694.46it/s]Calculating Metrics:  33%|███▎      | 3/9 [00:00<00:00, 389.42it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 332.48it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 267.83it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 421.19it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 398.48it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 396.81it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 395.05it/s]
_ test_case_runner_v3_api[pandas/multicolumn_map_expectations/expect_select_column_values_to_be_unique_within_record:mostly_default_fails_expectation] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:  22%|██▏       | 2/9 [00:00<00:00, 4301.85it/s]Calculating Metrics:  22%|██▏       | 2/9 [00:00<00:00, 516.92it/s]Calculating Metrics:  33%|███▎      | 3/9 [00:00<00:00, 724.99it/s]Calculating Metrics:  33%|███▎      | 3/9 [00:00<00:00, 386.16it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 384.61it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 298.77it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 451.36it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 426.62it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 424.91it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 421.98it/s]
_ test_case_runner_v3_api[pandas/multicolumn_map_expectations/expect_select_column_values_to_be_unique_within_record:mostly_set_successful_expectation] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:  22%|██▏       | 2/9 [00:00<00:00, 4082.05it/s]Calculating Metrics:  22%|██▏       | 2/9 [00:00<00:00, 486.63it/s]Calculating Metrics:  33%|███▎      | 3/9 [00:00<00:00, 683.56it/s]Calculating Metrics:  33%|███▎      | 3/9 [00:00<00:00, 392.16it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 382.89it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 294.94it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 429.49it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 406.53it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 404.99it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 403.21it/s]
_ test_case_runner_v3_api[pandas/multicolumn_map_expectations/expect_select_column_values_to_be_unique_within_record:mostly_set_fails_expectation] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:  22%|██▏       | 2/9 [00:00<00:00, 3466.37it/s]Calculating Metrics:  22%|██▏       | 2/9 [00:00<00:00, 499.50it/s]Calculating Metrics:  33%|███▎      | 3/9 [00:00<00:00, 704.57it/s]Calculating Metrics:  33%|███▎      | 3/9 [00:00<00:00, 399.36it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 402.42it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 311.50it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 453.36it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 426.94it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 425.17it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 423.03it/s]
_ test_case_runner_v3_api[pandas/multicolumn_map_expectations/expect_select_column_values_to_be_unique_within_record:basic_test_ignore_if_all_are_missing_the_default_behavior] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:  22%|██▏       | 2/9 [00:00<00:00, 4114.08it/s]Calculating Metrics:  22%|██▏       | 2/9 [00:00<00:00, 506.47it/s]Calculating Metrics:  33%|███▎      | 3/9 [00:00<00:00, 712.55it/s]Calculating Metrics:  33%|███▎      | 3/9 [00:00<00:00, 397.93it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 367.20it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 287.73it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 455.88it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 423.87it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 420.78it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 418.19it/s]
_ test_case_runner_v3_api[pandas/multicolumn_map_expectations/expect_select_column_values_to_be_unique_within_record:basic_test_ignore_if_any_are_missing] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:  22%|██▏       | 2/9 [00:00<00:00, 3648.81it/s]Calculating Metrics:  22%|██▏       | 2/9 [00:00<00:00, 469.61it/s]Calculating Metrics:  33%|███▎      | 3/9 [00:00<00:00, 661.98it/s]Calculating Metrics:  33%|███▎      | 3/9 [00:00<00:00, 384.22it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 383.88it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 301.98it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 465.17it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 439.20it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 437.47it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 435.44it/s]
_ test_case_runner_v3_api[pandas/multicolumn_map_expectations/expect_select_column_values_to_be_unique_within_record:unexpected_values_exact_match_out_without_unexpected_index_list] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:  22%|██▏       | 2/9 [00:00<00:00, 3412.78it/s]Calculating Metrics:  22%|██▏       | 2/9 [00:00<00:00, 403.78it/s]Calculating Metrics:  33%|███▎      | 3/9 [00:00<00:00, 568.28it/s]Calculating Metrics:  33%|███▎      | 3/9 [00:00<00:00, 321.27it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 321.13it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 258.00it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 396.16it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 377.09it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 375.76it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 374.22it/s]
_ test_case_runner_v3_api[pandas/multicolumn_map_expectations/expect_multicolumn_sum_to_equal:mostly_default_successful_expectation] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:  22%|██▏       | 2/9 [00:00<00:00, 3968.12it/s]Calculating Metrics:  22%|██▏       | 2/9 [00:00<00:00, 439.38it/s]Calculating Metrics:  33%|███▎      | 3/9 [00:00<00:00, 622.30it/s]Calculating Metrics:  33%|███▎      | 3/9 [00:00<00:00, 346.90it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 370.45it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 284.35it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 435.90it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 408.87it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 407.30it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 405.46it/s]
_ test_case_runner_v3_api[pandas/multicolumn_map_expectations/expect_multicolumn_sum_to_equal:mostly_default_fails_expectation] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:  22%|██▏       | 2/9 [00:00<00:00, 3753.29it/s]Calculating Metrics:  22%|██▏       | 2/9 [00:00<00:00, 442.39it/s]Calculating Metrics:  33%|███▎      | 3/9 [00:00<00:00, 627.61it/s]Calculating Metrics:  33%|███▎      | 3/9 [00:00<00:00, 349.32it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 366.17it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 276.80it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 426.91it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 400.39it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 398.85it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 397.09it/s]
_ test_case_runner_v3_api[pandas/multicolumn_map_expectations/expect_multicolumn_sum_to_equal:mostly_set_successful_expectation] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:  22%|██▏       | 2/9 [00:00<00:00, 4122.17it/s]Calculating Metrics:  22%|██▏       | 2/9 [00:00<00:00, 455.58it/s]Calculating Metrics:  33%|███▎      | 3/9 [00:00<00:00, 642.81it/s]Calculating Metrics:  33%|███▎      | 3/9 [00:00<00:00, 356.51it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 377.14it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 287.48it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 433.75it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 406.91it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 405.37it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 403.62it/s]
_ test_case_runner_v3_api[pandas/multicolumn_map_expectations/expect_multicolumn_sum_to_equal:mostly_set_fails_expectation] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:  22%|██▏       | 2/9 [00:00<00:00, 3921.74it/s]Calculating Metrics:  22%|██▏       | 2/9 [00:00<00:00, 441.48it/s]Calculating Metrics:  33%|███▎      | 3/9 [00:00<00:00, 624.46it/s]Calculating Metrics:  33%|███▎      | 3/9 [00:00<00:00, 348.22it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 360.58it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 273.67it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 424.76it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 399.77it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 398.33it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 396.63it/s]
_ test_case_runner_v3_api[pandas/multicolumn_map_expectations/expect_multicolumn_sum_to_equal:basic_positive_test] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:  22%|██▏       | 2/9 [00:00<00:00, 3874.65it/s]Calculating Metrics:  22%|██▏       | 2/9 [00:00<00:00, 447.32it/s]Calculating Metrics:  33%|███▎      | 3/9 [00:00<00:00, 630.79it/s]Calculating Metrics:  33%|███▎      | 3/9 [00:00<00:00, 346.73it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 366.90it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 281.81it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 426.09it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 400.34it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 398.86it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 397.12it/s]
_ test_case_runner_v3_api[pandas/multicolumn_map_expectations/expect_multicolumn_sum_to_equal:basic_positive_test_more_than_2_columns] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:  22%|██▏       | 2/9 [00:00<00:00, 3921.74it/s]Calculating Metrics:  22%|██▏       | 2/9 [00:00<00:00, 440.14it/s]Calculating Metrics:  33%|███▎      | 3/9 [00:00<00:00, 612.25it/s]Calculating Metrics:  33%|███▎      | 3/9 [00:00<00:00, 339.29it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 355.87it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 272.26it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 421.19it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 394.37it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 392.72it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 390.98it/s]
_ test_case_runner_v3_api[pandas/multicolumn_map_expectations/expect_multicolumn_sum_to_equal:row_condition_with_ignore_if_any_are_missing] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:  22%|██▏       | 2/9 [00:00<00:00, 468.51it/s]Calculating Metrics:  22%|██▏       | 2/9 [00:00<00:00, 275.48it/s]Calculating Metrics:  33%|███▎      | 3/9 [00:00<00:00, 399.25it/s]Calculating Metrics:  33%|███▎      | 3/9 [00:00<00:00, 291.57it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 262.19it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 226.60it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 291.73it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 282.15it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 281.38it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 280.48it/s]
_ test_case_runner_v3_api[sqlite/multicolumn_map_expectations/expect_compound_columns_to_be_unique:mostly_default_successful_expectation] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:  22%|██▏       | 2/9 [00:00<00:00, 3145.33it/s]Calculating Metrics:  22%|██▏       | 2/9 [00:00<00:00, 548.10it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 816.13it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 520.69it/s]Calculating Metrics:  56%|█████▌    | 5/9 [00:00<00:00, 559.15it/s]Calculating Metrics:  56%|█████▌    | 5/9 [00:00<00:00, 435.92it/s]Calculating Metrics:  67%|██████▋   | 6/9 [00:00<00:00, 480.97it/s]Calculating Metrics:  67%|██████▋   | 6/9 [00:00<00:00, 407.76it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 414.77it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 395.73it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 394.23it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 392.57it/s]
_ test_case_runner_v3_api[sqlite/multicolumn_map_expectations/expect_compound_columns_to_be_unique:mostly_default_fails_expectation] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:  22%|██▏       | 2/9 [00:00<00:00, 3310.42it/s]Calculating Metrics:  22%|██▏       | 2/9 [00:00<00:00, 560.55it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 893.07it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 556.48it/s]Calculating Metrics:  56%|█████▌    | 5/9 [00:00<00:00, 611.11it/s]Calculating Metrics:  56%|█████▌    | 5/9 [00:00<00:00, 456.20it/s]Calculating Metrics:  67%|██████▋   | 6/9 [00:00<00:00, 507.40it/s]Calculating Metrics:  67%|██████▋   | 6/9 [00:00<00:00, 430.47it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 439.65it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 417.56it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 415.72it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 413.72it/s]
_ test_case_runner_v3_api[sqlite/multicolumn_map_expectations/expect_compound_columns_to_be_unique:mostly_set_successful_expectation] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:  22%|██▏       | 2/9 [00:00<00:00, 3400.33it/s]Calculating Metrics:  22%|██▏       | 2/9 [00:00<00:00, 558.01it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 891.03it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 554.38it/s]Calculating Metrics:  56%|█████▌    | 5/9 [00:00<00:00, 608.49it/s]Calculating Metrics:  56%|█████▌    | 5/9 [00:00<00:00, 459.73it/s]Calculating Metrics:  67%|██████▋   | 6/9 [00:00<00:00, 500.17it/s]Calculating Metrics:  67%|██████▋   | 6/9 [00:00<00:00, 428.20it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 527.73it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 498.95it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 496.79it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 494.20it/s]
_ test_case_runner_v3_api[sqlite/multicolumn_map_expectations/expect_compound_columns_to_be_unique:mostly_set_fails_expectation] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:  22%|██▏       | 2/9 [00:00<00:00, 3163.13it/s]Calculating Metrics:  22%|██▏       | 2/9 [00:00<00:00, 532.17it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 850.69it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 531.61it/s]Calculating Metrics:  56%|█████▌    | 5/9 [00:00<00:00, 584.44it/s]Calculating Metrics:  56%|█████▌    | 5/9 [00:00<00:00, 447.43it/s]Calculating Metrics:  67%|██████▋   | 6/9 [00:00<00:00, 488.64it/s]Calculating Metrics:  67%|██████▋   | 6/9 [00:00<00:00, 420.76it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 522.31it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 494.07it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 491.94it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 489.37it/s]
_ test_case_runner_v3_api[sqlite/multicolumn_map_expectations/expect_compound_columns_to_be_unique:basic_test_ignore_if_all_are_missing_the_default_behavior] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:  22%|██▏       | 2/9 [00:00<00:00, 3114.97it/s]Calculating Metrics:  22%|██▏       | 2/9 [00:00<00:00, 546.03it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 838.94it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 530.87it/s]Calculating Metrics:  56%|█████▌    | 5/9 [00:00<00:00, 561.92it/s]Calculating Metrics:  56%|█████▌    | 5/9 [00:00<00:00, 437.49it/s]Calculating Metrics:  67%|██████▋   | 6/9 [00:00<00:00, 483.80it/s]Calculating Metrics:  67%|██████▋   | 6/9 [00:00<00:00, 412.12it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 439.35it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 415.98it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 414.03it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 411.96it/s]
_ test_case_runner_v3_api[sqlite/multicolumn_map_expectations/expect_compound_columns_to_be_unique:basic_test_ignore_if_any_are_missing] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:  22%|██▏       | 2/9 [00:00<00:00, 3085.18it/s]Calculating Metrics:  22%|██▏       | 2/9 [00:00<00:00, 541.97it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 847.81it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 528.45it/s]Calculating Metrics:  56%|█████▌    | 5/9 [00:00<00:00, 564.71it/s]Calculating Metrics:  56%|█████▌    | 5/9 [00:00<00:00, 441.88it/s]Calculating Metrics:  67%|██████▋   | 6/9 [00:00<00:00, 492.00it/s]Calculating Metrics:  67%|██████▋   | 6/9 [00:00<00:00, 421.91it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 447.13it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 425.51it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 423.82it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 421.88it/s]
_ test_case_runner_v3_api[sqlite/multicolumn_map_expectations/expect_compound_columns_to_be_unique:unexpected_values_without_unexpected_index_list] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:  22%|██▏       | 2/9 [00:00<00:00, 3284.50it/s]Calculating Metrics:  22%|██▏       | 2/9 [00:00<00:00, 536.05it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 840.88it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 524.19it/s]Calculating Metrics:  56%|█████▌    | 5/9 [00:00<00:00, 567.09it/s]Calculating Metrics:  56%|█████▌    | 5/9 [00:00<00:00, 427.82it/s]Calculating Metrics:  67%|██████▋   | 6/9 [00:00<00:00, 470.93it/s]Calculating Metrics:  67%|██████▋   | 6/9 [00:00<00:00, 400.37it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 430.85it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 410.85it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 409.28it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 407.44it/s]
_ test_case_runner_v3_api[sqlite/multicolumn_map_expectations/expect_compound_columns_to_be_unique:unexpected_values_exact_match_out_without_unexpected_index_list] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:  22%|██▏       | 2/9 [00:00<00:00, 3077.26it/s]Calculating Metrics:  22%|██▏       | 2/9 [00:00<00:00, 545.42it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 857.99it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 540.47it/s]Calculating Metrics:  56%|█████▌    | 5/9 [00:00<00:00, 584.59it/s]Calculating Metrics:  56%|█████▌    | 5/9 [00:00<00:00, 441.93it/s]Calculating Metrics:  67%|██████▋   | 6/9 [00:00<00:00, 479.12it/s]Calculating Metrics:  67%|██████▋   | 6/9 [00:00<00:00, 393.03it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 483.93it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 456.21it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 453.84it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 450.79it/s]
_ test_case_runner_v3_api[sqlite/multicolumn_map_expectations/expect_select_column_values_to_be_unique_within_record:mostly_default_successful_expectation] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/8 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/8 [00:00<?, ?it/s]Calculating Metrics:  25%|██▌       | 2/8 [00:00<00:00, 3261.51it/s]Calculating Metrics:  25%|██▌       | 2/8 [00:00<00:00, 624.80it/s]Calculating Metrics:  50%|█████     | 4/8 [00:00<00:00, 917.04it/s]Calculating Metrics:  50%|█████     | 4/8 [00:00<00:00, 603.15it/s]Calculating Metrics:  62%|██████▎   | 5/8 [00:00<00:00, 669.80it/s]Calculating Metrics:  62%|██████▎   | 5/8 [00:00<00:00, 534.47it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 572.44it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 536.52it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 533.08it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 529.48it/s]
_ test_case_runner_v3_api[sqlite/multicolumn_map_expectations/expect_select_column_values_to_be_unique_within_record:mostly_default_fails_expectation] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/8 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/8 [00:00<?, ?it/s]Calculating Metrics:  25%|██▌       | 2/8 [00:00<00:00, 2797.14it/s]Calculating Metrics:  25%|██▌       | 2/8 [00:00<00:00, 517.21it/s]Calculating Metrics:  50%|█████     | 4/8 [00:00<00:00, 790.33it/s]Calculating Metrics:  50%|█████     | 4/8 [00:00<00:00, 495.84it/s]Calculating Metrics:  62%|██████▎   | 5/8 [00:00<00:00, 550.53it/s]Calculating Metrics:  62%|██████▎   | 5/8 [00:00<00:00, 438.09it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 462.54it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 436.68it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 433.96it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 431.27it/s]
_ test_case_runner_v3_api[sqlite/multicolumn_map_expectations/expect_select_column_values_to_be_unique_within_record:mostly_set_successful_expectation] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/8 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/8 [00:00<?, ?it/s]Calculating Metrics:  25%|██▌       | 2/8 [00:00<00:00, 2927.96it/s]Calculating Metrics:  25%|██▌       | 2/8 [00:00<00:00, 574.72it/s]Calculating Metrics:  50%|█████     | 4/8 [00:00<00:00, 842.48it/s]Calculating Metrics:  50%|█████     | 4/8 [00:00<00:00, 563.15it/s]Calculating Metrics:  62%|██████▎   | 5/8 [00:00<00:00, 620.90it/s]Calculating Metrics:  62%|██████▎   | 5/8 [00:00<00:00, 484.36it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 573.84it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 540.00it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 536.88it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 533.41it/s]
_ test_case_runner_v3_api[sqlite/multicolumn_map_expectations/expect_select_column_values_to_be_unique_within_record:mostly_set_fails_expectation] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/8 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/8 [00:00<?, ?it/s]Calculating Metrics:  25%|██▌       | 2/8 [00:00<00:00, 3101.15it/s]Calculating Metrics:  25%|██▌       | 2/8 [00:00<00:00, 606.24it/s]Calculating Metrics:  50%|█████     | 4/8 [00:00<00:00, 879.54it/s]Calculating Metrics:  50%|█████     | 4/8 [00:00<00:00, 536.48it/s]Calculating Metrics:  62%|██████▎   | 5/8 [00:00<00:00, 593.76it/s]Calculating Metrics:  62%|██████▎   | 5/8 [00:00<00:00, 473.75it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 568.03it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 531.03it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 527.64it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 523.87it/s]
_ test_case_runner_v3_api[sqlite/multicolumn_map_expectations/expect_select_column_values_to_be_unique_within_record:basic_test_ignore_if_all_are_missing_the_default_behavior] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/8 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/8 [00:00<?, ?it/s]Calculating Metrics:  25%|██▌       | 2/8 [00:00<00:00, 2828.26it/s]Calculating Metrics:  25%|██▌       | 2/8 [00:00<00:00, 625.92it/s]Calculating Metrics:  50%|█████     | 4/8 [00:00<00:00, 914.04it/s]Calculating Metrics:  50%|█████     | 4/8 [00:00<00:00, 593.63it/s]Calculating Metrics:  62%|██████▎   | 5/8 [00:00<00:00, 654.26it/s]Calculating Metrics:  62%|██████▎   | 5/8 [00:00<00:00, 525.42it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 553.32it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 516.23it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 512.85it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 509.16it/s]
_ test_case_runner_v3_api[sqlite/multicolumn_map_expectations/expect_select_column_values_to_be_unique_within_record:basic_test_ignore_if_any_are_missing] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/8 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/8 [00:00<?, ?it/s]Calculating Metrics:  25%|██▌       | 2/8 [00:00<00:00, 2637.93it/s]Calculating Metrics:  25%|██▌       | 2/8 [00:00<00:00, 511.63it/s]Calculating Metrics:  50%|█████     | 4/8 [00:00<00:00, 787.22it/s]Calculating Metrics:  50%|█████     | 4/8 [00:00<00:00, 502.45it/s]Calculating Metrics:  62%|██████▎   | 5/8 [00:00<00:00, 554.68it/s]Calculating Metrics:  62%|██████▎   | 5/8 [00:00<00:00, 441.06it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 488.58it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 460.14it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 457.53it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 454.60it/s]
_ test_case_runner_v3_api[sqlite/multicolumn_map_expectations/expect_select_column_values_to_be_unique_within_record:unexpected_values] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/8 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/8 [00:00<?, ?it/s]Calculating Metrics:  25%|██▌       | 2/8 [00:00<00:00, 3140.62it/s]Calculating Metrics:  25%|██▌       | 2/8 [00:00<00:00, 624.62it/s]Calculating Metrics:  50%|█████     | 4/8 [00:00<00:00, 914.09it/s]Calculating Metrics:  50%|█████     | 4/8 [00:00<00:00, 592.44it/s]Calculating Metrics:  62%|██████▎   | 5/8 [00:00<00:00, 655.07it/s]Calculating Metrics:  62%|██████▎   | 5/8 [00:00<00:00, 493.39it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 533.09it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 503.22it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 500.38it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 497.33it/s]
_ test_case_runner_v3_api[sqlite/multicolumn_map_expectations/expect_select_column_values_to_be_unique_within_record:unexpected_values_exact_match_out_no_index_list] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/8 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/8 [00:00<?, ?it/s]Calculating Metrics:  25%|██▌       | 2/8 [00:00<00:00, 3119.60it/s]Calculating Metrics:  25%|██▌       | 2/8 [00:00<00:00, 607.17it/s]Calculating Metrics:  50%|█████     | 4/8 [00:00<00:00, 925.03it/s]Calculating Metrics:  50%|█████     | 4/8 [00:00<00:00, 607.61it/s]Calculating Metrics:  62%|██████▎   | 5/8 [00:00<00:00, 675.74it/s]Calculating Metrics:  62%|██████▎   | 5/8 [00:00<00:00, 539.43it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 639.84it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 592.49it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 588.32it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 583.69it/s]
_ test_case_runner_v3_api[sqlite/multicolumn_map_expectations/expect_select_column_values_to_be_unique_within_record:unexpected_values_exact_match_out_without_unexpected_index_list] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/8 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/8 [00:00<?, ?it/s]Calculating Metrics:  25%|██▌       | 2/8 [00:00<00:00, 3012.07it/s]Calculating Metrics:  25%|██▌       | 2/8 [00:00<00:00, 608.84it/s]Calculating Metrics:  50%|█████     | 4/8 [00:00<00:00, 924.82it/s]Calculating Metrics:  50%|█████     | 4/8 [00:00<00:00, 599.96it/s]Calculating Metrics:  62%|██████▎   | 5/8 [00:00<00:00, 659.88it/s]Calculating Metrics:  62%|██████▎   | 5/8 [00:00<00:00, 519.21it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 613.44it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 573.31it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 569.75it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 565.49it/s]
_ test_case_runner_v3_api[sqlite/multicolumn_map_expectations/expect_multicolumn_sum_to_equal:mostly_default_successful_expectation] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/8 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/8 [00:00<?, ?it/s]Calculating Metrics:  25%|██▌       | 2/8 [00:00<00:00, 2910.69it/s]Calculating Metrics:  25%|██▌       | 2/8 [00:00<00:00, 532.04it/s]Calculating Metrics:  50%|█████     | 4/8 [00:00<00:00, 803.85it/s]Calculating Metrics:  50%|█████     | 4/8 [00:00<00:00, 533.25it/s]Calculating Metrics:  62%|██████▎   | 5/8 [00:00<00:00, 598.09it/s]Calculating Metrics:  62%|██████▎   | 5/8 [00:00<00:00, 477.46it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 530.40it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 497.01it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 494.37it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 491.37it/s]
_ test_case_runner_v3_api[sqlite/multicolumn_map_expectations/expect_multicolumn_sum_to_equal:mostly_default_fails_expectation] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/8 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/8 [00:00<?, ?it/s]Calculating Metrics:  25%|██▌       | 2/8 [00:00<00:00, 3442.19it/s]Calculating Metrics:  25%|██▌       | 2/8 [00:00<00:00, 599.66it/s]Calculating Metrics:  50%|█████     | 4/8 [00:00<00:00, 926.97it/s]Calculating Metrics:  50%|█████     | 4/8 [00:00<00:00, 574.05it/s]Calculating Metrics:  62%|██████▎   | 5/8 [00:00<00:00, 626.73it/s]Calculating Metrics:  62%|██████▎   | 5/8 [00:00<00:00, 493.73it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 547.30it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 512.66it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 509.46it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 506.16it/s]
_ test_case_runner_v3_api[sqlite/multicolumn_map_expectations/expect_multicolumn_sum_to_equal:mostly_set_successful_expectation] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/8 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/8 [00:00<?, ?it/s]Calculating Metrics:  25%|██▌       | 2/8 [00:00<00:00, 3085.18it/s]Calculating Metrics:  25%|██▌       | 2/8 [00:00<00:00, 569.61it/s]Calculating Metrics:  50%|█████     | 4/8 [00:00<00:00, 883.76it/s]Calculating Metrics:  50%|█████     | 4/8 [00:00<00:00, 559.35it/s]Calculating Metrics:  62%|██████▎   | 5/8 [00:00<00:00, 620.48it/s]Calculating Metrics:  62%|██████▎   | 5/8 [00:00<00:00, 482.94it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 576.95it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 535.65it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 531.94it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 528.19it/s]
_ test_case_runner_v3_api[sqlite/multicolumn_map_expectations/expect_multicolumn_sum_to_equal:mostly_set_fails_expectation] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/8 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/8 [00:00<?, ?it/s]Calculating Metrics:  25%|██▌       | 2/8 [00:00<00:00, 3109.19it/s]Calculating Metrics:  25%|██▌       | 2/8 [00:00<00:00, 506.56it/s]Calculating Metrics:  50%|█████     | 4/8 [00:00<00:00, 792.35it/s]Calculating Metrics:  50%|█████     | 4/8 [00:00<00:00, 477.43it/s]Calculating Metrics:  62%|██████▎   | 5/8 [00:00<00:00, 532.70it/s]Calculating Metrics:  62%|██████▎   | 5/8 [00:00<00:00, 430.14it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 535.30it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 500.93it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 498.22it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 495.01it/s]
_ test_case_runner_v3_api[sqlite/multicolumn_map_expectations/expect_multicolumn_sum_to_equal:basic_positive_test] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/8 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/8 [00:00<?, ?it/s]Calculating Metrics:  25%|██▌       | 2/8 [00:00<00:00, 2923.88it/s]Calculating Metrics:  25%|██▌       | 2/8 [00:00<00:00, 553.70it/s]Calculating Metrics:  50%|█████     | 4/8 [00:00<00:00, 832.45it/s]Calculating Metrics:  50%|█████     | 4/8 [00:00<00:00, 530.57it/s]Calculating Metrics:  62%|██████▎   | 5/8 [00:00<00:00, 593.61it/s]Calculating Metrics:  62%|██████▎   | 5/8 [00:00<00:00, 462.75it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 462.56it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 434.41it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 431.97it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 429.39it/s]
_ test_case_runner_v3_api[sqlite/multicolumn_map_expectations/expect_multicolumn_sum_to_equal:basic_positive_test_more_than_2_columns] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/8 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/8 [00:00<?, ?it/s]Calculating Metrics:  25%|██▌       | 2/8 [00:00<00:00, 3077.26it/s]Calculating Metrics:  25%|██▌       | 2/8 [00:00<00:00, 568.99it/s]Calculating Metrics:  50%|█████     | 4/8 [00:00<00:00, 891.27it/s]Calculating Metrics:  50%|█████     | 4/8 [00:00<00:00, 567.93it/s]Calculating Metrics:  62%|██████▎   | 5/8 [00:00<00:00, 633.14it/s]Calculating Metrics:  62%|██████▎   | 5/8 [00:00<00:00, 497.21it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 535.72it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 502.04it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 499.22it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 496.19it/s]
_ test_case_runner_v3_api[sqlite/multicolumn_map_expectations/expect_multicolumn_sum_to_equal:basic_negative_test] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/8 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/8 [00:00<?, ?it/s]Calculating Metrics:  25%|██▌       | 2/8 [00:00<00:00, 2999.14it/s]Calculating Metrics:  25%|██▌       | 2/8 [00:00<00:00, 540.99it/s]Calculating Metrics:  50%|█████     | 4/8 [00:00<00:00, 851.25it/s]Calculating Metrics:  50%|█████     | 4/8 [00:00<00:00, 556.72it/s]Calculating Metrics:  62%|██████▎   | 5/8 [00:00<00:00, 619.98it/s]Calculating Metrics:  62%|██████▎   | 5/8 [00:00<00:00, 478.56it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 529.31it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 496.93it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 494.35it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 491.37it/s]
_ test_case_runner_v3_api[sqlite/multicolumn_map_expectations/expect_multicolumn_sum_to_equal:negative_test_with_ignore_if_any_are_missing] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/8 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/8 [00:00<?, ?it/s]Calculating Metrics:  25%|██▌       | 2/8 [00:00<00:00, 3198.10it/s]Calculating Metrics:  25%|██▌       | 2/8 [00:00<00:00, 591.33it/s]Calculating Metrics:  50%|█████     | 4/8 [00:00<00:00, 912.60it/s]Calculating Metrics:  50%|█████     | 4/8 [00:00<00:00, 555.92it/s]Calculating Metrics:  62%|██████▎   | 5/8 [00:00<00:00, 605.31it/s]Calculating Metrics:  62%|██████▎   | 5/8 [00:00<00:00, 470.89it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 498.81it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 468.68it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 466.12it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 463.47it/s]
_ test_case_runner_v3_api[sqlite/multicolumn_map_expectations/expect_multicolumn_sum_to_equal:negative_test_different_value] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/8 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/8 [00:00<?, ?it/s]Calculating Metrics:  25%|██▌       | 2/8 [00:00<00:00, 3067.13it/s]Calculating Metrics:  25%|██▌       | 2/8 [00:00<00:00, 552.94it/s]Calculating Metrics:  50%|█████     | 4/8 [00:00<00:00, 861.87it/s]Calculating Metrics:  50%|█████     | 4/8 [00:00<00:00, 549.30it/s]Calculating Metrics:  62%|██████▎   | 5/8 [00:00<00:00, 612.75it/s]Calculating Metrics:  62%|██████▎   | 5/8 [00:00<00:00, 476.14it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 517.90it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 482.54it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 479.70it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 476.73it/s]
_ test_case_runner_v3_api[pandas/column_pair_map_expectations/expect_column_pair_values_to_be_equal:trivial_case:_x__x] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:  22%|██▏       | 2/9 [00:00<00:00, 3466.37it/s]Calculating Metrics:  22%|██▏       | 2/9 [00:00<00:00, 468.04it/s]Calculating Metrics:  33%|███▎      | 3/9 [00:00<00:00, 656.90it/s]Calculating Metrics:  33%|███▎      | 3/9 [00:00<00:00, 383.64it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 415.59it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 318.72it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 435.75it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 412.61it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 410.94it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 409.05it/s]
_ test_case_runner_v3_api[pandas/column_pair_map_expectations/expect_column_pair_values_to_be_equal:basic_negative_example_compare_number_to_text] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:  22%|██▏       | 2/9 [00:00<00:00, 4072.14it/s]Calculating Metrics:  22%|██▏       | 2/9 [00:00<00:00, 506.19it/s]Calculating Metrics:  33%|███▎      | 3/9 [00:00<00:00, 706.67it/s]Calculating Metrics:  33%|███▎      | 3/9 [00:00<00:00, 387.54it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 427.21it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 321.80it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 483.26it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 445.52it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 443.14it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 440.37it/s]
_ test_case_runner_v3_api[pandas/column_pair_map_expectations/expect_column_pair_values_to_be_equal:basic_negative_example_compare_numbers] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:  22%|██▏       | 2/9 [00:00<00:00, 3800.91it/s]Calculating Metrics:  22%|██▏       | 2/9 [00:00<00:00, 483.66it/s]Calculating Metrics:  33%|███▎      | 3/9 [00:00<00:00, 676.86it/s]Calculating Metrics:  33%|███▎      | 3/9 [00:00<00:00, 377.77it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 412.66it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 320.13it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 465.12it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 436.49it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 434.32it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 432.19it/s]
_ test_case_runner_v3_api[pandas/column_pair_map_expectations/expect_column_pair_values_to_be_equal:positive_example_with_mostly_compare_number_to_text] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:  22%|██▏       | 2/9 [00:00<00:00, 3559.02it/s]Calculating Metrics:  22%|██▏       | 2/9 [00:00<00:00, 433.59it/s]Calculating Metrics:  33%|███▎      | 3/9 [00:00<00:00, 603.38it/s]Calculating Metrics:  33%|███▎      | 3/9 [00:00<00:00, 360.02it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 405.31it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 314.64it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 477.96it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 450.37it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 448.39it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 446.19it/s]
_ test_case_runner_v3_api[pandas/column_pair_map_expectations/expect_column_pair_values_to_be_equal:positive_example_with_mostly_compare_numbers] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:  22%|██▏       | 2/9 [00:00<00:00, 3766.78it/s]Calculating Metrics:  22%|██▏       | 2/9 [00:00<00:00, 486.47it/s]Calculating Metrics:  33%|███▎      | 3/9 [00:00<00:00, 683.19it/s]Calculating Metrics:  33%|███▎      | 3/9 [00:00<00:00, 383.67it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 412.45it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 307.91it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 439.84it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 415.99it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 414.21it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 412.25it/s]
_ test_case_runner_v3_api[pandas/column_pair_map_expectations/expect_column_pair_values_to_be_equal:positive_example_with_mismatched_null_values_and_ignore_row_if__either_value_is_missing_compare_number_to_text] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:  22%|██▏       | 2/9 [00:00<00:00, 4007.94it/s]Calculating Metrics:  22%|██▏       | 2/9 [00:00<00:00, 516.79it/s]Calculating Metrics:  33%|███▎      | 3/9 [00:00<00:00, 726.08it/s]Calculating Metrics:  33%|███▎      | 3/9 [00:00<00:00, 393.91it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 386.58it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 295.62it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 450.54it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 423.11it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 421.15it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 418.16it/s]
_ test_case_runner_v3_api[pandas/column_pair_map_expectations/expect_column_pair_values_to_be_equal:positive_example_with_mismatched_null_values_and_ignore_row_if__either_value_is_missing_compare_numbers] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:  22%|██▏       | 2/9 [00:00<00:00, 3975.64it/s]Calculating Metrics:  22%|██▏       | 2/9 [00:00<00:00, 494.79it/s]Calculating Metrics:  33%|███▎      | 3/9 [00:00<00:00, 692.97it/s]Calculating Metrics:  33%|███▎      | 3/9 [00:00<00:00, 384.76it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 424.71it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 323.34it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 460.24it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 430.71it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 428.18it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 425.95it/s]
_ test_case_runner_v3_api[pandas/column_pair_map_expectations/expect_column_pair_values_to_be_equal:a_negative_example_with_mismatched_null_values_and_ignore_row_if__both_values_are_missing_set_explicitly] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:  22%|██▏       | 2/9 [00:00<00:00, 3663.15it/s]Calculating Metrics:  22%|██▏       | 2/9 [00:00<00:00, 462.21it/s]Calculating Metrics:  33%|███▎      | 3/9 [00:00<00:00, 646.27it/s]Calculating Metrics:  33%|███▎      | 3/9 [00:00<00:00, 363.54it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 389.10it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 298.39it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 419.97it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 397.27it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 395.57it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 393.79it/s]
_ test_case_runner_v3_api[pandas/column_pair_map_expectations/expect_column_pair_values_to_be_equal:a_negative_example_with_mismatched_null_values_and_ignore_row_if__both_values_are_missing_set_by_default] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:  22%|██▏       | 2/9 [00:00<00:00, 3892.63it/s]Calculating Metrics:  22%|██▏       | 2/9 [00:00<00:00, 483.33it/s]Calculating Metrics:  33%|███▎      | 3/9 [00:00<00:00, 680.12it/s]Calculating Metrics:  33%|███▎      | 3/9 [00:00<00:00, 374.96it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 392.28it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 306.16it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 428.84it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 405.53it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 403.71it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 401.84it/s]
_ test_case_runner_v3_api[pandas/column_pair_map_expectations/expect_column_pair_values_to_be_equal:a_negative_example] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:  22%|██▏       | 2/9 [00:00<00:00, 3648.81it/s]Calculating Metrics:  22%|██▏       | 2/9 [00:00<00:00, 466.97it/s]Calculating Metrics:  33%|███▎      | 3/9 [00:00<00:00, 656.32it/s]Calculating Metrics:  33%|███▎      | 3/9 [00:00<00:00, 381.20it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 409.46it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 314.69it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 434.55it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 411.45it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 409.78it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 407.94it/s]
_ test_case_runner_v3_api[pandas/column_pair_map_expectations/expect_column_pair_values_to_be_equal:a_positive_example_with_mostly] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:  22%|██▏       | 2/9 [00:00<00:00, 3914.42it/s]Calculating Metrics:  22%|██▏       | 2/9 [00:00<00:00, 475.63it/s]Calculating Metrics:  33%|███▎      | 3/9 [00:00<00:00, 669.05it/s]Calculating Metrics:  33%|███▎      | 3/9 [00:00<00:00, 372.95it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 398.84it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 311.89it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 432.82it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 407.30it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 405.48it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 403.63it/s]
_ test_case_runner_v3_api[pandas/column_pair_map_expectations/expect_column_pair_values_to_be_equal:a_positive_example_with_ignore_row_if__either_value_is_missing] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:  22%|██▏       | 2/9 [00:00<00:00, 3663.15it/s]Calculating Metrics:  22%|██▏       | 2/9 [00:00<00:00, 486.61it/s]Calculating Metrics:  33%|███▎      | 3/9 [00:00<00:00, 680.75it/s]Calculating Metrics:  33%|███▎      | 3/9 [00:00<00:00, 385.16it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 410.30it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 311.36it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 449.14it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 424.21it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 422.40it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 420.44it/s]
_ test_case_runner_v3_api[pandas/column_pair_map_expectations/expect_column_pair_values_to_be_in_set:basic_positive_test_without_nulls] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 5384.22it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 706.19it/s]Calculating Metrics:  40%|████      | 4/10 [00:00<00:00, 897.85it/s]Calculating Metrics:  40%|████      | 4/10 [00:00<00:00, 481.63it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 460.95it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 353.95it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 485.50it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 453.87it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 451.89it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 449.84it/s]
_ test_case_runner_v3_api[pandas/column_pair_map_expectations/expect_column_pair_values_to_be_in_set:positive_test_with_nulls_and_ignore_row_if_either_value_is_missing] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 4622.67it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 699.48it/s]Calculating Metrics:  40%|████      | 4/10 [00:00<00:00, 877.61it/s]Calculating Metrics:  40%|████      | 4/10 [00:00<00:00, 496.90it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 485.06it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 380.93it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 521.36it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 487.54it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 485.24it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 482.79it/s]
_ test_case_runner_v3_api[pandas/column_pair_map_expectations/expect_column_pair_values_to_be_in_set:another_positive_test_with_nulls] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 5188.83it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 800.24it/s]Calculating Metrics:  40%|████      | 4/10 [00:00<00:00, 1001.74it/s]Calculating Metrics:  40%|████      | 4/10 [00:00<00:00, 547.18it/s] Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 514.56it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 404.76it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 521.82it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 491.08it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 488.99it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 486.60it/s]
_ test_case_runner_v3_api[pandas/column_pair_map_expectations/expect_column_pair_values_A_to_be_greater_than_B:basic_positive_test] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:  22%|██▏       | 2/9 [00:00<00:00, 3283.21it/s]Calculating Metrics:  22%|██▏       | 2/9 [00:00<00:00, 334.01it/s]Calculating Metrics:  33%|███▎      | 3/9 [00:00<00:00, 477.10it/s]Calculating Metrics:  33%|███▎      | 3/9 [00:00<00:00, 264.39it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 300.97it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 225.49it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 353.62it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 330.51it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 329.43it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 328.22it/s]
_ test_case_runner_v3_api[pandas/column_pair_map_expectations/expect_column_pair_values_A_to_be_greater_than_B:basic_positive_test_with_missing_values_and_ignore_row_if__either_value_is_missing] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:  22%|██▏       | 2/9 [00:00<00:00, 3914.42it/s]Calculating Metrics:  22%|██▏       | 2/9 [00:00<00:00, 329.00it/s]Calculating Metrics:  33%|███▎      | 3/9 [00:00<00:00, 469.34it/s]Calculating Metrics:  33%|███▎      | 3/9 [00:00<00:00, 260.99it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 303.61it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 226.71it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 363.28it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 339.15it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 337.91it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 336.66it/s]
_ test_case_runner_v3_api[pandas/column_pair_map_expectations/expect_column_pair_values_A_to_be_greater_than_B:test_ties_with_or_equal] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]Calculating Metrics:  22%|██▏       | 2/9 [00:00<00:00, 3919.91it/s]Calculating Metrics:  22%|██▏       | 2/9 [00:00<00:00, 384.30it/s]Calculating Metrics:  33%|███▎      | 3/9 [00:00<00:00, 549.16it/s]Calculating Metrics:  33%|███▎      | 3/9 [00:00<00:00, 291.49it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 326.29it/s]Calculating Metrics:  44%|████▍     | 4/9 [00:00<00:00, 235.93it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 372.62it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 345.90it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 344.39it/s]Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 343.00it/s]
_ test_case_runner_v3_api[sqlite/column_pair_map_expectations/expect_column_pair_values_to_be_equal:trivial_case:_x__x] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/8 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/8 [00:00<?, ?it/s]Calculating Metrics:  25%|██▌       | 2/8 [00:00<00:00, 2642.08it/s]Calculating Metrics:  25%|██▌       | 2/8 [00:00<00:00, 566.26it/s]Calculating Metrics:  50%|█████     | 4/8 [00:00<00:00, 831.58it/s]Calculating Metrics:  50%|█████     | 4/8 [00:00<00:00, 561.32it/s]Calculating Metrics:  62%|██████▎   | 5/8 [00:00<00:00, 624.91it/s]Calculating Metrics:  62%|██████▎   | 5/8 [00:00<00:00, 508.07it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 557.18it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 526.14it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 523.28it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 519.85it/s]
_ test_case_runner_v3_api[sqlite/column_pair_map_expectations/expect_column_pair_values_to_be_equal:basic_negative_example_compare_numbers] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/8 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/8 [00:00<?, ?it/s]Calculating Metrics:  25%|██▌       | 2/8 [00:00<00:00, 2801.81it/s]Calculating Metrics:  25%|██▌       | 2/8 [00:00<00:00, 484.61it/s]Calculating Metrics:  50%|█████     | 4/8 [00:00<00:00, 711.89it/s]Calculating Metrics:  50%|█████     | 4/8 [00:00<00:00, 492.68it/s]Calculating Metrics:  62%|██████▎   | 5/8 [00:00<00:00, 532.15it/s]Calculating Metrics:  62%|██████▎   | 5/8 [00:00<00:00, 436.23it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 498.25it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 472.26it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 469.87it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 467.07it/s]
_ test_case_runner_v3_api[sqlite/column_pair_map_expectations/expect_column_pair_values_to_be_equal:positive_example_with_mostly_compare_numbers] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/8 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/8 [00:00<?, ?it/s]Calculating Metrics:  25%|██▌       | 2/8 [00:00<00:00, 2736.01it/s]Calculating Metrics:  25%|██▌       | 2/8 [00:00<00:00, 580.73it/s]Calculating Metrics:  50%|█████     | 4/8 [00:00<00:00, 897.03it/s]Calculating Metrics:  50%|█████     | 4/8 [00:00<00:00, 593.21it/s]Calculating Metrics:  62%|██████▎   | 5/8 [00:00<00:00, 662.52it/s]Calculating Metrics:  62%|██████▎   | 5/8 [00:00<00:00, 532.20it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 642.37it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 601.45it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 597.78it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 593.47it/s]
_ test_case_runner_v3_api[sqlite/column_pair_map_expectations/expect_column_pair_values_to_be_equal:positive_example_with_mismatched_null_values_and_ignore_row_if__either_value_is_missing_compare_numbers] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/8 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/8 [00:00<?, ?it/s]Calculating Metrics:  25%|██▌       | 2/8 [00:00<00:00, 2946.47it/s]Calculating Metrics:  25%|██▌       | 2/8 [00:00<00:00, 594.52it/s]Calculating Metrics:  50%|█████     | 4/8 [00:00<00:00, 915.74it/s]Calculating Metrics:  50%|█████     | 4/8 [00:00<00:00, 609.95it/s]Calculating Metrics:  62%|██████▎   | 5/8 [00:00<00:00, 679.44it/s]Calculating Metrics:  62%|██████▎   | 5/8 [00:00<00:00, 545.27it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 583.47it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 544.33it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 540.72it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 537.02it/s]
_ test_case_runner_v3_api[sqlite/column_pair_map_expectations/expect_column_pair_values_to_be_equal:a_negative_example_with_mismatched_null_values_and_ignore_row_if__both_values_are_missing_set_explicitly] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/8 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/8 [00:00<?, ?it/s]Calculating Metrics:  25%|██▌       | 2/8 [00:00<00:00, 2732.45it/s]Calculating Metrics:  25%|██▌       | 2/8 [00:00<00:00, 561.00it/s]Calculating Metrics:  50%|█████     | 4/8 [00:00<00:00, 862.27it/s]Calculating Metrics:  50%|█████     | 4/8 [00:00<00:00, 566.32it/s]Calculating Metrics:  62%|██████▎   | 5/8 [00:00<00:00, 623.15it/s]Calculating Metrics:  62%|██████▎   | 5/8 [00:00<00:00, 492.90it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 526.70it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 496.80it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 493.99it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 490.92it/s]
_ test_case_runner_v3_api[sqlite/column_pair_map_expectations/expect_column_pair_values_to_be_equal:a_negative_example_with_mismatched_null_values_and_ignore_row_if__both_values_are_missing_set_by_default] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/8 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/8 [00:00<?, ?it/s]Calculating Metrics:  25%|██▌       | 2/8 [00:00<00:00, 3025.10it/s]Calculating Metrics:  25%|██▌       | 2/8 [00:00<00:00, 588.55it/s]Calculating Metrics:  50%|█████     | 4/8 [00:00<00:00, 908.45it/s]Calculating Metrics:  50%|█████     | 4/8 [00:00<00:00, 603.50it/s]Calculating Metrics:  62%|██████▎   | 5/8 [00:00<00:00, 672.03it/s]Calculating Metrics:  62%|██████▎   | 5/8 [00:00<00:00, 537.11it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 635.07it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 592.55it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 588.76it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 584.28it/s]
_ test_case_runner_v3_api[sqlite/column_pair_map_expectations/expect_column_pair_values_to_be_equal:a_negative_example] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/8 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/8 [00:00<?, ?it/s]Calculating Metrics:  25%|██▌       | 2/8 [00:00<00:00, 2928.98it/s]Calculating Metrics:  25%|██▌       | 2/8 [00:00<00:00, 569.80it/s]Calculating Metrics:  50%|█████     | 4/8 [00:00<00:00, 880.09it/s]Calculating Metrics:  50%|█████     | 4/8 [00:00<00:00, 570.23it/s]Calculating Metrics:  62%|██████▎   | 5/8 [00:00<00:00, 636.56it/s]Calculating Metrics:  62%|██████▎   | 5/8 [00:00<00:00, 514.63it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 620.07it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 581.83it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 578.13it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 574.06it/s]
_ test_case_runner_v3_api[sqlite/column_pair_map_expectations/expect_column_pair_values_to_be_equal:a_positive_example_with_mostly] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/8 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/8 [00:00<?, ?it/s]Calculating Metrics:  25%|██▌       | 2/8 [00:00<00:00, 2881.69it/s]Calculating Metrics:  25%|██▌       | 2/8 [00:00<00:00, 595.99it/s]Calculating Metrics:  50%|█████     | 4/8 [00:00<00:00, 899.49it/s]Calculating Metrics:  50%|█████     | 4/8 [00:00<00:00, 569.24it/s]Calculating Metrics:  62%|██████▎   | 5/8 [00:00<00:00, 625.01it/s]Calculating Metrics:  62%|██████▎   | 5/8 [00:00<00:00, 471.35it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 557.57it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 518.91it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 515.37it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 511.81it/s]
_ test_case_runner_v3_api[sqlite/column_pair_map_expectations/expect_column_pair_values_to_be_equal:a_positive_example_with_ignore_row_if__either_value_is_missing] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/8 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/8 [00:00<?, ?it/s]Calculating Metrics:  25%|██▌       | 2/8 [00:00<00:00, 2902.63it/s]Calculating Metrics:  25%|██▌       | 2/8 [00:00<00:00, 608.80it/s]Calculating Metrics:  50%|█████     | 4/8 [00:00<00:00, 934.98it/s]Calculating Metrics:  50%|█████     | 4/8 [00:00<00:00, 604.76it/s]Calculating Metrics:  62%|██████▎   | 5/8 [00:00<00:00, 673.11it/s]Calculating Metrics:  62%|██████▎   | 5/8 [00:00<00:00, 529.72it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 565.09it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 530.89it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 527.84it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 524.32it/s]
_ test_case_runner_v3_api[sqlite/column_pair_map_expectations/expect_column_pair_values_to_be_in_set:basic_positive_test_without_nulls] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 4093.34it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 922.23it/s]Calculating Metrics:  60%|██████    | 6/10 [00:00<00:00, 1209.72it/s]Calculating Metrics:  60%|██████    | 6/10 [00:00<00:00, 815.99it/s] Calculating Metrics:  70%|███████   | 7/10 [00:00<00:00, 805.07it/s]Calculating Metrics:  70%|███████   | 7/10 [00:00<00:00, 657.96it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 634.00it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 597.37it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 594.43it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 591.02it/s]
_ test_case_runner_v3_api[sqlite/column_pair_map_expectations/expect_column_pair_values_to_be_in_set:positive_test_with_nulls_and_ignore_row_if_either_value_is_missing] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 4465.19it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 993.05it/s]Calculating Metrics:  60%|██████    | 6/10 [00:00<00:00, 1374.58it/s]Calculating Metrics:  60%|██████    | 6/10 [00:00<00:00, 942.22it/s] Calculating Metrics:  70%|███████   | 7/10 [00:00<00:00, 957.98it/s]Calculating Metrics:  70%|███████   | 7/10 [00:00<00:00, 772.61it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 667.34it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 628.69it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 622.16it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 617.13it/s]
_ test_case_runner_v3_api[sqlite/column_pair_map_expectations/expect_column_pair_values_to_be_in_set:negative_test_with_nulls] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 4167.91it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 1020.43it/s]Calculating Metrics:  60%|██████    | 6/10 [00:00<00:00, 1424.86it/s]Calculating Metrics:  60%|██████    | 6/10 [00:00<00:00, 968.36it/s]Calculating Metrics:  70%|███████   | 7/10 [00:00<00:00, 984.68it/s]Calculating Metrics:  70%|███████   | 7/10 [00:00<00:00, 795.56it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 746.34it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 701.41it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 697.31it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 692.58it/s]
_ test_case_runner_v3_api[sqlite/column_pair_map_expectations/expect_column_pair_values_to_be_in_set:another_positive_test_with_nulls] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 4361.49it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 1027.76it/s]Calculating Metrics:  60%|██████    | 6/10 [00:00<00:00, 1292.01it/s]Calculating Metrics:  60%|██████    | 6/10 [00:00<00:00, 900.48it/s]Calculating Metrics:  70%|███████   | 7/10 [00:00<00:00, 927.42it/s]Calculating Metrics:  70%|███████   | 7/10 [00:00<00:00, 760.78it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 730.52it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 686.90it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 682.78it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 678.01it/s]
_ test_case_runner_v3_api[sqlite/column_pair_map_expectations/expect_column_pair_values_A_to_be_greater_than_B:basic_positive_test] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/8 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/8 [00:00<?, ?it/s]Calculating Metrics:  25%|██▌       | 2/8 [00:00<00:00, 3048.19it/s]Calculating Metrics:  25%|██▌       | 2/8 [00:00<00:00, 476.08it/s]Calculating Metrics:  50%|█████     | 4/8 [00:00<00:00, 754.00it/s]Calculating Metrics:  50%|█████     | 4/8 [00:00<00:00, 469.70it/s]Calculating Metrics:  62%|██████▎   | 5/8 [00:00<00:00, 539.43it/s]Calculating Metrics:  62%|██████▎   | 5/8 [00:00<00:00, 415.90it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 473.12it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 433.51it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 430.73it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 428.24it/s]
_ test_case_runner_v3_api[sqlite/column_pair_map_expectations/expect_column_pair_values_A_to_be_greater_than_B:basic_positive_test_with_missing_values_and_ignore_row_if__either_value_is_missing] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/8 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/8 [00:00<?, ?it/s]Calculating Metrics:  25%|██▌       | 2/8 [00:00<00:00, 3165.51it/s]Calculating Metrics:  25%|██▌       | 2/8 [00:00<00:00, 465.34it/s]Calculating Metrics:  50%|█████     | 4/8 [00:00<00:00, 759.60it/s]Calculating Metrics:  50%|█████     | 4/8 [00:00<00:00, 473.38it/s]Calculating Metrics:  62%|██████▎   | 5/8 [00:00<00:00, 542.47it/s]Calculating Metrics:  62%|██████▎   | 5/8 [00:00<00:00, 417.53it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 481.98it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 446.61it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 444.22it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 441.77it/s]
_ test_case_runner_v3_api[sqlite/column_pair_map_expectations/expect_column_pair_values_A_to_be_greater_than_B:test_ties] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/8 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/8 [00:00<?, ?it/s]Calculating Metrics:  25%|██▌       | 2/8 [00:00<00:00, 2836.86it/s]Calculating Metrics:  25%|██▌       | 2/8 [00:00<00:00, 453.29it/s]Calculating Metrics:  50%|█████     | 4/8 [00:00<00:00, 745.12it/s]Calculating Metrics:  50%|█████     | 4/8 [00:00<00:00, 463.80it/s]Calculating Metrics:  62%|██████▎   | 5/8 [00:00<00:00, 532.08it/s]Calculating Metrics:  62%|██████▎   | 5/8 [00:00<00:00, 412.64it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 478.66it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 443.58it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 441.30it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 438.86it/s]
_ test_case_runner_v3_api[sqlite/column_pair_map_expectations/expect_column_pair_values_A_to_be_greater_than_B:test_ties_with_or_equal] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/8 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/8 [00:00<?, ?it/s]Calculating Metrics:  25%|██▌       | 2/8 [00:00<00:00, 3130.08it/s]Calculating Metrics:  25%|██▌       | 2/8 [00:00<00:00, 514.51it/s]Calculating Metrics:  50%|█████     | 4/8 [00:00<00:00, 825.41it/s]Calculating Metrics:  50%|█████     | 4/8 [00:00<00:00, 501.68it/s]Calculating Metrics:  62%|██████▎   | 5/8 [00:00<00:00, 565.15it/s]Calculating Metrics:  62%|██████▎   | 5/8 [00:00<00:00, 438.86it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 510.04it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 474.55it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 472.25it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 469.17it/s]
_ test_case_runner_v3_api[sqlite/column_pair_map_expectations/expect_column_pair_values_A_to_be_greater_than_B:test_mostly] _
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/8 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/8 [00:00<?, ?it/s]Calculating Metrics:  25%|██▌       | 2/8 [00:00<00:00, 3252.66it/s]Calculating Metrics:  25%|██▌       | 2/8 [00:00<00:00, 484.36it/s]Calculating Metrics:  50%|█████     | 4/8 [00:00<00:00, 794.60it/s]Calculating Metrics:  50%|█████     | 4/8 [00:00<00:00, 471.92it/s]Calculating Metrics:  62%|██████▎   | 5/8 [00:00<00:00, 535.00it/s]Calculating Metrics:  62%|██████▎   | 5/8 [00:00<00:00, 412.98it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 478.56it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 444.74it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 442.55it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 440.19it/s]
=========================== short test summary info ============================
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/multi_table_expectations/expect_table_row_count_to_equal_other_table:basic_positive]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/multi_table_expectations/expect_table_row_count_to_equal_other_table:basic_negative]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_value_lengths_to_equal:positive_test_string_one_character_length]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_value_lengths_to_equal:negative_test_string_value_is_1_too_high]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_value_lengths_to_equal:positive_test_with_missing_value_in_column_mostly]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_value_lengths_to_equal:negative_one_length_too_small]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_value_lengths_to_equal:positive_one_length_too_small_mostly]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_value_lengths_to_equal:test_conditional_expectation_passes]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_value_lengths_to_equal:test_conditional_expectation_fails]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_value_lengths_to_equal:test_conditional_expectation_parser_errors]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_between:basic_positive_now_test]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_between:basic_negative_now_test]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_between:basic_positive_now_timedelta_test]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_between:basic_negative_now_timedelta_test]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_between:basic_positive_datetime_test]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_between:basic_negative_datetime_test]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_json_parseable:basic_positive_test]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_json_parseable:positive_test_with_nulls]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_json_parseable:negative_test_integers]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_json_parseable:negative_test_string_only]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_json_parseable:negative_test_null_only]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_between:basic_positive_test_with_timestamps_tz_informed]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_between:basic_negative_test_with_timestamps_tz_informed]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_not_be_null:basic_positive_case_complete_result_format]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_not_be_null:negative_all_missing_values]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_not_be_null:positive_mostly]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_not_be_null:negative_no_mostly_one_missing]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_not_be_null:negative_mostly]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_not_be_null:positive_case_with_mostly_and_no_unexpected_values]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_not_be_null:negative_case_with_75percent_null_values_no_mostly]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_match_regex_list:basic_positive_test]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_match_regex_list:positive_test_with_multiple_regexes]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_match_regex_list:basic_negative_test]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_match_regex_list:negative_test_with_more_string-ish_strings]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_match_regex_list:positive_test_with_match_on__any]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_match_regex_list:positive_test_column_name_has_space_and_match_on__any]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_value_z_scores_to_be_less_than:basic_successful_test]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_value_z_scores_to_be_less_than:basic_unsuccessful_test]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_value_z_scores_to_be_less_than:basic_test_with_outlier]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_value_z_scores_to_be_less_than:basic_test_mostly_zero]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_value_z_scores_to_be_less_than:basic_test_with_missing]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_between:basic_positive_test0]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_between:another_basic_positive_test0]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_between:basic_positive_test_with_timestamps0]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_between:missing_min_value0]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_between:null_min_value0]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_between:missing_max_value0]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_between:null_max_value0]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_between:basic_negative_test0]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_between:another_negative_test0]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_between:basic_negative_test_with_timestamps0]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_between:positive_test_with_result_format__boolean_only0]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_between:another_positive_test_with_result_format__boolean_only0]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_between:negative_test_with_result_format__boolean_only0]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_between:another_negative_test_with_result_format__boolean_only0]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_between:positive_test_with_mostly0]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_between:2nd_positive_test_with_mostly0]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_between:error:_improperly_mixed_types0]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_between:error:_improperly_mixed_types_again0]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_between:error:_improperly_mixed_types_once_more0]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_between:error:_missing_both_min_value_and_max_value0]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_between:error_on_string-to-int_comparisons0]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_between:test_min_value_is_greater_than_max_value0]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_between:test_strict_min_success0]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_between:test_strict_max_success0]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_between:test_conditional_expectation_passes0]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_between:test_conditional_expectation_parser_errors0]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_match_regex:positive_test_all_missing_values]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_match_regex:positive_test_all_missing_values_mostly]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_match_regex:positive_test_empty_regex]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_match_regex:positive_test_more_complicated_regex]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_between:basic_positive_test1]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_between:another_basic_positive_test1]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_between:basic_positive_test_with_timestamps1]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_between:missing_min_value1]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_between:null_min_value1]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_between:missing_max_value1]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_between:null_max_value1]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_between:positive_test_with_result_format__boolean_only1]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_between:another_positive_test_with_result_format__boolean_only1]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_between:negative_test_with_result_format__boolean_only1]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_between:another_negative_test_with_result_format__boolean_only1]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_between:positive_test_with_mostly1]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_between:2nd_positive_test_with_mostly1]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_between:error:_improperly_mixed_types1]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_between:error:_improperly_mixed_types_again1]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_between:error:_improperly_mixed_types_once_more1]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_between:error:_missing_both_min_value_and_max_value1]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_between:error_on_string-to-int_comparisons1]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_between:test_min_value_is_greater_than_max_value1]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_between:test_strict_min_success1]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_between:test_strict_max_success1]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_between:test_conditional_expectation_passes1]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_between:test_conditional_expectation_parser_errors1]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_not_be_in_set:basic_positive_test_case_single_value_not_in_column]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_not_be_in_set:positive_empty_values_set]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_not_be_in_set:positive_test_strings_set_extra_value_in_column]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_not_be_in_set:positive_test_values_set_is_null]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_not_be_in_set:raise_typeerror_when_values_set_is_missing]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_of_type:basic_python_int_positive_test]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_of_type:negative_test_python_ints_are_not_string]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_of_type:positive_test_pandas_floats]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_of_type:positive_test_pandas_strings]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_of_type:negative_test_python_floats_are_not_python_bools]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_of_type:dtype_object_and_type_object_still_has_aggregate_semantics]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_of_type:dtype_object_and_type_object_still_has_aggregate_semantics_object_underscore]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_of_type:dtype_object_and_type_object_still_has_aggregate_semantics_big_o]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_of_type:positive_pandas_datetime_no_timezone]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_of_type:positive_pandas_datetime_with_timezone]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_of_type:negative_pandas_datetime_with_timezone]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_of_type:negative_pandas_datetime_expected_int]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_in_set:basic_positive_test_case_number_set]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_in_set:vacuously_true_empty_value_set]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_in_set:basic_positive_strings_set]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_in_set:positive_test_float_set]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_in_set:test_conditional_expectation_passes]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_in_set:test_conditional_expectation_parser_errors]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_in_set:test_empty_column_should_be_vacuously_true]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_in_set:basic_positive_test_case_datetime_set]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_decreasing:basic_positive_test]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_decreasing:test_empty_column_should_be_vacuously_true]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_not_match_regex:positive_test_one_missing_value_no_exceptions]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_not_match_regex:positive_test_all_missing_values]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_not_match_regex:positive_test_all_missing_values_mostly]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_dateutil_parseable:positive_test]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_dateutil_parseable:positive_test_with_nulls]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_dateutil_parseable:negative_test]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_dateutil_parseable:test_raising_exception_for_wrong_input_data_type]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_not_match_regex_list:basic_positive_test]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_not_match_regex_list:positive_test_with_multiple_regexes]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_increasing:basic_positive_test]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_increasing:2nd_basic_positive_test]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_increasing:positive_test_with_parse_strings_as_datetimes]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_increasing:positive_test_with_interspersed_nulls]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_value_lengths_to_be_between:positive_test_exact_min_and_max]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_value_lengths_to_be_between:positive_test_with_null_min_value]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_value_lengths_to_be_between:positive_test_with_null_max_value]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_value_lengths_to_be_between:positive_test_with_missing_value_in_column]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_value_lengths_to_be_between:test_error_handling_for_non_int_min_value]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_value_lengths_to_be_between:test_error_handling_for_non_int_max_value]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_value_lengths_to_be_between:test_error_handling_for_both_null_max_and_min_values]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_value_lengths_to_be_between:test_error_handling_values_are_integers]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_value_lengths_to_be_between:test_conditional_expectation_passes]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_value_lengths_to_be_between:test_conditional_expectation_parser_errors]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_match_strftime_format:simple_positive_test]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_match_strftime_format:negative_test_wrong_format]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_match_strftime_format:positive_test_w_nulls]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_match_strftime_format:positive_test_w_mostly]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_match_strftime_format:simple_negative_test]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_match_strftime_format:negative_test_out_of_bounds_value_for_month]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_match_strftime_format:negative_test_iso8601]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_match_strftime_format:test_raising_exception_for_wrong_input_data_type]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_match_strftime_format:test_raising_exception_for_wrong_format]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_unique:basic_positive_test_case]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_unique:positive_case_multiple_null_values]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_unique:positive_case_all_null_values]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_match_json_schema:basic_positive_test]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_match_json_schema:positive_test_with_a_more_complex_schema]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_null:basic_positive_case_basic_result_format]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_null:positive_case_with_mostly_and_no_unexpected_values]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_in_type_list:positive_test_pandas_integer_column]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_in_type_list:negative_test_pandas_float_values_are_not_strings]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_in_type_list:positive_test_float_values]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_in_type_list:positive_test_string_values]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_in_type_list:positive_test_boolean_values]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_in_type_list:positive_test_string_and_int_values]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_in_type_list:positive_test_placeholder_expectation]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_value_lengths_to_equal:positive_test_string_one_character_length]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_value_lengths_to_equal:negative_test_string_value_is_1_too_high]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_value_lengths_to_equal:positive_test_with_missing_value_in_column_mostly]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_value_lengths_to_equal:negative_one_length_too_small]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_value_lengths_to_equal:positive_one_length_too_small_mostly]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_between:basic_positive_now_test]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_between:basic_negative_now_test]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_between:basic_positive_now_timedelta_test]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_between:basic_negative_now_timedelta_test]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_between:basic_positive_datetime_test]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_between:basic_negative_datetime_test]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_between:basic_positive_test_with_timestamps_tz_informed]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_between:basic_negative_test_with_timestamps_tz_informed]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_not_be_null:basic_positive_case_complete_result_format]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_not_be_null:negative_all_missing_values]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_not_be_null:positive_mostly]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_not_be_null:negative_no_mostly_one_missing]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_not_be_null:negative_mostly]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_not_be_null:positive_case_with_mostly_and_no_unexpected_values]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_not_be_null:negative_case_with_75percent_null_values_no_mostly]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_value_z_scores_to_be_less_than:basic_successful_test]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_value_z_scores_to_be_less_than:basic_unsuccessful_test]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_value_z_scores_to_be_less_than:basic_test_with_outlier]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_value_z_scores_to_be_less_than:basic_test_mostly_zero]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_value_z_scores_to_be_less_than:basic_test_with_missing]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_between:basic_positive_test0]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_between:another_basic_positive_test0]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_between:basic_positive_test_with_timestamps0]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_between:missing_min_value0]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_between:null_min_value0]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_between:missing_max_value0]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_between:null_max_value0]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_between:basic_negative_test0]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_between:another_negative_test0]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_between:basic_negative_test_with_timestamps0]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_between:positive_test_with_result_format__boolean_only0]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_between:another_positive_test_with_result_format__boolean_only0]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_between:negative_test_with_result_format__boolean_only0]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_between:another_negative_test_with_result_format__boolean_only0]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_between:positive_test_with_mostly0]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_between:2nd_positive_test_with_mostly0]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_between:3rd_positive_test_with_mostly0]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_between:negative_test_with_mostly0]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_between:error:_improperly_mixed_types0]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_between:error:_improperly_mixed_types_again0]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_between:error:_improperly_mixed_types_once_more0]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_between:error:_missing_both_min_value_and_max_value0]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_between:negative_test_to_verify_that_the_denominator_for_mostly_works_with_missing_values0]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_between:positive_test_to_verify_that_the_denominator_for_mostly_works_with_missing_values0]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_between:error_on_string-to-int_comparisons0]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_between:test_min_value_is_greater_than_max_value0]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_between:test_strict_min_failure0]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_between:test_strict_min_success0]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_between:test_strict_max_failure0]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_between:test_strict_max_success0]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_between:basic_positive_test1]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_between:another_basic_positive_test1]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_between:basic_positive_test_with_timestamps1]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_between:missing_min_value1]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_between:null_min_value1]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_between:missing_max_value1]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_between:null_max_value1]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_between:basic_negative_test1]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_between:another_negative_test1]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_between:basic_negative_test_with_timestamps1]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_between:positive_test_with_result_format__boolean_only1]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_between:another_positive_test_with_result_format__boolean_only1]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_between:negative_test_with_result_format__boolean_only1]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_between:another_negative_test_with_result_format__boolean_only1]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_between:positive_test_with_mostly1]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_between:2nd_positive_test_with_mostly1]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_between:3rd_positive_test_with_mostly1]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_between:negative_test_with_mostly1]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_between:error:_improperly_mixed_types1]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_between:error:_improperly_mixed_types_again1]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_between:error:_improperly_mixed_types_once_more1]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_between:error:_missing_both_min_value_and_max_value1]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_between:negative_test_to_verify_that_the_denominator_for_mostly_works_with_missing_values1]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_between:positive_test_to_verify_that_the_denominator_for_mostly_works_with_missing_values1]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_between:error_on_string-to-int_comparisons1]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_between:test_min_value_is_greater_than_max_value1]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_between:test_strict_min_failure1]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_between:test_strict_min_success1]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_between:test_strict_max_failure1]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_between:test_strict_max_success1]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_not_be_in_set:basic_positive_test_case_single_value_not_in_column]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_not_be_in_set:negative_test_case_include_one_existing_column_value]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_not_be_in_set:positive_empty_values_set]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_not_be_in_set:basic_negative_strings_set_all_character_values]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_not_be_in_set:positive_test_strings_set_extra_value_in_column]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_not_be_in_set:negative_test_float_set_two_out_of_three_column_values_included_mostly]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_not_be_in_set:positive_test_float_set_two_out_of_three_column_values_included_mostly]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_not_be_in_set:positive_test_values_set_is_null]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_not_be_in_set:raise_typeerror_when_values_set_is_missing]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_of_type:basic_sqlalchemy_int_positive_test]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_of_type:negative_sqlite_integer_is_not_varchar]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_of_type:positive_test_sql_non_postgres_floats]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_of_type:positive_test_sql_varchar]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_of_type:negative_test_sqlalchemy_floats_are_not_boolean]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_in_set:basic_positive_test_case_number_set]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_in_set:vacuously_true_empty_value_set]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_in_set:negative_test_case_exclude_existing_column_value]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_in_set:negative_empty_values_set]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_in_set:basic_positive_strings_set]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_in_set:negative_test_strings_set_extra_value_in_column]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_in_set:negative_test_numbers_set_no_matching_values]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_in_set:positive_test_float_set]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_in_set:negative_test_float_set]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_in_set:test_empty_column_should_be_vacuously_true]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_not_match_like_pattern_list:basic_positive_test]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_not_match_like_pattern_list:positive_test_with_multiple_like_patternes]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_not_match_like_pattern_list:negative_test_with_more_string-ish_strings]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_not_match_like_pattern_list:basic_negative_test]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_not_match_like_pattern:negative_test_insufficient_mostly_and_one_non_matching_value]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_not_match_like_pattern:positive_test_exact_mostly_w_one_non_matching_value]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_not_match_like_pattern:positive_test_sufficient_mostly_w_one_non_matching_value]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_not_match_like_pattern:negative_test_one_missing_value_and_insufficent_mostly]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_not_match_like_pattern:positive_test_one_missing_value_no_exceptions]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_not_match_like_pattern:positive_test_all_missing_values]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_not_match_like_pattern:positive_test_all_missing_values_mostly]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_not_match_like_pattern:negative_test_match_characters_not_at_the_beginning_of_string_exact_mostly]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_match_like_pattern:negative_test_insufficient_mostly_and_one_non_matching_value]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_match_like_pattern:positive_test_exact_mostly_w_one_non_matching_value]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_match_like_pattern:positive_test_column_name_has_space]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_match_like_pattern:positive_test_sufficient_mostly_w_one_non_matching_value]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_match_like_pattern:negative_test_one_missing_value_and_insufficent_mostly]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_match_like_pattern:positive_test_one_missing_value_and_exact_mostly]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_match_like_pattern:positive_test_one_missing_value_and_sufficent_mostly]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_match_like_pattern:positive_test_all_missing_values]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_match_like_pattern:positive_test_all_missing_values_mostly]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_match_like_pattern:positive_test_match_characters_not_at_the_beginning_of_string]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_value_lengths_to_be_between:positive_test_exact_min_and_max]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_value_lengths_to_be_between:negative_test_matching_min_max_too_small]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_value_lengths_to_be_between:negative_test_matching_max_min_too_large]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_value_lengths_to_be_between:positive_test_with_null_min_value]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_value_lengths_to_be_between:positive_test_with_null_max_value]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_value_lengths_to_be_between:negative_test_with_max_lt_min]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_value_lengths_to_be_between:positive_test_with_missing_value_in_column]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_value_lengths_to_be_between:test_error_handling_for_non_int_min_value]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_value_lengths_to_be_between:test_error_handling_for_non_int_max_value]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_value_lengths_to_be_between:test_error_handling_for_both_null_max_and_min_values]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_value_lengths_to_be_between:test_error_handling_values_are_integers]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_match_like_pattern_list:basic_negative_test]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_match_like_pattern_list:negative_test_with_more_string-ish_strings]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_match_like_pattern_list:basic_positive_test]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_match_like_pattern_list:positive_test_with_multiple_like_patternes]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_match_like_pattern_list:positive_test_with_match_on__any]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_match_like_pattern_list:positive_test_column_name_has_space_and_match_on__any]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_unique:basic_positive_test_case]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_unique:basic_negative_case_all_non_unique_character_values]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_unique:positive_case_using_mostly]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_unique:negative_case_using_mostly]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_unique:positive_case_multiple_null_values]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_unique:negative_case_non_unique_numeric_values]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_unique:positive_case_all_null_values]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_unique:negative_multiple_duplicate_values]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_null:basic_positive_case_basic_result_format]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_null:negative_no_missing_values]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_null:positive_mostly]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_null:negative_mostly]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_null:positive_case_with_mostly_and_no_unexpected_values]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_null:negative_case_with_75percent_non_null_values_no_mostly]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_in_type_list:positive_test_sqlalchemy_integer_column]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_in_type_list:negative_test_sqlalchemy_float_values_are_not_text]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_in_type_list:positive_test_float_values]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_in_type_list:positive_test_text_values]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_in_type_list:positive_test_boolean_values]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_in_type_list:positive_test_text_and_integer_values]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_map_expectations/expect_column_values_to_be_in_type_list:positive_test_placeholder_expectation]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_stdev_to_be_between:basic_positive_test]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_stdev_to_be_between:vacuously_true]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_stdev_to_be_between:positive_test__exact_min_and_max]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_stdev_to_be_between:basic_negative_test]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_stdev_to_be_between:positive_zero_stdev_exact_min_and_max]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_stdev_to_be_between:positive_test_null_max_exact_min]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_stdev_to_be_between:positive_test_null_min_exact_max]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_stdev_to_be_between:negative_test_null_max]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_stdev_to_be_between:negative_test_null_min]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_stdev_to_be_between:positive_test_missing_value_in_column_complete_result_format]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_distinct_values_to_be_in_set:basic_positive_test]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_distinct_values_to_be_in_set:vacuously_true_universal_set]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_distinct_values_to_be_in_set:positive_test_with_null_values_in_column]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_distinct_values_to_be_in_set:positive_test_with_duplicate_values_in_column]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_distinct_values_to_be_in_set:positive_test_duplicate_and_null_values]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_distinct_values_to_be_in_set:basic_negative_test_no_set_intersection]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_distinct_values_to_be_in_set:negative_test_some_set_intersection_and_extra]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_distinct_values_to_be_in_set:negative_test_with_null_values_in_column]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_distinct_values_to_be_in_set:negative_test_with_duplicate_values_in_column]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_distinct_values_to_be_in_set:negative_test_duplicate_and_null_values]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_most_common_value_to_be_in_set:negative_test_tie_for_most_common_with_missing_values_and_complete_result_format]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_most_common_value_to_be_in_set:negative_test__tie_for_most_common__full_value_set__ties_okay__false]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_most_common_value_to_be_in_set:positive_test__tie_for_most_common__ties_okay__true]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_most_common_value_to_be_in_set:positive_test__tie_for_most_common_but_test_for_last_value__ties_okay__true]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_most_common_value_to_be_in_set:negative_test__tie_for_most_common__value_set_does_not_match__ties_okay__false]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_most_common_value_to_be_in_set:negative_test__tie_for_most_common__value_set_does_not_match__ties_okay__true]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_most_common_value_to_be_in_set:positive_test_string_values_value_set_contains_more_than_actual_mode_list]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_most_common_value_to_be_in_set:negative_test_string_values_value_set_contains_more_than_actual_mode_list]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_quantile_values_to_be_between:basic_positive_test_extremes]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_quantile_values_to_be_between:basic_positive_test_normal_quantiles]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_quantile_values_to_be_between:basic_positive_test_uneven_spacing]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_quantile_values_to_be_between:basic_negative_test_normal_quantiles_wrong_distribution]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_quantile_values_to_be_between:basic_negative_test_disordered_quantile_ranges]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_max_to_be_between:basic_positive_test_case]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_max_to_be_between:basic_negative_test_case]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_max_to_be_between:test_case_with_result_format_summary_also_verifies_that_max_value_is_inclusive]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_max_to_be_between:test_case_with_only_a_lower_bound_and_a_missing_value]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_max_to_be_between:negative_test_case_with_only_a_lower_bound]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_max_to_be_between:datetime_except_sqlite]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_max_to_be_between:datetime_with_evaluation_parameter_except_sqlite]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_max_to_be_between:test_empty_column_should_be_false_no_observed_value_with_which_to_compare]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_mean_to_be_between:simple_mean_positive_range]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_mean_to_be_between:simple_mean_negative_range]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_mean_to_be_between:simple_mean_positive_exact_match0]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_mean_to_be_between:simple_mean_negative_exact_match]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_mean_to_be_between:simple_mean_positive_exact_match1]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_mean_to_be_between:simple_mean_negative_range_match]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_mean_to_be_between:simple_mean_includes_null]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_mean_to_be_between:vacuously_true_missing_min_and_max]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_mean_to_be_between:type_mismatch_null_observed_value]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_mean_to_be_between:coerced_types_true_false_values]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_mean_to_be_between:coerced_types_true_false_and_null]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_mean_to_be_between:catch_exceptions___non_number_min_value]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_mean_to_be_between:catch_exceptions___non_number_max_value]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_mean_to_be_between:test_empty_column_should_be_false_no_observed_value_with_which_to_compare]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_distinct_values_to_equal_set:basic_positive_test]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_distinct_values_to_equal_set:positive_test_with_null_values_in_column]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_distinct_values_to_equal_set:positive_test_with_duplicate_values_in_column]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_distinct_values_to_equal_set:positive_test_duplicate_and_null_values]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_distinct_values_to_equal_set:basic_negative_test_set_contained]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_distinct_values_to_equal_set:negative_test_some_set_intersection]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_distinct_values_to_equal_set:negative_test_with_null_values_in_column]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_distinct_values_to_equal_set:negative_test_with_duplicate_values_in_column]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_distinct_values_to_equal_set:negative_test_duplicate_and_null_values]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_sum_to_be_between:basic_positive_test_case]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_sum_to_be_between:basic_negative_test_case]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_sum_to_be_between:test_case_with_result_format_summary]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_sum_to_be_between:test_case_with_only_a_lower_bound_and_a_missing_value]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_sum_to_be_between:negative_test_case_with_only_a_lower_bound]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_sum_to_be_between:negative_test_case_with_only_a_max]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_sum_to_be_between:raise_valueerror_with_both_max_and_min_are_missing]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_unique_value_count_to_be_between:basic_positive_test]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_unique_value_count_to_be_between:positive_test_with_null_values_in_column]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_unique_value_count_to_be_between:null_min_duplicate_values_in_column]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_unique_value_count_to_be_between:null_max_duplicate_and_null_values]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_unique_value_count_to_be_between:vacuously_true_null_min_and_max]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_median_to_be_between:positive_test_min_equal_max]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_median_to_be_between:positive_test_null_min]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_median_to_be_between:positive_test_null_max]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_median_to_be_between:negative_test_min_equal_max]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_median_to_be_between:negative_test_null_min]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_median_to_be_between:negative_test_null_max]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_median_to_be_between:positive_test_missing_value_in_column_exact_min_and_max]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_median_to_be_between:negative_test_missing_value_in_column_complete_result_format]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_median_to_be_between:vacuously_true_both_min_and_max_null]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_median_to_be_between:test_empty_column_should_be_false_no_observed_value_with_which_to_compare]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_min_to_be_between:basic_positive_test_case]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_min_to_be_between:negative_test_case_with_max_value_none]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_min_to_be_between:test_case_with_result_format_summary_also_verifies_that_max_value_is_inclusive]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_min_to_be_between:test_case_with_only_a_lower_bound_and_a_missing_value]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_min_to_be_between:negative_test_case_with_only_a_lower_bound]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_min_to_be_between:test_on_a_series_with_mostly_nulls]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_min_to_be_between:negative_test_case_with_only_a_max]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_min_to_be_between:raise_valueerror_with_both_max_and_min_are_missing]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_proportion_of_unique_values_to_be_between:basic_positive_test]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_proportion_of_unique_values_to_be_between:positive_test_with_null_values_in_column]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_proportion_of_unique_values_to_be_between:duplicate_values_in_column]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_proportion_of_unique_values_to_be_between:null_max_duplicate_and_null_values]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_proportion_of_unique_values_to_be_between:vacuously_true_null_min_and_max]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_distinct_values_to_contain_set:basic_positive_test]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_distinct_values_to_contain_set:positive_test_with_null_values_in_column]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_distinct_values_to_contain_set:positive_test_with_duplicate_values_in_column]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_distinct_values_to_contain_set:positive_test_duplicate_and_null_values]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_distinct_values_to_contain_set:basic_negative_test_no_set_intersection]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_distinct_values_to_contain_set:negative_test_some_set_intersection]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_distinct_values_to_contain_set:negative_test_with_null_values_in_column]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_distinct_values_to_contain_set:negative_test_with_duplicate_values_in_column]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_distinct_values_to_contain_set:negative_test_duplicate_and_null_values]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_distinct_values_to_contain_set:positive_test_string_values]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_distinct_values_to_contain_set:negative_test_string_values]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_distinct_values_to_contain_set:basic_positive_test_case_date_set]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_distinct_values_to_contain_set:basic_negative_test_case_date_set]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_distinct_values_to_contain_set:basic_positive_test_case_datetime_set]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_aggregate_expectations/expect_column_distinct_values_to_contain_set:basic_negative_test_case_datetime_set]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_stdev_to_be_between:basic_positive_test]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_stdev_to_be_between:vacuously_true]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_stdev_to_be_between:positive_test__exact_min_and_max]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_stdev_to_be_between:basic_negative_test]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_stdev_to_be_between:positive_zero_stdev_exact_min_and_max]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_stdev_to_be_between:positive_test_null_max_exact_min]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_stdev_to_be_between:positive_test_null_min_exact_max]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_stdev_to_be_between:negative_test_null_max]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_stdev_to_be_between:negative_test_null_min]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_stdev_to_be_between:positive_test_missing_value_in_column_complete_result_format]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_distinct_values_to_be_in_set:basic_positive_test]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_distinct_values_to_be_in_set:vacuously_true_universal_set]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_distinct_values_to_be_in_set:positive_test_with_null_values_in_column]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_distinct_values_to_be_in_set:positive_test_with_duplicate_values_in_column]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_distinct_values_to_be_in_set:positive_test_duplicate_and_null_values]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_distinct_values_to_be_in_set:basic_negative_test_no_set_intersection]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_distinct_values_to_be_in_set:negative_test_some_set_intersection_and_extra]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_distinct_values_to_be_in_set:negative_test_with_null_values_in_column]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_distinct_values_to_be_in_set:negative_test_with_duplicate_values_in_column]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_distinct_values_to_be_in_set:negative_test_duplicate_and_null_values]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_most_common_value_to_be_in_set:negative_test_tie_for_most_common_with_missing_values_and_complete_result_format]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_most_common_value_to_be_in_set:negative_test__tie_for_most_common__full_value_set__ties_okay__false]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_most_common_value_to_be_in_set:positive_test__tie_for_most_common__ties_okay__true]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_most_common_value_to_be_in_set:positive_test__tie_for_most_common_but_test_for_last_value__ties_okay__true]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_most_common_value_to_be_in_set:negative_test__tie_for_most_common__value_set_does_not_match__ties_okay__false]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_most_common_value_to_be_in_set:negative_test__tie_for_most_common__value_set_does_not_match__ties_okay__true]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_most_common_value_to_be_in_set:positive_test_string_values_value_set_contains_more_than_actual_mode_list]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_most_common_value_to_be_in_set:negative_test_string_values_value_set_contains_more_than_actual_mode_list]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_quantile_values_to_be_between:basic_positive_test_extremes]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_quantile_values_to_be_between:basic_positive_test_normal_quantiles]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_quantile_values_to_be_between:basic_positive_test_uneven_spacing]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_quantile_values_to_be_between:basic_negative_test_normal_quantiles_wrong_distribution]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_quantile_values_to_be_between:basic_negative_test_disordered_quantile_ranges]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_max_to_be_between:basic_positive_test_case]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_max_to_be_between:basic_negative_test_case]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_max_to_be_between:test_case_with_result_format_summary_also_verifies_that_max_value_is_inclusive]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_max_to_be_between:test_case_with_only_a_lower_bound_and_a_missing_value]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_max_to_be_between:negative_test_case_with_only_a_lower_bound]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_max_to_be_between:datetime_sqlite]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_max_to_be_between:datetime_with_evaluation_parameter_sqlite]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_max_to_be_between:test_empty_column_should_be_false_no_observed_value_with_which_to_compare]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_mean_to_be_between:simple_mean_positive_range]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_mean_to_be_between:simple_mean_negative_range]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_mean_to_be_between:simple_mean_positive_exact_match0]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_mean_to_be_between:simple_mean_negative_exact_match]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_mean_to_be_between:simple_mean_positive_exact_match1]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_mean_to_be_between:simple_mean_negative_range_match]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_mean_to_be_between:simple_mean_includes_null]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_mean_to_be_between:vacuously_true_missing_min_and_max]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_mean_to_be_between:coerced_types_true_false_values]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_mean_to_be_between:coerced_types_true_false_and_null]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_mean_to_be_between:catch_exceptions___non_number_min_value]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_mean_to_be_between:catch_exceptions___non_number_max_value]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_mean_to_be_between:test_empty_column_should_be_false_no_observed_value_with_which_to_compare]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_distinct_values_to_equal_set:basic_positive_test]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_distinct_values_to_equal_set:positive_test_with_null_values_in_column]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_distinct_values_to_equal_set:positive_test_with_duplicate_values_in_column]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_distinct_values_to_equal_set:positive_test_duplicate_and_null_values]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_distinct_values_to_equal_set:basic_negative_test_set_contained]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_distinct_values_to_equal_set:negative_test_some_set_intersection]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_distinct_values_to_equal_set:negative_test_with_null_values_in_column]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_distinct_values_to_equal_set:negative_test_with_duplicate_values_in_column]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_distinct_values_to_equal_set:negative_test_duplicate_and_null_values]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_sum_to_be_between:basic_positive_test_case]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_sum_to_be_between:basic_negative_test_case]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_sum_to_be_between:test_case_with_result_format_summary]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_sum_to_be_between:test_case_with_only_a_lower_bound_and_a_missing_value]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_sum_to_be_between:negative_test_case_with_only_a_lower_bound]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_sum_to_be_between:negative_test_case_with_only_a_max]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_sum_to_be_between:raise_valueerror_with_both_max_and_min_are_missing]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_unique_value_count_to_be_between:basic_positive_test]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_unique_value_count_to_be_between:positive_test_with_null_values_in_column]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_unique_value_count_to_be_between:null_min_duplicate_values_in_column]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_unique_value_count_to_be_between:null_max_duplicate_and_null_values]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_unique_value_count_to_be_between:vacuously_true_null_min_and_max]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_median_to_be_between:positive_test_min_equal_max]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_median_to_be_between:positive_test_null_min]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_median_to_be_between:positive_test_null_max]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_median_to_be_between:negative_test_min_equal_max]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_median_to_be_between:negative_test_null_min]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_median_to_be_between:negative_test_null_max]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_median_to_be_between:positive_test_missing_value_in_column_exact_min_and_max]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_median_to_be_between:negative_test_missing_value_in_column_complete_result_format]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_median_to_be_between:vacuously_true_both_min_and_max_null]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_median_to_be_between:test_empty_column_should_be_false_no_observed_value_with_which_to_compare]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_min_to_be_between:basic_positive_test_case]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_min_to_be_between:negative_test_case_with_max_value_none]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_min_to_be_between:test_case_with_result_format_summary_also_verifies_that_max_value_is_inclusive]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_min_to_be_between:test_case_with_only_a_lower_bound_and_a_missing_value]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_min_to_be_between:negative_test_case_with_only_a_lower_bound]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_min_to_be_between:test_on_a_series_with_mostly_nulls]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_min_to_be_between:negative_test_case_with_only_a_max]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_min_to_be_between:raise_valueerror_with_both_max_and_min_are_missing]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_proportion_of_unique_values_to_be_between:basic_positive_test]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_proportion_of_unique_values_to_be_between:positive_test_with_null_values_in_column]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_proportion_of_unique_values_to_be_between:duplicate_values_in_column]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_proportion_of_unique_values_to_be_between:null_max_duplicate_and_null_values]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_proportion_of_unique_values_to_be_between:vacuously_true_null_min_and_max]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_distinct_values_to_contain_set:basic_positive_test]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_distinct_values_to_contain_set:positive_test_with_null_values_in_column]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_distinct_values_to_contain_set:positive_test_with_duplicate_values_in_column]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_distinct_values_to_contain_set:positive_test_duplicate_and_null_values]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_distinct_values_to_contain_set:basic_negative_test_no_set_intersection]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_distinct_values_to_contain_set:negative_test_some_set_intersection]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_distinct_values_to_contain_set:negative_test_with_null_values_in_column]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_distinct_values_to_contain_set:negative_test_with_duplicate_values_in_column]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_distinct_values_to_contain_set:negative_test_duplicate_and_null_values]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_distinct_values_to_contain_set:positive_test_string_values]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_aggregate_expectations/expect_column_distinct_values_to_contain_set:negative_test_string_values]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_distributional_expectations/expect_column_kl_divergence_to_be_less_than:discrete_baseline_categorical_fixed_partition]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_distributional_expectations/expect_column_kl_divergence_to_be_less_than:discrete_categorical_fixed_alternate_partition]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_distributional_expectations/expect_column_kl_divergence_to_be_less_than:internal_holdout_with_categorical_partition]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_distributional_expectations/expect_column_kl_divergence_to_be_less_than:holdout_0_1]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_distributional_expectations/expect_column_kl_divergence_to_be_less_than:holdout_0_05]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_distributional_expectations/expect_column_kl_divergence_to_be_less_than:missing_vals_no_holdout]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_distributional_expectations/expect_column_kl_divergence_to_be_less_than:norm_0_1_auto_explicit_infinite_endpoints]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_distributional_expectations/expect_column_kl_divergence_to_be_less_than:norm_0_1_auto_inf_partition_test_tail_weights]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_distributional_expectations/expect_column_kl_divergence_to_be_less_than:norm_0_1_auto_inf_partition_should_fail_with_no_holdout]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_distributional_expectations/expect_column_kl_divergence_to_be_less_than:norm_0_1_auto_inf_partition_null_threshold_should_always_succeed]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_distributional_expectations/expect_column_kl_divergence_to_be_less_than:norm_0_1_auto_inf_partition_null_threshold_and_partition_object_supports_profiling]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_distributional_expectations/expect_column_kl_divergence_to_be_less_than:norm_0_1_auto_inf_partition_tail_weights_should_fail_with_no_internal_holdout]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_distributional_expectations/expect_column_kl_divergence_to_be_less_than:positive_norm_0_1_auto_partition]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_distributional_expectations/expect_column_kl_divergence_to_be_less_than:positive_norm_0_1_uniform_partition]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_distributional_expectations/expect_column_kl_divergence_to_be_less_than:positive_norm_0_1_ntile_partition]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_distributional_expectations/expect_column_kl_divergence_to_be_less_than:positive_norm_0_1_kde_partition]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_distributional_expectations/expect_column_kl_divergence_to_be_less_than:negative_norm_1_1_column_norm_0_1_auto_partition]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_distributional_expectations/expect_column_kl_divergence_to_be_less_than:negative_norm_1_1_column_norm_0_1_uniform_partition]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_distributional_expectations/expect_column_kl_divergence_to_be_less_than:negative_norm_1_1_column_norm_0_1_ntile_partition]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_distributional_expectations/expect_column_kl_divergence_to_be_less_than:negative_norm_1_1_column_norm_0_1_kde_partition]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_distributional_expectations/expect_column_kl_divergence_to_be_less_than:positive_bimodal_auto_partition]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_distributional_expectations/expect_column_kl_divergence_to_be_less_than:negative_bimodal_column_norm_0_1_auto_partition]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_distributional_expectations/expect_column_kl_divergence_to_be_less_than:negative_bimodal_column_norm_0_1_uniform_partition]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_distributional_expectations/expect_column_kl_divergence_to_be_less_than:manual_partition_lower_inf_bound]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_distributional_expectations/expect_column_kl_divergence_to_be_less_than:manual_partition_lower_inf_bound_tail_weights]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_distributional_expectations/expect_column_kl_divergence_to_be_less_than:manual_partition_upper_inf_bound]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_distributional_expectations/expect_column_kl_divergence_to_be_less_than:manual_partition_upper_inf_bound_tail_weights]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_distributional_expectations/expect_column_kl_divergence_to_be_less_than:manual_partition_lower_and_upper_inf_bounds0]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_distributional_expectations/expect_column_kl_divergence_to_be_less_than:manual_partition_lower_and_upper_inf_bounds1]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_distributional_expectations/expect_column_kl_divergence_to_be_less_than:tail_weight_holdout_is_not_defined_for_partitions_already_extending_to_inifinity]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_distributional_expectations/expect_column_kl_divergence_to_be_less_than:tail_weight_holdout_is_not_defined_for_partitions_already_extending_to_inifinity_tail_weight]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_distributional_expectations/expect_column_kl_divergence_to_be_less_than:empty_partition]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_distributional_expectations/expect_column_kl_divergence_to_be_less_than:too_big_tail_weight]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_distributional_expectations/expect_column_kl_divergence_to_be_less_than:too_big_internal_weight]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_distributional_expectations/expect_column_kl_divergence_to_be_less_than:manual_partition_undefined_infinite_kl_divergence0]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_distributional_expectations/expect_column_kl_divergence_to_be_less_than:manual_partition_undefined_infinite_kl_divergence1]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_distributional_expectations/expect_column_kl_divergence_to_be_less_than:manual_partition_undefined_infinite_kl_divergence_bins_do_not_cover_all_data]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_distributional_expectations/expect_column_kl_divergence_to_be_less_than:manual_partition_return_partitions_should_have_tail_weights]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_distributional_expectations/expect_column_kl_divergence_to_be_less_than:manual_partition_tail_weights_return_partitions_should_have_tail_weights]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_distributional_expectations/expect_column_kl_divergence_to_be_less_than:manual_partition_bounded_endpoints_empty_tail_weights_in_return]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_distributional_expectations/expect_column_kl_divergence_to_be_less_than:manual_partition_bounded_endpoints_observed_with_tail_weight_infinite_kl_divergence]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_distributional_expectations/expect_column_kl_divergence_to_be_less_than:manual_partition_infinite_endpoints_non_zero_tail_hold_out]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_distributional_expectations/expect_column_kl_divergence_to_be_less_than:manual_partition_upper_infinite_endpoint_non_zero_tail_hold_out]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_distributional_expectations/expect_column_kl_divergence_to_be_less_than:manual_partition_lower_infinite_endpoint_non_zero_tail_hold_out]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_distributional_expectations/expect_column_kl_divergence_to_be_less_than:manual_partition_bounded_endpoints_non_zero_tail_hold_out]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_distributional_expectations/expect_column_kl_divergence_to_be_less_than:discrete_baseline_categorical_fixed_partition]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_distributional_expectations/expect_column_kl_divergence_to_be_less_than:discrete_categorical_fixed_alternate_partition]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_distributional_expectations/expect_column_kl_divergence_to_be_less_than:internal_holdout_with_categorical_partition]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_distributional_expectations/expect_column_kl_divergence_to_be_less_than:holdout_0_1]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_distributional_expectations/expect_column_kl_divergence_to_be_less_than:holdout_0_05]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_distributional_expectations/expect_column_kl_divergence_to_be_less_than:missing_vals_no_holdout]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_distributional_expectations/expect_column_kl_divergence_to_be_less_than:norm_0_1_auto_explicit_infinite_endpoints]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_distributional_expectations/expect_column_kl_divergence_to_be_less_than:norm_0_1_auto_inf_partition_test_tail_weights]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_distributional_expectations/expect_column_kl_divergence_to_be_less_than:norm_0_1_auto_inf_partition_should_fail_with_no_holdout]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_distributional_expectations/expect_column_kl_divergence_to_be_less_than:norm_0_1_auto_inf_partition_null_threshold_should_always_succeed]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_distributional_expectations/expect_column_kl_divergence_to_be_less_than:norm_0_1_auto_inf_partition_tail_weights_should_fail_with_no_internal_holdout]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_distributional_expectations/expect_column_kl_divergence_to_be_less_than:positive_norm_0_1_auto_partition]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_distributional_expectations/expect_column_kl_divergence_to_be_less_than:positive_norm_0_1_uniform_partition]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_distributional_expectations/expect_column_kl_divergence_to_be_less_than:positive_norm_0_1_ntile_partition]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_distributional_expectations/expect_column_kl_divergence_to_be_less_than:positive_norm_0_1_kde_partition]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_distributional_expectations/expect_column_kl_divergence_to_be_less_than:negative_norm_1_1_column_norm_0_1_auto_partition]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_distributional_expectations/expect_column_kl_divergence_to_be_less_than:negative_norm_1_1_column_norm_0_1_uniform_partition]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_distributional_expectations/expect_column_kl_divergence_to_be_less_than:negative_norm_1_1_column_norm_0_1_ntile_partition]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_distributional_expectations/expect_column_kl_divergence_to_be_less_than:negative_norm_1_1_column_norm_0_1_kde_partition]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_distributional_expectations/expect_column_kl_divergence_to_be_less_than:positive_bimodal_auto_partition]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_distributional_expectations/expect_column_kl_divergence_to_be_less_than:negative_bimodal_column_norm_0_1_auto_partition]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_distributional_expectations/expect_column_kl_divergence_to_be_less_than:negative_bimodal_column_norm_0_1_uniform_partition]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_distributional_expectations/expect_column_kl_divergence_to_be_less_than:manual_partition_lower_inf_bound]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_distributional_expectations/expect_column_kl_divergence_to_be_less_than:manual_partition_lower_inf_bound_tail_weights]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_distributional_expectations/expect_column_kl_divergence_to_be_less_than:manual_partition_upper_inf_bound]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_distributional_expectations/expect_column_kl_divergence_to_be_less_than:manual_partition_upper_inf_bound_tail_weights]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_distributional_expectations/expect_column_kl_divergence_to_be_less_than:manual_partition_lower_and_upper_inf_bounds0]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_distributional_expectations/expect_column_kl_divergence_to_be_less_than:manual_partition_lower_and_upper_inf_bounds1]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_distributional_expectations/expect_column_kl_divergence_to_be_less_than:tail_weight_holdout_is_not_defined_for_partitions_already_extending_to_inifinity]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_distributional_expectations/expect_column_kl_divergence_to_be_less_than:tail_weight_holdout_is_not_defined_for_partitions_already_extending_to_inifinity_tail_weight]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_distributional_expectations/expect_column_kl_divergence_to_be_less_than:empty_partition]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_distributional_expectations/expect_column_kl_divergence_to_be_less_than:too_big_tail_weight]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_distributional_expectations/expect_column_kl_divergence_to_be_less_than:too_big_internal_weight]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_distributional_expectations/expect_column_kl_divergence_to_be_less_than:manual_partition_undefined_infinite_kl_divergence0]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_distributional_expectations/expect_column_kl_divergence_to_be_less_than:manual_partition_undefined_infinite_kl_divergence1]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_distributional_expectations/expect_column_kl_divergence_to_be_less_than:manual_partition_undefined_infinite_kl_divergence_bins_do_not_cover_all_data]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_distributional_expectations/expect_column_kl_divergence_to_be_less_than:manual_partition_return_partitions_should_have_tail_weights]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_distributional_expectations/expect_column_kl_divergence_to_be_less_than:manual_partition_tail_weights_return_partitions_should_have_tail_weights]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_distributional_expectations/expect_column_kl_divergence_to_be_less_than:manual_partition_bounded_endpoints_empty_tail_weights_in_return]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_distributional_expectations/expect_column_kl_divergence_to_be_less_than:manual_partition_bounded_endpoints_observed_with_tail_weight_infinite_kl_divergence]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_distributional_expectations/expect_column_kl_divergence_to_be_less_than:manual_partition_infinite_endpoints_non_zero_tail_hold_out]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_distributional_expectations/expect_column_kl_divergence_to_be_less_than:manual_partition_upper_infinite_endpoint_non_zero_tail_hold_out]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_distributional_expectations/expect_column_kl_divergence_to_be_less_than:manual_partition_lower_infinite_endpoint_non_zero_tail_hold_out]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_distributional_expectations/expect_column_kl_divergence_to_be_less_than:manual_partition_bounded_endpoints_non_zero_tail_hold_out]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/other_expectations/expect_table_row_count_to_equal:basic_positive_case]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/other_expectations/expect_table_row_count_to_equal:basic_negative_case_upper_error]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/other_expectations/expect_table_row_count_to_equal:basic_negative_case_lower_error]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/other_expectations/expect_table_row_count_to_equal:basic_negative_case_kwargs_args]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/other_expectations/expect_table_row_count_to_equal:invalid_arguments_throws_exception]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/other_expectations/expect_column_to_exist:basic_positive_test]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/other_expectations/expect_column_to_exist:basic_negative_test]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/other_expectations/expect_column_to_exist:positive_test_with_column_order]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/other_expectations/expect_column_to_exist:column_exists_but_wrong_index]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/other_expectations/expect_table_row_count_to_be_between:basic_positive_test]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/other_expectations/expect_table_row_count_to_be_between:vacuously_true]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/other_expectations/expect_table_row_count_to_be_between:basic_negative_test]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/other_expectations/expect_table_row_count_to_be_between:positive_test_with_null_min_value]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/other_expectations/expect_table_row_count_to_be_between:test_error_handling_for_min_greater_than_max]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/other_expectations/expect_table_row_count_to_be_between:positive_test_with_null_min]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/other_expectations/expect_table_row_count_to_be_between:test_error_handling_for_non_int_min_value]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/other_expectations/expect_table_row_count_to_be_between:test_error_handling_for_non_int_max_value]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/other_expectations/expect_table_columns_to_match_ordered_list:basic_positive_test]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/other_expectations/expect_table_columns_to_match_ordered_list:negative_test_column_is_missing]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/other_expectations/expect_table_columns_to_match_ordered_list:negative_test_column_is_misnamed]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/other_expectations/expect_table_columns_to_match_ordered_list:negative_test_columns_are_right_but_ordering_wrong]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/other_expectations/expect_table_columns_to_match_ordered_list:negative_test_extra_column]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/other_expectations/expect_table_columns_to_match_ordered_list:null_list_provides_vacuously_true_expectation]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/other_expectations/expect_table_column_count_to_be_between:basic_positive_test]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/other_expectations/expect_table_column_count_to_be_between:vacuously_true]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/other_expectations/expect_table_column_count_to_be_between:basic_negative_test]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/other_expectations/expect_table_column_count_to_be_between:positive_test_with_null_min_value]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/other_expectations/expect_table_column_count_to_be_between:test_error_handling_for_max_lt_min]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/other_expectations/expect_table_column_count_to_be_between:positive_test_with_null_min]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/other_expectations/expect_table_column_count_to_be_between:test_error_handling_for_non_int_min_value]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/other_expectations/expect_table_column_count_to_be_between:test_error_handling_for_non_int_max_value]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/other_expectations/expect_table_columns_to_match_set:basic_positive_test_exact_match_true]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/other_expectations/expect_table_columns_to_match_set:basic_positive_test_exact_match_false]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/other_expectations/expect_table_columns_to_match_set:negative_test_column_is_missing_exact_match_true]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/other_expectations/expect_table_columns_to_match_set:positive_test_column_is_missing_exact_match_false]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/other_expectations/expect_table_columns_to_match_set:negative_test_column_is_misnamed_exact_match_true]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/other_expectations/expect_table_columns_to_match_set:negative_test_column_is_misnamed_exact_match_false]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/other_expectations/expect_table_columns_to_match_set:positive_test_columns_are_right_but_ordering_wrong_exact_match_true]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/other_expectations/expect_table_columns_to_match_set:positive_test_columns_are_right_but_ordering_wrong_exact_match_false]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/other_expectations/expect_table_columns_to_match_set:negative_test_extra_column_exact_match_true]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/other_expectations/expect_table_columns_to_match_set:negative_test_extra_column_exact_match_false]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/other_expectations/expect_table_columns_to_match_set:negative_test_null_set_exact_match_true]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/other_expectations/expect_table_columns_to_match_set:positive_test_vacuously_true_null_set_exact_match_false]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/other_expectations/expect_table_column_count_to_equal:basic_positive_case]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/other_expectations/expect_table_column_count_to_equal:basic_negative_case_upper_error]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/other_expectations/expect_table_column_count_to_equal:basic_negative_case_lower_error]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/other_expectations/expect_table_column_count_to_equal:basic_negative_case_kwargs_args]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/other_expectations/expect_table_column_count_to_equal:invalid_arguments_throws_exception]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/other_expectations/expect_table_row_count_to_equal:basic_positive_case]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/other_expectations/expect_table_row_count_to_equal:basic_negative_case_upper_error]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/other_expectations/expect_table_row_count_to_equal:basic_negative_case_lower_error]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/other_expectations/expect_table_row_count_to_equal:basic_negative_case_kwargs_args]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/other_expectations/expect_table_row_count_to_equal:invalid_arguments_throws_exception]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/other_expectations/expect_column_to_exist:basic_positive_test]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/other_expectations/expect_column_to_exist:basic_negative_test]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/other_expectations/expect_column_to_exist:positive_test_with_column_order]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/other_expectations/expect_column_to_exist:column_exists_but_wrong_index]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/other_expectations/expect_table_row_count_to_be_between:basic_positive_test]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/other_expectations/expect_table_row_count_to_be_between:vacuously_true]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/other_expectations/expect_table_row_count_to_be_between:basic_negative_test]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/other_expectations/expect_table_row_count_to_be_between:positive_test_with_null_min_value]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/other_expectations/expect_table_row_count_to_be_between:test_error_handling_for_min_greater_than_max]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/other_expectations/expect_table_row_count_to_be_between:positive_test_with_null_min]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/other_expectations/expect_table_row_count_to_be_between:test_error_handling_for_non_int_min_value]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/other_expectations/expect_table_row_count_to_be_between:test_error_handling_for_non_int_max_value]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/other_expectations/expect_table_columns_to_match_ordered_list:basic_positive_test]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/other_expectations/expect_table_columns_to_match_ordered_list:negative_test_column_is_missing]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/other_expectations/expect_table_columns_to_match_ordered_list:negative_test_column_is_misnamed]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/other_expectations/expect_table_columns_to_match_ordered_list:negative_test_columns_are_right_but_ordering_wrong]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/other_expectations/expect_table_columns_to_match_ordered_list:negative_test_extra_column]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/other_expectations/expect_table_columns_to_match_ordered_list:null_list_provides_vacuously_true_expectation]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/other_expectations/expect_table_column_count_to_be_between:basic_positive_test]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/other_expectations/expect_table_column_count_to_be_between:vacuously_true]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/other_expectations/expect_table_column_count_to_be_between:basic_negative_test]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/other_expectations/expect_table_column_count_to_be_between:positive_test_with_null_min_value]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/other_expectations/expect_table_column_count_to_be_between:test_error_handling_for_max_lt_min]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/other_expectations/expect_table_column_count_to_be_between:positive_test_with_null_min]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/other_expectations/expect_table_column_count_to_be_between:test_error_handling_for_non_int_min_value]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/other_expectations/expect_table_column_count_to_be_between:test_error_handling_for_non_int_max_value]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/other_expectations/expect_table_columns_to_match_set:basic_positive_test_exact_match_true]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/other_expectations/expect_table_columns_to_match_set:basic_positive_test_exact_match_false]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/other_expectations/expect_table_columns_to_match_set:negative_test_column_is_missing_exact_match_true]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/other_expectations/expect_table_columns_to_match_set:positive_test_column_is_missing_exact_match_false]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/other_expectations/expect_table_columns_to_match_set:negative_test_column_is_misnamed_exact_match_true]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/other_expectations/expect_table_columns_to_match_set:negative_test_column_is_misnamed_exact_match_false]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/other_expectations/expect_table_columns_to_match_set:positive_test_columns_are_right_but_ordering_wrong_exact_match_true]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/other_expectations/expect_table_columns_to_match_set:positive_test_columns_are_right_but_ordering_wrong_exact_match_false]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/other_expectations/expect_table_columns_to_match_set:negative_test_extra_column_exact_match_true]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/other_expectations/expect_table_columns_to_match_set:negative_test_extra_column_exact_match_false]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/other_expectations/expect_table_columns_to_match_set:negative_test_null_set_exact_match_true]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/other_expectations/expect_table_columns_to_match_set:positive_test_vacuously_true_null_set_exact_match_false]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/other_expectations/expect_table_column_count_to_equal:basic_positive_case]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/other_expectations/expect_table_column_count_to_equal:basic_negative_case_upper_error]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/other_expectations/expect_table_column_count_to_equal:basic_negative_case_lower_error]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/other_expectations/expect_table_column_count_to_equal:basic_negative_case_kwargs_args]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/other_expectations/expect_table_column_count_to_equal:invalid_arguments_throws_exception]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/multicolumn_map_expectations/expect_compound_columns_to_be_unique:mostly_default_successful_expectation]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/multicolumn_map_expectations/expect_compound_columns_to_be_unique:mostly_default_fails_expectation]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/multicolumn_map_expectations/expect_compound_columns_to_be_unique:mostly_set_successful_expectation]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/multicolumn_map_expectations/expect_compound_columns_to_be_unique:mostly_set_fails_expectation]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/multicolumn_map_expectations/expect_compound_columns_to_be_unique:mostly_set_incorrectly]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/multicolumn_map_expectations/expect_compound_columns_to_be_unique:basic_test_ignore_if_all_are_missing_the_default_behavior]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/multicolumn_map_expectations/expect_compound_columns_to_be_unique:basic_test_ignore_if_any_are_missing]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/multicolumn_map_expectations/expect_compound_columns_to_be_unique:row_condition_with_ignore_if_any_are_missing]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/multicolumn_map_expectations/expect_compound_columns_to_be_unique:unexpected_values_without_unexpected_index_list]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/multicolumn_map_expectations/expect_compound_columns_to_be_unique:unexpected_values_exact_match_out_without_unexpected_index_list]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/multicolumn_map_expectations/expect_select_column_values_to_be_unique_within_record:mostly_default_successful_expectation]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/multicolumn_map_expectations/expect_select_column_values_to_be_unique_within_record:mostly_default_fails_expectation]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/multicolumn_map_expectations/expect_select_column_values_to_be_unique_within_record:mostly_set_successful_expectation]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/multicolumn_map_expectations/expect_select_column_values_to_be_unique_within_record:mostly_set_fails_expectation]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/multicolumn_map_expectations/expect_select_column_values_to_be_unique_within_record:mostly_set_incorrectly]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/multicolumn_map_expectations/expect_select_column_values_to_be_unique_within_record:basic_test_ignore_if_all_are_missing_the_default_behavior]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/multicolumn_map_expectations/expect_select_column_values_to_be_unique_within_record:basic_test_ignore_if_any_are_missing]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/multicolumn_map_expectations/expect_select_column_values_to_be_unique_within_record:unexpected_values_exact_match_out_without_unexpected_index_list]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/multicolumn_map_expectations/expect_multicolumn_sum_to_equal:mostly_default_successful_expectation]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/multicolumn_map_expectations/expect_multicolumn_sum_to_equal:mostly_default_fails_expectation]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/multicolumn_map_expectations/expect_multicolumn_sum_to_equal:mostly_set_successful_expectation]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/multicolumn_map_expectations/expect_multicolumn_sum_to_equal:mostly_set_fails_expectation]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/multicolumn_map_expectations/expect_multicolumn_sum_to_equal:mostly_set_incorrectly]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/multicolumn_map_expectations/expect_multicolumn_sum_to_equal:basic_positive_test]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/multicolumn_map_expectations/expect_multicolumn_sum_to_equal:basic_positive_test_more_than_2_columns]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/multicolumn_map_expectations/expect_multicolumn_sum_to_equal:row_condition_with_ignore_if_any_are_missing]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/multicolumn_map_expectations/expect_compound_columns_to_be_unique:mostly_default_successful_expectation]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/multicolumn_map_expectations/expect_compound_columns_to_be_unique:mostly_default_fails_expectation]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/multicolumn_map_expectations/expect_compound_columns_to_be_unique:mostly_set_successful_expectation]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/multicolumn_map_expectations/expect_compound_columns_to_be_unique:mostly_set_fails_expectation]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/multicolumn_map_expectations/expect_compound_columns_to_be_unique:mostly_set_incorrectly]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/multicolumn_map_expectations/expect_compound_columns_to_be_unique:basic_test_ignore_if_all_are_missing_the_default_behavior]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/multicolumn_map_expectations/expect_compound_columns_to_be_unique:basic_test_ignore_if_any_are_missing]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/multicolumn_map_expectations/expect_compound_columns_to_be_unique:unexpected_values_without_unexpected_index_list]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/multicolumn_map_expectations/expect_compound_columns_to_be_unique:unexpected_values_exact_match_out_without_unexpected_index_list]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/multicolumn_map_expectations/expect_select_column_values_to_be_unique_within_record:mostly_default_successful_expectation]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/multicolumn_map_expectations/expect_select_column_values_to_be_unique_within_record:mostly_default_fails_expectation]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/multicolumn_map_expectations/expect_select_column_values_to_be_unique_within_record:mostly_set_successful_expectation]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/multicolumn_map_expectations/expect_select_column_values_to_be_unique_within_record:mostly_set_fails_expectation]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/multicolumn_map_expectations/expect_select_column_values_to_be_unique_within_record:mostly_set_incorrectly]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/multicolumn_map_expectations/expect_select_column_values_to_be_unique_within_record:basic_test_ignore_if_all_are_missing_the_default_behavior]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/multicolumn_map_expectations/expect_select_column_values_to_be_unique_within_record:basic_test_ignore_if_any_are_missing]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/multicolumn_map_expectations/expect_select_column_values_to_be_unique_within_record:unexpected_values]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/multicolumn_map_expectations/expect_select_column_values_to_be_unique_within_record:unexpected_values_exact_match_out_no_index_list]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/multicolumn_map_expectations/expect_select_column_values_to_be_unique_within_record:unexpected_values_exact_match_out_without_unexpected_index_list]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/multicolumn_map_expectations/expect_multicolumn_sum_to_equal:mostly_default_successful_expectation]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/multicolumn_map_expectations/expect_multicolumn_sum_to_equal:mostly_default_fails_expectation]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/multicolumn_map_expectations/expect_multicolumn_sum_to_equal:mostly_set_successful_expectation]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/multicolumn_map_expectations/expect_multicolumn_sum_to_equal:mostly_set_fails_expectation]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/multicolumn_map_expectations/expect_multicolumn_sum_to_equal:mostly_set_incorrectly]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/multicolumn_map_expectations/expect_multicolumn_sum_to_equal:basic_positive_test]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/multicolumn_map_expectations/expect_multicolumn_sum_to_equal:basic_positive_test_more_than_2_columns]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/multicolumn_map_expectations/expect_multicolumn_sum_to_equal:basic_negative_test]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/multicolumn_map_expectations/expect_multicolumn_sum_to_equal:negative_test_with_ignore_if_any_are_missing]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/multicolumn_map_expectations/expect_multicolumn_sum_to_equal:negative_test_different_value]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_pair_map_expectations/expect_column_pair_values_to_be_equal:trivial_case:_x__x]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_pair_map_expectations/expect_column_pair_values_to_be_equal:basic_negative_example_compare_number_to_text]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_pair_map_expectations/expect_column_pair_values_to_be_equal:basic_negative_example_compare_numbers]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_pair_map_expectations/expect_column_pair_values_to_be_equal:positive_example_with_mostly_compare_number_to_text]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_pair_map_expectations/expect_column_pair_values_to_be_equal:positive_example_with_mostly_compare_numbers]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_pair_map_expectations/expect_column_pair_values_to_be_equal:positive_example_with_mismatched_null_values_and_ignore_row_if__either_value_is_missing_compare_number_to_text]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_pair_map_expectations/expect_column_pair_values_to_be_equal:positive_example_with_mismatched_null_values_and_ignore_row_if__either_value_is_missing_compare_numbers]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_pair_map_expectations/expect_column_pair_values_to_be_equal:a_negative_example_with_mismatched_null_values_and_ignore_row_if__both_values_are_missing_set_explicitly]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_pair_map_expectations/expect_column_pair_values_to_be_equal:a_negative_example_with_mismatched_null_values_and_ignore_row_if__both_values_are_missing_set_by_default]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_pair_map_expectations/expect_column_pair_values_to_be_equal:a_negative_example]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_pair_map_expectations/expect_column_pair_values_to_be_equal:a_positive_example_with_mostly]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_pair_map_expectations/expect_column_pair_values_to_be_equal:a_positive_example_with_ignore_row_if__either_value_is_missing]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_pair_map_expectations/expect_column_pair_values_to_be_in_set:basic_positive_test_without_nulls]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_pair_map_expectations/expect_column_pair_values_to_be_in_set:positive_test_with_nulls_and_ignore_row_if_either_value_is_missing]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_pair_map_expectations/expect_column_pair_values_to_be_in_set:another_positive_test_with_nulls]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_pair_map_expectations/expect_column_pair_values_A_to_be_greater_than_B:basic_positive_test]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_pair_map_expectations/expect_column_pair_values_A_to_be_greater_than_B:basic_positive_test_with_missing_values_and_ignore_row_if__either_value_is_missing]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_pair_map_expectations/expect_column_pair_values_A_to_be_greater_than_B:test_ties_with_or_equal]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_pair_map_expectations/expect_column_pair_values_to_be_equal:trivial_case:_x__x]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_pair_map_expectations/expect_column_pair_values_to_be_equal:basic_negative_example_compare_numbers]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_pair_map_expectations/expect_column_pair_values_to_be_equal:positive_example_with_mostly_compare_numbers]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_pair_map_expectations/expect_column_pair_values_to_be_equal:positive_example_with_mismatched_null_values_and_ignore_row_if__either_value_is_missing_compare_numbers]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_pair_map_expectations/expect_column_pair_values_to_be_equal:a_negative_example_with_mismatched_null_values_and_ignore_row_if__both_values_are_missing_set_explicitly]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_pair_map_expectations/expect_column_pair_values_to_be_equal:a_negative_example_with_mismatched_null_values_and_ignore_row_if__both_values_are_missing_set_by_default]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_pair_map_expectations/expect_column_pair_values_to_be_equal:a_negative_example]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_pair_map_expectations/expect_column_pair_values_to_be_equal:a_positive_example_with_mostly]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_pair_map_expectations/expect_column_pair_values_to_be_equal:a_positive_example_with_ignore_row_if__either_value_is_missing]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_pair_map_expectations/expect_column_pair_values_to_be_in_set:basic_positive_test_without_nulls]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_pair_map_expectations/expect_column_pair_values_to_be_in_set:positive_test_with_nulls_and_ignore_row_if_either_value_is_missing]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_pair_map_expectations/expect_column_pair_values_to_be_in_set:negative_test_with_nulls]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_pair_map_expectations/expect_column_pair_values_to_be_in_set:another_positive_test_with_nulls]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_pair_map_expectations/expect_column_pair_values_A_to_be_greater_than_B:basic_positive_test]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_pair_map_expectations/expect_column_pair_values_A_to_be_greater_than_B:basic_positive_test_with_missing_values_and_ignore_row_if__either_value_is_missing]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_pair_map_expectations/expect_column_pair_values_A_to_be_greater_than_B:test_ties]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_pair_map_expectations/expect_column_pair_values_A_to_be_greater_than_B:test_ties_with_or_equal]
PASSED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[sqlite/column_pair_map_expectations/expect_column_pair_values_A_to_be_greater_than_B:test_mostly]
SKIPPED [190] tests/test_definitions/test_expectations_v3_api.py:405: Skipped
FAILED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_between:3rd_positive_test_with_mostly0]
FAILED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_between:negative_test_with_mostly0]
FAILED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_between:negative_test_to_verify_that_the_denominator_for_mostly_works_with_missing_values0]
FAILED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_between:positive_test_to_verify_that_the_denominator_for_mostly_works_with_missing_values0]
FAILED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_between:test_allow_cross_type_comparisons0]
FAILED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_between:test_allow_cross_type_comparisons_again0]
FAILED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_between:test_strict_min_failure0]
FAILED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_between:test_strict_max_failure0]
FAILED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_between:test_conditional_expectation_fails0]
FAILED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_match_regex:negative_test_insufficient_mostly_and_one_non_matching_value]
FAILED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_match_regex:negative_test_insufficient_mostly_and_one_non_matching_value_summary_output]
FAILED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_match_regex:positive_test_exact_mostly_w_one_non_matching_value]
FAILED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_match_regex:positive_test_column_name_has_space]
FAILED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_match_regex:positive_test_sufficient_mostly_w_one_non_matching_value]
FAILED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_match_regex:negative_test_one_missing_value_and_insufficent_mostly]
FAILED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_match_regex:positive_test_one_missing_value_and_exact_mostly]
FAILED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_match_regex:positive_test_one_missing_value_and_sufficent_mostly]
FAILED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_match_regex:positive_test_match_characters_not_at_the_beginning_of_string]
FAILED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_between:basic_negative_test1]
FAILED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_between:another_negative_test1]
FAILED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_between:basic_negative_test_with_timestamps1]
FAILED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_between:3rd_positive_test_with_mostly1]
FAILED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_between:negative_test_with_mostly1]
FAILED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_between:negative_test_to_verify_that_the_denominator_for_mostly_works_with_missing_values1]
FAILED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_between:positive_test_to_verify_that_the_denominator_for_mostly_works_with_missing_values1]
FAILED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_between:test_allow_cross_type_comparisons1]
FAILED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_between:test_allow_cross_type_comparisons_again1]
FAILED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_between:test_strict_min_failure1]
FAILED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_between:test_strict_max_failure1]
FAILED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_between:test_conditional_expectation_fails1]
FAILED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_not_be_in_set:negative_test_case_include_one_existing_column_value]
FAILED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_not_be_in_set:basic_negative_strings_set_all_character_values]
FAILED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_not_be_in_set:negative_test_float_set_two_out_of_three_column_values_included_mostly]
FAILED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_not_be_in_set:negative_test_float_set_two_out_of_three_column_values_included_mostly_summary_output]
FAILED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_not_be_in_set:positive_test_float_set_two_out_of_three_column_values_included_mostly]
FAILED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_in_set:negative_test_case_exclude_existing_column_value]
FAILED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_in_set:negative_empty_values_set]
FAILED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_in_set:negative_test_strings_set_extra_value_in_column]
FAILED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_in_set:negative_test_numbers_set_no_matching_values]
FAILED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_in_set:negative_test_float_set]
FAILED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_in_set:test_conditional_expectation_fails]
FAILED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_in_set:basic_negative_test_case_datetime_set]
FAILED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_decreasing:positive_test_with_mostly]
FAILED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_decreasing:basic_negative_test]
FAILED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_decreasing:basic_negative_test_with_strictly]
FAILED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_not_match_regex:negative_test_insufficient_mostly_and_one_non_matching_value]
FAILED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_not_match_regex:positive_test_exact_mostly_w_one_non_matching_value]
FAILED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_not_match_regex:positive_test_sufficient_mostly_w_one_non_matching_value]
FAILED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_not_match_regex:negative_test_one_missing_value_and_insufficent_mostly]
FAILED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_not_match_regex:negative_test_empty_regex]
FAILED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_not_match_regex:negative_test_match_characters_not_at_the_beginning_of_string_exact_mostly]
FAILED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_not_match_regex_list:basic_negative_test]
FAILED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_not_match_regex_list:negative_test_with_more_string-ish_strings]
FAILED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_increasing:positive_test_with_strictly]
FAILED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_increasing:basic_negative_test]
FAILED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_increasing:negative_test_with_interspersed_nulls]
FAILED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_value_lengths_to_be_between:negative_test_matching_min_max_too_small]
FAILED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_value_lengths_to_be_between:negative_test_matching_max_min_too_large]
FAILED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_value_lengths_to_be_between:negative_test_with_max_lt_min]
FAILED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_value_lengths_to_be_between:test_conditional_expectation_fails]
FAILED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_unique:basic_negative_case_all_non_unique_character_values]
FAILED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_unique:multi_type_column_contains_2_and_quoted_2_suppressed_for_sqalchemy]
FAILED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_unique:positive_case_using_mostly]
FAILED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_unique:negative_case_using_mostly]
FAILED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_unique:negative_case_non_unique_numeric_values]
FAILED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_unique:negative_multiple_duplicate_values]
FAILED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_match_json_schema:basic_negative_test]
FAILED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_null:negative_no_missing_values]
FAILED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_null:positive_mostly]
FAILED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_null:negative_mostly]
FAILED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_null:negative_case_with_75percent_non_null_values_no_mostly]
FAILED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_in_type_list:negative_test_string_and_int_values_summary_output]
FAILED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_in_type_list:negative_test_string_and_int_values_complete_output]
FAILED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/multicolumn_map_expectations/expect_compound_columns_to_be_unique:unexpected_values_with_unexpected_index_list]
FAILED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/multicolumn_map_expectations/expect_compound_columns_to_be_unique:unexpected_values_exact_match_out_with_unexpected_index_list]
FAILED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/multicolumn_map_expectations/expect_select_column_values_to_be_unique_within_record:unexpected_values]
FAILED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/multicolumn_map_expectations/expect_select_column_values_to_be_unique_within_record:unexpected_values_exact_match_out_with_index_list]
FAILED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/multicolumn_map_expectations/expect_multicolumn_sum_to_equal:basic_negative_test]
FAILED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/multicolumn_map_expectations/expect_multicolumn_sum_to_equal:negative_test_with_ignore_if_any_are_missing]
FAILED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/multicolumn_map_expectations/expect_multicolumn_sum_to_equal:negative_test_different_value]
FAILED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_pair_map_expectations/expect_column_pair_values_to_be_in_set:negative_test_with_nulls]
FAILED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_pair_map_expectations/expect_column_pair_values_A_to_be_greater_than_B:test_mostly]
========== 84 failed, 835 passed, 190 skipped, 45 warnings in 56.19s ===========
