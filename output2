=================================== FAILURES ===================================
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_between:negative_test_to_verify_that_the_denominator_for_mostly_works_with_missing_values0] _

test_case = {'expectation_type': 'expect_column_values_to_be_between', 'pk_column': True, 'skip': False, 'test': {'exact_match_out..._value': 1, 'mostly': 0.9}, 'out': {'success': False, 'unexpected_index_list': [4], 'unexpected_list': [5]}, ...}, ...}

    @pytest.mark.order(index=0)
    @pytest.mark.integration
    @pytest.mark.slow  # 12.68s
    def test_case_runner_v3_api(test_case):
        if test_case["skip"]:
            pytest.skip()

        # Note: this should never be done in practice, but we are wiping expectations to reuse batches during testing.
        # test_case["batch"]._initialize_expectations()
        if "parse_strings_as_datetimes" in test_case["test"]["in"]:
            with pytest.deprecated_call():
                evaluate_json_test_v3_api(
                    validator=test_case["validator_with_data"],
                    expectation_type=test_case["expectation_type"],
                    test=test_case["test"],
                    pk_column=test_case["pk_column"],
                )
        else:
>           evaluate_json_test_v3_api(
                validator=test_case["validator_with_data"],
                expectation_type=test_case["expectation_type"],
                test=test_case["test"],
                pk_column=test_case["pk_column"],
            )

tests/test_definitions/test_expectations_v3_api.py:418:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
great_expectations/self_check/util.py:2901: in evaluate_json_test_v3_api
    check_json_test_result(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

test = {'exact_match_out': False, 'in': {'column': 'z', 'max_value': 4, 'min_value': 1, 'mostly': 0.9}, 'input': {'column': '..., 'min_value': 1, 'mostly': 0.9}, 'out': {'success': False, 'unexpected_index_list': [4], 'unexpected_list': [5]}, ...}
result = {'exception_info': {'exception_message': None, 'exception_traceback': None, 'raised_exception': False}, 'expectation_c...': 10, 'missing_count': 5, 'missing_percent': 50.0, 'partial_unexpected_counts': [{'count': 1, 'value': 5}], ...}, ...}
data_asset = <great_expectations.execution_engine.pandas_batch_data.PandasBatchData object at 0x7f9502a651f0>
pk_column = True

    def check_json_test_result(  # noqa: C901 - 52
        test, result, data_asset=None, pk_column=False
    ) -> None:

        # check for id_pk results in cases where pk_column is true and unexpected_index_list already exists
        # this will work for testing since result_format is COMPLETE
        if pk_column:
            if not result["success"]:
                if "unexpected_index_list" in result["result"]:
                    assert "unexpected_index_query" in result["result"]

        if "unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )
            elif "unexpected_list" in test["output"]:
                (
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )

        if "partial_unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "partial_unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )
            elif "partial_unexpected_list" in test["output"]:
                (
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )

        # Determine if np.allclose(..) might be needed for float comparison
        try_allclose = False
        if "observed_value" in test["output"]:
            if RX_FLOAT.match(repr(test["output"]["observed_value"])):
                try_allclose = True

        # Check results
        if test["exact_match_out"] is True:
            if "result" in result and "observed_value" in result["result"]:
                if isinstance(result["result"]["observed_value"], (np.floating, float)):
                    assert np.allclose(
                        result["result"]["observed_value"],
                        expectationValidationResultSchema.load(test["output"])["result"][
                            "observed_value"
                        ],
                        rtol=RTOL,
                        atol=ATOL,
                    ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {expectationValidationResultSchema.load(test['output'])['result']['observed_value']}"
                else:
                    assert result == expectationValidationResultSchema.load(
                        test["output"]
                    ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
            else:
                assert result == expectationValidationResultSchema.load(
                    test["output"]
                ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
        else:
            # Convert result to json since our tests are reading from json so cannot easily contain richer types (e.g. NaN)
            # NOTE - 20191031 - JPC - we may eventually want to change these tests as we update our view on how
            # representations, serializations, and objects should interact and how much of that is shown to the user.
            result = result.to_json_dict()
            for key, value in test["output"].items():
                # Apply our great expectations-specific test logic

                if key == "success":
                    if isinstance(value, (np.floating, float)):
                        try:
                            assert np.allclose(
                                result["success"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['success']} not np.allclose to {value}"
                        except TypeError:
                            assert (
                                result["success"] == value
                            ), f"{result['success']} != {value}"
                    else:
                        assert result["success"] == value, f"{result['success']} != {value}"

                elif key == "observed_value":
                    if "tolerance" in test:
                        if isinstance(value, dict):
                            assert set(result["result"]["observed_value"].keys()) == set(
                                value.keys()
                            ), f"{set(result['result']['observed_value'].keys())} != {set(value.keys())}"
                            for k, v in value.items():
                                assert np.allclose(
                                    result["result"]["observed_value"][k],
                                    v,
                                    rtol=test["tolerance"],
                                )
                        else:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=test["tolerance"],
                            )
                    else:
                        if isinstance(value, dict) and "values" in value:
                            try:
                                assert np.allclose(
                                    result["result"]["observed_value"]["values"],
                                    value["values"],
                                    rtol=RTOL,
                                    atol=ATOL,
                                ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']['values']} not np.allclose to {value['values']}"
                            except TypeError as e:
                                print(e)
                                assert (
                                    result["result"]["observed_value"] == value
                                ), f"{result['result']['observed_value']} != {value}"
                        elif try_allclose:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {value}"
                        else:
                            assert (
                                result["result"]["observed_value"] == value
                            ), f"{result['result']['observed_value']} != {value}"

                # NOTE: This is a key used ONLY for testing cases where an expectation is legitimately allowed to return
                # any of multiple possible observed_values. expect_column_values_to_be_of_type is one such expectation.
                elif key == "observed_value_list":
                    assert result["result"]["observed_value"] in value

                elif key == "unexpected_index_list":
                    if isinstance(data_asset, (SqlAlchemyDataset, SparkDFDataset)):
                        pass
                    elif isinstance(data_asset, (SqlAlchemyBatchData, SparkDFBatchData)):
                        pass
                    else:
>                       assert (
                            result["result"]["unexpected_index_list"] == value
                        ), f"{result['result']['unexpected_index_list']} != {value}"
E                       AssertionError: [{'z': 5, 'pk_index': 4}] != [4]

great_expectations/self_check/util.py:3075: AssertionError
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 4247.40it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 386.39it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 553.61it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 304.10it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 421.22it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 321.81it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 501.75it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 465.33it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 463.67it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 461.68it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_between:positive_test_to_verify_that_the_denominator_for_mostly_works_with_missing_values0] _

test_case = {'expectation_type': 'expect_column_values_to_be_between', 'pk_column': True, 'skip': False, 'test': {'exact_match_out...n_value': 1, 'mostly': 0.8}, 'out': {'success': True, 'unexpected_index_list': [4], 'unexpected_list': [5]}, ...}, ...}

    @pytest.mark.order(index=0)
    @pytest.mark.integration
    @pytest.mark.slow  # 12.68s
    def test_case_runner_v3_api(test_case):
        if test_case["skip"]:
            pytest.skip()

        # Note: this should never be done in practice, but we are wiping expectations to reuse batches during testing.
        # test_case["batch"]._initialize_expectations()
        if "parse_strings_as_datetimes" in test_case["test"]["in"]:
            with pytest.deprecated_call():
                evaluate_json_test_v3_api(
                    validator=test_case["validator_with_data"],
                    expectation_type=test_case["expectation_type"],
                    test=test_case["test"],
                    pk_column=test_case["pk_column"],
                )
        else:
>           evaluate_json_test_v3_api(
                validator=test_case["validator_with_data"],
                expectation_type=test_case["expectation_type"],
                test=test_case["test"],
                pk_column=test_case["pk_column"],
            )

tests/test_definitions/test_expectations_v3_api.py:418:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
great_expectations/self_check/util.py:2901: in evaluate_json_test_v3_api
    check_json_test_result(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

test = {'exact_match_out': False, 'in': {'column': 'z', 'max_value': 4, 'min_value': 1, 'mostly': 0.8}, 'input': {'column': '...4, 'min_value': 1, 'mostly': 0.8}, 'out': {'success': True, 'unexpected_index_list': [4], 'unexpected_list': [5]}, ...}
result = {'exception_info': {'exception_message': None, 'exception_traceback': None, 'raised_exception': False}, 'expectation_c...': 10, 'missing_count': 5, 'missing_percent': 50.0, 'partial_unexpected_counts': [{'count': 1, 'value': 5}], ...}, ...}
data_asset = <great_expectations.execution_engine.pandas_batch_data.PandasBatchData object at 0x7f9502a651f0>
pk_column = True

    def check_json_test_result(  # noqa: C901 - 52
        test, result, data_asset=None, pk_column=False
    ) -> None:

        # check for id_pk results in cases where pk_column is true and unexpected_index_list already exists
        # this will work for testing since result_format is COMPLETE
        if pk_column:
            if not result["success"]:
                if "unexpected_index_list" in result["result"]:
                    assert "unexpected_index_query" in result["result"]

        if "unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )
            elif "unexpected_list" in test["output"]:
                (
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )

        if "partial_unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "partial_unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )
            elif "partial_unexpected_list" in test["output"]:
                (
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )

        # Determine if np.allclose(..) might be needed for float comparison
        try_allclose = False
        if "observed_value" in test["output"]:
            if RX_FLOAT.match(repr(test["output"]["observed_value"])):
                try_allclose = True

        # Check results
        if test["exact_match_out"] is True:
            if "result" in result and "observed_value" in result["result"]:
                if isinstance(result["result"]["observed_value"], (np.floating, float)):
                    assert np.allclose(
                        result["result"]["observed_value"],
                        expectationValidationResultSchema.load(test["output"])["result"][
                            "observed_value"
                        ],
                        rtol=RTOL,
                        atol=ATOL,
                    ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {expectationValidationResultSchema.load(test['output'])['result']['observed_value']}"
                else:
                    assert result == expectationValidationResultSchema.load(
                        test["output"]
                    ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
            else:
                assert result == expectationValidationResultSchema.load(
                    test["output"]
                ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
        else:
            # Convert result to json since our tests are reading from json so cannot easily contain richer types (e.g. NaN)
            # NOTE - 20191031 - JPC - we may eventually want to change these tests as we update our view on how
            # representations, serializations, and objects should interact and how much of that is shown to the user.
            result = result.to_json_dict()
            for key, value in test["output"].items():
                # Apply our great expectations-specific test logic

                if key == "success":
                    if isinstance(value, (np.floating, float)):
                        try:
                            assert np.allclose(
                                result["success"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['success']} not np.allclose to {value}"
                        except TypeError:
                            assert (
                                result["success"] == value
                            ), f"{result['success']} != {value}"
                    else:
                        assert result["success"] == value, f"{result['success']} != {value}"

                elif key == "observed_value":
                    if "tolerance" in test:
                        if isinstance(value, dict):
                            assert set(result["result"]["observed_value"].keys()) == set(
                                value.keys()
                            ), f"{set(result['result']['observed_value'].keys())} != {set(value.keys())}"
                            for k, v in value.items():
                                assert np.allclose(
                                    result["result"]["observed_value"][k],
                                    v,
                                    rtol=test["tolerance"],
                                )
                        else:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=test["tolerance"],
                            )
                    else:
                        if isinstance(value, dict) and "values" in value:
                            try:
                                assert np.allclose(
                                    result["result"]["observed_value"]["values"],
                                    value["values"],
                                    rtol=RTOL,
                                    atol=ATOL,
                                ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']['values']} not np.allclose to {value['values']}"
                            except TypeError as e:
                                print(e)
                                assert (
                                    result["result"]["observed_value"] == value
                                ), f"{result['result']['observed_value']} != {value}"
                        elif try_allclose:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {value}"
                        else:
                            assert (
                                result["result"]["observed_value"] == value
                            ), f"{result['result']['observed_value']} != {value}"

                # NOTE: This is a key used ONLY for testing cases where an expectation is legitimately allowed to return
                # any of multiple possible observed_values. expect_column_values_to_be_of_type is one such expectation.
                elif key == "observed_value_list":
                    assert result["result"]["observed_value"] in value

                elif key == "unexpected_index_list":
                    if isinstance(data_asset, (SqlAlchemyDataset, SparkDFDataset)):
                        pass
                    elif isinstance(data_asset, (SqlAlchemyBatchData, SparkDFBatchData)):
                        pass
                    else:
>                       assert (
                            result["result"]["unexpected_index_list"] == value
                        ), f"{result['result']['unexpected_index_list']} != {value}"
E                       AssertionError: [{'z': 5, 'pk_index': 4}] != [4]

great_expectations/self_check/util.py:3075: AssertionError
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 4009.85it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 373.62it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 534.87it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 293.57it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 420.70it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 317.42it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 485.77it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 451.06it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 449.40it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 447.49it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_between:test_allow_cross_type_comparisons0] _

test_case = {'expectation_type': 'expect_column_values_to_be_between', 'pk_column': True, 'skip': False, 'test': {'exact_match_out...': 10, 'min_value': 0}, 'out': {'success': False, 'unexpected_index_list': [9], 'unexpected_list': ['abc']}, ...}, ...}

    @pytest.mark.order(index=0)
    @pytest.mark.integration
    @pytest.mark.slow  # 12.68s
    def test_case_runner_v3_api(test_case):
        if test_case["skip"]:
            pytest.skip()

        # Note: this should never be done in practice, but we are wiping expectations to reuse batches during testing.
        # test_case["batch"]._initialize_expectations()
        if "parse_strings_as_datetimes" in test_case["test"]["in"]:
            with pytest.deprecated_call():
                evaluate_json_test_v3_api(
                    validator=test_case["validator_with_data"],
                    expectation_type=test_case["expectation_type"],
                    test=test_case["test"],
                    pk_column=test_case["pk_column"],
                )
        else:
>           evaluate_json_test_v3_api(
                validator=test_case["validator_with_data"],
                expectation_type=test_case["expectation_type"],
                test=test_case["test"],
                pk_column=test_case["pk_column"],
            )

tests/test_definitions/test_expectations_v3_api.py:418:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
great_expectations/self_check/util.py:2901: in evaluate_json_test_v3_api
    check_json_test_result(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

test = {'exact_match_out': False, 'in': {'allow_cross_type_comparisons': True, 'column': 'y', 'max_value': 10, 'min_value': 0..._value': 10, 'min_value': 0}, 'out': {'success': False, 'unexpected_index_list': [9], 'unexpected_list': ['abc']}, ...}
result = {'exception_info': {'exception_message': None, 'exception_traceback': None, 'raised_exception': False}, 'expectation_c...10, 'missing_count': 0, 'missing_percent': 0.0, 'partial_unexpected_counts': [{'count': 1, 'value': 'abc'}], ...}, ...}
data_asset = <great_expectations.execution_engine.pandas_batch_data.PandasBatchData object at 0x7f9502a651f0>
pk_column = True

    def check_json_test_result(  # noqa: C901 - 52
        test, result, data_asset=None, pk_column=False
    ) -> None:

        # check for id_pk results in cases where pk_column is true and unexpected_index_list already exists
        # this will work for testing since result_format is COMPLETE
        if pk_column:
            if not result["success"]:
                if "unexpected_index_list" in result["result"]:
                    assert "unexpected_index_query" in result["result"]

        if "unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )
            elif "unexpected_list" in test["output"]:
                (
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )

        if "partial_unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "partial_unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )
            elif "partial_unexpected_list" in test["output"]:
                (
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )

        # Determine if np.allclose(..) might be needed for float comparison
        try_allclose = False
        if "observed_value" in test["output"]:
            if RX_FLOAT.match(repr(test["output"]["observed_value"])):
                try_allclose = True

        # Check results
        if test["exact_match_out"] is True:
            if "result" in result and "observed_value" in result["result"]:
                if isinstance(result["result"]["observed_value"], (np.floating, float)):
                    assert np.allclose(
                        result["result"]["observed_value"],
                        expectationValidationResultSchema.load(test["output"])["result"][
                            "observed_value"
                        ],
                        rtol=RTOL,
                        atol=ATOL,
                    ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {expectationValidationResultSchema.load(test['output'])['result']['observed_value']}"
                else:
                    assert result == expectationValidationResultSchema.load(
                        test["output"]
                    ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
            else:
                assert result == expectationValidationResultSchema.load(
                    test["output"]
                ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
        else:
            # Convert result to json since our tests are reading from json so cannot easily contain richer types (e.g. NaN)
            # NOTE - 20191031 - JPC - we may eventually want to change these tests as we update our view on how
            # representations, serializations, and objects should interact and how much of that is shown to the user.
            result = result.to_json_dict()
            for key, value in test["output"].items():
                # Apply our great expectations-specific test logic

                if key == "success":
                    if isinstance(value, (np.floating, float)):
                        try:
                            assert np.allclose(
                                result["success"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['success']} not np.allclose to {value}"
                        except TypeError:
                            assert (
                                result["success"] == value
                            ), f"{result['success']} != {value}"
                    else:
                        assert result["success"] == value, f"{result['success']} != {value}"

                elif key == "observed_value":
                    if "tolerance" in test:
                        if isinstance(value, dict):
                            assert set(result["result"]["observed_value"].keys()) == set(
                                value.keys()
                            ), f"{set(result['result']['observed_value'].keys())} != {set(value.keys())}"
                            for k, v in value.items():
                                assert np.allclose(
                                    result["result"]["observed_value"][k],
                                    v,
                                    rtol=test["tolerance"],
                                )
                        else:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=test["tolerance"],
                            )
                    else:
                        if isinstance(value, dict) and "values" in value:
                            try:
                                assert np.allclose(
                                    result["result"]["observed_value"]["values"],
                                    value["values"],
                                    rtol=RTOL,
                                    atol=ATOL,
                                ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']['values']} not np.allclose to {value['values']}"
                            except TypeError as e:
                                print(e)
                                assert (
                                    result["result"]["observed_value"] == value
                                ), f"{result['result']['observed_value']} != {value}"
                        elif try_allclose:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {value}"
                        else:
                            assert (
                                result["result"]["observed_value"] == value
                            ), f"{result['result']['observed_value']} != {value}"

                # NOTE: This is a key used ONLY for testing cases where an expectation is legitimately allowed to return
                # any of multiple possible observed_values. expect_column_values_to_be_of_type is one such expectation.
                elif key == "observed_value_list":
                    assert result["result"]["observed_value"] in value

                elif key == "unexpected_index_list":
                    if isinstance(data_asset, (SqlAlchemyDataset, SparkDFDataset)):
                        pass
                    elif isinstance(data_asset, (SqlAlchemyBatchData, SparkDFBatchData)):
                        pass
                    else:
>                       assert (
                            result["result"]["unexpected_index_list"] == value
                        ), f"{result['result']['unexpected_index_list']} != {value}"
E                       AssertionError: [{'y': 'abc', 'pk_index': 9}] != [9]

great_expectations/self_check/util.py:3075: AssertionError
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 4082.05it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 408.66it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 582.41it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 322.51it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 456.88it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 348.46it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 529.30it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 491.57it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 489.65it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 487.38it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_between:test_allow_cross_type_comparisons_again0] _

test_case = {'expectation_type': 'expect_column_values_to_be_between', 'pk_column': True, 'skip': False, 'test': {'exact_match_out... 'unexpected_index_list': [0, 1, 2, 3, 4, 5, ...], 'unexpected_list': ['1', '10', '2', '3', '4', '5', ...]}, ...}, ...}

    @pytest.mark.order(index=0)
    @pytest.mark.integration
    @pytest.mark.slow  # 12.68s
    def test_case_runner_v3_api(test_case):
        if test_case["skip"]:
            pytest.skip()

        # Note: this should never be done in practice, but we are wiping expectations to reuse batches during testing.
        # test_case["batch"]._initialize_expectations()
        if "parse_strings_as_datetimes" in test_case["test"]["in"]:
            with pytest.deprecated_call():
                evaluate_json_test_v3_api(
                    validator=test_case["validator_with_data"],
                    expectation_type=test_case["expectation_type"],
                    test=test_case["test"],
                    pk_column=test_case["pk_column"],
                )
        else:
>           evaluate_json_test_v3_api(
                validator=test_case["validator_with_data"],
                expectation_type=test_case["expectation_type"],
                test=test_case["test"],
                pk_column=test_case["pk_column"],
            )

tests/test_definitions/test_expectations_v3_api.py:418:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
great_expectations/self_check/util.py:2901: in evaluate_json_test_v3_api
    check_json_test_result(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

test = {'exact_match_out': False, 'in': {'allow_cross_type_comparisons': True, 'column': 'numeric', 'max_value': 10, 'min_val...False, 'unexpected_index_list': [0, 1, 2, 3, 4, 5, ...], 'unexpected_list': ['1', '10', '2', '3', '4', '5', ...]}, ...}
result = {'exception_info': {'exception_message': None, 'exception_traceback': None, 'raised_exception': False}, 'expectation_c... 1, 'value': '2'}, {'count': 1, 'value': '3'}, {'count': 1, 'value': '4'}, {'count': 1, 'value': '5'}, ...], ...}, ...}
data_asset = <great_expectations.execution_engine.pandas_batch_data.PandasBatchData object at 0x7f9502a651f0>
pk_column = True

    def check_json_test_result(  # noqa: C901 - 52
        test, result, data_asset=None, pk_column=False
    ) -> None:

        # check for id_pk results in cases where pk_column is true and unexpected_index_list already exists
        # this will work for testing since result_format is COMPLETE
        if pk_column:
            if not result["success"]:
                if "unexpected_index_list" in result["result"]:
                    assert "unexpected_index_query" in result["result"]

        if "unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )
            elif "unexpected_list" in test["output"]:
                (
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )

        if "partial_unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "partial_unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )
            elif "partial_unexpected_list" in test["output"]:
                (
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )

        # Determine if np.allclose(..) might be needed for float comparison
        try_allclose = False
        if "observed_value" in test["output"]:
            if RX_FLOAT.match(repr(test["output"]["observed_value"])):
                try_allclose = True

        # Check results
        if test["exact_match_out"] is True:
            if "result" in result and "observed_value" in result["result"]:
                if isinstance(result["result"]["observed_value"], (np.floating, float)):
                    assert np.allclose(
                        result["result"]["observed_value"],
                        expectationValidationResultSchema.load(test["output"])["result"][
                            "observed_value"
                        ],
                        rtol=RTOL,
                        atol=ATOL,
                    ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {expectationValidationResultSchema.load(test['output'])['result']['observed_value']}"
                else:
                    assert result == expectationValidationResultSchema.load(
                        test["output"]
                    ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
            else:
                assert result == expectationValidationResultSchema.load(
                    test["output"]
                ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
        else:
            # Convert result to json since our tests are reading from json so cannot easily contain richer types (e.g. NaN)
            # NOTE - 20191031 - JPC - we may eventually want to change these tests as we update our view on how
            # representations, serializations, and objects should interact and how much of that is shown to the user.
            result = result.to_json_dict()
            for key, value in test["output"].items():
                # Apply our great expectations-specific test logic

                if key == "success":
                    if isinstance(value, (np.floating, float)):
                        try:
                            assert np.allclose(
                                result["success"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['success']} not np.allclose to {value}"
                        except TypeError:
                            assert (
                                result["success"] == value
                            ), f"{result['success']} != {value}"
                    else:
                        assert result["success"] == value, f"{result['success']} != {value}"

                elif key == "observed_value":
                    if "tolerance" in test:
                        if isinstance(value, dict):
                            assert set(result["result"]["observed_value"].keys()) == set(
                                value.keys()
                            ), f"{set(result['result']['observed_value'].keys())} != {set(value.keys())}"
                            for k, v in value.items():
                                assert np.allclose(
                                    result["result"]["observed_value"][k],
                                    v,
                                    rtol=test["tolerance"],
                                )
                        else:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=test["tolerance"],
                            )
                    else:
                        if isinstance(value, dict) and "values" in value:
                            try:
                                assert np.allclose(
                                    result["result"]["observed_value"]["values"],
                                    value["values"],
                                    rtol=RTOL,
                                    atol=ATOL,
                                ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']['values']} not np.allclose to {value['values']}"
                            except TypeError as e:
                                print(e)
                                assert (
                                    result["result"]["observed_value"] == value
                                ), f"{result['result']['observed_value']} != {value}"
                        elif try_allclose:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {value}"
                        else:
                            assert (
                                result["result"]["observed_value"] == value
                            ), f"{result['result']['observed_value']} != {value}"

                # NOTE: This is a key used ONLY for testing cases where an expectation is legitimately allowed to return
                # any of multiple possible observed_values. expect_column_values_to_be_of_type is one such expectation.
                elif key == "observed_value_list":
                    assert result["result"]["observed_value"] in value

                elif key == "unexpected_index_list":
                    if isinstance(data_asset, (SqlAlchemyDataset, SparkDFDataset)):
                        pass
                    elif isinstance(data_asset, (SqlAlchemyBatchData, SparkDFBatchData)):
                        pass
                    else:
>                       assert (
                            result["result"]["unexpected_index_list"] == value
                        ), f"{result['result']['unexpected_index_list']} != {value}"
E                       AssertionError: [{'numeric': '1', 'pk_index': 0}, {'numeric': '2', 'pk_index': 1}, {'numeric': '3', 'pk_index': 2}, {'numeric': '4', 'pk_index': 3}, {'numeric': '5', 'pk_index': 4}, {'numeric': '6', 'pk_index': 5}, {'numeric': '7', 'pk_index': 6}, {'numeric': '8', 'pk_index': 7}, {'numeric': '9', 'pk_index': 8}, {'numeric': '10', 'pk_index': 9}] != [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]

great_expectations/self_check/util.py:3075: AssertionError
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 4074.12it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 410.52it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 586.07it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 323.73it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 459.77it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 350.21it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 532.57it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 494.37it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 492.39it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 490.10it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_between:test_strict_min_failure0] _

test_case = {'expectation_type': 'expect_column_values_to_be_between', 'pk_column': True, 'skip': False, 'test': {'exact_match_out...e': 1, 'strict_min': True}, 'out': {'success': False, 'unexpected_index_list': [0], 'unexpected_list': [1]}, ...}, ...}

    @pytest.mark.order(index=0)
    @pytest.mark.integration
    @pytest.mark.slow  # 12.68s
    def test_case_runner_v3_api(test_case):
        if test_case["skip"]:
            pytest.skip()

        # Note: this should never be done in practice, but we are wiping expectations to reuse batches during testing.
        # test_case["batch"]._initialize_expectations()
        if "parse_strings_as_datetimes" in test_case["test"]["in"]:
            with pytest.deprecated_call():
                evaluate_json_test_v3_api(
                    validator=test_case["validator_with_data"],
                    expectation_type=test_case["expectation_type"],
                    test=test_case["test"],
                    pk_column=test_case["pk_column"],
                )
        else:
>           evaluate_json_test_v3_api(
                validator=test_case["validator_with_data"],
                expectation_type=test_case["expectation_type"],
                test=test_case["test"],
                pk_column=test_case["pk_column"],
            )

tests/test_definitions/test_expectations_v3_api.py:418:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
great_expectations/self_check/util.py:2901: in evaluate_json_test_v3_api
    check_json_test_result(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

test = {'exact_match_out': False, 'in': {'column': 'x', 'max_value': 10, 'min_value': 1, 'strict_min': True}, 'input': {'colu...n_value': 1, 'strict_min': True}, 'out': {'success': False, 'unexpected_index_list': [0], 'unexpected_list': [1]}, ...}
result = {'exception_info': {'exception_message': None, 'exception_traceback': None, 'raised_exception': False}, 'expectation_c...t': 10, 'missing_count': 0, 'missing_percent': 0.0, 'partial_unexpected_counts': [{'count': 1, 'value': 1}], ...}, ...}
data_asset = <great_expectations.execution_engine.pandas_batch_data.PandasBatchData object at 0x7f9502a651f0>
pk_column = True

    def check_json_test_result(  # noqa: C901 - 52
        test, result, data_asset=None, pk_column=False
    ) -> None:

        # check for id_pk results in cases where pk_column is true and unexpected_index_list already exists
        # this will work for testing since result_format is COMPLETE
        if pk_column:
            if not result["success"]:
                if "unexpected_index_list" in result["result"]:
                    assert "unexpected_index_query" in result["result"]

        if "unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )
            elif "unexpected_list" in test["output"]:
                (
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )

        if "partial_unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "partial_unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )
            elif "partial_unexpected_list" in test["output"]:
                (
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )

        # Determine if np.allclose(..) might be needed for float comparison
        try_allclose = False
        if "observed_value" in test["output"]:
            if RX_FLOAT.match(repr(test["output"]["observed_value"])):
                try_allclose = True

        # Check results
        if test["exact_match_out"] is True:
            if "result" in result and "observed_value" in result["result"]:
                if isinstance(result["result"]["observed_value"], (np.floating, float)):
                    assert np.allclose(
                        result["result"]["observed_value"],
                        expectationValidationResultSchema.load(test["output"])["result"][
                            "observed_value"
                        ],
                        rtol=RTOL,
                        atol=ATOL,
                    ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {expectationValidationResultSchema.load(test['output'])['result']['observed_value']}"
                else:
                    assert result == expectationValidationResultSchema.load(
                        test["output"]
                    ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
            else:
                assert result == expectationValidationResultSchema.load(
                    test["output"]
                ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
        else:
            # Convert result to json since our tests are reading from json so cannot easily contain richer types (e.g. NaN)
            # NOTE - 20191031 - JPC - we may eventually want to change these tests as we update our view on how
            # representations, serializations, and objects should interact and how much of that is shown to the user.
            result = result.to_json_dict()
            for key, value in test["output"].items():
                # Apply our great expectations-specific test logic

                if key == "success":
                    if isinstance(value, (np.floating, float)):
                        try:
                            assert np.allclose(
                                result["success"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['success']} not np.allclose to {value}"
                        except TypeError:
                            assert (
                                result["success"] == value
                            ), f"{result['success']} != {value}"
                    else:
                        assert result["success"] == value, f"{result['success']} != {value}"

                elif key == "observed_value":
                    if "tolerance" in test:
                        if isinstance(value, dict):
                            assert set(result["result"]["observed_value"].keys()) == set(
                                value.keys()
                            ), f"{set(result['result']['observed_value'].keys())} != {set(value.keys())}"
                            for k, v in value.items():
                                assert np.allclose(
                                    result["result"]["observed_value"][k],
                                    v,
                                    rtol=test["tolerance"],
                                )
                        else:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=test["tolerance"],
                            )
                    else:
                        if isinstance(value, dict) and "values" in value:
                            try:
                                assert np.allclose(
                                    result["result"]["observed_value"]["values"],
                                    value["values"],
                                    rtol=RTOL,
                                    atol=ATOL,
                                ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']['values']} not np.allclose to {value['values']}"
                            except TypeError as e:
                                print(e)
                                assert (
                                    result["result"]["observed_value"] == value
                                ), f"{result['result']['observed_value']} != {value}"
                        elif try_allclose:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {value}"
                        else:
                            assert (
                                result["result"]["observed_value"] == value
                            ), f"{result['result']['observed_value']} != {value}"

                # NOTE: This is a key used ONLY for testing cases where an expectation is legitimately allowed to return
                # any of multiple possible observed_values. expect_column_values_to_be_of_type is one such expectation.
                elif key == "observed_value_list":
                    assert result["result"]["observed_value"] in value

                elif key == "unexpected_index_list":
                    if isinstance(data_asset, (SqlAlchemyDataset, SparkDFDataset)):
                        pass
                    elif isinstance(data_asset, (SqlAlchemyBatchData, SparkDFBatchData)):
                        pass
                    else:
>                       assert (
                            result["result"]["unexpected_index_list"] == value
                        ), f"{result['result']['unexpected_index_list']} != {value}"
E                       AssertionError: [{'x': 1, 'pk_index': 0}] != [0]

great_expectations/self_check/util.py:3075: AssertionError
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 3036.05it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 349.10it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 498.53it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 278.35it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 392.56it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 300.96it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 471.83it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 437.95it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 436.43it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 434.58it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_between:test_conditional_expectation_fails0] _

test_case = {'expectation_type': 'expect_column_values_to_be_between', 'pk_column': True, 'skip': False, 'test': {'exact_match_out...'column': 'x', 'condition_parser': 'pandas', 'max_value': 10, 'min_value': 10, ...}, 'only_for': ['pandas'], ...}, ...}

    @pytest.mark.order(index=0)
    @pytest.mark.integration
    @pytest.mark.slow  # 12.68s
    def test_case_runner_v3_api(test_case):
        if test_case["skip"]:
            pytest.skip()

        # Note: this should never be done in practice, but we are wiping expectations to reuse batches during testing.
        # test_case["batch"]._initialize_expectations()
        if "parse_strings_as_datetimes" in test_case["test"]["in"]:
            with pytest.deprecated_call():
                evaluate_json_test_v3_api(
                    validator=test_case["validator_with_data"],
                    expectation_type=test_case["expectation_type"],
                    test=test_case["test"],
                    pk_column=test_case["pk_column"],
                )
        else:
>           evaluate_json_test_v3_api(
                validator=test_case["validator_with_data"],
                expectation_type=test_case["expectation_type"],
                test=test_case["test"],
                pk_column=test_case["pk_column"],
            )

tests/test_definitions/test_expectations_v3_api.py:418:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
great_expectations/self_check/util.py:2901: in evaluate_json_test_v3_api
    check_json_test_result(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

test = {'exact_match_out': False, 'in': {'column': 'x', 'condition_parser': 'pandas', 'max_value': 10, 'min_value': 10, ...},...ut': {'column': 'x', 'condition_parser': 'pandas', 'max_value': 10, 'min_value': 10, ...}, 'only_for': ['pandas'], ...}
result = {'exception_info': {'exception_message': None, 'exception_traceback': None, 'raised_exception': False}, 'expectation_c...nt': 1, 'missing_count': 0, 'missing_percent': 0.0, 'partial_unexpected_counts': [{'count': 1, 'value': 9}], ...}, ...}
data_asset = <great_expectations.execution_engine.pandas_batch_data.PandasBatchData object at 0x7f9502a651f0>
pk_column = True

    def check_json_test_result(  # noqa: C901 - 52
        test, result, data_asset=None, pk_column=False
    ) -> None:

        # check for id_pk results in cases where pk_column is true and unexpected_index_list already exists
        # this will work for testing since result_format is COMPLETE
        if pk_column:
            if not result["success"]:
                if "unexpected_index_list" in result["result"]:
                    assert "unexpected_index_query" in result["result"]

        if "unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )
            elif "unexpected_list" in test["output"]:
                (
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )

        if "partial_unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "partial_unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )
            elif "partial_unexpected_list" in test["output"]:
                (
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )

        # Determine if np.allclose(..) might be needed for float comparison
        try_allclose = False
        if "observed_value" in test["output"]:
            if RX_FLOAT.match(repr(test["output"]["observed_value"])):
                try_allclose = True

        # Check results
        if test["exact_match_out"] is True:
            if "result" in result and "observed_value" in result["result"]:
                if isinstance(result["result"]["observed_value"], (np.floating, float)):
                    assert np.allclose(
                        result["result"]["observed_value"],
                        expectationValidationResultSchema.load(test["output"])["result"][
                            "observed_value"
                        ],
                        rtol=RTOL,
                        atol=ATOL,
                    ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {expectationValidationResultSchema.load(test['output'])['result']['observed_value']}"
                else:
                    assert result == expectationValidationResultSchema.load(
                        test["output"]
                    ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
            else:
                assert result == expectationValidationResultSchema.load(
                    test["output"]
                ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
        else:
            # Convert result to json since our tests are reading from json so cannot easily contain richer types (e.g. NaN)
            # NOTE - 20191031 - JPC - we may eventually want to change these tests as we update our view on how
            # representations, serializations, and objects should interact and how much of that is shown to the user.
            result = result.to_json_dict()
            for key, value in test["output"].items():
                # Apply our great expectations-specific test logic

                if key == "success":
                    if isinstance(value, (np.floating, float)):
                        try:
                            assert np.allclose(
                                result["success"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['success']} not np.allclose to {value}"
                        except TypeError:
                            assert (
                                result["success"] == value
                            ), f"{result['success']} != {value}"
                    else:
                        assert result["success"] == value, f"{result['success']} != {value}"

                elif key == "observed_value":
                    if "tolerance" in test:
                        if isinstance(value, dict):
                            assert set(result["result"]["observed_value"].keys()) == set(
                                value.keys()
                            ), f"{set(result['result']['observed_value'].keys())} != {set(value.keys())}"
                            for k, v in value.items():
                                assert np.allclose(
                                    result["result"]["observed_value"][k],
                                    v,
                                    rtol=test["tolerance"],
                                )
                        else:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=test["tolerance"],
                            )
                    else:
                        if isinstance(value, dict) and "values" in value:
                            try:
                                assert np.allclose(
                                    result["result"]["observed_value"]["values"],
                                    value["values"],
                                    rtol=RTOL,
                                    atol=ATOL,
                                ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']['values']} not np.allclose to {value['values']}"
                            except TypeError as e:
                                print(e)
                                assert (
                                    result["result"]["observed_value"] == value
                                ), f"{result['result']['observed_value']} != {value}"
                        elif try_allclose:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {value}"
                        else:
                            assert (
                                result["result"]["observed_value"] == value
                            ), f"{result['result']['observed_value']} != {value}"

                # NOTE: This is a key used ONLY for testing cases where an expectation is legitimately allowed to return
                # any of multiple possible observed_values. expect_column_values_to_be_of_type is one such expectation.
                elif key == "observed_value_list":
                    assert result["result"]["observed_value"] in value

                elif key == "unexpected_index_list":
                    if isinstance(data_asset, (SqlAlchemyDataset, SparkDFDataset)):
                        pass
                    elif isinstance(data_asset, (SqlAlchemyBatchData, SparkDFBatchData)):
                        pass
                    else:
>                       assert (
                            result["result"]["unexpected_index_list"] == value
                        ), f"{result['result']['unexpected_index_list']} != {value}"
E                       AssertionError: [{'x': 9, 'pk_index': 8}] != [8]

great_expectations/self_check/util.py:3075: AssertionError
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 724.40it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 314.27it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 455.24it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 300.22it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 365.58it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 303.21it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 422.58it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 401.58it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 400.26it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 398.68it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_match_regex:negative_test_insufficient_mostly_and_one_non_matching_value] _

test_case = {'expectation_type': 'expect_column_values_to_match_regex', 'pk_column': True, 'skip': False, 'test': {'exact_match_ou...y': 0.9, 'regex': '^a'}, 'include_in_gallery': True, 'input': {'column': 'a', 'mostly': 0.9, 'regex': '^a'}, ...}, ...}

    @pytest.mark.order(index=0)
    @pytest.mark.integration
    @pytest.mark.slow  # 12.68s
    def test_case_runner_v3_api(test_case):
        if test_case["skip"]:
            pytest.skip()

        # Note: this should never be done in practice, but we are wiping expectations to reuse batches during testing.
        # test_case["batch"]._initialize_expectations()
        if "parse_strings_as_datetimes" in test_case["test"]["in"]:
            with pytest.deprecated_call():
                evaluate_json_test_v3_api(
                    validator=test_case["validator_with_data"],
                    expectation_type=test_case["expectation_type"],
                    test=test_case["test"],
                    pk_column=test_case["pk_column"],
                )
        else:
>           evaluate_json_test_v3_api(
                validator=test_case["validator_with_data"],
                expectation_type=test_case["expectation_type"],
                test=test_case["test"],
                pk_column=test_case["pk_column"],
            )

tests/test_definitions/test_expectations_v3_api.py:418:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
great_expectations/self_check/util.py:2901: in evaluate_json_test_v3_api
    check_json_test_result(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

test = {'exact_match_out': False, 'in': {'column': 'a', 'mostly': 0.9, 'regex': '^a'}, 'include_in_gallery': True, 'input': {'column': 'a', 'mostly': 0.9, 'regex': '^a'}, ...}
result = {'exception_info': {'exception_message': None, 'exception_traceback': None, 'raised_exception': False}, 'expectation_c... 5, 'missing_count': 0, 'missing_percent': 0.0, 'partial_unexpected_counts': [{'count': 1, 'value': 'bee'}], ...}, ...}
data_asset = <great_expectations.execution_engine.pandas_batch_data.PandasBatchData object at 0x7f9502945e20>
pk_column = True

    def check_json_test_result(  # noqa: C901 - 52
        test, result, data_asset=None, pk_column=False
    ) -> None:

        # check for id_pk results in cases where pk_column is true and unexpected_index_list already exists
        # this will work for testing since result_format is COMPLETE
        if pk_column:
            if not result["success"]:
                if "unexpected_index_list" in result["result"]:
                    assert "unexpected_index_query" in result["result"]

        if "unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )
            elif "unexpected_list" in test["output"]:
                (
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )

        if "partial_unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "partial_unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )
            elif "partial_unexpected_list" in test["output"]:
                (
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )

        # Determine if np.allclose(..) might be needed for float comparison
        try_allclose = False
        if "observed_value" in test["output"]:
            if RX_FLOAT.match(repr(test["output"]["observed_value"])):
                try_allclose = True

        # Check results
        if test["exact_match_out"] is True:
            if "result" in result and "observed_value" in result["result"]:
                if isinstance(result["result"]["observed_value"], (np.floating, float)):
                    assert np.allclose(
                        result["result"]["observed_value"],
                        expectationValidationResultSchema.load(test["output"])["result"][
                            "observed_value"
                        ],
                        rtol=RTOL,
                        atol=ATOL,
                    ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {expectationValidationResultSchema.load(test['output'])['result']['observed_value']}"
                else:
                    assert result == expectationValidationResultSchema.load(
                        test["output"]
                    ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
            else:
                assert result == expectationValidationResultSchema.load(
                    test["output"]
                ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
        else:
            # Convert result to json since our tests are reading from json so cannot easily contain richer types (e.g. NaN)
            # NOTE - 20191031 - JPC - we may eventually want to change these tests as we update our view on how
            # representations, serializations, and objects should interact and how much of that is shown to the user.
            result = result.to_json_dict()
            for key, value in test["output"].items():
                # Apply our great expectations-specific test logic

                if key == "success":
                    if isinstance(value, (np.floating, float)):
                        try:
                            assert np.allclose(
                                result["success"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['success']} not np.allclose to {value}"
                        except TypeError:
                            assert (
                                result["success"] == value
                            ), f"{result['success']} != {value}"
                    else:
                        assert result["success"] == value, f"{result['success']} != {value}"

                elif key == "observed_value":
                    if "tolerance" in test:
                        if isinstance(value, dict):
                            assert set(result["result"]["observed_value"].keys()) == set(
                                value.keys()
                            ), f"{set(result['result']['observed_value'].keys())} != {set(value.keys())}"
                            for k, v in value.items():
                                assert np.allclose(
                                    result["result"]["observed_value"][k],
                                    v,
                                    rtol=test["tolerance"],
                                )
                        else:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=test["tolerance"],
                            )
                    else:
                        if isinstance(value, dict) and "values" in value:
                            try:
                                assert np.allclose(
                                    result["result"]["observed_value"]["values"],
                                    value["values"],
                                    rtol=RTOL,
                                    atol=ATOL,
                                ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']['values']} not np.allclose to {value['values']}"
                            except TypeError as e:
                                print(e)
                                assert (
                                    result["result"]["observed_value"] == value
                                ), f"{result['result']['observed_value']} != {value}"
                        elif try_allclose:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {value}"
                        else:
                            assert (
                                result["result"]["observed_value"] == value
                            ), f"{result['result']['observed_value']} != {value}"

                # NOTE: This is a key used ONLY for testing cases where an expectation is legitimately allowed to return
                # any of multiple possible observed_values. expect_column_values_to_be_of_type is one such expectation.
                elif key == "observed_value_list":
                    assert result["result"]["observed_value"] in value

                elif key == "unexpected_index_list":
                    if isinstance(data_asset, (SqlAlchemyDataset, SparkDFDataset)):
                        pass
                    elif isinstance(data_asset, (SqlAlchemyBatchData, SparkDFBatchData)):
                        pass
                    else:
>                       assert (
                            result["result"]["unexpected_index_list"] == value
                        ), f"{result['result']['unexpected_index_list']} != {value}"
E                       AssertionError: [{'a': 'bee', 'pk_index': 4}] != [4]

great_expectations/self_check/util.py:3075: AssertionError
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 3938.31it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 447.54it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 635.34it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 352.66it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 484.63it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 373.56it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 581.43it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 540.45it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 538.10it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 535.33it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_match_regex:positive_test_column_name_has_space] _

test_case = {'expectation_type': 'expect_column_values_to_match_regex', 'pk_column': True, 'skip': False, 'test': {'exact_match_ou...out': {'success': True, 'unexpected_index_list': [{'a': 'bee', 'pk_index': 4}], 'unexpected_list': ['bee']}, ...}, ...}

    @pytest.mark.order(index=0)
    @pytest.mark.integration
    @pytest.mark.slow  # 12.68s
    def test_case_runner_v3_api(test_case):
        if test_case["skip"]:
            pytest.skip()

        # Note: this should never be done in practice, but we are wiping expectations to reuse batches during testing.
        # test_case["batch"]._initialize_expectations()
        if "parse_strings_as_datetimes" in test_case["test"]["in"]:
            with pytest.deprecated_call():
                evaluate_json_test_v3_api(
                    validator=test_case["validator_with_data"],
                    expectation_type=test_case["expectation_type"],
                    test=test_case["test"],
                    pk_column=test_case["pk_column"],
                )
        else:
>           evaluate_json_test_v3_api(
                validator=test_case["validator_with_data"],
                expectation_type=test_case["expectation_type"],
                test=test_case["test"],
                pk_column=test_case["pk_column"],
            )

tests/test_definitions/test_expectations_v3_api.py:418:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
great_expectations/self_check/util.py:2901: in evaluate_json_test_v3_api
    check_json_test_result(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

test = {'exact_match_out': False, 'in': {'column': 'column_name with space', 'mostly': 0.8, 'regex': '^a'}, 'input': {'column...a'}, 'out': {'success': True, 'unexpected_index_list': [{'a': 'bee', 'pk_index': 4}], 'unexpected_list': ['bee']}, ...}
result = {'exception_info': {'exception_message': None, 'exception_traceback': None, 'raised_exception': False}, 'expectation_c... 5, 'missing_count': 0, 'missing_percent': 0.0, 'partial_unexpected_counts': [{'count': 1, 'value': 'bee'}], ...}, ...}
data_asset = <great_expectations.execution_engine.pandas_batch_data.PandasBatchData object at 0x7f9502945e20>
pk_column = True

    def check_json_test_result(  # noqa: C901 - 52
        test, result, data_asset=None, pk_column=False
    ) -> None:

        # check for id_pk results in cases where pk_column is true and unexpected_index_list already exists
        # this will work for testing since result_format is COMPLETE
        if pk_column:
            if not result["success"]:
                if "unexpected_index_list" in result["result"]:
                    assert "unexpected_index_query" in result["result"]

        if "unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )
            elif "unexpected_list" in test["output"]:
                (
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )

        if "partial_unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "partial_unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )
            elif "partial_unexpected_list" in test["output"]:
                (
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )

        # Determine if np.allclose(..) might be needed for float comparison
        try_allclose = False
        if "observed_value" in test["output"]:
            if RX_FLOAT.match(repr(test["output"]["observed_value"])):
                try_allclose = True

        # Check results
        if test["exact_match_out"] is True:
            if "result" in result and "observed_value" in result["result"]:
                if isinstance(result["result"]["observed_value"], (np.floating, float)):
                    assert np.allclose(
                        result["result"]["observed_value"],
                        expectationValidationResultSchema.load(test["output"])["result"][
                            "observed_value"
                        ],
                        rtol=RTOL,
                        atol=ATOL,
                    ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {expectationValidationResultSchema.load(test['output'])['result']['observed_value']}"
                else:
                    assert result == expectationValidationResultSchema.load(
                        test["output"]
                    ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
            else:
                assert result == expectationValidationResultSchema.load(
                    test["output"]
                ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
        else:
            # Convert result to json since our tests are reading from json so cannot easily contain richer types (e.g. NaN)
            # NOTE - 20191031 - JPC - we may eventually want to change these tests as we update our view on how
            # representations, serializations, and objects should interact and how much of that is shown to the user.
            result = result.to_json_dict()
            for key, value in test["output"].items():
                # Apply our great expectations-specific test logic

                if key == "success":
                    if isinstance(value, (np.floating, float)):
                        try:
                            assert np.allclose(
                                result["success"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['success']} not np.allclose to {value}"
                        except TypeError:
                            assert (
                                result["success"] == value
                            ), f"{result['success']} != {value}"
                    else:
                        assert result["success"] == value, f"{result['success']} != {value}"

                elif key == "observed_value":
                    if "tolerance" in test:
                        if isinstance(value, dict):
                            assert set(result["result"]["observed_value"].keys()) == set(
                                value.keys()
                            ), f"{set(result['result']['observed_value'].keys())} != {set(value.keys())}"
                            for k, v in value.items():
                                assert np.allclose(
                                    result["result"]["observed_value"][k],
                                    v,
                                    rtol=test["tolerance"],
                                )
                        else:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=test["tolerance"],
                            )
                    else:
                        if isinstance(value, dict) and "values" in value:
                            try:
                                assert np.allclose(
                                    result["result"]["observed_value"]["values"],
                                    value["values"],
                                    rtol=RTOL,
                                    atol=ATOL,
                                ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']['values']} not np.allclose to {value['values']}"
                            except TypeError as e:
                                print(e)
                                assert (
                                    result["result"]["observed_value"] == value
                                ), f"{result['result']['observed_value']} != {value}"
                        elif try_allclose:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {value}"
                        else:
                            assert (
                                result["result"]["observed_value"] == value
                            ), f"{result['result']['observed_value']} != {value}"

                # NOTE: This is a key used ONLY for testing cases where an expectation is legitimately allowed to return
                # any of multiple possible observed_values. expect_column_values_to_be_of_type is one such expectation.
                elif key == "observed_value_list":
                    assert result["result"]["observed_value"] in value

                elif key == "unexpected_index_list":
                    if isinstance(data_asset, (SqlAlchemyDataset, SparkDFDataset)):
                        pass
                    elif isinstance(data_asset, (SqlAlchemyBatchData, SparkDFBatchData)):
                        pass
                    else:
>                       assert (
                            result["result"]["unexpected_index_list"] == value
                        ), f"{result['result']['unexpected_index_list']} != {value}"
E                       AssertionError: [{'column_name with space': 'bee', 'pk_index': 4}] != [{'a': 'bee', 'pk_index': 4}]

great_expectations/self_check/util.py:3075: AssertionError
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 4464.40it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 452.68it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 639.93it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 352.98it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 490.57it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 375.11it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 585.99it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 543.59it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 541.21it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 538.35it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_between:basic_negative_test1] _

test_case = {'expectation_type': 'expect_column_values_to_be_between', 'pk_column': True, 'skip': False, 'test': {'exact_match_out...': 9, 'min_value': 1}, 'include_in_gallery': True, 'input': {'column': 'x', 'max_value': 9, 'min_value': 1}, ...}, ...}

    @pytest.mark.order(index=0)
    @pytest.mark.integration
    @pytest.mark.slow  # 12.68s
    def test_case_runner_v3_api(test_case):
        if test_case["skip"]:
            pytest.skip()

        # Note: this should never be done in practice, but we are wiping expectations to reuse batches during testing.
        # test_case["batch"]._initialize_expectations()
        if "parse_strings_as_datetimes" in test_case["test"]["in"]:
            with pytest.deprecated_call():
                evaluate_json_test_v3_api(
                    validator=test_case["validator_with_data"],
                    expectation_type=test_case["expectation_type"],
                    test=test_case["test"],
                    pk_column=test_case["pk_column"],
                )
        else:
>           evaluate_json_test_v3_api(
                validator=test_case["validator_with_data"],
                expectation_type=test_case["expectation_type"],
                test=test_case["test"],
                pk_column=test_case["pk_column"],
            )

tests/test_definitions/test_expectations_v3_api.py:418:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
great_expectations/self_check/util.py:2901: in evaluate_json_test_v3_api
    check_json_test_result(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

test = {'exact_match_out': False, 'in': {'column': 'x', 'max_value': 9, 'min_value': 1}, 'include_in_gallery': True, 'input': {'column': 'x', 'max_value': 9, 'min_value': 1}, ...}
result = {'exception_info': {'exception_message': None, 'exception_traceback': None, 'raised_exception': False}, 'expectation_c...': 10, 'missing_count': 0, 'missing_percent': 0.0, 'partial_unexpected_counts': [{'count': 1, 'value': 10}], ...}, ...}
data_asset = <great_expectations.execution_engine.pandas_batch_data.PandasBatchData object at 0x7f9502ac0190>
pk_column = True

    def check_json_test_result(  # noqa: C901 - 52
        test, result, data_asset=None, pk_column=False
    ) -> None:

        # check for id_pk results in cases where pk_column is true and unexpected_index_list already exists
        # this will work for testing since result_format is COMPLETE
        if pk_column:
            if not result["success"]:
                if "unexpected_index_list" in result["result"]:
                    assert "unexpected_index_query" in result["result"]

        if "unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )
            elif "unexpected_list" in test["output"]:
                (
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )

        if "partial_unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "partial_unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )
            elif "partial_unexpected_list" in test["output"]:
                (
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )

        # Determine if np.allclose(..) might be needed for float comparison
        try_allclose = False
        if "observed_value" in test["output"]:
            if RX_FLOAT.match(repr(test["output"]["observed_value"])):
                try_allclose = True

        # Check results
        if test["exact_match_out"] is True:
            if "result" in result and "observed_value" in result["result"]:
                if isinstance(result["result"]["observed_value"], (np.floating, float)):
                    assert np.allclose(
                        result["result"]["observed_value"],
                        expectationValidationResultSchema.load(test["output"])["result"][
                            "observed_value"
                        ],
                        rtol=RTOL,
                        atol=ATOL,
                    ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {expectationValidationResultSchema.load(test['output'])['result']['observed_value']}"
                else:
                    assert result == expectationValidationResultSchema.load(
                        test["output"]
                    ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
            else:
                assert result == expectationValidationResultSchema.load(
                    test["output"]
                ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
        else:
            # Convert result to json since our tests are reading from json so cannot easily contain richer types (e.g. NaN)
            # NOTE - 20191031 - JPC - we may eventually want to change these tests as we update our view on how
            # representations, serializations, and objects should interact and how much of that is shown to the user.
            result = result.to_json_dict()
            for key, value in test["output"].items():
                # Apply our great expectations-specific test logic

                if key == "success":
                    if isinstance(value, (np.floating, float)):
                        try:
                            assert np.allclose(
                                result["success"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['success']} not np.allclose to {value}"
                        except TypeError:
                            assert (
                                result["success"] == value
                            ), f"{result['success']} != {value}"
                    else:
                        assert result["success"] == value, f"{result['success']} != {value}"

                elif key == "observed_value":
                    if "tolerance" in test:
                        if isinstance(value, dict):
                            assert set(result["result"]["observed_value"].keys()) == set(
                                value.keys()
                            ), f"{set(result['result']['observed_value'].keys())} != {set(value.keys())}"
                            for k, v in value.items():
                                assert np.allclose(
                                    result["result"]["observed_value"][k],
                                    v,
                                    rtol=test["tolerance"],
                                )
                        else:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=test["tolerance"],
                            )
                    else:
                        if isinstance(value, dict) and "values" in value:
                            try:
                                assert np.allclose(
                                    result["result"]["observed_value"]["values"],
                                    value["values"],
                                    rtol=RTOL,
                                    atol=ATOL,
                                ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']['values']} not np.allclose to {value['values']}"
                            except TypeError as e:
                                print(e)
                                assert (
                                    result["result"]["observed_value"] == value
                                ), f"{result['result']['observed_value']} != {value}"
                        elif try_allclose:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {value}"
                        else:
                            assert (
                                result["result"]["observed_value"] == value
                            ), f"{result['result']['observed_value']} != {value}"

                # NOTE: This is a key used ONLY for testing cases where an expectation is legitimately allowed to return
                # any of multiple possible observed_values. expect_column_values_to_be_of_type is one such expectation.
                elif key == "observed_value_list":
                    assert result["result"]["observed_value"] in value

                elif key == "unexpected_index_list":
                    if isinstance(data_asset, (SqlAlchemyDataset, SparkDFDataset)):
                        pass
                    elif isinstance(data_asset, (SqlAlchemyBatchData, SparkDFBatchData)):
                        pass
                    else:
>                       assert (
                            result["result"]["unexpected_index_list"] == value
                        ), f"{result['result']['unexpected_index_list']} != {value}"
E                       AssertionError: [{'x': 10, 'pk_index': 9}] != [9]

great_expectations/self_check/util.py:3075: AssertionError
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 4156.89it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 377.85it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 540.90it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 297.56it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 420.77it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 318.43it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 492.88it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 456.37it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 454.66it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 452.67it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_between:another_negative_test1] _

test_case = {'expectation_type': 'expect_column_values_to_be_between', 'pk_column': True, 'skip': False, 'test': {'exact_match_out... 10, 'min_value': 3}, 'out': {'success': False, 'unexpected_index_list': [0, 1], 'unexpected_list': [1, 2]}, ...}, ...}

    @pytest.mark.order(index=0)
    @pytest.mark.integration
    @pytest.mark.slow  # 12.68s
    def test_case_runner_v3_api(test_case):
        if test_case["skip"]:
            pytest.skip()

        # Note: this should never be done in practice, but we are wiping expectations to reuse batches during testing.
        # test_case["batch"]._initialize_expectations()
        if "parse_strings_as_datetimes" in test_case["test"]["in"]:
            with pytest.deprecated_call():
                evaluate_json_test_v3_api(
                    validator=test_case["validator_with_data"],
                    expectation_type=test_case["expectation_type"],
                    test=test_case["test"],
                    pk_column=test_case["pk_column"],
                )
        else:
>           evaluate_json_test_v3_api(
                validator=test_case["validator_with_data"],
                expectation_type=test_case["expectation_type"],
                test=test_case["test"],
                pk_column=test_case["pk_column"],
            )

tests/test_definitions/test_expectations_v3_api.py:418:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
great_expectations/self_check/util.py:2901: in evaluate_json_test_v3_api
    check_json_test_result(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

test = {'exact_match_out': False, 'in': {'column': 'x', 'max_value': 10, 'min_value': 3}, 'input': {'column': 'x', 'max_value': 10, 'min_value': 3}, 'out': {'success': False, 'unexpected_index_list': [0, 1], 'unexpected_list': [1, 2]}, ...}
result = {'exception_info': {'exception_message': None, 'exception_traceback': None, 'raised_exception': False}, 'expectation_c..., 'missing_percent': 0.0, 'partial_unexpected_counts': [{'count': 1, 'value': 1}, {'count': 1, 'value': 2}], ...}, ...}
data_asset = <great_expectations.execution_engine.pandas_batch_data.PandasBatchData object at 0x7f9502ac0190>
pk_column = True

    def check_json_test_result(  # noqa: C901 - 52
        test, result, data_asset=None, pk_column=False
    ) -> None:

        # check for id_pk results in cases where pk_column is true and unexpected_index_list already exists
        # this will work for testing since result_format is COMPLETE
        if pk_column:
            if not result["success"]:
                if "unexpected_index_list" in result["result"]:
                    assert "unexpected_index_query" in result["result"]

        if "unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )
            elif "unexpected_list" in test["output"]:
                (
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )

        if "partial_unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "partial_unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )
            elif "partial_unexpected_list" in test["output"]:
                (
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )

        # Determine if np.allclose(..) might be needed for float comparison
        try_allclose = False
        if "observed_value" in test["output"]:
            if RX_FLOAT.match(repr(test["output"]["observed_value"])):
                try_allclose = True

        # Check results
        if test["exact_match_out"] is True:
            if "result" in result and "observed_value" in result["result"]:
                if isinstance(result["result"]["observed_value"], (np.floating, float)):
                    assert np.allclose(
                        result["result"]["observed_value"],
                        expectationValidationResultSchema.load(test["output"])["result"][
                            "observed_value"
                        ],
                        rtol=RTOL,
                        atol=ATOL,
                    ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {expectationValidationResultSchema.load(test['output'])['result']['observed_value']}"
                else:
                    assert result == expectationValidationResultSchema.load(
                        test["output"]
                    ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
            else:
                assert result == expectationValidationResultSchema.load(
                    test["output"]
                ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
        else:
            # Convert result to json since our tests are reading from json so cannot easily contain richer types (e.g. NaN)
            # NOTE - 20191031 - JPC - we may eventually want to change these tests as we update our view on how
            # representations, serializations, and objects should interact and how much of that is shown to the user.
            result = result.to_json_dict()
            for key, value in test["output"].items():
                # Apply our great expectations-specific test logic

                if key == "success":
                    if isinstance(value, (np.floating, float)):
                        try:
                            assert np.allclose(
                                result["success"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['success']} not np.allclose to {value}"
                        except TypeError:
                            assert (
                                result["success"] == value
                            ), f"{result['success']} != {value}"
                    else:
                        assert result["success"] == value, f"{result['success']} != {value}"

                elif key == "observed_value":
                    if "tolerance" in test:
                        if isinstance(value, dict):
                            assert set(result["result"]["observed_value"].keys()) == set(
                                value.keys()
                            ), f"{set(result['result']['observed_value'].keys())} != {set(value.keys())}"
                            for k, v in value.items():
                                assert np.allclose(
                                    result["result"]["observed_value"][k],
                                    v,
                                    rtol=test["tolerance"],
                                )
                        else:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=test["tolerance"],
                            )
                    else:
                        if isinstance(value, dict) and "values" in value:
                            try:
                                assert np.allclose(
                                    result["result"]["observed_value"]["values"],
                                    value["values"],
                                    rtol=RTOL,
                                    atol=ATOL,
                                ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']['values']} not np.allclose to {value['values']}"
                            except TypeError as e:
                                print(e)
                                assert (
                                    result["result"]["observed_value"] == value
                                ), f"{result['result']['observed_value']} != {value}"
                        elif try_allclose:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {value}"
                        else:
                            assert (
                                result["result"]["observed_value"] == value
                            ), f"{result['result']['observed_value']} != {value}"

                # NOTE: This is a key used ONLY for testing cases where an expectation is legitimately allowed to return
                # any of multiple possible observed_values. expect_column_values_to_be_of_type is one such expectation.
                elif key == "observed_value_list":
                    assert result["result"]["observed_value"] in value

                elif key == "unexpected_index_list":
                    if isinstance(data_asset, (SqlAlchemyDataset, SparkDFDataset)):
                        pass
                    elif isinstance(data_asset, (SqlAlchemyBatchData, SparkDFBatchData)):
                        pass
                    else:
>                       assert (
                            result["result"]["unexpected_index_list"] == value
                        ), f"{result['result']['unexpected_index_list']} != {value}"
E                       AssertionError: [{'x': 1, 'pk_index': 0}, {'x': 2, 'pk_index': 1}] != [0, 1]

great_expectations/self_check/util.py:3075: AssertionError
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 3985.09it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 377.93it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 539.97it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 295.78it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 417.29it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 317.22it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 493.24it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 457.33it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 455.70it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 453.76it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_between:basic_negative_test_with_timestamps1] _

test_case = {'expectation_type': 'expect_column_values_to_be_between', 'pk_column': True, 'skip': False, 'test': {'exact_match_out... 12:00:00', 'parse_strings_as_datetimes': True}, 'out': {'success': False, 'unexpected_index_list': [0, 9]}, ...}, ...}

    @pytest.mark.order(index=0)
    @pytest.mark.integration
    @pytest.mark.slow  # 12.68s
    def test_case_runner_v3_api(test_case):
        if test_case["skip"]:
            pytest.skip()

        # Note: this should never be done in practice, but we are wiping expectations to reuse batches during testing.
        # test_case["batch"]._initialize_expectations()
        if "parse_strings_as_datetimes" in test_case["test"]["in"]:
            with pytest.deprecated_call():
>               evaluate_json_test_v3_api(
                    validator=test_case["validator_with_data"],
                    expectation_type=test_case["expectation_type"],
                    test=test_case["test"],
                    pk_column=test_case["pk_column"],
                )

tests/test_definitions/test_expectations_v3_api.py:411:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
great_expectations/self_check/util.py:2901: in evaluate_json_test_v3_api
    check_json_test_result(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

test = {'exact_match_out': False, 'in': {'column': 'ts', 'max_value': 'Jan 01 2001 12:00:00', 'min_value': 'Jan 01 1990 12:00...1 1990 12:00:00', 'parse_strings_as_datetimes': True}, 'out': {'success': False, 'unexpected_index_list': [0, 9]}, ...}
result = {'exception_info': {'exception_message': None, 'exception_traceback': None, 'raised_exception': False}, 'expectation_c...ected_counts': [{'count': 1, 'value': '1970-01-01T12:00:01'}, {'count': 1, 'value': '2001-01-01T12:00:01'}], ...}, ...}
data_asset = <great_expectations.execution_engine.pandas_batch_data.PandasBatchData object at 0x7f9502ac0190>
pk_column = True

    def check_json_test_result(  # noqa: C901 - 52
        test, result, data_asset=None, pk_column=False
    ) -> None:

        # check for id_pk results in cases where pk_column is true and unexpected_index_list already exists
        # this will work for testing since result_format is COMPLETE
        if pk_column:
            if not result["success"]:
                if "unexpected_index_list" in result["result"]:
                    assert "unexpected_index_query" in result["result"]

        if "unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )
            elif "unexpected_list" in test["output"]:
                (
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )

        if "partial_unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "partial_unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )
            elif "partial_unexpected_list" in test["output"]:
                (
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )

        # Determine if np.allclose(..) might be needed for float comparison
        try_allclose = False
        if "observed_value" in test["output"]:
            if RX_FLOAT.match(repr(test["output"]["observed_value"])):
                try_allclose = True

        # Check results
        if test["exact_match_out"] is True:
            if "result" in result and "observed_value" in result["result"]:
                if isinstance(result["result"]["observed_value"], (np.floating, float)):
                    assert np.allclose(
                        result["result"]["observed_value"],
                        expectationValidationResultSchema.load(test["output"])["result"][
                            "observed_value"
                        ],
                        rtol=RTOL,
                        atol=ATOL,
                    ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {expectationValidationResultSchema.load(test['output'])['result']['observed_value']}"
                else:
                    assert result == expectationValidationResultSchema.load(
                        test["output"]
                    ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
            else:
                assert result == expectationValidationResultSchema.load(
                    test["output"]
                ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
        else:
            # Convert result to json since our tests are reading from json so cannot easily contain richer types (e.g. NaN)
            # NOTE - 20191031 - JPC - we may eventually want to change these tests as we update our view on how
            # representations, serializations, and objects should interact and how much of that is shown to the user.
            result = result.to_json_dict()
            for key, value in test["output"].items():
                # Apply our great expectations-specific test logic

                if key == "success":
                    if isinstance(value, (np.floating, float)):
                        try:
                            assert np.allclose(
                                result["success"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['success']} not np.allclose to {value}"
                        except TypeError:
                            assert (
                                result["success"] == value
                            ), f"{result['success']} != {value}"
                    else:
                        assert result["success"] == value, f"{result['success']} != {value}"

                elif key == "observed_value":
                    if "tolerance" in test:
                        if isinstance(value, dict):
                            assert set(result["result"]["observed_value"].keys()) == set(
                                value.keys()
                            ), f"{set(result['result']['observed_value'].keys())} != {set(value.keys())}"
                            for k, v in value.items():
                                assert np.allclose(
                                    result["result"]["observed_value"][k],
                                    v,
                                    rtol=test["tolerance"],
                                )
                        else:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=test["tolerance"],
                            )
                    else:
                        if isinstance(value, dict) and "values" in value:
                            try:
                                assert np.allclose(
                                    result["result"]["observed_value"]["values"],
                                    value["values"],
                                    rtol=RTOL,
                                    atol=ATOL,
                                ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']['values']} not np.allclose to {value['values']}"
                            except TypeError as e:
                                print(e)
                                assert (
                                    result["result"]["observed_value"] == value
                                ), f"{result['result']['observed_value']} != {value}"
                        elif try_allclose:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {value}"
                        else:
                            assert (
                                result["result"]["observed_value"] == value
                            ), f"{result['result']['observed_value']} != {value}"

                # NOTE: This is a key used ONLY for testing cases where an expectation is legitimately allowed to return
                # any of multiple possible observed_values. expect_column_values_to_be_of_type is one such expectation.
                elif key == "observed_value_list":
                    assert result["result"]["observed_value"] in value

                elif key == "unexpected_index_list":
                    if isinstance(data_asset, (SqlAlchemyDataset, SparkDFDataset)):
                        pass
                    elif isinstance(data_asset, (SqlAlchemyBatchData, SparkDFBatchData)):
                        pass
                    else:
>                       assert (
                            result["result"]["unexpected_index_list"] == value
                        ), f"{result['result']['unexpected_index_list']} != {value}"
E                       AssertionError: [{'ts': '1970-01-01T12:00:01', 'pk_index': 0}, {'ts': '2001-01-01T12:00:01', 'pk_index': 9}] != [0, 9]

great_expectations/self_check/util.py:3075: AssertionError
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 3977.53it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 373.47it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 534.76it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 290.33it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 397.23it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 303.97it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 474.79it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 441.46it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 439.85it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 437.98it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_not_be_in_set:negative_test_case_include_one_existing_column_value] _

test_case = {'expectation_type': 'expect_column_values_to_not_be_in_set', 'pk_column': True, 'skip': False, 'test': {'exact_match_...: {'column': 'x', 'value_set': [1]}, 'include_in_gallery': True, 'input': {'column': 'x', 'value_set': [1]}, ...}, ...}

    @pytest.mark.order(index=0)
    @pytest.mark.integration
    @pytest.mark.slow  # 12.68s
    def test_case_runner_v3_api(test_case):
        if test_case["skip"]:
            pytest.skip()

        # Note: this should never be done in practice, but we are wiping expectations to reuse batches during testing.
        # test_case["batch"]._initialize_expectations()
        if "parse_strings_as_datetimes" in test_case["test"]["in"]:
            with pytest.deprecated_call():
                evaluate_json_test_v3_api(
                    validator=test_case["validator_with_data"],
                    expectation_type=test_case["expectation_type"],
                    test=test_case["test"],
                    pk_column=test_case["pk_column"],
                )
        else:
>           evaluate_json_test_v3_api(
                validator=test_case["validator_with_data"],
                expectation_type=test_case["expectation_type"],
                test=test_case["test"],
                pk_column=test_case["pk_column"],
            )

tests/test_definitions/test_expectations_v3_api.py:418:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
great_expectations/self_check/util.py:2901: in evaluate_json_test_v3_api
    check_json_test_result(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

test = {'exact_match_out': False, 'in': {'column': 'x', 'value_set': [1]}, 'include_in_gallery': True, 'input': {'column': 'x', 'value_set': [1]}, ...}
result = {'exception_info': {'exception_message': None, 'exception_traceback': None, 'raised_exception': False}, 'expectation_c...nt': 3, 'missing_count': 0, 'missing_percent': 0.0, 'partial_unexpected_counts': [{'count': 1, 'value': 1}], ...}, ...}
data_asset = <great_expectations.execution_engine.pandas_batch_data.PandasBatchData object at 0x7f9502a5a460>
pk_column = True

    def check_json_test_result(  # noqa: C901 - 52
        test, result, data_asset=None, pk_column=False
    ) -> None:

        # check for id_pk results in cases where pk_column is true and unexpected_index_list already exists
        # this will work for testing since result_format is COMPLETE
        if pk_column:
            if not result["success"]:
                if "unexpected_index_list" in result["result"]:
                    assert "unexpected_index_query" in result["result"]

        if "unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )
            elif "unexpected_list" in test["output"]:
                (
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )

        if "partial_unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "partial_unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )
            elif "partial_unexpected_list" in test["output"]:
                (
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )

        # Determine if np.allclose(..) might be needed for float comparison
        try_allclose = False
        if "observed_value" in test["output"]:
            if RX_FLOAT.match(repr(test["output"]["observed_value"])):
                try_allclose = True

        # Check results
        if test["exact_match_out"] is True:
            if "result" in result and "observed_value" in result["result"]:
                if isinstance(result["result"]["observed_value"], (np.floating, float)):
                    assert np.allclose(
                        result["result"]["observed_value"],
                        expectationValidationResultSchema.load(test["output"])["result"][
                            "observed_value"
                        ],
                        rtol=RTOL,
                        atol=ATOL,
                    ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {expectationValidationResultSchema.load(test['output'])['result']['observed_value']}"
                else:
                    assert result == expectationValidationResultSchema.load(
                        test["output"]
                    ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
            else:
                assert result == expectationValidationResultSchema.load(
                    test["output"]
                ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
        else:
            # Convert result to json since our tests are reading from json so cannot easily contain richer types (e.g. NaN)
            # NOTE - 20191031 - JPC - we may eventually want to change these tests as we update our view on how
            # representations, serializations, and objects should interact and how much of that is shown to the user.
            result = result.to_json_dict()
            for key, value in test["output"].items():
                # Apply our great expectations-specific test logic

                if key == "success":
                    if isinstance(value, (np.floating, float)):
                        try:
                            assert np.allclose(
                                result["success"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['success']} not np.allclose to {value}"
                        except TypeError:
                            assert (
                                result["success"] == value
                            ), f"{result['success']} != {value}"
                    else:
                        assert result["success"] == value, f"{result['success']} != {value}"

                elif key == "observed_value":
                    if "tolerance" in test:
                        if isinstance(value, dict):
                            assert set(result["result"]["observed_value"].keys()) == set(
                                value.keys()
                            ), f"{set(result['result']['observed_value'].keys())} != {set(value.keys())}"
                            for k, v in value.items():
                                assert np.allclose(
                                    result["result"]["observed_value"][k],
                                    v,
                                    rtol=test["tolerance"],
                                )
                        else:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=test["tolerance"],
                            )
                    else:
                        if isinstance(value, dict) and "values" in value:
                            try:
                                assert np.allclose(
                                    result["result"]["observed_value"]["values"],
                                    value["values"],
                                    rtol=RTOL,
                                    atol=ATOL,
                                ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']['values']} not np.allclose to {value['values']}"
                            except TypeError as e:
                                print(e)
                                assert (
                                    result["result"]["observed_value"] == value
                                ), f"{result['result']['observed_value']} != {value}"
                        elif try_allclose:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {value}"
                        else:
                            assert (
                                result["result"]["observed_value"] == value
                            ), f"{result['result']['observed_value']} != {value}"

                # NOTE: This is a key used ONLY for testing cases where an expectation is legitimately allowed to return
                # any of multiple possible observed_values. expect_column_values_to_be_of_type is one such expectation.
                elif key == "observed_value_list":
                    assert result["result"]["observed_value"] in value

                elif key == "unexpected_index_list":
                    if isinstance(data_asset, (SqlAlchemyDataset, SparkDFDataset)):
                        pass
                    elif isinstance(data_asset, (SqlAlchemyBatchData, SparkDFBatchData)):
                        pass
                    else:
>                       assert (
                            result["result"]["unexpected_index_list"] == value
                        ), f"{result['result']['unexpected_index_list']} != {value}"
E                       AssertionError: [{'x': 1, 'pk_index': 0}] != [0]

great_expectations/self_check/util.py:3075: AssertionError
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 4228.13it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 433.47it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 617.41it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 340.52it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 485.81it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 370.13it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 574.74it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 533.14it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 530.90it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 528.20it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_not_be_in_set:basic_negative_strings_set_all_character_values] _

test_case = {'expectation_type': 'expect_column_values_to_not_be_in_set', 'pk_column': True, 'skip': False, 'test': {'exact_match_...ut': {'success': False, 'unexpected_index_list': [0, 1, 2], 'unexpected_list': ['hello', 'jello', 'mello']}, ...}, ...}

    @pytest.mark.order(index=0)
    @pytest.mark.integration
    @pytest.mark.slow  # 12.68s
    def test_case_runner_v3_api(test_case):
        if test_case["skip"]:
            pytest.skip()

        # Note: this should never be done in practice, but we are wiping expectations to reuse batches during testing.
        # test_case["batch"]._initialize_expectations()
        if "parse_strings_as_datetimes" in test_case["test"]["in"]:
            with pytest.deprecated_call():
                evaluate_json_test_v3_api(
                    validator=test_case["validator_with_data"],
                    expectation_type=test_case["expectation_type"],
                    test=test_case["test"],
                    pk_column=test_case["pk_column"],
                )
        else:
>           evaluate_json_test_v3_api(
                validator=test_case["validator_with_data"],
                expectation_type=test_case["expectation_type"],
                test=test_case["test"],
                pk_column=test_case["pk_column"],
            )

tests/test_definitions/test_expectations_v3_api.py:418:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
great_expectations/self_check/util.py:2901: in evaluate_json_test_v3_api
    check_json_test_result(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

test = {'exact_match_out': False, 'in': {'column': 'z', 'value_set': ['hello', 'jello', 'mello']}, 'input': {'column': 'z', '...]}, 'out': {'success': False, 'unexpected_index_list': [0, 1, 2], 'unexpected_list': ['hello', 'jello', 'mello']}, ...}
result = {'exception_info': {'exception_message': None, 'exception_traceback': None, 'raised_exception': False}, 'expectation_c...d_counts': [{'count': 1, 'value': 'hello'}, {'count': 1, 'value': 'jello'}, {'count': 1, 'value': 'mello'}], ...}, ...}
data_asset = <great_expectations.execution_engine.pandas_batch_data.PandasBatchData object at 0x7f9502a5a460>
pk_column = True

    def check_json_test_result(  # noqa: C901 - 52
        test, result, data_asset=None, pk_column=False
    ) -> None:

        # check for id_pk results in cases where pk_column is true and unexpected_index_list already exists
        # this will work for testing since result_format is COMPLETE
        if pk_column:
            if not result["success"]:
                if "unexpected_index_list" in result["result"]:
                    assert "unexpected_index_query" in result["result"]

        if "unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )
            elif "unexpected_list" in test["output"]:
                (
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )

        if "partial_unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "partial_unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )
            elif "partial_unexpected_list" in test["output"]:
                (
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )

        # Determine if np.allclose(..) might be needed for float comparison
        try_allclose = False
        if "observed_value" in test["output"]:
            if RX_FLOAT.match(repr(test["output"]["observed_value"])):
                try_allclose = True

        # Check results
        if test["exact_match_out"] is True:
            if "result" in result and "observed_value" in result["result"]:
                if isinstance(result["result"]["observed_value"], (np.floating, float)):
                    assert np.allclose(
                        result["result"]["observed_value"],
                        expectationValidationResultSchema.load(test["output"])["result"][
                            "observed_value"
                        ],
                        rtol=RTOL,
                        atol=ATOL,
                    ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {expectationValidationResultSchema.load(test['output'])['result']['observed_value']}"
                else:
                    assert result == expectationValidationResultSchema.load(
                        test["output"]
                    ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
            else:
                assert result == expectationValidationResultSchema.load(
                    test["output"]
                ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
        else:
            # Convert result to json since our tests are reading from json so cannot easily contain richer types (e.g. NaN)
            # NOTE - 20191031 - JPC - we may eventually want to change these tests as we update our view on how
            # representations, serializations, and objects should interact and how much of that is shown to the user.
            result = result.to_json_dict()
            for key, value in test["output"].items():
                # Apply our great expectations-specific test logic

                if key == "success":
                    if isinstance(value, (np.floating, float)):
                        try:
                            assert np.allclose(
                                result["success"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['success']} not np.allclose to {value}"
                        except TypeError:
                            assert (
                                result["success"] == value
                            ), f"{result['success']} != {value}"
                    else:
                        assert result["success"] == value, f"{result['success']} != {value}"

                elif key == "observed_value":
                    if "tolerance" in test:
                        if isinstance(value, dict):
                            assert set(result["result"]["observed_value"].keys()) == set(
                                value.keys()
                            ), f"{set(result['result']['observed_value'].keys())} != {set(value.keys())}"
                            for k, v in value.items():
                                assert np.allclose(
                                    result["result"]["observed_value"][k],
                                    v,
                                    rtol=test["tolerance"],
                                )
                        else:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=test["tolerance"],
                            )
                    else:
                        if isinstance(value, dict) and "values" in value:
                            try:
                                assert np.allclose(
                                    result["result"]["observed_value"]["values"],
                                    value["values"],
                                    rtol=RTOL,
                                    atol=ATOL,
                                ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']['values']} not np.allclose to {value['values']}"
                            except TypeError as e:
                                print(e)
                                assert (
                                    result["result"]["observed_value"] == value
                                ), f"{result['result']['observed_value']} != {value}"
                        elif try_allclose:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {value}"
                        else:
                            assert (
                                result["result"]["observed_value"] == value
                            ), f"{result['result']['observed_value']} != {value}"

                # NOTE: This is a key used ONLY for testing cases where an expectation is legitimately allowed to return
                # any of multiple possible observed_values. expect_column_values_to_be_of_type is one such expectation.
                elif key == "observed_value_list":
                    assert result["result"]["observed_value"] in value

                elif key == "unexpected_index_list":
                    if isinstance(data_asset, (SqlAlchemyDataset, SparkDFDataset)):
                        pass
                    elif isinstance(data_asset, (SqlAlchemyBatchData, SparkDFBatchData)):
                        pass
                    else:
>                       assert (
                            result["result"]["unexpected_index_list"] == value
                        ), f"{result['result']['unexpected_index_list']} != {value}"
E                       AssertionError: [{'z': 'hello', 'pk_index': 0}, {'z': 'jello', 'pk_index': 1}, {'z': 'mello', 'pk_index': 2}] != [0, 1, 2]

great_expectations/self_check/util.py:3075: AssertionError
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 4245.25it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 424.07it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 600.70it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 336.69it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 472.45it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 362.79it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 569.11it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 527.57it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 525.37it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 522.79it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_not_be_in_set:negative_test_float_set_two_out_of_three_column_values_included_mostly] _

test_case = {'expectation_type': 'expect_column_values_to_not_be_in_set', 'pk_column': True, 'skip': False, 'test': {'exact_match_...et': [1.1, 2.2]}, 'out': {'success': False, 'unexpected_index_list': [0, 1], 'unexpected_list': [1.1, 2.2]}, ...}, ...}

    @pytest.mark.order(index=0)
    @pytest.mark.integration
    @pytest.mark.slow  # 12.68s
    def test_case_runner_v3_api(test_case):
        if test_case["skip"]:
            pytest.skip()

        # Note: this should never be done in practice, but we are wiping expectations to reuse batches during testing.
        # test_case["batch"]._initialize_expectations()
        if "parse_strings_as_datetimes" in test_case["test"]["in"]:
            with pytest.deprecated_call():
                evaluate_json_test_v3_api(
                    validator=test_case["validator_with_data"],
                    expectation_type=test_case["expectation_type"],
                    test=test_case["test"],
                    pk_column=test_case["pk_column"],
                )
        else:
>           evaluate_json_test_v3_api(
                validator=test_case["validator_with_data"],
                expectation_type=test_case["expectation_type"],
                test=test_case["test"],
                pk_column=test_case["pk_column"],
            )

tests/test_definitions/test_expectations_v3_api.py:418:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
great_expectations/self_check/util.py:2901: in evaluate_json_test_v3_api
    check_json_test_result(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

test = {'exact_match_out': False, 'in': {'column': 'y', 'mostly': 0.65, 'value_set': [1.1, 2.2]}, 'input': {'column': 'y', 'm...alue_set': [1.1, 2.2]}, 'out': {'success': False, 'unexpected_index_list': [0, 1], 'unexpected_list': [1.1, 2.2]}, ...}
result = {'exception_info': {'exception_message': None, 'exception_traceback': None, 'raised_exception': False}, 'expectation_c...issing_percent': 0.0, 'partial_unexpected_counts': [{'count': 1, 'value': 1.1}, {'count': 1, 'value': 2.2}], ...}, ...}
data_asset = <great_expectations.execution_engine.pandas_batch_data.PandasBatchData object at 0x7f9502a5a460>
pk_column = True

    def check_json_test_result(  # noqa: C901 - 52
        test, result, data_asset=None, pk_column=False
    ) -> None:

        # check for id_pk results in cases where pk_column is true and unexpected_index_list already exists
        # this will work for testing since result_format is COMPLETE
        if pk_column:
            if not result["success"]:
                if "unexpected_index_list" in result["result"]:
                    assert "unexpected_index_query" in result["result"]

        if "unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )
            elif "unexpected_list" in test["output"]:
                (
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )

        if "partial_unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "partial_unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )
            elif "partial_unexpected_list" in test["output"]:
                (
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )

        # Determine if np.allclose(..) might be needed for float comparison
        try_allclose = False
        if "observed_value" in test["output"]:
            if RX_FLOAT.match(repr(test["output"]["observed_value"])):
                try_allclose = True

        # Check results
        if test["exact_match_out"] is True:
            if "result" in result and "observed_value" in result["result"]:
                if isinstance(result["result"]["observed_value"], (np.floating, float)):
                    assert np.allclose(
                        result["result"]["observed_value"],
                        expectationValidationResultSchema.load(test["output"])["result"][
                            "observed_value"
                        ],
                        rtol=RTOL,
                        atol=ATOL,
                    ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {expectationValidationResultSchema.load(test['output'])['result']['observed_value']}"
                else:
                    assert result == expectationValidationResultSchema.load(
                        test["output"]
                    ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
            else:
                assert result == expectationValidationResultSchema.load(
                    test["output"]
                ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
        else:
            # Convert result to json since our tests are reading from json so cannot easily contain richer types (e.g. NaN)
            # NOTE - 20191031 - JPC - we may eventually want to change these tests as we update our view on how
            # representations, serializations, and objects should interact and how much of that is shown to the user.
            result = result.to_json_dict()
            for key, value in test["output"].items():
                # Apply our great expectations-specific test logic

                if key == "success":
                    if isinstance(value, (np.floating, float)):
                        try:
                            assert np.allclose(
                                result["success"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['success']} not np.allclose to {value}"
                        except TypeError:
                            assert (
                                result["success"] == value
                            ), f"{result['success']} != {value}"
                    else:
                        assert result["success"] == value, f"{result['success']} != {value}"

                elif key == "observed_value":
                    if "tolerance" in test:
                        if isinstance(value, dict):
                            assert set(result["result"]["observed_value"].keys()) == set(
                                value.keys()
                            ), f"{set(result['result']['observed_value'].keys())} != {set(value.keys())}"
                            for k, v in value.items():
                                assert np.allclose(
                                    result["result"]["observed_value"][k],
                                    v,
                                    rtol=test["tolerance"],
                                )
                        else:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=test["tolerance"],
                            )
                    else:
                        if isinstance(value, dict) and "values" in value:
                            try:
                                assert np.allclose(
                                    result["result"]["observed_value"]["values"],
                                    value["values"],
                                    rtol=RTOL,
                                    atol=ATOL,
                                ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']['values']} not np.allclose to {value['values']}"
                            except TypeError as e:
                                print(e)
                                assert (
                                    result["result"]["observed_value"] == value
                                ), f"{result['result']['observed_value']} != {value}"
                        elif try_allclose:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {value}"
                        else:
                            assert (
                                result["result"]["observed_value"] == value
                            ), f"{result['result']['observed_value']} != {value}"

                # NOTE: This is a key used ONLY for testing cases where an expectation is legitimately allowed to return
                # any of multiple possible observed_values. expect_column_values_to_be_of_type is one such expectation.
                elif key == "observed_value_list":
                    assert result["result"]["observed_value"] in value

                elif key == "unexpected_index_list":
                    if isinstance(data_asset, (SqlAlchemyDataset, SparkDFDataset)):
                        pass
                    elif isinstance(data_asset, (SqlAlchemyBatchData, SparkDFBatchData)):
                        pass
                    else:
>                       assert (
                            result["result"]["unexpected_index_list"] == value
                        ), f"{result['result']['unexpected_index_list']} != {value}"
E                       AssertionError: [{'y': 1.1, 'pk_index': 0}, {'y': 2.2, 'pk_index': 1}] != [0, 1]

great_expectations/self_check/util.py:3075: AssertionError
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 3938.31it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 375.36it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 533.22it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 300.99it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 417.22it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 321.64it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 511.51it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 474.31it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 472.39it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 470.23it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_not_be_in_set:negative_test_float_set_two_out_of_three_column_values_included_mostly_summary_output] _

test_case = {'expectation_type': 'expect_column_values_to_not_be_in_set', 'pk_column': True, 'skip': False, 'test': {'exact_match_...et': [1.1, 2.2]}, 'input': {'column': 'y', 'mostly': 0.65, 'value_set': [1.1, 2.2]}, 'only_for': ['pandas'], ...}, ...}

    @pytest.mark.order(index=0)
    @pytest.mark.integration
    @pytest.mark.slow  # 12.68s
    def test_case_runner_v3_api(test_case):
        if test_case["skip"]:
            pytest.skip()

        # Note: this should never be done in practice, but we are wiping expectations to reuse batches during testing.
        # test_case["batch"]._initialize_expectations()
        if "parse_strings_as_datetimes" in test_case["test"]["in"]:
            with pytest.deprecated_call():
                evaluate_json_test_v3_api(
                    validator=test_case["validator_with_data"],
                    expectation_type=test_case["expectation_type"],
                    test=test_case["test"],
                    pk_column=test_case["pk_column"],
                )
        else:
>           evaluate_json_test_v3_api(
                validator=test_case["validator_with_data"],
                expectation_type=test_case["expectation_type"],
                test=test_case["test"],
                pk_column=test_case["pk_column"],
            )

tests/test_definitions/test_expectations_v3_api.py:418:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
great_expectations/self_check/util.py:2901: in evaluate_json_test_v3_api
    check_json_test_result(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

test = {'exact_match_out': False, 'in': {'column': 'y', 'mostly': 0.65, 'value_set': [1.1, 2.2]}, 'input': {'column': 'y', 'mostly': 0.65, 'value_set': [1.1, 2.2]}, 'only_for': ['pandas'], ...}
result = {'exception_info': {'exception_message': None, 'exception_traceback': None, 'raised_exception': False}, 'expectation_c...issing_percent': 0.0, 'partial_unexpected_counts': [{'count': 1, 'value': 1.1}, {'count': 1, 'value': 2.2}], ...}, ...}
data_asset = <great_expectations.execution_engine.pandas_batch_data.PandasBatchData object at 0x7f9502a5a460>
pk_column = True

    def check_json_test_result(  # noqa: C901 - 52
        test, result, data_asset=None, pk_column=False
    ) -> None:

        # check for id_pk results in cases where pk_column is true and unexpected_index_list already exists
        # this will work for testing since result_format is COMPLETE
        if pk_column:
            if not result["success"]:
                if "unexpected_index_list" in result["result"]:
                    assert "unexpected_index_query" in result["result"]

        if "unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )
            elif "unexpected_list" in test["output"]:
                (
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )

        if "partial_unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "partial_unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )
            elif "partial_unexpected_list" in test["output"]:
                (
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )

        # Determine if np.allclose(..) might be needed for float comparison
        try_allclose = False
        if "observed_value" in test["output"]:
            if RX_FLOAT.match(repr(test["output"]["observed_value"])):
                try_allclose = True

        # Check results
        if test["exact_match_out"] is True:
            if "result" in result and "observed_value" in result["result"]:
                if isinstance(result["result"]["observed_value"], (np.floating, float)):
                    assert np.allclose(
                        result["result"]["observed_value"],
                        expectationValidationResultSchema.load(test["output"])["result"][
                            "observed_value"
                        ],
                        rtol=RTOL,
                        atol=ATOL,
                    ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {expectationValidationResultSchema.load(test['output'])['result']['observed_value']}"
                else:
                    assert result == expectationValidationResultSchema.load(
                        test["output"]
                    ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
            else:
                assert result == expectationValidationResultSchema.load(
                    test["output"]
                ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
        else:
            # Convert result to json since our tests are reading from json so cannot easily contain richer types (e.g. NaN)
            # NOTE - 20191031 - JPC - we may eventually want to change these tests as we update our view on how
            # representations, serializations, and objects should interact and how much of that is shown to the user.
            result = result.to_json_dict()
            for key, value in test["output"].items():
                # Apply our great expectations-specific test logic

                if key == "success":
                    if isinstance(value, (np.floating, float)):
                        try:
                            assert np.allclose(
                                result["success"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['success']} not np.allclose to {value}"
                        except TypeError:
                            assert (
                                result["success"] == value
                            ), f"{result['success']} != {value}"
                    else:
                        assert result["success"] == value, f"{result['success']} != {value}"

                elif key == "observed_value":
                    if "tolerance" in test:
                        if isinstance(value, dict):
                            assert set(result["result"]["observed_value"].keys()) == set(
                                value.keys()
                            ), f"{set(result['result']['observed_value'].keys())} != {set(value.keys())}"
                            for k, v in value.items():
                                assert np.allclose(
                                    result["result"]["observed_value"][k],
                                    v,
                                    rtol=test["tolerance"],
                                )
                        else:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=test["tolerance"],
                            )
                    else:
                        if isinstance(value, dict) and "values" in value:
                            try:
                                assert np.allclose(
                                    result["result"]["observed_value"]["values"],
                                    value["values"],
                                    rtol=RTOL,
                                    atol=ATOL,
                                ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']['values']} not np.allclose to {value['values']}"
                            except TypeError as e:
                                print(e)
                                assert (
                                    result["result"]["observed_value"] == value
                                ), f"{result['result']['observed_value']} != {value}"
                        elif try_allclose:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {value}"
                        else:
                            assert (
                                result["result"]["observed_value"] == value
                            ), f"{result['result']['observed_value']} != {value}"

                # NOTE: This is a key used ONLY for testing cases where an expectation is legitimately allowed to return
                # any of multiple possible observed_values. expect_column_values_to_be_of_type is one such expectation.
                elif key == "observed_value_list":
                    assert result["result"]["observed_value"] in value

                elif key == "unexpected_index_list":
                    if isinstance(data_asset, (SqlAlchemyDataset, SparkDFDataset)):
                        pass
                    elif isinstance(data_asset, (SqlAlchemyBatchData, SparkDFBatchData)):
                        pass
                    else:
>                       assert (
                            result["result"]["unexpected_index_list"] == value
                        ), f"{result['result']['unexpected_index_list']} != {value}"
E                       AssertionError: [{'y': 1.1, 'pk_index': 0}, {'y': 2.2, 'pk_index': 1}] != [0, 1]

great_expectations/self_check/util.py:3075: AssertionError
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 4253.86it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 397.68it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 566.24it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 311.88it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 443.21it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 335.93it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 530.22it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 489.64it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 487.42it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 485.01it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_not_be_in_set:positive_test_float_set_two_out_of_three_column_values_included_mostly] _

test_case = {'expectation_type': 'expect_column_values_to_not_be_in_set', 'pk_column': True, 'skip': False, 'test': {'exact_match_...et': [1.1, 2.2]}, 'out': {'success': False, 'unexpected_index_list': [0, 1], 'unexpected_list': [1.1, 2.2]}, ...}, ...}

    @pytest.mark.order(index=0)
    @pytest.mark.integration
    @pytest.mark.slow  # 12.68s
    def test_case_runner_v3_api(test_case):
        if test_case["skip"]:
            pytest.skip()

        # Note: this should never be done in practice, but we are wiping expectations to reuse batches during testing.
        # test_case["batch"]._initialize_expectations()
        if "parse_strings_as_datetimes" in test_case["test"]["in"]:
            with pytest.deprecated_call():
                evaluate_json_test_v3_api(
                    validator=test_case["validator_with_data"],
                    expectation_type=test_case["expectation_type"],
                    test=test_case["test"],
                    pk_column=test_case["pk_column"],
                )
        else:
>           evaluate_json_test_v3_api(
                validator=test_case["validator_with_data"],
                expectation_type=test_case["expectation_type"],
                test=test_case["test"],
                pk_column=test_case["pk_column"],
            )

tests/test_definitions/test_expectations_v3_api.py:418:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
great_expectations/self_check/util.py:2901: in evaluate_json_test_v3_api
    check_json_test_result(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

test = {'exact_match_out': False, 'in': {'column': 'y', 'mostly': 0.7, 'value_set': [1.1, 2.2]}, 'input': {'column': 'y', 'mo...alue_set': [1.1, 2.2]}, 'out': {'success': False, 'unexpected_index_list': [0, 1], 'unexpected_list': [1.1, 2.2]}, ...}
result = {'exception_info': {'exception_message': None, 'exception_traceback': None, 'raised_exception': False}, 'expectation_c...issing_percent': 0.0, 'partial_unexpected_counts': [{'count': 1, 'value': 1.1}, {'count': 1, 'value': 2.2}], ...}, ...}
data_asset = <great_expectations.execution_engine.pandas_batch_data.PandasBatchData object at 0x7f9502a5a460>
pk_column = True

    def check_json_test_result(  # noqa: C901 - 52
        test, result, data_asset=None, pk_column=False
    ) -> None:

        # check for id_pk results in cases where pk_column is true and unexpected_index_list already exists
        # this will work for testing since result_format is COMPLETE
        if pk_column:
            if not result["success"]:
                if "unexpected_index_list" in result["result"]:
                    assert "unexpected_index_query" in result["result"]

        if "unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )
            elif "unexpected_list" in test["output"]:
                (
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )

        if "partial_unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "partial_unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )
            elif "partial_unexpected_list" in test["output"]:
                (
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )

        # Determine if np.allclose(..) might be needed for float comparison
        try_allclose = False
        if "observed_value" in test["output"]:
            if RX_FLOAT.match(repr(test["output"]["observed_value"])):
                try_allclose = True

        # Check results
        if test["exact_match_out"] is True:
            if "result" in result and "observed_value" in result["result"]:
                if isinstance(result["result"]["observed_value"], (np.floating, float)):
                    assert np.allclose(
                        result["result"]["observed_value"],
                        expectationValidationResultSchema.load(test["output"])["result"][
                            "observed_value"
                        ],
                        rtol=RTOL,
                        atol=ATOL,
                    ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {expectationValidationResultSchema.load(test['output'])['result']['observed_value']}"
                else:
                    assert result == expectationValidationResultSchema.load(
                        test["output"]
                    ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
            else:
                assert result == expectationValidationResultSchema.load(
                    test["output"]
                ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
        else:
            # Convert result to json since our tests are reading from json so cannot easily contain richer types (e.g. NaN)
            # NOTE - 20191031 - JPC - we may eventually want to change these tests as we update our view on how
            # representations, serializations, and objects should interact and how much of that is shown to the user.
            result = result.to_json_dict()
            for key, value in test["output"].items():
                # Apply our great expectations-specific test logic

                if key == "success":
                    if isinstance(value, (np.floating, float)):
                        try:
                            assert np.allclose(
                                result["success"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['success']} not np.allclose to {value}"
                        except TypeError:
                            assert (
                                result["success"] == value
                            ), f"{result['success']} != {value}"
                    else:
                        assert result["success"] == value, f"{result['success']} != {value}"

                elif key == "observed_value":
                    if "tolerance" in test:
                        if isinstance(value, dict):
                            assert set(result["result"]["observed_value"].keys()) == set(
                                value.keys()
                            ), f"{set(result['result']['observed_value'].keys())} != {set(value.keys())}"
                            for k, v in value.items():
                                assert np.allclose(
                                    result["result"]["observed_value"][k],
                                    v,
                                    rtol=test["tolerance"],
                                )
                        else:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=test["tolerance"],
                            )
                    else:
                        if isinstance(value, dict) and "values" in value:
                            try:
                                assert np.allclose(
                                    result["result"]["observed_value"]["values"],
                                    value["values"],
                                    rtol=RTOL,
                                    atol=ATOL,
                                ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']['values']} not np.allclose to {value['values']}"
                            except TypeError as e:
                                print(e)
                                assert (
                                    result["result"]["observed_value"] == value
                                ), f"{result['result']['observed_value']} != {value}"
                        elif try_allclose:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {value}"
                        else:
                            assert (
                                result["result"]["observed_value"] == value
                            ), f"{result['result']['observed_value']} != {value}"

                # NOTE: This is a key used ONLY for testing cases where an expectation is legitimately allowed to return
                # any of multiple possible observed_values. expect_column_values_to_be_of_type is one such expectation.
                elif key == "observed_value_list":
                    assert result["result"]["observed_value"] in value

                elif key == "unexpected_index_list":
                    if isinstance(data_asset, (SqlAlchemyDataset, SparkDFDataset)):
                        pass
                    elif isinstance(data_asset, (SqlAlchemyBatchData, SparkDFBatchData)):
                        pass
                    else:
>                       assert (
                            result["result"]["unexpected_index_list"] == value
                        ), f"{result['result']['unexpected_index_list']} != {value}"
E                       AssertionError: [{'y': 1.1, 'pk_index': 0}, {'y': 2.2, 'pk_index': 1}] != [0, 1]

great_expectations/self_check/util.py:3075: AssertionError
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 4090.01it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 398.07it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 568.59it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 308.16it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 438.86it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 335.12it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 529.20it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 489.61it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 487.47it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 485.10it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_in_set:negative_test_case_exclude_existing_column_value] _

test_case = {'expectation_type': 'expect_column_values_to_be_in_set', 'pk_column': True, 'skip': False, 'test': {'exact_match_out'...lumn': 'x', 'value_set': [2, 4]}, 'include_in_gallery': True, 'input': {'column': 'x', 'value_set': [2, 4]}, ...}, ...}

    @pytest.mark.order(index=0)
    @pytest.mark.integration
    @pytest.mark.slow  # 12.68s
    def test_case_runner_v3_api(test_case):
        if test_case["skip"]:
            pytest.skip()

        # Note: this should never be done in practice, but we are wiping expectations to reuse batches during testing.
        # test_case["batch"]._initialize_expectations()
        if "parse_strings_as_datetimes" in test_case["test"]["in"]:
            with pytest.deprecated_call():
                evaluate_json_test_v3_api(
                    validator=test_case["validator_with_data"],
                    expectation_type=test_case["expectation_type"],
                    test=test_case["test"],
                    pk_column=test_case["pk_column"],
                )
        else:
>           evaluate_json_test_v3_api(
                validator=test_case["validator_with_data"],
                expectation_type=test_case["expectation_type"],
                test=test_case["test"],
                pk_column=test_case["pk_column"],
            )

tests/test_definitions/test_expectations_v3_api.py:418:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
great_expectations/self_check/util.py:2901: in evaluate_json_test_v3_api
    check_json_test_result(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

test = {'exact_match_out': False, 'in': {'column': 'x', 'value_set': [2, 4]}, 'include_in_gallery': True, 'input': {'column': 'x', 'value_set': [2, 4]}, ...}
result = {'exception_info': {'exception_message': None, 'exception_traceback': None, 'raised_exception': False}, 'expectation_c...nt': 3, 'missing_count': 0, 'missing_percent': 0.0, 'partial_unexpected_counts': [{'count': 1, 'value': 1}], ...}, ...}
data_asset = <great_expectations.execution_engine.pandas_batch_data.PandasBatchData object at 0x7f9502b566d0>
pk_column = True

    def check_json_test_result(  # noqa: C901 - 52
        test, result, data_asset=None, pk_column=False
    ) -> None:

        # check for id_pk results in cases where pk_column is true and unexpected_index_list already exists
        # this will work for testing since result_format is COMPLETE
        if pk_column:
            if not result["success"]:
                if "unexpected_index_list" in result["result"]:
                    assert "unexpected_index_query" in result["result"]

        if "unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )
            elif "unexpected_list" in test["output"]:
                (
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )

        if "partial_unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "partial_unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )
            elif "partial_unexpected_list" in test["output"]:
                (
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )

        # Determine if np.allclose(..) might be needed for float comparison
        try_allclose = False
        if "observed_value" in test["output"]:
            if RX_FLOAT.match(repr(test["output"]["observed_value"])):
                try_allclose = True

        # Check results
        if test["exact_match_out"] is True:
            if "result" in result and "observed_value" in result["result"]:
                if isinstance(result["result"]["observed_value"], (np.floating, float)):
                    assert np.allclose(
                        result["result"]["observed_value"],
                        expectationValidationResultSchema.load(test["output"])["result"][
                            "observed_value"
                        ],
                        rtol=RTOL,
                        atol=ATOL,
                    ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {expectationValidationResultSchema.load(test['output'])['result']['observed_value']}"
                else:
                    assert result == expectationValidationResultSchema.load(
                        test["output"]
                    ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
            else:
                assert result == expectationValidationResultSchema.load(
                    test["output"]
                ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
        else:
            # Convert result to json since our tests are reading from json so cannot easily contain richer types (e.g. NaN)
            # NOTE - 20191031 - JPC - we may eventually want to change these tests as we update our view on how
            # representations, serializations, and objects should interact and how much of that is shown to the user.
            result = result.to_json_dict()
            for key, value in test["output"].items():
                # Apply our great expectations-specific test logic

                if key == "success":
                    if isinstance(value, (np.floating, float)):
                        try:
                            assert np.allclose(
                                result["success"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['success']} not np.allclose to {value}"
                        except TypeError:
                            assert (
                                result["success"] == value
                            ), f"{result['success']} != {value}"
                    else:
                        assert result["success"] == value, f"{result['success']} != {value}"

                elif key == "observed_value":
                    if "tolerance" in test:
                        if isinstance(value, dict):
                            assert set(result["result"]["observed_value"].keys()) == set(
                                value.keys()
                            ), f"{set(result['result']['observed_value'].keys())} != {set(value.keys())}"
                            for k, v in value.items():
                                assert np.allclose(
                                    result["result"]["observed_value"][k],
                                    v,
                                    rtol=test["tolerance"],
                                )
                        else:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=test["tolerance"],
                            )
                    else:
                        if isinstance(value, dict) and "values" in value:
                            try:
                                assert np.allclose(
                                    result["result"]["observed_value"]["values"],
                                    value["values"],
                                    rtol=RTOL,
                                    atol=ATOL,
                                ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']['values']} not np.allclose to {value['values']}"
                            except TypeError as e:
                                print(e)
                                assert (
                                    result["result"]["observed_value"] == value
                                ), f"{result['result']['observed_value']} != {value}"
                        elif try_allclose:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {value}"
                        else:
                            assert (
                                result["result"]["observed_value"] == value
                            ), f"{result['result']['observed_value']} != {value}"

                # NOTE: This is a key used ONLY for testing cases where an expectation is legitimately allowed to return
                # any of multiple possible observed_values. expect_column_values_to_be_of_type is one such expectation.
                elif key == "observed_value_list":
                    assert result["result"]["observed_value"] in value

                elif key == "unexpected_index_list":
                    if isinstance(data_asset, (SqlAlchemyDataset, SparkDFDataset)):
                        pass
                    elif isinstance(data_asset, (SqlAlchemyBatchData, SparkDFBatchData)):
                        pass
                    else:
>                       assert (
                            result["result"]["unexpected_index_list"] == value
                        ), f"{result['result']['unexpected_index_list']} != {value}"
E                       AssertionError: [{'x': 1, 'pk_index': 0}] != [0]

great_expectations/self_check/util.py:3075: AssertionError
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 4042.70it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 420.19it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 597.51it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 325.07it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 459.61it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 349.65it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 544.99it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 505.08it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 502.94it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 500.46it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_in_set:negative_empty_values_set] _

test_case = {'expectation_type': 'expect_column_values_to_be_in_set', 'pk_column': True, 'skip': False, 'test': {'exact_match_out'...alue_set': []}, 'out': {'success': False, 'unexpected_index_list': [0, 1, 2], 'unexpected_list': [1, 2, 4]}, ...}, ...}

    @pytest.mark.order(index=0)
    @pytest.mark.integration
    @pytest.mark.slow  # 12.68s
    def test_case_runner_v3_api(test_case):
        if test_case["skip"]:
            pytest.skip()

        # Note: this should never be done in practice, but we are wiping expectations to reuse batches during testing.
        # test_case["batch"]._initialize_expectations()
        if "parse_strings_as_datetimes" in test_case["test"]["in"]:
            with pytest.deprecated_call():
                evaluate_json_test_v3_api(
                    validator=test_case["validator_with_data"],
                    expectation_type=test_case["expectation_type"],
                    test=test_case["test"],
                    pk_column=test_case["pk_column"],
                )
        else:
>           evaluate_json_test_v3_api(
                validator=test_case["validator_with_data"],
                expectation_type=test_case["expectation_type"],
                test=test_case["test"],
                pk_column=test_case["pk_column"],
            )

tests/test_definitions/test_expectations_v3_api.py:418:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
great_expectations/self_check/util.py:2901: in evaluate_json_test_v3_api
    check_json_test_result(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

test = {'exact_match_out': False, 'in': {'column': 'x', 'value_set': []}, 'input': {'column': 'x', 'value_set': []}, 'out': {'success': False, 'unexpected_index_list': [0, 1, 2], 'unexpected_list': [1, 2, 4]}, ...}
result = {'exception_info': {'exception_message': None, 'exception_traceback': None, 'raised_exception': False}, 'expectation_c...'partial_unexpected_counts': [{'count': 1, 'value': 1}, {'count': 1, 'value': 2}, {'count': 1, 'value': 4}], ...}, ...}
data_asset = <great_expectations.execution_engine.pandas_batch_data.PandasBatchData object at 0x7f9502b566d0>
pk_column = True

    def check_json_test_result(  # noqa: C901 - 52
        test, result, data_asset=None, pk_column=False
    ) -> None:

        # check for id_pk results in cases where pk_column is true and unexpected_index_list already exists
        # this will work for testing since result_format is COMPLETE
        if pk_column:
            if not result["success"]:
                if "unexpected_index_list" in result["result"]:
                    assert "unexpected_index_query" in result["result"]

        if "unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )
            elif "unexpected_list" in test["output"]:
                (
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )

        if "partial_unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "partial_unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )
            elif "partial_unexpected_list" in test["output"]:
                (
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )

        # Determine if np.allclose(..) might be needed for float comparison
        try_allclose = False
        if "observed_value" in test["output"]:
            if RX_FLOAT.match(repr(test["output"]["observed_value"])):
                try_allclose = True

        # Check results
        if test["exact_match_out"] is True:
            if "result" in result and "observed_value" in result["result"]:
                if isinstance(result["result"]["observed_value"], (np.floating, float)):
                    assert np.allclose(
                        result["result"]["observed_value"],
                        expectationValidationResultSchema.load(test["output"])["result"][
                            "observed_value"
                        ],
                        rtol=RTOL,
                        atol=ATOL,
                    ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {expectationValidationResultSchema.load(test['output'])['result']['observed_value']}"
                else:
                    assert result == expectationValidationResultSchema.load(
                        test["output"]
                    ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
            else:
                assert result == expectationValidationResultSchema.load(
                    test["output"]
                ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
        else:
            # Convert result to json since our tests are reading from json so cannot easily contain richer types (e.g. NaN)
            # NOTE - 20191031 - JPC - we may eventually want to change these tests as we update our view on how
            # representations, serializations, and objects should interact and how much of that is shown to the user.
            result = result.to_json_dict()
            for key, value in test["output"].items():
                # Apply our great expectations-specific test logic

                if key == "success":
                    if isinstance(value, (np.floating, float)):
                        try:
                            assert np.allclose(
                                result["success"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['success']} not np.allclose to {value}"
                        except TypeError:
                            assert (
                                result["success"] == value
                            ), f"{result['success']} != {value}"
                    else:
                        assert result["success"] == value, f"{result['success']} != {value}"

                elif key == "observed_value":
                    if "tolerance" in test:
                        if isinstance(value, dict):
                            assert set(result["result"]["observed_value"].keys()) == set(
                                value.keys()
                            ), f"{set(result['result']['observed_value'].keys())} != {set(value.keys())}"
                            for k, v in value.items():
                                assert np.allclose(
                                    result["result"]["observed_value"][k],
                                    v,
                                    rtol=test["tolerance"],
                                )
                        else:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=test["tolerance"],
                            )
                    else:
                        if isinstance(value, dict) and "values" in value:
                            try:
                                assert np.allclose(
                                    result["result"]["observed_value"]["values"],
                                    value["values"],
                                    rtol=RTOL,
                                    atol=ATOL,
                                ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']['values']} not np.allclose to {value['values']}"
                            except TypeError as e:
                                print(e)
                                assert (
                                    result["result"]["observed_value"] == value
                                ), f"{result['result']['observed_value']} != {value}"
                        elif try_allclose:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {value}"
                        else:
                            assert (
                                result["result"]["observed_value"] == value
                            ), f"{result['result']['observed_value']} != {value}"

                # NOTE: This is a key used ONLY for testing cases where an expectation is legitimately allowed to return
                # any of multiple possible observed_values. expect_column_values_to_be_of_type is one such expectation.
                elif key == "observed_value_list":
                    assert result["result"]["observed_value"] in value

                elif key == "unexpected_index_list":
                    if isinstance(data_asset, (SqlAlchemyDataset, SparkDFDataset)):
                        pass
                    elif isinstance(data_asset, (SqlAlchemyBatchData, SparkDFBatchData)):
                        pass
                    else:
>                       assert (
                            result["result"]["unexpected_index_list"] == value
                        ), f"{result['result']['unexpected_index_list']} != {value}"
E                       AssertionError: [{'x': 1, 'pk_index': 0}, {'x': 2, 'pk_index': 1}, {'x': 4, 'pk_index': 2}] != [0, 1, 2]

great_expectations/self_check/util.py:3075: AssertionError
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 3968.12it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 424.01it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 599.53it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 330.29it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 465.90it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 355.26it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 547.07it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 508.13it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 505.97it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 503.45it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_in_set:negative_test_strings_set_extra_value_in_column] _

test_case = {'expectation_type': 'expect_column_values_to_be_in_set', 'pk_column': True, 'skip': False, 'test': {'exact_match_out'... ['hello', 'jello']}, 'out': {'success': False, 'unexpected_index_list': [2], 'unexpected_list': ['mello']}, ...}, ...}

    @pytest.mark.order(index=0)
    @pytest.mark.integration
    @pytest.mark.slow  # 12.68s
    def test_case_runner_v3_api(test_case):
        if test_case["skip"]:
            pytest.skip()

        # Note: this should never be done in practice, but we are wiping expectations to reuse batches during testing.
        # test_case["batch"]._initialize_expectations()
        if "parse_strings_as_datetimes" in test_case["test"]["in"]:
            with pytest.deprecated_call():
                evaluate_json_test_v3_api(
                    validator=test_case["validator_with_data"],
                    expectation_type=test_case["expectation_type"],
                    test=test_case["test"],
                    pk_column=test_case["pk_column"],
                )
        else:
>           evaluate_json_test_v3_api(
                validator=test_case["validator_with_data"],
                expectation_type=test_case["expectation_type"],
                test=test_case["test"],
                pk_column=test_case["pk_column"],
            )

tests/test_definitions/test_expectations_v3_api.py:418:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
great_expectations/self_check/util.py:2901: in evaluate_json_test_v3_api
    check_json_test_result(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

test = {'exact_match_out': False, 'in': {'column': 'z', 'value_set': ['hello', 'jello']}, 'input': {'column': 'z', 'value_set': ['hello', 'jello']}, 'out': {'success': False, 'unexpected_index_list': [2], 'unexpected_list': ['mello']}, ...}
result = {'exception_info': {'exception_message': None, 'exception_traceback': None, 'raised_exception': False}, 'expectation_c..., 'missing_count': 0, 'missing_percent': 0.0, 'partial_unexpected_counts': [{'count': 1, 'value': 'mello'}], ...}, ...}
data_asset = <great_expectations.execution_engine.pandas_batch_data.PandasBatchData object at 0x7f9502b566d0>
pk_column = True

    def check_json_test_result(  # noqa: C901 - 52
        test, result, data_asset=None, pk_column=False
    ) -> None:

        # check for id_pk results in cases where pk_column is true and unexpected_index_list already exists
        # this will work for testing since result_format is COMPLETE
        if pk_column:
            if not result["success"]:
                if "unexpected_index_list" in result["result"]:
                    assert "unexpected_index_query" in result["result"]

        if "unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )
            elif "unexpected_list" in test["output"]:
                (
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )

        if "partial_unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "partial_unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )
            elif "partial_unexpected_list" in test["output"]:
                (
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )

        # Determine if np.allclose(..) might be needed for float comparison
        try_allclose = False
        if "observed_value" in test["output"]:
            if RX_FLOAT.match(repr(test["output"]["observed_value"])):
                try_allclose = True

        # Check results
        if test["exact_match_out"] is True:
            if "result" in result and "observed_value" in result["result"]:
                if isinstance(result["result"]["observed_value"], (np.floating, float)):
                    assert np.allclose(
                        result["result"]["observed_value"],
                        expectationValidationResultSchema.load(test["output"])["result"][
                            "observed_value"
                        ],
                        rtol=RTOL,
                        atol=ATOL,
                    ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {expectationValidationResultSchema.load(test['output'])['result']['observed_value']}"
                else:
                    assert result == expectationValidationResultSchema.load(
                        test["output"]
                    ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
            else:
                assert result == expectationValidationResultSchema.load(
                    test["output"]
                ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
        else:
            # Convert result to json since our tests are reading from json so cannot easily contain richer types (e.g. NaN)
            # NOTE - 20191031 - JPC - we may eventually want to change these tests as we update our view on how
            # representations, serializations, and objects should interact and how much of that is shown to the user.
            result = result.to_json_dict()
            for key, value in test["output"].items():
                # Apply our great expectations-specific test logic

                if key == "success":
                    if isinstance(value, (np.floating, float)):
                        try:
                            assert np.allclose(
                                result["success"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['success']} not np.allclose to {value}"
                        except TypeError:
                            assert (
                                result["success"] == value
                            ), f"{result['success']} != {value}"
                    else:
                        assert result["success"] == value, f"{result['success']} != {value}"

                elif key == "observed_value":
                    if "tolerance" in test:
                        if isinstance(value, dict):
                            assert set(result["result"]["observed_value"].keys()) == set(
                                value.keys()
                            ), f"{set(result['result']['observed_value'].keys())} != {set(value.keys())}"
                            for k, v in value.items():
                                assert np.allclose(
                                    result["result"]["observed_value"][k],
                                    v,
                                    rtol=test["tolerance"],
                                )
                        else:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=test["tolerance"],
                            )
                    else:
                        if isinstance(value, dict) and "values" in value:
                            try:
                                assert np.allclose(
                                    result["result"]["observed_value"]["values"],
                                    value["values"],
                                    rtol=RTOL,
                                    atol=ATOL,
                                ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']['values']} not np.allclose to {value['values']}"
                            except TypeError as e:
                                print(e)
                                assert (
                                    result["result"]["observed_value"] == value
                                ), f"{result['result']['observed_value']} != {value}"
                        elif try_allclose:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {value}"
                        else:
                            assert (
                                result["result"]["observed_value"] == value
                            ), f"{result['result']['observed_value']} != {value}"

                # NOTE: This is a key used ONLY for testing cases where an expectation is legitimately allowed to return
                # any of multiple possible observed_values. expect_column_values_to_be_of_type is one such expectation.
                elif key == "observed_value_list":
                    assert result["result"]["observed_value"] in value

                elif key == "unexpected_index_list":
                    if isinstance(data_asset, (SqlAlchemyDataset, SparkDFDataset)):
                        pass
                    elif isinstance(data_asset, (SqlAlchemyBatchData, SparkDFBatchData)):
                        pass
                    else:
>                       assert (
                            result["result"]["unexpected_index_list"] == value
                        ), f"{result['result']['unexpected_index_list']} != {value}"
E                       AssertionError: [{'z': 'mello', 'pk_index': 2}] != [2]

great_expectations/self_check/util.py:3075: AssertionError
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 3921.74it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 414.76it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 589.14it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 319.02it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 451.51it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 346.06it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 543.57it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 503.86it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 501.73it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 499.30it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_in_set:negative_test_numbers_set_no_matching_values] _

test_case = {'expectation_type': 'expect_column_values_to_be_in_set', 'pk_column': True, 'skip': False, 'test': {'exact_match_out'...lue_set': [3]}, 'out': {'success': False, 'unexpected_index_list': [0, 1, 2], 'unexpected_list': [1, 2, 4]}, ...}, ...}

    @pytest.mark.order(index=0)
    @pytest.mark.integration
    @pytest.mark.slow  # 12.68s
    def test_case_runner_v3_api(test_case):
        if test_case["skip"]:
            pytest.skip()

        # Note: this should never be done in practice, but we are wiping expectations to reuse batches during testing.
        # test_case["batch"]._initialize_expectations()
        if "parse_strings_as_datetimes" in test_case["test"]["in"]:
            with pytest.deprecated_call():
                evaluate_json_test_v3_api(
                    validator=test_case["validator_with_data"],
                    expectation_type=test_case["expectation_type"],
                    test=test_case["test"],
                    pk_column=test_case["pk_column"],
                )
        else:
>           evaluate_json_test_v3_api(
                validator=test_case["validator_with_data"],
                expectation_type=test_case["expectation_type"],
                test=test_case["test"],
                pk_column=test_case["pk_column"],
            )

tests/test_definitions/test_expectations_v3_api.py:418:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
great_expectations/self_check/util.py:2901: in evaluate_json_test_v3_api
    check_json_test_result(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

test = {'exact_match_out': False, 'in': {'column': 'x', 'value_set': [3]}, 'input': {'column': 'x', 'value_set': [3]}, 'out': {'success': False, 'unexpected_index_list': [0, 1, 2], 'unexpected_list': [1, 2, 4]}, ...}
result = {'exception_info': {'exception_message': None, 'exception_traceback': None, 'raised_exception': False}, 'expectation_c...'partial_unexpected_counts': [{'count': 1, 'value': 1}, {'count': 1, 'value': 2}, {'count': 1, 'value': 4}], ...}, ...}
data_asset = <great_expectations.execution_engine.pandas_batch_data.PandasBatchData object at 0x7f9502b566d0>
pk_column = True

    def check_json_test_result(  # noqa: C901 - 52
        test, result, data_asset=None, pk_column=False
    ) -> None:

        # check for id_pk results in cases where pk_column is true and unexpected_index_list already exists
        # this will work for testing since result_format is COMPLETE
        if pk_column:
            if not result["success"]:
                if "unexpected_index_list" in result["result"]:
                    assert "unexpected_index_query" in result["result"]

        if "unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )
            elif "unexpected_list" in test["output"]:
                (
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )

        if "partial_unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "partial_unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )
            elif "partial_unexpected_list" in test["output"]:
                (
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )

        # Determine if np.allclose(..) might be needed for float comparison
        try_allclose = False
        if "observed_value" in test["output"]:
            if RX_FLOAT.match(repr(test["output"]["observed_value"])):
                try_allclose = True

        # Check results
        if test["exact_match_out"] is True:
            if "result" in result and "observed_value" in result["result"]:
                if isinstance(result["result"]["observed_value"], (np.floating, float)):
                    assert np.allclose(
                        result["result"]["observed_value"],
                        expectationValidationResultSchema.load(test["output"])["result"][
                            "observed_value"
                        ],
                        rtol=RTOL,
                        atol=ATOL,
                    ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {expectationValidationResultSchema.load(test['output'])['result']['observed_value']}"
                else:
                    assert result == expectationValidationResultSchema.load(
                        test["output"]
                    ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
            else:
                assert result == expectationValidationResultSchema.load(
                    test["output"]
                ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
        else:
            # Convert result to json since our tests are reading from json so cannot easily contain richer types (e.g. NaN)
            # NOTE - 20191031 - JPC - we may eventually want to change these tests as we update our view on how
            # representations, serializations, and objects should interact and how much of that is shown to the user.
            result = result.to_json_dict()
            for key, value in test["output"].items():
                # Apply our great expectations-specific test logic

                if key == "success":
                    if isinstance(value, (np.floating, float)):
                        try:
                            assert np.allclose(
                                result["success"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['success']} not np.allclose to {value}"
                        except TypeError:
                            assert (
                                result["success"] == value
                            ), f"{result['success']} != {value}"
                    else:
                        assert result["success"] == value, f"{result['success']} != {value}"

                elif key == "observed_value":
                    if "tolerance" in test:
                        if isinstance(value, dict):
                            assert set(result["result"]["observed_value"].keys()) == set(
                                value.keys()
                            ), f"{set(result['result']['observed_value'].keys())} != {set(value.keys())}"
                            for k, v in value.items():
                                assert np.allclose(
                                    result["result"]["observed_value"][k],
                                    v,
                                    rtol=test["tolerance"],
                                )
                        else:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=test["tolerance"],
                            )
                    else:
                        if isinstance(value, dict) and "values" in value:
                            try:
                                assert np.allclose(
                                    result["result"]["observed_value"]["values"],
                                    value["values"],
                                    rtol=RTOL,
                                    atol=ATOL,
                                ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']['values']} not np.allclose to {value['values']}"
                            except TypeError as e:
                                print(e)
                                assert (
                                    result["result"]["observed_value"] == value
                                ), f"{result['result']['observed_value']} != {value}"
                        elif try_allclose:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {value}"
                        else:
                            assert (
                                result["result"]["observed_value"] == value
                            ), f"{result['result']['observed_value']} != {value}"

                # NOTE: This is a key used ONLY for testing cases where an expectation is legitimately allowed to return
                # any of multiple possible observed_values. expect_column_values_to_be_of_type is one such expectation.
                elif key == "observed_value_list":
                    assert result["result"]["observed_value"] in value

                elif key == "unexpected_index_list":
                    if isinstance(data_asset, (SqlAlchemyDataset, SparkDFDataset)):
                        pass
                    elif isinstance(data_asset, (SqlAlchemyBatchData, SparkDFBatchData)):
                        pass
                    else:
>                       assert (
                            result["result"]["unexpected_index_list"] == value
                        ), f"{result['result']['unexpected_index_list']} != {value}"
E                       AssertionError: [{'x': 1, 'pk_index': 0}, {'x': 2, 'pk_index': 1}, {'x': 4, 'pk_index': 2}] != [0, 1, 2]

great_expectations/self_check/util.py:3075: AssertionError
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 3914.42it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 420.63it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 598.33it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 329.27it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 467.43it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 356.84it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 556.89it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 515.68it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 513.29it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 510.68it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_in_set:negative_test_float_set] _

test_case = {'expectation_type': 'expect_column_values_to_be_in_set', 'pk_column': True, 'skip': False, 'test': {'exact_match_out'..., 5.51]}, 'out': {'success': False, 'unexpected_index_list': [0, 1, 2], 'unexpected_list': [1.1, 2.2, 5.5]}, ...}, ...}

    @pytest.mark.order(index=0)
    @pytest.mark.integration
    @pytest.mark.slow  # 12.68s
    def test_case_runner_v3_api(test_case):
        if test_case["skip"]:
            pytest.skip()

        # Note: this should never be done in practice, but we are wiping expectations to reuse batches during testing.
        # test_case["batch"]._initialize_expectations()
        if "parse_strings_as_datetimes" in test_case["test"]["in"]:
            with pytest.deprecated_call():
                evaluate_json_test_v3_api(
                    validator=test_case["validator_with_data"],
                    expectation_type=test_case["expectation_type"],
                    test=test_case["test"],
                    pk_column=test_case["pk_column"],
                )
        else:
>           evaluate_json_test_v3_api(
                validator=test_case["validator_with_data"],
                expectation_type=test_case["expectation_type"],
                test=test_case["test"],
                pk_column=test_case["pk_column"],
            )

tests/test_definitions/test_expectations_v3_api.py:418:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
great_expectations/self_check/util.py:2901: in evaluate_json_test_v3_api
    check_json_test_result(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

test = {'exact_match_out': False, 'in': {'column': 'y', 'value_set': [1.11, 2.22, 5.51]}, 'input': {'column': 'y', 'value_set..., 2.22, 5.51]}, 'out': {'success': False, 'unexpected_index_list': [0, 1, 2], 'unexpected_list': [1.1, 2.2, 5.5]}, ...}
result = {'exception_info': {'exception_message': None, 'exception_traceback': None, 'raised_exception': False}, 'expectation_c...al_unexpected_counts': [{'count': 1, 'value': 1.1}, {'count': 1, 'value': 2.2}, {'count': 1, 'value': 5.5}], ...}, ...}
data_asset = <great_expectations.execution_engine.pandas_batch_data.PandasBatchData object at 0x7f9502b566d0>
pk_column = True

    def check_json_test_result(  # noqa: C901 - 52
        test, result, data_asset=None, pk_column=False
    ) -> None:

        # check for id_pk results in cases where pk_column is true and unexpected_index_list already exists
        # this will work for testing since result_format is COMPLETE
        if pk_column:
            if not result["success"]:
                if "unexpected_index_list" in result["result"]:
                    assert "unexpected_index_query" in result["result"]

        if "unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )
            elif "unexpected_list" in test["output"]:
                (
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )

        if "partial_unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "partial_unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )
            elif "partial_unexpected_list" in test["output"]:
                (
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )

        # Determine if np.allclose(..) might be needed for float comparison
        try_allclose = False
        if "observed_value" in test["output"]:
            if RX_FLOAT.match(repr(test["output"]["observed_value"])):
                try_allclose = True

        # Check results
        if test["exact_match_out"] is True:
            if "result" in result and "observed_value" in result["result"]:
                if isinstance(result["result"]["observed_value"], (np.floating, float)):
                    assert np.allclose(
                        result["result"]["observed_value"],
                        expectationValidationResultSchema.load(test["output"])["result"][
                            "observed_value"
                        ],
                        rtol=RTOL,
                        atol=ATOL,
                    ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {expectationValidationResultSchema.load(test['output'])['result']['observed_value']}"
                else:
                    assert result == expectationValidationResultSchema.load(
                        test["output"]
                    ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
            else:
                assert result == expectationValidationResultSchema.load(
                    test["output"]
                ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
        else:
            # Convert result to json since our tests are reading from json so cannot easily contain richer types (e.g. NaN)
            # NOTE - 20191031 - JPC - we may eventually want to change these tests as we update our view on how
            # representations, serializations, and objects should interact and how much of that is shown to the user.
            result = result.to_json_dict()
            for key, value in test["output"].items():
                # Apply our great expectations-specific test logic

                if key == "success":
                    if isinstance(value, (np.floating, float)):
                        try:
                            assert np.allclose(
                                result["success"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['success']} not np.allclose to {value}"
                        except TypeError:
                            assert (
                                result["success"] == value
                            ), f"{result['success']} != {value}"
                    else:
                        assert result["success"] == value, f"{result['success']} != {value}"

                elif key == "observed_value":
                    if "tolerance" in test:
                        if isinstance(value, dict):
                            assert set(result["result"]["observed_value"].keys()) == set(
                                value.keys()
                            ), f"{set(result['result']['observed_value'].keys())} != {set(value.keys())}"
                            for k, v in value.items():
                                assert np.allclose(
                                    result["result"]["observed_value"][k],
                                    v,
                                    rtol=test["tolerance"],
                                )
                        else:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=test["tolerance"],
                            )
                    else:
                        if isinstance(value, dict) and "values" in value:
                            try:
                                assert np.allclose(
                                    result["result"]["observed_value"]["values"],
                                    value["values"],
                                    rtol=RTOL,
                                    atol=ATOL,
                                ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']['values']} not np.allclose to {value['values']}"
                            except TypeError as e:
                                print(e)
                                assert (
                                    result["result"]["observed_value"] == value
                                ), f"{result['result']['observed_value']} != {value}"
                        elif try_allclose:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {value}"
                        else:
                            assert (
                                result["result"]["observed_value"] == value
                            ), f"{result['result']['observed_value']} != {value}"

                # NOTE: This is a key used ONLY for testing cases where an expectation is legitimately allowed to return
                # any of multiple possible observed_values. expect_column_values_to_be_of_type is one such expectation.
                elif key == "observed_value_list":
                    assert result["result"]["observed_value"] in value

                elif key == "unexpected_index_list":
                    if isinstance(data_asset, (SqlAlchemyDataset, SparkDFDataset)):
                        pass
                    elif isinstance(data_asset, (SqlAlchemyBatchData, SparkDFBatchData)):
                        pass
                    else:
>                       assert (
                            result["result"]["unexpected_index_list"] == value
                        ), f"{result['result']['unexpected_index_list']} != {value}"
E                       AssertionError: [{'y': 1.1, 'pk_index': 0}, {'y': 2.2, 'pk_index': 1}, {'y': 5.5, 'pk_index': 2}] != [0, 1, 2]

great_expectations/self_check/util.py:3075: AssertionError
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 4142.52it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 392.25it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 560.04it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 305.16it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 428.12it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 323.92it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 510.34it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 472.10it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 470.06it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 467.88it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_in_set:test_conditional_expectation_fails] _

test_case = {'expectation_type': 'expect_column_values_to_be_in_set', 'pk_column': True, 'skip': False, 'test': {'exact_match_out'... True, 'column': 'z', 'condition_parser': 'pandas', 'row_condition': 'x == 1', ...}, 'only_for': ['pandas'], ...}, ...}

    @pytest.mark.order(index=0)
    @pytest.mark.integration
    @pytest.mark.slow  # 12.68s
    def test_case_runner_v3_api(test_case):
        if test_case["skip"]:
            pytest.skip()

        # Note: this should never be done in practice, but we are wiping expectations to reuse batches during testing.
        # test_case["batch"]._initialize_expectations()
        if "parse_strings_as_datetimes" in test_case["test"]["in"]:
            with pytest.deprecated_call():
                evaluate_json_test_v3_api(
                    validator=test_case["validator_with_data"],
                    expectation_type=test_case["expectation_type"],
                    test=test_case["test"],
                    pk_column=test_case["pk_column"],
                )
        else:
>           evaluate_json_test_v3_api(
                validator=test_case["validator_with_data"],
                expectation_type=test_case["expectation_type"],
                test=test_case["test"],
                pk_column=test_case["pk_column"],
            )

tests/test_definitions/test_expectations_v3_api.py:418:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
great_expectations/self_check/util.py:2901: in evaluate_json_test_v3_api
    check_json_test_result(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

test = {'exact_match_out': False, 'in': {'catch_exceptions': True, 'column': 'z', 'condition_parser': 'pandas', 'row_conditio...ions': True, 'column': 'z', 'condition_parser': 'pandas', 'row_condition': 'x == 1', ...}, 'only_for': ['pandas'], ...}
result = {'exception_info': {'exception_message': None, 'exception_traceback': None, 'raised_exception': False}, 'expectation_c..., 'missing_count': 0, 'missing_percent': 0.0, 'partial_unexpected_counts': [{'count': 1, 'value': 'hello'}], ...}, ...}
data_asset = <great_expectations.execution_engine.pandas_batch_data.PandasBatchData object at 0x7f9502b566d0>
pk_column = True

    def check_json_test_result(  # noqa: C901 - 52
        test, result, data_asset=None, pk_column=False
    ) -> None:

        # check for id_pk results in cases where pk_column is true and unexpected_index_list already exists
        # this will work for testing since result_format is COMPLETE
        if pk_column:
            if not result["success"]:
                if "unexpected_index_list" in result["result"]:
                    assert "unexpected_index_query" in result["result"]

        if "unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )
            elif "unexpected_list" in test["output"]:
                (
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )

        if "partial_unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "partial_unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )
            elif "partial_unexpected_list" in test["output"]:
                (
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )

        # Determine if np.allclose(..) might be needed for float comparison
        try_allclose = False
        if "observed_value" in test["output"]:
            if RX_FLOAT.match(repr(test["output"]["observed_value"])):
                try_allclose = True

        # Check results
        if test["exact_match_out"] is True:
            if "result" in result and "observed_value" in result["result"]:
                if isinstance(result["result"]["observed_value"], (np.floating, float)):
                    assert np.allclose(
                        result["result"]["observed_value"],
                        expectationValidationResultSchema.load(test["output"])["result"][
                            "observed_value"
                        ],
                        rtol=RTOL,
                        atol=ATOL,
                    ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {expectationValidationResultSchema.load(test['output'])['result']['observed_value']}"
                else:
                    assert result == expectationValidationResultSchema.load(
                        test["output"]
                    ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
            else:
                assert result == expectationValidationResultSchema.load(
                    test["output"]
                ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
        else:
            # Convert result to json since our tests are reading from json so cannot easily contain richer types (e.g. NaN)
            # NOTE - 20191031 - JPC - we may eventually want to change these tests as we update our view on how
            # representations, serializations, and objects should interact and how much of that is shown to the user.
            result = result.to_json_dict()
            for key, value in test["output"].items():
                # Apply our great expectations-specific test logic

                if key == "success":
                    if isinstance(value, (np.floating, float)):
                        try:
                            assert np.allclose(
                                result["success"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['success']} not np.allclose to {value}"
                        except TypeError:
                            assert (
                                result["success"] == value
                            ), f"{result['success']} != {value}"
                    else:
                        assert result["success"] == value, f"{result['success']} != {value}"

                elif key == "observed_value":
                    if "tolerance" in test:
                        if isinstance(value, dict):
                            assert set(result["result"]["observed_value"].keys()) == set(
                                value.keys()
                            ), f"{set(result['result']['observed_value'].keys())} != {set(value.keys())}"
                            for k, v in value.items():
                                assert np.allclose(
                                    result["result"]["observed_value"][k],
                                    v,
                                    rtol=test["tolerance"],
                                )
                        else:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=test["tolerance"],
                            )
                    else:
                        if isinstance(value, dict) and "values" in value:
                            try:
                                assert np.allclose(
                                    result["result"]["observed_value"]["values"],
                                    value["values"],
                                    rtol=RTOL,
                                    atol=ATOL,
                                ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']['values']} not np.allclose to {value['values']}"
                            except TypeError as e:
                                print(e)
                                assert (
                                    result["result"]["observed_value"] == value
                                ), f"{result['result']['observed_value']} != {value}"
                        elif try_allclose:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {value}"
                        else:
                            assert (
                                result["result"]["observed_value"] == value
                            ), f"{result['result']['observed_value']} != {value}"

                # NOTE: This is a key used ONLY for testing cases where an expectation is legitimately allowed to return
                # any of multiple possible observed_values. expect_column_values_to_be_of_type is one such expectation.
                elif key == "observed_value_list":
                    assert result["result"]["observed_value"] in value

                elif key == "unexpected_index_list":
                    if isinstance(data_asset, (SqlAlchemyDataset, SparkDFDataset)):
                        pass
                    elif isinstance(data_asset, (SqlAlchemyBatchData, SparkDFBatchData)):
                        pass
                    else:
>                       assert (
                            result["result"]["unexpected_index_list"] == value
                        ), f"{result['result']['unexpected_index_list']} != {value}"
E                       AssertionError: [{'z': 'hello', 'pk_index': 0}] != [0]

great_expectations/self_check/util.py:3075: AssertionError
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 884.22it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 380.95it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 547.63it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 362.62it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 440.18it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 364.91it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 505.53it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 479.87it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 477.87it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 475.63it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_in_set:basic_negative_test_case_datetime_set] _

test_case = {'expectation_type': 'expect_column_values_to_be_in_set', 'pk_column': True, 'skip': False, 'test': {'exact_match_out'...:01']}, 'out': {'success': False, 'unexpected_index_list': [0], 'unexpected_list': ['2018-01-01T00:00:00']}, ...}, ...}

    @pytest.mark.order(index=0)
    @pytest.mark.integration
    @pytest.mark.slow  # 12.68s
    def test_case_runner_v3_api(test_case):
        if test_case["skip"]:
            pytest.skip()

        # Note: this should never be done in practice, but we are wiping expectations to reuse batches during testing.
        # test_case["batch"]._initialize_expectations()
        if "parse_strings_as_datetimes" in test_case["test"]["in"]:
            with pytest.deprecated_call():
>               evaluate_json_test_v3_api(
                    validator=test_case["validator_with_data"],
                    expectation_type=test_case["expectation_type"],
                    test=test_case["test"],
                    pk_column=test_case["pk_column"],
                )

tests/test_definitions/test_expectations_v3_api.py:411:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
great_expectations/self_check/util.py:2901: in evaluate_json_test_v3_api
    check_json_test_result(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

test = {'exact_match_out': False, 'in': {'column': 'dates', 'parse_strings_as_datetimes': True, 'value_set': ['2018-01-02', '... 00:34:01']}, 'out': {'success': False, 'unexpected_index_list': [0], 'unexpected_list': ['2018-01-01T00:00:00']}, ...}
result = {'exception_info': {'exception_message': None, 'exception_traceback': None, 'raised_exception': False}, 'expectation_c...nt': 0, 'missing_percent': 0.0, 'partial_unexpected_counts': [{'count': 1, 'value': '2018-01-01T00:00:00'}], ...}, ...}
data_asset = <great_expectations.execution_engine.pandas_batch_data.PandasBatchData object at 0x7f9502b93550>
pk_column = True

    def check_json_test_result(  # noqa: C901 - 52
        test, result, data_asset=None, pk_column=False
    ) -> None:

        # check for id_pk results in cases where pk_column is true and unexpected_index_list already exists
        # this will work for testing since result_format is COMPLETE
        if pk_column:
            if not result["success"]:
                if "unexpected_index_list" in result["result"]:
                    assert "unexpected_index_query" in result["result"]

        if "unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )
            elif "unexpected_list" in test["output"]:
                (
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )

        if "partial_unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "partial_unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )
            elif "partial_unexpected_list" in test["output"]:
                (
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )

        # Determine if np.allclose(..) might be needed for float comparison
        try_allclose = False
        if "observed_value" in test["output"]:
            if RX_FLOAT.match(repr(test["output"]["observed_value"])):
                try_allclose = True

        # Check results
        if test["exact_match_out"] is True:
            if "result" in result and "observed_value" in result["result"]:
                if isinstance(result["result"]["observed_value"], (np.floating, float)):
                    assert np.allclose(
                        result["result"]["observed_value"],
                        expectationValidationResultSchema.load(test["output"])["result"][
                            "observed_value"
                        ],
                        rtol=RTOL,
                        atol=ATOL,
                    ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {expectationValidationResultSchema.load(test['output'])['result']['observed_value']}"
                else:
                    assert result == expectationValidationResultSchema.load(
                        test["output"]
                    ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
            else:
                assert result == expectationValidationResultSchema.load(
                    test["output"]
                ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
        else:
            # Convert result to json since our tests are reading from json so cannot easily contain richer types (e.g. NaN)
            # NOTE - 20191031 - JPC - we may eventually want to change these tests as we update our view on how
            # representations, serializations, and objects should interact and how much of that is shown to the user.
            result = result.to_json_dict()
            for key, value in test["output"].items():
                # Apply our great expectations-specific test logic

                if key == "success":
                    if isinstance(value, (np.floating, float)):
                        try:
                            assert np.allclose(
                                result["success"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['success']} not np.allclose to {value}"
                        except TypeError:
                            assert (
                                result["success"] == value
                            ), f"{result['success']} != {value}"
                    else:
                        assert result["success"] == value, f"{result['success']} != {value}"

                elif key == "observed_value":
                    if "tolerance" in test:
                        if isinstance(value, dict):
                            assert set(result["result"]["observed_value"].keys()) == set(
                                value.keys()
                            ), f"{set(result['result']['observed_value'].keys())} != {set(value.keys())}"
                            for k, v in value.items():
                                assert np.allclose(
                                    result["result"]["observed_value"][k],
                                    v,
                                    rtol=test["tolerance"],
                                )
                        else:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=test["tolerance"],
                            )
                    else:
                        if isinstance(value, dict) and "values" in value:
                            try:
                                assert np.allclose(
                                    result["result"]["observed_value"]["values"],
                                    value["values"],
                                    rtol=RTOL,
                                    atol=ATOL,
                                ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']['values']} not np.allclose to {value['values']}"
                            except TypeError as e:
                                print(e)
                                assert (
                                    result["result"]["observed_value"] == value
                                ), f"{result['result']['observed_value']} != {value}"
                        elif try_allclose:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {value}"
                        else:
                            assert (
                                result["result"]["observed_value"] == value
                            ), f"{result['result']['observed_value']} != {value}"

                # NOTE: This is a key used ONLY for testing cases where an expectation is legitimately allowed to return
                # any of multiple possible observed_values. expect_column_values_to_be_of_type is one such expectation.
                elif key == "observed_value_list":
                    assert result["result"]["observed_value"] in value

                elif key == "unexpected_index_list":
                    if isinstance(data_asset, (SqlAlchemyDataset, SparkDFDataset)):
                        pass
                    elif isinstance(data_asset, (SqlAlchemyBatchData, SparkDFBatchData)):
                        pass
                    else:
>                       assert (
                            result["result"]["unexpected_index_list"] == value
                        ), f"{result['result']['unexpected_index_list']} != {value}"
E                       AssertionError: [{'dates': '2018-01-01T00:00:00', 'pk_index': 0}] != [0]

great_expectations/self_check/util.py:3075: AssertionError
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 3968.12it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 414.42it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 587.33it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 325.59it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 438.52it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 336.43it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 522.66it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 485.96it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 483.91it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 481.53it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_decreasing:positive_test_with_mostly] _

test_case = {'expectation_type': 'expect_column_values_to_be_decreasing', 'pk_column': True, 'skip': False, 'test': {'exact_match_..., 'in': {'column': 'w', 'mostly': 0.6}, 'include_in_gallery': True, 'input': {'column': 'w', 'mostly': 0.6}, ...}, ...}

    @pytest.mark.order(index=0)
    @pytest.mark.integration
    @pytest.mark.slow  # 12.68s
    def test_case_runner_v3_api(test_case):
        if test_case["skip"]:
            pytest.skip()

        # Note: this should never be done in practice, but we are wiping expectations to reuse batches during testing.
        # test_case["batch"]._initialize_expectations()
        if "parse_strings_as_datetimes" in test_case["test"]["in"]:
            with pytest.deprecated_call():
                evaluate_json_test_v3_api(
                    validator=test_case["validator_with_data"],
                    expectation_type=test_case["expectation_type"],
                    test=test_case["test"],
                    pk_column=test_case["pk_column"],
                )
        else:
>           evaluate_json_test_v3_api(
                validator=test_case["validator_with_data"],
                expectation_type=test_case["expectation_type"],
                test=test_case["test"],
                pk_column=test_case["pk_column"],
            )

tests/test_definitions/test_expectations_v3_api.py:418:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
great_expectations/self_check/util.py:2901: in evaluate_json_test_v3_api
    check_json_test_result(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

test = {'exact_match_out': False, 'in': {'column': 'w', 'mostly': 0.6}, 'include_in_gallery': True, 'input': {'column': 'w', 'mostly': 0.6}, ...}
result = {'exception_info': {'exception_message': None, 'exception_traceback': None, 'raised_exception': False}, 'expectation_c...'partial_unexpected_counts': [{'count': 1, 'value': 2}, {'count': 1, 'value': 3}, {'count': 1, 'value': 7}], ...}, ...}
data_asset = <great_expectations.execution_engine.pandas_batch_data.PandasBatchData object at 0x7f9502b68d00>
pk_column = True

    def check_json_test_result(  # noqa: C901 - 52
        test, result, data_asset=None, pk_column=False
    ) -> None:

        # check for id_pk results in cases where pk_column is true and unexpected_index_list already exists
        # this will work for testing since result_format is COMPLETE
        if pk_column:
            if not result["success"]:
                if "unexpected_index_list" in result["result"]:
                    assert "unexpected_index_query" in result["result"]

        if "unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )
            elif "unexpected_list" in test["output"]:
                (
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )

        if "partial_unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "partial_unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )
            elif "partial_unexpected_list" in test["output"]:
                (
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )

        # Determine if np.allclose(..) might be needed for float comparison
        try_allclose = False
        if "observed_value" in test["output"]:
            if RX_FLOAT.match(repr(test["output"]["observed_value"])):
                try_allclose = True

        # Check results
        if test["exact_match_out"] is True:
            if "result" in result and "observed_value" in result["result"]:
                if isinstance(result["result"]["observed_value"], (np.floating, float)):
                    assert np.allclose(
                        result["result"]["observed_value"],
                        expectationValidationResultSchema.load(test["output"])["result"][
                            "observed_value"
                        ],
                        rtol=RTOL,
                        atol=ATOL,
                    ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {expectationValidationResultSchema.load(test['output'])['result']['observed_value']}"
                else:
                    assert result == expectationValidationResultSchema.load(
                        test["output"]
                    ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
            else:
                assert result == expectationValidationResultSchema.load(
                    test["output"]
                ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
        else:
            # Convert result to json since our tests are reading from json so cannot easily contain richer types (e.g. NaN)
            # NOTE - 20191031 - JPC - we may eventually want to change these tests as we update our view on how
            # representations, serializations, and objects should interact and how much of that is shown to the user.
            result = result.to_json_dict()
            for key, value in test["output"].items():
                # Apply our great expectations-specific test logic

                if key == "success":
                    if isinstance(value, (np.floating, float)):
                        try:
                            assert np.allclose(
                                result["success"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['success']} not np.allclose to {value}"
                        except TypeError:
                            assert (
                                result["success"] == value
                            ), f"{result['success']} != {value}"
                    else:
                        assert result["success"] == value, f"{result['success']} != {value}"

                elif key == "observed_value":
                    if "tolerance" in test:
                        if isinstance(value, dict):
                            assert set(result["result"]["observed_value"].keys()) == set(
                                value.keys()
                            ), f"{set(result['result']['observed_value'].keys())} != {set(value.keys())}"
                            for k, v in value.items():
                                assert np.allclose(
                                    result["result"]["observed_value"][k],
                                    v,
                                    rtol=test["tolerance"],
                                )
                        else:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=test["tolerance"],
                            )
                    else:
                        if isinstance(value, dict) and "values" in value:
                            try:
                                assert np.allclose(
                                    result["result"]["observed_value"]["values"],
                                    value["values"],
                                    rtol=RTOL,
                                    atol=ATOL,
                                ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']['values']} not np.allclose to {value['values']}"
                            except TypeError as e:
                                print(e)
                                assert (
                                    result["result"]["observed_value"] == value
                                ), f"{result['result']['observed_value']} != {value}"
                        elif try_allclose:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {value}"
                        else:
                            assert (
                                result["result"]["observed_value"] == value
                            ), f"{result['result']['observed_value']} != {value}"

                # NOTE: This is a key used ONLY for testing cases where an expectation is legitimately allowed to return
                # any of multiple possible observed_values. expect_column_values_to_be_of_type is one such expectation.
                elif key == "observed_value_list":
                    assert result["result"]["observed_value"] in value

                elif key == "unexpected_index_list":
                    if isinstance(data_asset, (SqlAlchemyDataset, SparkDFDataset)):
                        pass
                    elif isinstance(data_asset, (SqlAlchemyBatchData, SparkDFBatchData)):
                        pass
                    else:
>                       assert (
                            result["result"]["unexpected_index_list"] == value
                        ), f"{result['result']['unexpected_index_list']} != {value}"
E                       AssertionError: [{'w': 2, 'pk_index': 1}, {'w': 3, 'pk_index': 2}, {'w': 7, 'pk_index': 3}] != [1, 2, 3]

great_expectations/self_check/util.py:3075: AssertionError
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 3929.09it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 431.60it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 609.02it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 331.71it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 422.19it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 329.66it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 527.51it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 491.93it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 490.01it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 487.69it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_decreasing:basic_negative_test] _

test_case = {'expectation_type': 'expect_column_values_to_be_decreasing', 'pk_column': True, 'skip': False, 'test': {'exact_match_out': False, 'in': {'column': 'y'}, 'include_in_gallery': True, 'input': {'column': 'y'}, ...}, ...}

    @pytest.mark.order(index=0)
    @pytest.mark.integration
    @pytest.mark.slow  # 12.68s
    def test_case_runner_v3_api(test_case):
        if test_case["skip"]:
            pytest.skip()

        # Note: this should never be done in practice, but we are wiping expectations to reuse batches during testing.
        # test_case["batch"]._initialize_expectations()
        if "parse_strings_as_datetimes" in test_case["test"]["in"]:
            with pytest.deprecated_call():
                evaluate_json_test_v3_api(
                    validator=test_case["validator_with_data"],
                    expectation_type=test_case["expectation_type"],
                    test=test_case["test"],
                    pk_column=test_case["pk_column"],
                )
        else:
>           evaluate_json_test_v3_api(
                validator=test_case["validator_with_data"],
                expectation_type=test_case["expectation_type"],
                test=test_case["test"],
                pk_column=test_case["pk_column"],
            )

tests/test_definitions/test_expectations_v3_api.py:418:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
great_expectations/self_check/util.py:2901: in evaluate_json_test_v3_api
    check_json_test_result(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

test = {'exact_match_out': False, 'in': {'column': 'y'}, 'include_in_gallery': True, 'input': {'column': 'y'}, ...}
result = {'exception_info': {'exception_message': None, 'exception_traceback': None, 'raised_exception': False}, 'expectation_c...'partial_unexpected_counts': [{'count': 1, 'value': 2}, {'count': 1, 'value': 3}, {'count': 1, 'value': 4}], ...}, ...}
data_asset = <great_expectations.execution_engine.pandas_batch_data.PandasBatchData object at 0x7f9502b68d00>
pk_column = True

    def check_json_test_result(  # noqa: C901 - 52
        test, result, data_asset=None, pk_column=False
    ) -> None:

        # check for id_pk results in cases where pk_column is true and unexpected_index_list already exists
        # this will work for testing since result_format is COMPLETE
        if pk_column:
            if not result["success"]:
                if "unexpected_index_list" in result["result"]:
                    assert "unexpected_index_query" in result["result"]

        if "unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )
            elif "unexpected_list" in test["output"]:
                (
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )

        if "partial_unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "partial_unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )
            elif "partial_unexpected_list" in test["output"]:
                (
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )

        # Determine if np.allclose(..) might be needed for float comparison
        try_allclose = False
        if "observed_value" in test["output"]:
            if RX_FLOAT.match(repr(test["output"]["observed_value"])):
                try_allclose = True

        # Check results
        if test["exact_match_out"] is True:
            if "result" in result and "observed_value" in result["result"]:
                if isinstance(result["result"]["observed_value"], (np.floating, float)):
                    assert np.allclose(
                        result["result"]["observed_value"],
                        expectationValidationResultSchema.load(test["output"])["result"][
                            "observed_value"
                        ],
                        rtol=RTOL,
                        atol=ATOL,
                    ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {expectationValidationResultSchema.load(test['output'])['result']['observed_value']}"
                else:
                    assert result == expectationValidationResultSchema.load(
                        test["output"]
                    ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
            else:
                assert result == expectationValidationResultSchema.load(
                    test["output"]
                ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
        else:
            # Convert result to json since our tests are reading from json so cannot easily contain richer types (e.g. NaN)
            # NOTE - 20191031 - JPC - we may eventually want to change these tests as we update our view on how
            # representations, serializations, and objects should interact and how much of that is shown to the user.
            result = result.to_json_dict()
            for key, value in test["output"].items():
                # Apply our great expectations-specific test logic

                if key == "success":
                    if isinstance(value, (np.floating, float)):
                        try:
                            assert np.allclose(
                                result["success"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['success']} not np.allclose to {value}"
                        except TypeError:
                            assert (
                                result["success"] == value
                            ), f"{result['success']} != {value}"
                    else:
                        assert result["success"] == value, f"{result['success']} != {value}"

                elif key == "observed_value":
                    if "tolerance" in test:
                        if isinstance(value, dict):
                            assert set(result["result"]["observed_value"].keys()) == set(
                                value.keys()
                            ), f"{set(result['result']['observed_value'].keys())} != {set(value.keys())}"
                            for k, v in value.items():
                                assert np.allclose(
                                    result["result"]["observed_value"][k],
                                    v,
                                    rtol=test["tolerance"],
                                )
                        else:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=test["tolerance"],
                            )
                    else:
                        if isinstance(value, dict) and "values" in value:
                            try:
                                assert np.allclose(
                                    result["result"]["observed_value"]["values"],
                                    value["values"],
                                    rtol=RTOL,
                                    atol=ATOL,
                                ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']['values']} not np.allclose to {value['values']}"
                            except TypeError as e:
                                print(e)
                                assert (
                                    result["result"]["observed_value"] == value
                                ), f"{result['result']['observed_value']} != {value}"
                        elif try_allclose:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {value}"
                        else:
                            assert (
                                result["result"]["observed_value"] == value
                            ), f"{result['result']['observed_value']} != {value}"

                # NOTE: This is a key used ONLY for testing cases where an expectation is legitimately allowed to return
                # any of multiple possible observed_values. expect_column_values_to_be_of_type is one such expectation.
                elif key == "observed_value_list":
                    assert result["result"]["observed_value"] in value

                elif key == "unexpected_index_list":
                    if isinstance(data_asset, (SqlAlchemyDataset, SparkDFDataset)):
                        pass
                    elif isinstance(data_asset, (SqlAlchemyBatchData, SparkDFBatchData)):
                        pass
                    else:
>                       assert (
                            result["result"]["unexpected_index_list"] == value
                        ), f"{result['result']['unexpected_index_list']} != {value}"
E                       AssertionError: [{'y': 2, 'pk_index': 3}, {'y': 3, 'pk_index': 6}, {'y': 4, 'pk_index': 9}] != [3, 6, 9]

great_expectations/self_check/util.py:3075: AssertionError
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 2439.26it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 323.01it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 460.69it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 276.40it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 375.57it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 300.41it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 484.87it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 453.87it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 452.10it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 450.13it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_decreasing:basic_negative_test_with_strictly] _

test_case = {'expectation_type': 'expect_column_values_to_be_decreasing', 'pk_column': True, 'skip': False, 'test': {'exact_match_...cess': False, 'unexpected_index_list': [1, 2, 3, 4, 5, 6, ...], 'unexpected_list': [1, 1, 2, 2, 2, 3, ...]}, ...}, ...}

    @pytest.mark.order(index=0)
    @pytest.mark.integration
    @pytest.mark.slow  # 12.68s
    def test_case_runner_v3_api(test_case):
        if test_case["skip"]:
            pytest.skip()

        # Note: this should never be done in practice, but we are wiping expectations to reuse batches during testing.
        # test_case["batch"]._initialize_expectations()
        if "parse_strings_as_datetimes" in test_case["test"]["in"]:
            with pytest.deprecated_call():
                evaluate_json_test_v3_api(
                    validator=test_case["validator_with_data"],
                    expectation_type=test_case["expectation_type"],
                    test=test_case["test"],
                    pk_column=test_case["pk_column"],
                )
        else:
>           evaluate_json_test_v3_api(
                validator=test_case["validator_with_data"],
                expectation_type=test_case["expectation_type"],
                test=test_case["test"],
                pk_column=test_case["pk_column"],
            )

tests/test_definitions/test_expectations_v3_api.py:418:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
great_expectations/self_check/util.py:2901: in evaluate_json_test_v3_api
    check_json_test_result(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

test = {'exact_match_out': False, 'in': {'column': 'y', 'strictly': True}, 'input': {'column': 'y', 'strictly': True}, 'out': {'success': False, 'unexpected_index_list': [1, 2, 3, 4, 5, 6, ...], 'unexpected_list': [1, 1, 2, 2, 2, 3, ...]}, ...}
result = {'exception_info': {'exception_message': None, 'exception_traceback': None, 'raised_exception': False}, 'expectation_c...': [{'count': 3, 'value': 2}, {'count': 3, 'value': 3}, {'count': 2, 'value': 1}, {'count': 1, 'value': 4}], ...}, ...}
data_asset = <great_expectations.execution_engine.pandas_batch_data.PandasBatchData object at 0x7f9502b68d00>
pk_column = True

    def check_json_test_result(  # noqa: C901 - 52
        test, result, data_asset=None, pk_column=False
    ) -> None:

        # check for id_pk results in cases where pk_column is true and unexpected_index_list already exists
        # this will work for testing since result_format is COMPLETE
        if pk_column:
            if not result["success"]:
                if "unexpected_index_list" in result["result"]:
                    assert "unexpected_index_query" in result["result"]

        if "unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )
            elif "unexpected_list" in test["output"]:
                (
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )

        if "partial_unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "partial_unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )
            elif "partial_unexpected_list" in test["output"]:
                (
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )

        # Determine if np.allclose(..) might be needed for float comparison
        try_allclose = False
        if "observed_value" in test["output"]:
            if RX_FLOAT.match(repr(test["output"]["observed_value"])):
                try_allclose = True

        # Check results
        if test["exact_match_out"] is True:
            if "result" in result and "observed_value" in result["result"]:
                if isinstance(result["result"]["observed_value"], (np.floating, float)):
                    assert np.allclose(
                        result["result"]["observed_value"],
                        expectationValidationResultSchema.load(test["output"])["result"][
                            "observed_value"
                        ],
                        rtol=RTOL,
                        atol=ATOL,
                    ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {expectationValidationResultSchema.load(test['output'])['result']['observed_value']}"
                else:
                    assert result == expectationValidationResultSchema.load(
                        test["output"]
                    ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
            else:
                assert result == expectationValidationResultSchema.load(
                    test["output"]
                ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
        else:
            # Convert result to json since our tests are reading from json so cannot easily contain richer types (e.g. NaN)
            # NOTE - 20191031 - JPC - we may eventually want to change these tests as we update our view on how
            # representations, serializations, and objects should interact and how much of that is shown to the user.
            result = result.to_json_dict()
            for key, value in test["output"].items():
                # Apply our great expectations-specific test logic

                if key == "success":
                    if isinstance(value, (np.floating, float)):
                        try:
                            assert np.allclose(
                                result["success"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['success']} not np.allclose to {value}"
                        except TypeError:
                            assert (
                                result["success"] == value
                            ), f"{result['success']} != {value}"
                    else:
                        assert result["success"] == value, f"{result['success']} != {value}"

                elif key == "observed_value":
                    if "tolerance" in test:
                        if isinstance(value, dict):
                            assert set(result["result"]["observed_value"].keys()) == set(
                                value.keys()
                            ), f"{set(result['result']['observed_value'].keys())} != {set(value.keys())}"
                            for k, v in value.items():
                                assert np.allclose(
                                    result["result"]["observed_value"][k],
                                    v,
                                    rtol=test["tolerance"],
                                )
                        else:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=test["tolerance"],
                            )
                    else:
                        if isinstance(value, dict) and "values" in value:
                            try:
                                assert np.allclose(
                                    result["result"]["observed_value"]["values"],
                                    value["values"],
                                    rtol=RTOL,
                                    atol=ATOL,
                                ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']['values']} not np.allclose to {value['values']}"
                            except TypeError as e:
                                print(e)
                                assert (
                                    result["result"]["observed_value"] == value
                                ), f"{result['result']['observed_value']} != {value}"
                        elif try_allclose:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {value}"
                        else:
                            assert (
                                result["result"]["observed_value"] == value
                            ), f"{result['result']['observed_value']} != {value}"

                # NOTE: This is a key used ONLY for testing cases where an expectation is legitimately allowed to return
                # any of multiple possible observed_values. expect_column_values_to_be_of_type is one such expectation.
                elif key == "observed_value_list":
                    assert result["result"]["observed_value"] in value

                elif key == "unexpected_index_list":
                    if isinstance(data_asset, (SqlAlchemyDataset, SparkDFDataset)):
                        pass
                    elif isinstance(data_asset, (SqlAlchemyBatchData, SparkDFBatchData)):
                        pass
                    else:
>                       assert (
                            result["result"]["unexpected_index_list"] == value
                        ), f"{result['result']['unexpected_index_list']} != {value}"
E                       AssertionError: [{'y': 1, 'pk_index': 1}, {'y': 1, 'pk_index': 2}, {'y': 2, 'pk_index': 3}, {'y': 2, 'pk_index': 4}, {'y': 2, 'pk_index': 5}, {'y': 3, 'pk_index': 6}, {'y': 3, 'pk_index': 7}, {'y': 3, 'pk_index': 8}, {'y': 4, 'pk_index': 9}] != [1, 2, 3, 4, 5, 6, 7, 8, 9]

great_expectations/self_check/util.py:3075: AssertionError
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 4082.05it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 433.86it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 615.51it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 340.42it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 446.84it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 345.88it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 540.61it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 499.83it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 497.24it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 494.89it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_not_match_regex:negative_test_insufficient_mostly_and_one_non_matching_value] _

test_case = {'expectation_type': 'expect_column_values_to_not_match_regex', 'pk_column': True, 'skip': False, 'test': {'exact_matc...y': 0.3, 'regex': '^a'}, 'include_in_gallery': True, 'input': {'column': 'a', 'mostly': 0.3, 'regex': '^a'}, ...}, ...}

    @pytest.mark.order(index=0)
    @pytest.mark.integration
    @pytest.mark.slow  # 12.68s
    def test_case_runner_v3_api(test_case):
        if test_case["skip"]:
            pytest.skip()

        # Note: this should never be done in practice, but we are wiping expectations to reuse batches during testing.
        # test_case["batch"]._initialize_expectations()
        if "parse_strings_as_datetimes" in test_case["test"]["in"]:
            with pytest.deprecated_call():
                evaluate_json_test_v3_api(
                    validator=test_case["validator_with_data"],
                    expectation_type=test_case["expectation_type"],
                    test=test_case["test"],
                    pk_column=test_case["pk_column"],
                )
        else:
>           evaluate_json_test_v3_api(
                validator=test_case["validator_with_data"],
                expectation_type=test_case["expectation_type"],
                test=test_case["test"],
                pk_column=test_case["pk_column"],
            )

tests/test_definitions/test_expectations_v3_api.py:418:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
great_expectations/self_check/util.py:2901: in evaluate_json_test_v3_api
    check_json_test_result(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

test = {'exact_match_out': False, 'in': {'column': 'a', 'mostly': 0.3, 'regex': '^a'}, 'include_in_gallery': True, 'input': {'column': 'a', 'mostly': 0.3, 'regex': '^a'}, ...}
result = {'exception_info': {'exception_message': None, 'exception_traceback': None, 'raised_exception': False}, 'expectation_c... 'value': 'aaa'}, {'count': 1, 'value': 'abb'}, {'count': 1, 'value': 'acc'}, {'count': 1, 'value': 'add'}], ...}, ...}
data_asset = <great_expectations.execution_engine.pandas_batch_data.PandasBatchData object at 0x7f9502a6e310>
pk_column = True

    def check_json_test_result(  # noqa: C901 - 52
        test, result, data_asset=None, pk_column=False
    ) -> None:

        # check for id_pk results in cases where pk_column is true and unexpected_index_list already exists
        # this will work for testing since result_format is COMPLETE
        if pk_column:
            if not result["success"]:
                if "unexpected_index_list" in result["result"]:
                    assert "unexpected_index_query" in result["result"]

        if "unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )
            elif "unexpected_list" in test["output"]:
                (
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )

        if "partial_unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "partial_unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )
            elif "partial_unexpected_list" in test["output"]:
                (
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )

        # Determine if np.allclose(..) might be needed for float comparison
        try_allclose = False
        if "observed_value" in test["output"]:
            if RX_FLOAT.match(repr(test["output"]["observed_value"])):
                try_allclose = True

        # Check results
        if test["exact_match_out"] is True:
            if "result" in result and "observed_value" in result["result"]:
                if isinstance(result["result"]["observed_value"], (np.floating, float)):
                    assert np.allclose(
                        result["result"]["observed_value"],
                        expectationValidationResultSchema.load(test["output"])["result"][
                            "observed_value"
                        ],
                        rtol=RTOL,
                        atol=ATOL,
                    ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {expectationValidationResultSchema.load(test['output'])['result']['observed_value']}"
                else:
                    assert result == expectationValidationResultSchema.load(
                        test["output"]
                    ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
            else:
                assert result == expectationValidationResultSchema.load(
                    test["output"]
                ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
        else:
            # Convert result to json since our tests are reading from json so cannot easily contain richer types (e.g. NaN)
            # NOTE - 20191031 - JPC - we may eventually want to change these tests as we update our view on how
            # representations, serializations, and objects should interact and how much of that is shown to the user.
            result = result.to_json_dict()
            for key, value in test["output"].items():
                # Apply our great expectations-specific test logic

                if key == "success":
                    if isinstance(value, (np.floating, float)):
                        try:
                            assert np.allclose(
                                result["success"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['success']} not np.allclose to {value}"
                        except TypeError:
                            assert (
                                result["success"] == value
                            ), f"{result['success']} != {value}"
                    else:
                        assert result["success"] == value, f"{result['success']} != {value}"

                elif key == "observed_value":
                    if "tolerance" in test:
                        if isinstance(value, dict):
                            assert set(result["result"]["observed_value"].keys()) == set(
                                value.keys()
                            ), f"{set(result['result']['observed_value'].keys())} != {set(value.keys())}"
                            for k, v in value.items():
                                assert np.allclose(
                                    result["result"]["observed_value"][k],
                                    v,
                                    rtol=test["tolerance"],
                                )
                        else:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=test["tolerance"],
                            )
                    else:
                        if isinstance(value, dict) and "values" in value:
                            try:
                                assert np.allclose(
                                    result["result"]["observed_value"]["values"],
                                    value["values"],
                                    rtol=RTOL,
                                    atol=ATOL,
                                ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']['values']} not np.allclose to {value['values']}"
                            except TypeError as e:
                                print(e)
                                assert (
                                    result["result"]["observed_value"] == value
                                ), f"{result['result']['observed_value']} != {value}"
                        elif try_allclose:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {value}"
                        else:
                            assert (
                                result["result"]["observed_value"] == value
                            ), f"{result['result']['observed_value']} != {value}"

                # NOTE: This is a key used ONLY for testing cases where an expectation is legitimately allowed to return
                # any of multiple possible observed_values. expect_column_values_to_be_of_type is one such expectation.
                elif key == "observed_value_list":
                    assert result["result"]["observed_value"] in value

                elif key == "unexpected_index_list":
                    if isinstance(data_asset, (SqlAlchemyDataset, SparkDFDataset)):
                        pass
                    elif isinstance(data_asset, (SqlAlchemyBatchData, SparkDFBatchData)):
                        pass
                    else:
>                       assert (
                            result["result"]["unexpected_index_list"] == value
                        ), f"{result['result']['unexpected_index_list']} != {value}"
E                       AssertionError: [{'a': 'aaa', 'pk_index': 0}, {'a': 'abb', 'pk_index': 1}, {'a': 'acc', 'pk_index': 2}, {'a': 'add', 'pk_index': 3}] != [0, 1, 2, 3]

great_expectations/self_check/util.py:3075: AssertionError
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 3795.75it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 429.28it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 607.93it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 343.53it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 475.11it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 367.14it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 576.57it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 535.59it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 533.17it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 530.40it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_not_match_regex:positive_test_exact_mostly_w_one_non_matching_value] _

test_case = {'expectation_type': 'expect_column_values_to_not_match_regex', 'pk_column': True, 'skip': False, 'test': {'exact_matc...y': 0.2, 'regex': '^a'}, 'include_in_gallery': True, 'input': {'column': 'a', 'mostly': 0.2, 'regex': '^a'}, ...}, ...}

    @pytest.mark.order(index=0)
    @pytest.mark.integration
    @pytest.mark.slow  # 12.68s
    def test_case_runner_v3_api(test_case):
        if test_case["skip"]:
            pytest.skip()

        # Note: this should never be done in practice, but we are wiping expectations to reuse batches during testing.
        # test_case["batch"]._initialize_expectations()
        if "parse_strings_as_datetimes" in test_case["test"]["in"]:
            with pytest.deprecated_call():
                evaluate_json_test_v3_api(
                    validator=test_case["validator_with_data"],
                    expectation_type=test_case["expectation_type"],
                    test=test_case["test"],
                    pk_column=test_case["pk_column"],
                )
        else:
>           evaluate_json_test_v3_api(
                validator=test_case["validator_with_data"],
                expectation_type=test_case["expectation_type"],
                test=test_case["test"],
                pk_column=test_case["pk_column"],
            )

tests/test_definitions/test_expectations_v3_api.py:418:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
great_expectations/self_check/util.py:2901: in evaluate_json_test_v3_api
    check_json_test_result(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

test = {'exact_match_out': False, 'in': {'column': 'a', 'mostly': 0.2, 'regex': '^a'}, 'include_in_gallery': True, 'input': {'column': 'a', 'mostly': 0.2, 'regex': '^a'}, ...}
result = {'exception_info': {'exception_message': None, 'exception_traceback': None, 'raised_exception': False}, 'expectation_c... 'value': 'aaa'}, {'count': 1, 'value': 'abb'}, {'count': 1, 'value': 'acc'}, {'count': 1, 'value': 'add'}], ...}, ...}
data_asset = <great_expectations.execution_engine.pandas_batch_data.PandasBatchData object at 0x7f9502a6e310>
pk_column = True

    def check_json_test_result(  # noqa: C901 - 52
        test, result, data_asset=None, pk_column=False
    ) -> None:

        # check for id_pk results in cases where pk_column is true and unexpected_index_list already exists
        # this will work for testing since result_format is COMPLETE
        if pk_column:
            if not result["success"]:
                if "unexpected_index_list" in result["result"]:
                    assert "unexpected_index_query" in result["result"]

        if "unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )
            elif "unexpected_list" in test["output"]:
                (
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )

        if "partial_unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "partial_unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )
            elif "partial_unexpected_list" in test["output"]:
                (
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )

        # Determine if np.allclose(..) might be needed for float comparison
        try_allclose = False
        if "observed_value" in test["output"]:
            if RX_FLOAT.match(repr(test["output"]["observed_value"])):
                try_allclose = True

        # Check results
        if test["exact_match_out"] is True:
            if "result" in result and "observed_value" in result["result"]:
                if isinstance(result["result"]["observed_value"], (np.floating, float)):
                    assert np.allclose(
                        result["result"]["observed_value"],
                        expectationValidationResultSchema.load(test["output"])["result"][
                            "observed_value"
                        ],
                        rtol=RTOL,
                        atol=ATOL,
                    ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {expectationValidationResultSchema.load(test['output'])['result']['observed_value']}"
                else:
                    assert result == expectationValidationResultSchema.load(
                        test["output"]
                    ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
            else:
                assert result == expectationValidationResultSchema.load(
                    test["output"]
                ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
        else:
            # Convert result to json since our tests are reading from json so cannot easily contain richer types (e.g. NaN)
            # NOTE - 20191031 - JPC - we may eventually want to change these tests as we update our view on how
            # representations, serializations, and objects should interact and how much of that is shown to the user.
            result = result.to_json_dict()
            for key, value in test["output"].items():
                # Apply our great expectations-specific test logic

                if key == "success":
                    if isinstance(value, (np.floating, float)):
                        try:
                            assert np.allclose(
                                result["success"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['success']} not np.allclose to {value}"
                        except TypeError:
                            assert (
                                result["success"] == value
                            ), f"{result['success']} != {value}"
                    else:
                        assert result["success"] == value, f"{result['success']} != {value}"

                elif key == "observed_value":
                    if "tolerance" in test:
                        if isinstance(value, dict):
                            assert set(result["result"]["observed_value"].keys()) == set(
                                value.keys()
                            ), f"{set(result['result']['observed_value'].keys())} != {set(value.keys())}"
                            for k, v in value.items():
                                assert np.allclose(
                                    result["result"]["observed_value"][k],
                                    v,
                                    rtol=test["tolerance"],
                                )
                        else:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=test["tolerance"],
                            )
                    else:
                        if isinstance(value, dict) and "values" in value:
                            try:
                                assert np.allclose(
                                    result["result"]["observed_value"]["values"],
                                    value["values"],
                                    rtol=RTOL,
                                    atol=ATOL,
                                ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']['values']} not np.allclose to {value['values']}"
                            except TypeError as e:
                                print(e)
                                assert (
                                    result["result"]["observed_value"] == value
                                ), f"{result['result']['observed_value']} != {value}"
                        elif try_allclose:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {value}"
                        else:
                            assert (
                                result["result"]["observed_value"] == value
                            ), f"{result['result']['observed_value']} != {value}"

                # NOTE: This is a key used ONLY for testing cases where an expectation is legitimately allowed to return
                # any of multiple possible observed_values. expect_column_values_to_be_of_type is one such expectation.
                elif key == "observed_value_list":
                    assert result["result"]["observed_value"] in value

                elif key == "unexpected_index_list":
                    if isinstance(data_asset, (SqlAlchemyDataset, SparkDFDataset)):
                        pass
                    elif isinstance(data_asset, (SqlAlchemyBatchData, SparkDFBatchData)):
                        pass
                    else:
>                       assert (
                            result["result"]["unexpected_index_list"] == value
                        ), f"{result['result']['unexpected_index_list']} != {value}"
E                       AssertionError: [{'a': 'aaa', 'pk_index': 0}, {'a': 'abb', 'pk_index': 1}, {'a': 'acc', 'pk_index': 2}, {'a': 'add', 'pk_index': 3}] != [0, 1, 2, 3]

great_expectations/self_check/util.py:3075: AssertionError
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 4090.01it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 449.94it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 636.66it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 354.07it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 491.55it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 378.25it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 590.88it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 547.49it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 544.64it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 541.74it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_not_match_regex:positive_test_sufficient_mostly_w_one_non_matching_value] _

test_case = {'expectation_type': 'expect_column_values_to_not_match_regex', 'pk_column': True, 'skip': False, 'test': {'exact_matc...: {'success': True, 'unexpected_index_list': [0, 1, 2, 3], 'unexpected_list': ['aaa', 'abb', 'acc', 'add']}, ...}, ...}

    @pytest.mark.order(index=0)
    @pytest.mark.integration
    @pytest.mark.slow  # 12.68s
    def test_case_runner_v3_api(test_case):
        if test_case["skip"]:
            pytest.skip()

        # Note: this should never be done in practice, but we are wiping expectations to reuse batches during testing.
        # test_case["batch"]._initialize_expectations()
        if "parse_strings_as_datetimes" in test_case["test"]["in"]:
            with pytest.deprecated_call():
                evaluate_json_test_v3_api(
                    validator=test_case["validator_with_data"],
                    expectation_type=test_case["expectation_type"],
                    test=test_case["test"],
                    pk_column=test_case["pk_column"],
                )
        else:
>           evaluate_json_test_v3_api(
                validator=test_case["validator_with_data"],
                expectation_type=test_case["expectation_type"],
                test=test_case["test"],
                pk_column=test_case["pk_column"],
            )

tests/test_definitions/test_expectations_v3_api.py:418:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
great_expectations/self_check/util.py:2901: in evaluate_json_test_v3_api
    check_json_test_result(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

test = {'exact_match_out': False, 'in': {'column': 'a', 'mostly': 0.1, 'regex': '^a'}, 'input': {'column': 'a', 'mostly': 0.1... 'out': {'success': True, 'unexpected_index_list': [0, 1, 2, 3], 'unexpected_list': ['aaa', 'abb', 'acc', 'add']}, ...}
result = {'exception_info': {'exception_message': None, 'exception_traceback': None, 'raised_exception': False}, 'expectation_c... 'value': 'aaa'}, {'count': 1, 'value': 'abb'}, {'count': 1, 'value': 'acc'}, {'count': 1, 'value': 'add'}], ...}, ...}
data_asset = <great_expectations.execution_engine.pandas_batch_data.PandasBatchData object at 0x7f9502a6e310>
pk_column = True

    def check_json_test_result(  # noqa: C901 - 52
        test, result, data_asset=None, pk_column=False
    ) -> None:

        # check for id_pk results in cases where pk_column is true and unexpected_index_list already exists
        # this will work for testing since result_format is COMPLETE
        if pk_column:
            if not result["success"]:
                if "unexpected_index_list" in result["result"]:
                    assert "unexpected_index_query" in result["result"]

        if "unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )
            elif "unexpected_list" in test["output"]:
                (
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )

        if "partial_unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "partial_unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )
            elif "partial_unexpected_list" in test["output"]:
                (
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )

        # Determine if np.allclose(..) might be needed for float comparison
        try_allclose = False
        if "observed_value" in test["output"]:
            if RX_FLOAT.match(repr(test["output"]["observed_value"])):
                try_allclose = True

        # Check results
        if test["exact_match_out"] is True:
            if "result" in result and "observed_value" in result["result"]:
                if isinstance(result["result"]["observed_value"], (np.floating, float)):
                    assert np.allclose(
                        result["result"]["observed_value"],
                        expectationValidationResultSchema.load(test["output"])["result"][
                            "observed_value"
                        ],
                        rtol=RTOL,
                        atol=ATOL,
                    ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {expectationValidationResultSchema.load(test['output'])['result']['observed_value']}"
                else:
                    assert result == expectationValidationResultSchema.load(
                        test["output"]
                    ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
            else:
                assert result == expectationValidationResultSchema.load(
                    test["output"]
                ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
        else:
            # Convert result to json since our tests are reading from json so cannot easily contain richer types (e.g. NaN)
            # NOTE - 20191031 - JPC - we may eventually want to change these tests as we update our view on how
            # representations, serializations, and objects should interact and how much of that is shown to the user.
            result = result.to_json_dict()
            for key, value in test["output"].items():
                # Apply our great expectations-specific test logic

                if key == "success":
                    if isinstance(value, (np.floating, float)):
                        try:
                            assert np.allclose(
                                result["success"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['success']} not np.allclose to {value}"
                        except TypeError:
                            assert (
                                result["success"] == value
                            ), f"{result['success']} != {value}"
                    else:
                        assert result["success"] == value, f"{result['success']} != {value}"

                elif key == "observed_value":
                    if "tolerance" in test:
                        if isinstance(value, dict):
                            assert set(result["result"]["observed_value"].keys()) == set(
                                value.keys()
                            ), f"{set(result['result']['observed_value'].keys())} != {set(value.keys())}"
                            for k, v in value.items():
                                assert np.allclose(
                                    result["result"]["observed_value"][k],
                                    v,
                                    rtol=test["tolerance"],
                                )
                        else:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=test["tolerance"],
                            )
                    else:
                        if isinstance(value, dict) and "values" in value:
                            try:
                                assert np.allclose(
                                    result["result"]["observed_value"]["values"],
                                    value["values"],
                                    rtol=RTOL,
                                    atol=ATOL,
                                ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']['values']} not np.allclose to {value['values']}"
                            except TypeError as e:
                                print(e)
                                assert (
                                    result["result"]["observed_value"] == value
                                ), f"{result['result']['observed_value']} != {value}"
                        elif try_allclose:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {value}"
                        else:
                            assert (
                                result["result"]["observed_value"] == value
                            ), f"{result['result']['observed_value']} != {value}"

                # NOTE: This is a key used ONLY for testing cases where an expectation is legitimately allowed to return
                # any of multiple possible observed_values. expect_column_values_to_be_of_type is one such expectation.
                elif key == "observed_value_list":
                    assert result["result"]["observed_value"] in value

                elif key == "unexpected_index_list":
                    if isinstance(data_asset, (SqlAlchemyDataset, SparkDFDataset)):
                        pass
                    elif isinstance(data_asset, (SqlAlchemyBatchData, SparkDFBatchData)):
                        pass
                    else:
>                       assert (
                            result["result"]["unexpected_index_list"] == value
                        ), f"{result['result']['unexpected_index_list']} != {value}"
E                       AssertionError: [{'a': 'aaa', 'pk_index': 0}, {'a': 'abb', 'pk_index': 1}, {'a': 'acc', 'pk_index': 2}, {'a': 'add', 'pk_index': 3}] != [0, 1, 2, 3]

great_expectations/self_check/util.py:3075: AssertionError
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 4148.67it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 450.98it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 637.24it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 356.51it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 495.59it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 379.40it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 584.73it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 541.95it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 539.15it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 536.31it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_not_match_regex:negative_test_one_missing_value_and_insufficent_mostly] _

test_case = {'expectation_type': 'expect_column_values_to_not_match_regex', 'pk_column': True, 'skip': False, 'test': {'exact_matc...'}, 'out': {'success': False, 'unexpected_index_list': [0, 1, 2], 'unexpected_list': ['aaa', 'abb', 'acc']}, ...}, ...}

    @pytest.mark.order(index=0)
    @pytest.mark.integration
    @pytest.mark.slow  # 12.68s
    def test_case_runner_v3_api(test_case):
        if test_case["skip"]:
            pytest.skip()

        # Note: this should never be done in practice, but we are wiping expectations to reuse batches during testing.
        # test_case["batch"]._initialize_expectations()
        if "parse_strings_as_datetimes" in test_case["test"]["in"]:
            with pytest.deprecated_call():
                evaluate_json_test_v3_api(
                    validator=test_case["validator_with_data"],
                    expectation_type=test_case["expectation_type"],
                    test=test_case["test"],
                    pk_column=test_case["pk_column"],
                )
        else:
>           evaluate_json_test_v3_api(
                validator=test_case["validator_with_data"],
                expectation_type=test_case["expectation_type"],
                test=test_case["test"],
                pk_column=test_case["pk_column"],
            )

tests/test_definitions/test_expectations_v3_api.py:418:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
great_expectations/self_check/util.py:2901: in evaluate_json_test_v3_api
    check_json_test_result(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

test = {'exact_match_out': False, 'in': {'column': 'b', 'mostly': 0.5, 'regex': '^a'}, 'input': {'column': 'b', 'mostly': 0.5...': '^a'}, 'out': {'success': False, 'unexpected_index_list': [0, 1, 2], 'unexpected_list': ['aaa', 'abb', 'acc']}, ...}
result = {'exception_info': {'exception_message': None, 'exception_traceback': None, 'raised_exception': False}, 'expectation_c...xpected_counts': [{'count': 1, 'value': 'aaa'}, {'count': 1, 'value': 'abb'}, {'count': 1, 'value': 'acc'}], ...}, ...}
data_asset = <great_expectations.execution_engine.pandas_batch_data.PandasBatchData object at 0x7f9502a6e310>
pk_column = True

    def check_json_test_result(  # noqa: C901 - 52
        test, result, data_asset=None, pk_column=False
    ) -> None:

        # check for id_pk results in cases where pk_column is true and unexpected_index_list already exists
        # this will work for testing since result_format is COMPLETE
        if pk_column:
            if not result["success"]:
                if "unexpected_index_list" in result["result"]:
                    assert "unexpected_index_query" in result["result"]

        if "unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )
            elif "unexpected_list" in test["output"]:
                (
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )

        if "partial_unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "partial_unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )
            elif "partial_unexpected_list" in test["output"]:
                (
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )

        # Determine if np.allclose(..) might be needed for float comparison
        try_allclose = False
        if "observed_value" in test["output"]:
            if RX_FLOAT.match(repr(test["output"]["observed_value"])):
                try_allclose = True

        # Check results
        if test["exact_match_out"] is True:
            if "result" in result and "observed_value" in result["result"]:
                if isinstance(result["result"]["observed_value"], (np.floating, float)):
                    assert np.allclose(
                        result["result"]["observed_value"],
                        expectationValidationResultSchema.load(test["output"])["result"][
                            "observed_value"
                        ],
                        rtol=RTOL,
                        atol=ATOL,
                    ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {expectationValidationResultSchema.load(test['output'])['result']['observed_value']}"
                else:
                    assert result == expectationValidationResultSchema.load(
                        test["output"]
                    ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
            else:
                assert result == expectationValidationResultSchema.load(
                    test["output"]
                ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
        else:
            # Convert result to json since our tests are reading from json so cannot easily contain richer types (e.g. NaN)
            # NOTE - 20191031 - JPC - we may eventually want to change these tests as we update our view on how
            # representations, serializations, and objects should interact and how much of that is shown to the user.
            result = result.to_json_dict()
            for key, value in test["output"].items():
                # Apply our great expectations-specific test logic

                if key == "success":
                    if isinstance(value, (np.floating, float)):
                        try:
                            assert np.allclose(
                                result["success"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['success']} not np.allclose to {value}"
                        except TypeError:
                            assert (
                                result["success"] == value
                            ), f"{result['success']} != {value}"
                    else:
                        assert result["success"] == value, f"{result['success']} != {value}"

                elif key == "observed_value":
                    if "tolerance" in test:
                        if isinstance(value, dict):
                            assert set(result["result"]["observed_value"].keys()) == set(
                                value.keys()
                            ), f"{set(result['result']['observed_value'].keys())} != {set(value.keys())}"
                            for k, v in value.items():
                                assert np.allclose(
                                    result["result"]["observed_value"][k],
                                    v,
                                    rtol=test["tolerance"],
                                )
                        else:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=test["tolerance"],
                            )
                    else:
                        if isinstance(value, dict) and "values" in value:
                            try:
                                assert np.allclose(
                                    result["result"]["observed_value"]["values"],
                                    value["values"],
                                    rtol=RTOL,
                                    atol=ATOL,
                                ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']['values']} not np.allclose to {value['values']}"
                            except TypeError as e:
                                print(e)
                                assert (
                                    result["result"]["observed_value"] == value
                                ), f"{result['result']['observed_value']} != {value}"
                        elif try_allclose:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {value}"
                        else:
                            assert (
                                result["result"]["observed_value"] == value
                            ), f"{result['result']['observed_value']} != {value}"

                # NOTE: This is a key used ONLY for testing cases where an expectation is legitimately allowed to return
                # any of multiple possible observed_values. expect_column_values_to_be_of_type is one such expectation.
                elif key == "observed_value_list":
                    assert result["result"]["observed_value"] in value

                elif key == "unexpected_index_list":
                    if isinstance(data_asset, (SqlAlchemyDataset, SparkDFDataset)):
                        pass
                    elif isinstance(data_asset, (SqlAlchemyBatchData, SparkDFBatchData)):
                        pass
                    else:
>                       assert (
                            result["result"]["unexpected_index_list"] == value
                        ), f"{result['result']['unexpected_index_list']} != {value}"
E                       AssertionError: [{'b': 'aaa', 'pk_index': 0}, {'b': 'abb', 'pk_index': 1}, {'b': 'acc', 'pk_index': 2}] != [0, 1, 2]

great_expectations/self_check/util.py:3075: AssertionError
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 2553.61it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 333.38it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 470.57it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 265.10it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 362.68it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 277.50it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 440.70it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 410.88it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 409.18it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 407.13it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_not_match_regex:negative_test_empty_regex] _

test_case = {'expectation_type': 'expect_column_values_to_not_match_regex', 'pk_column': True, 'skip': False, 'test': {'exact_matc... {'success': False, 'unexpected_index_list': [0, 1, 2, 3], 'unexpected_list': ['aaa', 'abb', 'acc', 'bdd']}, ...}, ...}

    @pytest.mark.order(index=0)
    @pytest.mark.integration
    @pytest.mark.slow  # 12.68s
    def test_case_runner_v3_api(test_case):
        if test_case["skip"]:
            pytest.skip()

        # Note: this should never be done in practice, but we are wiping expectations to reuse batches during testing.
        # test_case["batch"]._initialize_expectations()
        if "parse_strings_as_datetimes" in test_case["test"]["in"]:
            with pytest.deprecated_call():
                evaluate_json_test_v3_api(
                    validator=test_case["validator_with_data"],
                    expectation_type=test_case["expectation_type"],
                    test=test_case["test"],
                    pk_column=test_case["pk_column"],
                )
        else:
>           evaluate_json_test_v3_api(
                validator=test_case["validator_with_data"],
                expectation_type=test_case["expectation_type"],
                test=test_case["test"],
                pk_column=test_case["pk_column"],
            )

tests/test_definitions/test_expectations_v3_api.py:418:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
great_expectations/self_check/util.py:2901: in evaluate_json_test_v3_api
    check_json_test_result(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

test = {'exact_match_out': False, 'in': {'column': 'b', 'regex': ''}, 'input': {'column': 'b', 'regex': ''}, 'out': {'success': False, 'unexpected_index_list': [0, 1, 2, 3], 'unexpected_list': ['aaa', 'abb', 'acc', 'bdd']}, ...}
result = {'exception_info': {'exception_message': None, 'exception_traceback': None, 'raised_exception': False}, 'expectation_c... 'value': 'aaa'}, {'count': 1, 'value': 'abb'}, {'count': 1, 'value': 'acc'}, {'count': 1, 'value': 'bdd'}], ...}, ...}
data_asset = <great_expectations.execution_engine.pandas_batch_data.PandasBatchData object at 0x7f9502a6e310>
pk_column = True

    def check_json_test_result(  # noqa: C901 - 52
        test, result, data_asset=None, pk_column=False
    ) -> None:

        # check for id_pk results in cases where pk_column is true and unexpected_index_list already exists
        # this will work for testing since result_format is COMPLETE
        if pk_column:
            if not result["success"]:
                if "unexpected_index_list" in result["result"]:
                    assert "unexpected_index_query" in result["result"]

        if "unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )
            elif "unexpected_list" in test["output"]:
                (
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )

        if "partial_unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "partial_unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )
            elif "partial_unexpected_list" in test["output"]:
                (
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )

        # Determine if np.allclose(..) might be needed for float comparison
        try_allclose = False
        if "observed_value" in test["output"]:
            if RX_FLOAT.match(repr(test["output"]["observed_value"])):
                try_allclose = True

        # Check results
        if test["exact_match_out"] is True:
            if "result" in result and "observed_value" in result["result"]:
                if isinstance(result["result"]["observed_value"], (np.floating, float)):
                    assert np.allclose(
                        result["result"]["observed_value"],
                        expectationValidationResultSchema.load(test["output"])["result"][
                            "observed_value"
                        ],
                        rtol=RTOL,
                        atol=ATOL,
                    ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {expectationValidationResultSchema.load(test['output'])['result']['observed_value']}"
                else:
                    assert result == expectationValidationResultSchema.load(
                        test["output"]
                    ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
            else:
                assert result == expectationValidationResultSchema.load(
                    test["output"]
                ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
        else:
            # Convert result to json since our tests are reading from json so cannot easily contain richer types (e.g. NaN)
            # NOTE - 20191031 - JPC - we may eventually want to change these tests as we update our view on how
            # representations, serializations, and objects should interact and how much of that is shown to the user.
            result = result.to_json_dict()
            for key, value in test["output"].items():
                # Apply our great expectations-specific test logic

                if key == "success":
                    if isinstance(value, (np.floating, float)):
                        try:
                            assert np.allclose(
                                result["success"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['success']} not np.allclose to {value}"
                        except TypeError:
                            assert (
                                result["success"] == value
                            ), f"{result['success']} != {value}"
                    else:
                        assert result["success"] == value, f"{result['success']} != {value}"

                elif key == "observed_value":
                    if "tolerance" in test:
                        if isinstance(value, dict):
                            assert set(result["result"]["observed_value"].keys()) == set(
                                value.keys()
                            ), f"{set(result['result']['observed_value'].keys())} != {set(value.keys())}"
                            for k, v in value.items():
                                assert np.allclose(
                                    result["result"]["observed_value"][k],
                                    v,
                                    rtol=test["tolerance"],
                                )
                        else:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=test["tolerance"],
                            )
                    else:
                        if isinstance(value, dict) and "values" in value:
                            try:
                                assert np.allclose(
                                    result["result"]["observed_value"]["values"],
                                    value["values"],
                                    rtol=RTOL,
                                    atol=ATOL,
                                ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']['values']} not np.allclose to {value['values']}"
                            except TypeError as e:
                                print(e)
                                assert (
                                    result["result"]["observed_value"] == value
                                ), f"{result['result']['observed_value']} != {value}"
                        elif try_allclose:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {value}"
                        else:
                            assert (
                                result["result"]["observed_value"] == value
                            ), f"{result['result']['observed_value']} != {value}"

                # NOTE: This is a key used ONLY for testing cases where an expectation is legitimately allowed to return
                # any of multiple possible observed_values. expect_column_values_to_be_of_type is one such expectation.
                elif key == "observed_value_list":
                    assert result["result"]["observed_value"] in value

                elif key == "unexpected_index_list":
                    if isinstance(data_asset, (SqlAlchemyDataset, SparkDFDataset)):
                        pass
                    elif isinstance(data_asset, (SqlAlchemyBatchData, SparkDFBatchData)):
                        pass
                    else:
>                       assert (
                            result["result"]["unexpected_index_list"] == value
                        ), f"{result['result']['unexpected_index_list']} != {value}"
E                       AssertionError: [{'b': 'aaa', 'pk_index': 0}, {'b': 'abb', 'pk_index': 1}, {'b': 'acc', 'pk_index': 2}, {'b': 'bdd', 'pk_index': 3}] != [0, 1, 2, 3]

great_expectations/self_check/util.py:3075: AssertionError
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 3130.08it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 333.90it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 470.76it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 262.75it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 365.82it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 226.77it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 362.68it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 340.69it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 339.23it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 337.67it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_not_match_regex:negative_test_match_characters_not_at_the_beginning_of_string_exact_mostly] _

test_case = {'expectation_type': 'expect_column_values_to_not_match_regex', 'pk_column': True, 'skip': False, 'test': {'exact_matc...'regex': 'b'}, 'out': {'success': True, 'unexpected_index_list': [1, 4], 'unexpected_list': ['abb', 'bee']}, ...}, ...}

    @pytest.mark.order(index=0)
    @pytest.mark.integration
    @pytest.mark.slow  # 12.68s
    def test_case_runner_v3_api(test_case):
        if test_case["skip"]:
            pytest.skip()

        # Note: this should never be done in practice, but we are wiping expectations to reuse batches during testing.
        # test_case["batch"]._initialize_expectations()
        if "parse_strings_as_datetimes" in test_case["test"]["in"]:
            with pytest.deprecated_call():
                evaluate_json_test_v3_api(
                    validator=test_case["validator_with_data"],
                    expectation_type=test_case["expectation_type"],
                    test=test_case["test"],
                    pk_column=test_case["pk_column"],
                )
        else:
>           evaluate_json_test_v3_api(
                validator=test_case["validator_with_data"],
                expectation_type=test_case["expectation_type"],
                test=test_case["test"],
                pk_column=test_case["pk_column"],
            )

tests/test_definitions/test_expectations_v3_api.py:418:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
great_expectations/self_check/util.py:2901: in evaluate_json_test_v3_api
    check_json_test_result(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

test = {'exact_match_out': False, 'in': {'column': 'a', 'mostly': 0.6, 'regex': 'b'}, 'input': {'column': 'a', 'mostly': 0.6, 'regex': 'b'}, 'out': {'success': True, 'unexpected_index_list': [1, 4], 'unexpected_list': ['abb', 'bee']}, ...}
result = {'exception_info': {'exception_message': None, 'exception_traceback': None, 'raised_exception': False}, 'expectation_c...ng_percent': 0.0, 'partial_unexpected_counts': [{'count': 1, 'value': 'abb'}, {'count': 1, 'value': 'bee'}], ...}, ...}
data_asset = <great_expectations.execution_engine.pandas_batch_data.PandasBatchData object at 0x7f9502a6e310>
pk_column = True

    def check_json_test_result(  # noqa: C901 - 52
        test, result, data_asset=None, pk_column=False
    ) -> None:

        # check for id_pk results in cases where pk_column is true and unexpected_index_list already exists
        # this will work for testing since result_format is COMPLETE
        if pk_column:
            if not result["success"]:
                if "unexpected_index_list" in result["result"]:
                    assert "unexpected_index_query" in result["result"]

        if "unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )
            elif "unexpected_list" in test["output"]:
                (
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )

        if "partial_unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "partial_unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )
            elif "partial_unexpected_list" in test["output"]:
                (
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )

        # Determine if np.allclose(..) might be needed for float comparison
        try_allclose = False
        if "observed_value" in test["output"]:
            if RX_FLOAT.match(repr(test["output"]["observed_value"])):
                try_allclose = True

        # Check results
        if test["exact_match_out"] is True:
            if "result" in result and "observed_value" in result["result"]:
                if isinstance(result["result"]["observed_value"], (np.floating, float)):
                    assert np.allclose(
                        result["result"]["observed_value"],
                        expectationValidationResultSchema.load(test["output"])["result"][
                            "observed_value"
                        ],
                        rtol=RTOL,
                        atol=ATOL,
                    ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {expectationValidationResultSchema.load(test['output'])['result']['observed_value']}"
                else:
                    assert result == expectationValidationResultSchema.load(
                        test["output"]
                    ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
            else:
                assert result == expectationValidationResultSchema.load(
                    test["output"]
                ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
        else:
            # Convert result to json since our tests are reading from json so cannot easily contain richer types (e.g. NaN)
            # NOTE - 20191031 - JPC - we may eventually want to change these tests as we update our view on how
            # representations, serializations, and objects should interact and how much of that is shown to the user.
            result = result.to_json_dict()
            for key, value in test["output"].items():
                # Apply our great expectations-specific test logic

                if key == "success":
                    if isinstance(value, (np.floating, float)):
                        try:
                            assert np.allclose(
                                result["success"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['success']} not np.allclose to {value}"
                        except TypeError:
                            assert (
                                result["success"] == value
                            ), f"{result['success']} != {value}"
                    else:
                        assert result["success"] == value, f"{result['success']} != {value}"

                elif key == "observed_value":
                    if "tolerance" in test:
                        if isinstance(value, dict):
                            assert set(result["result"]["observed_value"].keys()) == set(
                                value.keys()
                            ), f"{set(result['result']['observed_value'].keys())} != {set(value.keys())}"
                            for k, v in value.items():
                                assert np.allclose(
                                    result["result"]["observed_value"][k],
                                    v,
                                    rtol=test["tolerance"],
                                )
                        else:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=test["tolerance"],
                            )
                    else:
                        if isinstance(value, dict) and "values" in value:
                            try:
                                assert np.allclose(
                                    result["result"]["observed_value"]["values"],
                                    value["values"],
                                    rtol=RTOL,
                                    atol=ATOL,
                                ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']['values']} not np.allclose to {value['values']}"
                            except TypeError as e:
                                print(e)
                                assert (
                                    result["result"]["observed_value"] == value
                                ), f"{result['result']['observed_value']} != {value}"
                        elif try_allclose:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {value}"
                        else:
                            assert (
                                result["result"]["observed_value"] == value
                            ), f"{result['result']['observed_value']} != {value}"

                # NOTE: This is a key used ONLY for testing cases where an expectation is legitimately allowed to return
                # any of multiple possible observed_values. expect_column_values_to_be_of_type is one such expectation.
                elif key == "observed_value_list":
                    assert result["result"]["observed_value"] in value

                elif key == "unexpected_index_list":
                    if isinstance(data_asset, (SqlAlchemyDataset, SparkDFDataset)):
                        pass
                    elif isinstance(data_asset, (SqlAlchemyBatchData, SparkDFBatchData)):
                        pass
                    else:
>                       assert (
                            result["result"]["unexpected_index_list"] == value
                        ), f"{result['result']['unexpected_index_list']} != {value}"
E                       AssertionError: [{'a': 'abb', 'pk_index': 1}, {'a': 'bee', 'pk_index': 4}] != [1, 4]

great_expectations/self_check/util.py:3075: AssertionError
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 4140.48it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 451.46it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 640.91it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 356.34it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 496.04it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 379.68it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 596.26it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 553.37it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 551.00it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 548.07it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_not_match_regex_list:basic_negative_test] _

test_case = {'expectation_type': 'expect_column_values_to_not_match_regex_list', 'pk_column': True, 'skip': False, 'test': {'exact...['[12]+', '[45]+']}, 'include_in_gallery': True, 'input': {'column': 'w', 'regex_list': ['[12]+', '[45]+']}, ...}, ...}

    @pytest.mark.order(index=0)
    @pytest.mark.integration
    @pytest.mark.slow  # 12.68s
    def test_case_runner_v3_api(test_case):
        if test_case["skip"]:
            pytest.skip()

        # Note: this should never be done in practice, but we are wiping expectations to reuse batches during testing.
        # test_case["batch"]._initialize_expectations()
        if "parse_strings_as_datetimes" in test_case["test"]["in"]:
            with pytest.deprecated_call():
                evaluate_json_test_v3_api(
                    validator=test_case["validator_with_data"],
                    expectation_type=test_case["expectation_type"],
                    test=test_case["test"],
                    pk_column=test_case["pk_column"],
                )
        else:
>           evaluate_json_test_v3_api(
                validator=test_case["validator_with_data"],
                expectation_type=test_case["expectation_type"],
                test=test_case["test"],
                pk_column=test_case["pk_column"],
            )

tests/test_definitions/test_expectations_v3_api.py:418:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
great_expectations/self_check/util.py:2901: in evaluate_json_test_v3_api
    check_json_test_result(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

test = {'exact_match_out': False, 'in': {'column': 'w', 'regex_list': ['[12]+', '[45]+']}, 'include_in_gallery': True, 'input': {'column': 'w', 'regex_list': ['[12]+', '[45]+']}, ...}
result = {'exception_info': {'exception_message': None, 'exception_traceback': None, 'raised_exception': False}, 'expectation_c...ue': '222'}, {'count': 1, 'value': '321'}, {'count': 1, 'value': '444'}, {'count': 1, 'value': '456'}, ...], ...}, ...}
data_asset = <great_expectations.execution_engine.pandas_batch_data.PandasBatchData object at 0x7f9502ba0eb0>
pk_column = True

    def check_json_test_result(  # noqa: C901 - 52
        test, result, data_asset=None, pk_column=False
    ) -> None:

        # check for id_pk results in cases where pk_column is true and unexpected_index_list already exists
        # this will work for testing since result_format is COMPLETE
        if pk_column:
            if not result["success"]:
                if "unexpected_index_list" in result["result"]:
                    assert "unexpected_index_query" in result["result"]

        if "unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )
            elif "unexpected_list" in test["output"]:
                (
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )

        if "partial_unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "partial_unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )
            elif "partial_unexpected_list" in test["output"]:
                (
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )

        # Determine if np.allclose(..) might be needed for float comparison
        try_allclose = False
        if "observed_value" in test["output"]:
            if RX_FLOAT.match(repr(test["output"]["observed_value"])):
                try_allclose = True

        # Check results
        if test["exact_match_out"] is True:
            if "result" in result and "observed_value" in result["result"]:
                if isinstance(result["result"]["observed_value"], (np.floating, float)):
                    assert np.allclose(
                        result["result"]["observed_value"],
                        expectationValidationResultSchema.load(test["output"])["result"][
                            "observed_value"
                        ],
                        rtol=RTOL,
                        atol=ATOL,
                    ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {expectationValidationResultSchema.load(test['output'])['result']['observed_value']}"
                else:
                    assert result == expectationValidationResultSchema.load(
                        test["output"]
                    ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
            else:
                assert result == expectationValidationResultSchema.load(
                    test["output"]
                ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
        else:
            # Convert result to json since our tests are reading from json so cannot easily contain richer types (e.g. NaN)
            # NOTE - 20191031 - JPC - we may eventually want to change these tests as we update our view on how
            # representations, serializations, and objects should interact and how much of that is shown to the user.
            result = result.to_json_dict()
            for key, value in test["output"].items():
                # Apply our great expectations-specific test logic

                if key == "success":
                    if isinstance(value, (np.floating, float)):
                        try:
                            assert np.allclose(
                                result["success"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['success']} not np.allclose to {value}"
                        except TypeError:
                            assert (
                                result["success"] == value
                            ), f"{result['success']} != {value}"
                    else:
                        assert result["success"] == value, f"{result['success']} != {value}"

                elif key == "observed_value":
                    if "tolerance" in test:
                        if isinstance(value, dict):
                            assert set(result["result"]["observed_value"].keys()) == set(
                                value.keys()
                            ), f"{set(result['result']['observed_value'].keys())} != {set(value.keys())}"
                            for k, v in value.items():
                                assert np.allclose(
                                    result["result"]["observed_value"][k],
                                    v,
                                    rtol=test["tolerance"],
                                )
                        else:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=test["tolerance"],
                            )
                    else:
                        if isinstance(value, dict) and "values" in value:
                            try:
                                assert np.allclose(
                                    result["result"]["observed_value"]["values"],
                                    value["values"],
                                    rtol=RTOL,
                                    atol=ATOL,
                                ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']['values']} not np.allclose to {value['values']}"
                            except TypeError as e:
                                print(e)
                                assert (
                                    result["result"]["observed_value"] == value
                                ), f"{result['result']['observed_value']} != {value}"
                        elif try_allclose:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {value}"
                        else:
                            assert (
                                result["result"]["observed_value"] == value
                            ), f"{result['result']['observed_value']} != {value}"

                # NOTE: This is a key used ONLY for testing cases where an expectation is legitimately allowed to return
                # any of multiple possible observed_values. expect_column_values_to_be_of_type is one such expectation.
                elif key == "observed_value_list":
                    assert result["result"]["observed_value"] in value

                elif key == "unexpected_index_list":
                    if isinstance(data_asset, (SqlAlchemyDataset, SparkDFDataset)):
                        pass
                    elif isinstance(data_asset, (SqlAlchemyBatchData, SparkDFBatchData)):
                        pass
                    else:
>                       assert (
                            result["result"]["unexpected_index_list"] == value
                        ), f"{result['result']['unexpected_index_list']} != {value}"
E                       AssertionError: [{'w': '111', 'pk_index': 0}, {'w': '222', 'pk_index': 1}, {'w': '123', 'pk_index': 3}, {'w': '321', 'pk_index': 4}, {'w': '444', 'pk_index': 5}, {'w': '456', 'pk_index': 6}, {'w': '654', 'pk_index': 7}, {'w': '555', 'pk_index': 8}] != [0, 1, 3, 4, 5, 6, 7, 8]

great_expectations/self_check/util.py:3075: AssertionError
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 4348.68it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 451.39it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 641.04it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 353.32it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 458.98it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 356.36it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 565.94it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 526.37it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 524.08it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 521.52it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_not_match_regex_list:negative_test_with_more_string-ish_strings] _

test_case = {'expectation_type': 'expect_column_values_to_not_match_regex_list', 'pk_column': True, 'skip': False, 'test': {'exact...mus', 'ovat', 'h.*t']}, 'out': {'success': False, 'unexpected_index_list': [4], 'unexpected_list': ['hat']}, ...}, ...}

    @pytest.mark.order(index=0)
    @pytest.mark.integration
    @pytest.mark.slow  # 12.68s
    def test_case_runner_v3_api(test_case):
        if test_case["skip"]:
            pytest.skip()

        # Note: this should never be done in practice, but we are wiping expectations to reuse batches during testing.
        # test_case["batch"]._initialize_expectations()
        if "parse_strings_as_datetimes" in test_case["test"]["in"]:
            with pytest.deprecated_call():
                evaluate_json_test_v3_api(
                    validator=test_case["validator_with_data"],
                    expectation_type=test_case["expectation_type"],
                    test=test_case["test"],
                    pk_column=test_case["pk_column"],
                )
        else:
>           evaluate_json_test_v3_api(
                validator=test_case["validator_with_data"],
                expectation_type=test_case["expectation_type"],
                test=test_case["test"],
                pk_column=test_case["pk_column"],
            )

tests/test_definitions/test_expectations_v3_api.py:418:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
great_expectations/self_check/util.py:2901: in evaluate_json_test_v3_api
    check_json_test_result(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

test = {'exact_match_out': False, 'in': {'column': 'x', 'regex_list': ['opatomus', 'ovat', 'h.*t']}, 'input': {'column': 'x',...'opatomus', 'ovat', 'h.*t']}, 'out': {'success': False, 'unexpected_index_list': [4], 'unexpected_list': ['hat']}, ...}
result = {'exception_info': {'exception_message': None, 'exception_traceback': None, 'raised_exception': False}, 'expectation_c...10, 'missing_count': 0, 'missing_percent': 0.0, 'partial_unexpected_counts': [{'count': 1, 'value': 'hat'}], ...}, ...}
data_asset = <great_expectations.execution_engine.pandas_batch_data.PandasBatchData object at 0x7f9502ba0eb0>
pk_column = True

    def check_json_test_result(  # noqa: C901 - 52
        test, result, data_asset=None, pk_column=False
    ) -> None:

        # check for id_pk results in cases where pk_column is true and unexpected_index_list already exists
        # this will work for testing since result_format is COMPLETE
        if pk_column:
            if not result["success"]:
                if "unexpected_index_list" in result["result"]:
                    assert "unexpected_index_query" in result["result"]

        if "unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )
            elif "unexpected_list" in test["output"]:
                (
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )

        if "partial_unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "partial_unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )
            elif "partial_unexpected_list" in test["output"]:
                (
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )

        # Determine if np.allclose(..) might be needed for float comparison
        try_allclose = False
        if "observed_value" in test["output"]:
            if RX_FLOAT.match(repr(test["output"]["observed_value"])):
                try_allclose = True

        # Check results
        if test["exact_match_out"] is True:
            if "result" in result and "observed_value" in result["result"]:
                if isinstance(result["result"]["observed_value"], (np.floating, float)):
                    assert np.allclose(
                        result["result"]["observed_value"],
                        expectationValidationResultSchema.load(test["output"])["result"][
                            "observed_value"
                        ],
                        rtol=RTOL,
                        atol=ATOL,
                    ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {expectationValidationResultSchema.load(test['output'])['result']['observed_value']}"
                else:
                    assert result == expectationValidationResultSchema.load(
                        test["output"]
                    ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
            else:
                assert result == expectationValidationResultSchema.load(
                    test["output"]
                ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
        else:
            # Convert result to json since our tests are reading from json so cannot easily contain richer types (e.g. NaN)
            # NOTE - 20191031 - JPC - we may eventually want to change these tests as we update our view on how
            # representations, serializations, and objects should interact and how much of that is shown to the user.
            result = result.to_json_dict()
            for key, value in test["output"].items():
                # Apply our great expectations-specific test logic

                if key == "success":
                    if isinstance(value, (np.floating, float)):
                        try:
                            assert np.allclose(
                                result["success"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['success']} not np.allclose to {value}"
                        except TypeError:
                            assert (
                                result["success"] == value
                            ), f"{result['success']} != {value}"
                    else:
                        assert result["success"] == value, f"{result['success']} != {value}"

                elif key == "observed_value":
                    if "tolerance" in test:
                        if isinstance(value, dict):
                            assert set(result["result"]["observed_value"].keys()) == set(
                                value.keys()
                            ), f"{set(result['result']['observed_value'].keys())} != {set(value.keys())}"
                            for k, v in value.items():
                                assert np.allclose(
                                    result["result"]["observed_value"][k],
                                    v,
                                    rtol=test["tolerance"],
                                )
                        else:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=test["tolerance"],
                            )
                    else:
                        if isinstance(value, dict) and "values" in value:
                            try:
                                assert np.allclose(
                                    result["result"]["observed_value"]["values"],
                                    value["values"],
                                    rtol=RTOL,
                                    atol=ATOL,
                                ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']['values']} not np.allclose to {value['values']}"
                            except TypeError as e:
                                print(e)
                                assert (
                                    result["result"]["observed_value"] == value
                                ), f"{result['result']['observed_value']} != {value}"
                        elif try_allclose:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {value}"
                        else:
                            assert (
                                result["result"]["observed_value"] == value
                            ), f"{result['result']['observed_value']} != {value}"

                # NOTE: This is a key used ONLY for testing cases where an expectation is legitimately allowed to return
                # any of multiple possible observed_values. expect_column_values_to_be_of_type is one such expectation.
                elif key == "observed_value_list":
                    assert result["result"]["observed_value"] in value

                elif key == "unexpected_index_list":
                    if isinstance(data_asset, (SqlAlchemyDataset, SparkDFDataset)):
                        pass
                    elif isinstance(data_asset, (SqlAlchemyBatchData, SparkDFBatchData)):
                        pass
                    else:
>                       assert (
                            result["result"]["unexpected_index_list"] == value
                        ), f"{result['result']['unexpected_index_list']} != {value}"
E                       AssertionError: [{'x': 'hat', 'pk_index': 4}] != [4]

great_expectations/self_check/util.py:3075: AssertionError
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 3724.96it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 408.09it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 582.08it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 333.68it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 418.84it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 330.43it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 530.17it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 494.79it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 492.73it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 490.34it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_increasing:positive_test_with_strictly] _

test_case = {'expectation_type': 'expect_column_values_to_be_increasing', 'pk_column': True, 'skip': False, 'test': {'exact_match_...ut': {'success': False, 'unexpected_index_list': [1, 2, 4, 5, 7, 8], 'unexpected_list': [1, 1, 2, 2, 3, 3]}, ...}, ...}

    @pytest.mark.order(index=0)
    @pytest.mark.integration
    @pytest.mark.slow  # 12.68s
    def test_case_runner_v3_api(test_case):
        if test_case["skip"]:
            pytest.skip()

        # Note: this should never be done in practice, but we are wiping expectations to reuse batches during testing.
        # test_case["batch"]._initialize_expectations()
        if "parse_strings_as_datetimes" in test_case["test"]["in"]:
            with pytest.deprecated_call():
                evaluate_json_test_v3_api(
                    validator=test_case["validator_with_data"],
                    expectation_type=test_case["expectation_type"],
                    test=test_case["test"],
                    pk_column=test_case["pk_column"],
                )
        else:
>           evaluate_json_test_v3_api(
                validator=test_case["validator_with_data"],
                expectation_type=test_case["expectation_type"],
                test=test_case["test"],
                pk_column=test_case["pk_column"],
            )

tests/test_definitions/test_expectations_v3_api.py:418:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
great_expectations/self_check/util.py:2901: in evaluate_json_test_v3_api
    check_json_test_result(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

test = {'exact_match_out': False, 'in': {'column': 'y', 'strictly': True}, 'input': {'column': 'y', 'strictly': True}, 'out': {'success': False, 'unexpected_index_list': [1, 2, 4, 5, 7, 8], 'unexpected_list': [1, 1, 2, 2, 3, 3]}, ...}
result = {'exception_info': {'exception_message': None, 'exception_traceback': None, 'raised_exception': False}, 'expectation_c...'partial_unexpected_counts': [{'count': 2, 'value': 1}, {'count': 2, 'value': 2}, {'count': 2, 'value': 3}], ...}, ...}
data_asset = <great_expectations.execution_engine.pandas_batch_data.PandasBatchData object at 0x7f9502bf7b50>
pk_column = True

    def check_json_test_result(  # noqa: C901 - 52
        test, result, data_asset=None, pk_column=False
    ) -> None:

        # check for id_pk results in cases where pk_column is true and unexpected_index_list already exists
        # this will work for testing since result_format is COMPLETE
        if pk_column:
            if not result["success"]:
                if "unexpected_index_list" in result["result"]:
                    assert "unexpected_index_query" in result["result"]

        if "unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )
            elif "unexpected_list" in test["output"]:
                (
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )

        if "partial_unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "partial_unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )
            elif "partial_unexpected_list" in test["output"]:
                (
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )

        # Determine if np.allclose(..) might be needed for float comparison
        try_allclose = False
        if "observed_value" in test["output"]:
            if RX_FLOAT.match(repr(test["output"]["observed_value"])):
                try_allclose = True

        # Check results
        if test["exact_match_out"] is True:
            if "result" in result and "observed_value" in result["result"]:
                if isinstance(result["result"]["observed_value"], (np.floating, float)):
                    assert np.allclose(
                        result["result"]["observed_value"],
                        expectationValidationResultSchema.load(test["output"])["result"][
                            "observed_value"
                        ],
                        rtol=RTOL,
                        atol=ATOL,
                    ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {expectationValidationResultSchema.load(test['output'])['result']['observed_value']}"
                else:
                    assert result == expectationValidationResultSchema.load(
                        test["output"]
                    ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
            else:
                assert result == expectationValidationResultSchema.load(
                    test["output"]
                ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
        else:
            # Convert result to json since our tests are reading from json so cannot easily contain richer types (e.g. NaN)
            # NOTE - 20191031 - JPC - we may eventually want to change these tests as we update our view on how
            # representations, serializations, and objects should interact and how much of that is shown to the user.
            result = result.to_json_dict()
            for key, value in test["output"].items():
                # Apply our great expectations-specific test logic

                if key == "success":
                    if isinstance(value, (np.floating, float)):
                        try:
                            assert np.allclose(
                                result["success"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['success']} not np.allclose to {value}"
                        except TypeError:
                            assert (
                                result["success"] == value
                            ), f"{result['success']} != {value}"
                    else:
                        assert result["success"] == value, f"{result['success']} != {value}"

                elif key == "observed_value":
                    if "tolerance" in test:
                        if isinstance(value, dict):
                            assert set(result["result"]["observed_value"].keys()) == set(
                                value.keys()
                            ), f"{set(result['result']['observed_value'].keys())} != {set(value.keys())}"
                            for k, v in value.items():
                                assert np.allclose(
                                    result["result"]["observed_value"][k],
                                    v,
                                    rtol=test["tolerance"],
                                )
                        else:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=test["tolerance"],
                            )
                    else:
                        if isinstance(value, dict) and "values" in value:
                            try:
                                assert np.allclose(
                                    result["result"]["observed_value"]["values"],
                                    value["values"],
                                    rtol=RTOL,
                                    atol=ATOL,
                                ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']['values']} not np.allclose to {value['values']}"
                            except TypeError as e:
                                print(e)
                                assert (
                                    result["result"]["observed_value"] == value
                                ), f"{result['result']['observed_value']} != {value}"
                        elif try_allclose:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {value}"
                        else:
                            assert (
                                result["result"]["observed_value"] == value
                            ), f"{result['result']['observed_value']} != {value}"

                # NOTE: This is a key used ONLY for testing cases where an expectation is legitimately allowed to return
                # any of multiple possible observed_values. expect_column_values_to_be_of_type is one such expectation.
                elif key == "observed_value_list":
                    assert result["result"]["observed_value"] in value

                elif key == "unexpected_index_list":
                    if isinstance(data_asset, (SqlAlchemyDataset, SparkDFDataset)):
                        pass
                    elif isinstance(data_asset, (SqlAlchemyBatchData, SparkDFBatchData)):
                        pass
                    else:
>                       assert (
                            result["result"]["unexpected_index_list"] == value
                        ), f"{result['result']['unexpected_index_list']} != {value}"
E                       AssertionError: [{'y': 1, 'pk_index': 1}, {'y': 1, 'pk_index': 2}, {'y': 2, 'pk_index': 4}, {'y': 2, 'pk_index': 5}, {'y': 3, 'pk_index': 7}, {'y': 3, 'pk_index': 8}] != [1, 2, 4, 5, 7, 8]

great_expectations/self_check/util.py:3075: AssertionError
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 4213.26it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 431.42it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 613.65it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 336.22it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 433.62it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 337.29it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 535.45it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 498.78it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 496.80it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 494.42it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_increasing:basic_negative_test] _

test_case = {'expectation_type': 'expect_column_values_to_be_increasing', 'pk_column': True, 'skip': False, 'test': {'exact_match_out': False, 'in': {'column': 'w'}, 'include_in_gallery': True, 'input': {'column': 'w'}, ...}, ...}

    @pytest.mark.order(index=0)
    @pytest.mark.integration
    @pytest.mark.slow  # 12.68s
    def test_case_runner_v3_api(test_case):
        if test_case["skip"]:
            pytest.skip()

        # Note: this should never be done in practice, but we are wiping expectations to reuse batches during testing.
        # test_case["batch"]._initialize_expectations()
        if "parse_strings_as_datetimes" in test_case["test"]["in"]:
            with pytest.deprecated_call():
                evaluate_json_test_v3_api(
                    validator=test_case["validator_with_data"],
                    expectation_type=test_case["expectation_type"],
                    test=test_case["test"],
                    pk_column=test_case["pk_column"],
                )
        else:
>           evaluate_json_test_v3_api(
                validator=test_case["validator_with_data"],
                expectation_type=test_case["expectation_type"],
                test=test_case["test"],
                pk_column=test_case["pk_column"],
            )

tests/test_definitions/test_expectations_v3_api.py:418:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
great_expectations/self_check/util.py:2901: in evaluate_json_test_v3_api
    check_json_test_result(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

test = {'exact_match_out': False, 'in': {'column': 'w'}, 'include_in_gallery': True, 'input': {'column': 'w'}, ...}
result = {'exception_info': {'exception_message': None, 'exception_traceback': None, 'raised_exception': False}, 'expectation_c...': [{'count': 1, 'value': 1}, {'count': 1, 'value': 2}, {'count': 1, 'value': 3}, {'count': 1, 'value': 4}], ...}, ...}
data_asset = <great_expectations.execution_engine.pandas_batch_data.PandasBatchData object at 0x7f9502bf7b50>
pk_column = True

    def check_json_test_result(  # noqa: C901 - 52
        test, result, data_asset=None, pk_column=False
    ) -> None:

        # check for id_pk results in cases where pk_column is true and unexpected_index_list already exists
        # this will work for testing since result_format is COMPLETE
        if pk_column:
            if not result["success"]:
                if "unexpected_index_list" in result["result"]:
                    assert "unexpected_index_query" in result["result"]

        if "unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )
            elif "unexpected_list" in test["output"]:
                (
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )

        if "partial_unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "partial_unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )
            elif "partial_unexpected_list" in test["output"]:
                (
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )

        # Determine if np.allclose(..) might be needed for float comparison
        try_allclose = False
        if "observed_value" in test["output"]:
            if RX_FLOAT.match(repr(test["output"]["observed_value"])):
                try_allclose = True

        # Check results
        if test["exact_match_out"] is True:
            if "result" in result and "observed_value" in result["result"]:
                if isinstance(result["result"]["observed_value"], (np.floating, float)):
                    assert np.allclose(
                        result["result"]["observed_value"],
                        expectationValidationResultSchema.load(test["output"])["result"][
                            "observed_value"
                        ],
                        rtol=RTOL,
                        atol=ATOL,
                    ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {expectationValidationResultSchema.load(test['output'])['result']['observed_value']}"
                else:
                    assert result == expectationValidationResultSchema.load(
                        test["output"]
                    ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
            else:
                assert result == expectationValidationResultSchema.load(
                    test["output"]
                ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
        else:
            # Convert result to json since our tests are reading from json so cannot easily contain richer types (e.g. NaN)
            # NOTE - 20191031 - JPC - we may eventually want to change these tests as we update our view on how
            # representations, serializations, and objects should interact and how much of that is shown to the user.
            result = result.to_json_dict()
            for key, value in test["output"].items():
                # Apply our great expectations-specific test logic

                if key == "success":
                    if isinstance(value, (np.floating, float)):
                        try:
                            assert np.allclose(
                                result["success"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['success']} not np.allclose to {value}"
                        except TypeError:
                            assert (
                                result["success"] == value
                            ), f"{result['success']} != {value}"
                    else:
                        assert result["success"] == value, f"{result['success']} != {value}"

                elif key == "observed_value":
                    if "tolerance" in test:
                        if isinstance(value, dict):
                            assert set(result["result"]["observed_value"].keys()) == set(
                                value.keys()
                            ), f"{set(result['result']['observed_value'].keys())} != {set(value.keys())}"
                            for k, v in value.items():
                                assert np.allclose(
                                    result["result"]["observed_value"][k],
                                    v,
                                    rtol=test["tolerance"],
                                )
                        else:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=test["tolerance"],
                            )
                    else:
                        if isinstance(value, dict) and "values" in value:
                            try:
                                assert np.allclose(
                                    result["result"]["observed_value"]["values"],
                                    value["values"],
                                    rtol=RTOL,
                                    atol=ATOL,
                                ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']['values']} not np.allclose to {value['values']}"
                            except TypeError as e:
                                print(e)
                                assert (
                                    result["result"]["observed_value"] == value
                                ), f"{result['result']['observed_value']} != {value}"
                        elif try_allclose:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {value}"
                        else:
                            assert (
                                result["result"]["observed_value"] == value
                            ), f"{result['result']['observed_value']} != {value}"

                # NOTE: This is a key used ONLY for testing cases where an expectation is legitimately allowed to return
                # any of multiple possible observed_values. expect_column_values_to_be_of_type is one such expectation.
                elif key == "observed_value_list":
                    assert result["result"]["observed_value"] in value

                elif key == "unexpected_index_list":
                    if isinstance(data_asset, (SqlAlchemyDataset, SparkDFDataset)):
                        pass
                    elif isinstance(data_asset, (SqlAlchemyBatchData, SparkDFBatchData)):
                        pass
                    else:
>                       assert (
                            result["result"]["unexpected_index_list"] == value
                        ), f"{result['result']['unexpected_index_list']} != {value}"
E                       AssertionError: [{'w': 4, 'pk_index': 6}, {'w': 3, 'pk_index': 7}, {'w': 2, 'pk_index': 8}, {'w': 1, 'pk_index': 9}] != [6, 7, 8, 9]

great_expectations/self_check/util.py:3075: AssertionError
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 4072.14it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 433.09it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 613.98it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 340.06it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 445.82it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 345.79it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 549.18it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 510.91it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 508.88it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 506.41it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_increasing:negative_test_with_interspersed_nulls] _

test_case = {'expectation_type': 'expect_column_values_to_be_increasing', 'pk_column': True, 'skip': False, 'test': {'exact_match_..., 'input': {'column': 'b'}, 'out': {'success': False, 'unexpected_index_list': [7], 'unexpected_list': [1]}, ...}, ...}

    @pytest.mark.order(index=0)
    @pytest.mark.integration
    @pytest.mark.slow  # 12.68s
    def test_case_runner_v3_api(test_case):
        if test_case["skip"]:
            pytest.skip()

        # Note: this should never be done in practice, but we are wiping expectations to reuse batches during testing.
        # test_case["batch"]._initialize_expectations()
        if "parse_strings_as_datetimes" in test_case["test"]["in"]:
            with pytest.deprecated_call():
                evaluate_json_test_v3_api(
                    validator=test_case["validator_with_data"],
                    expectation_type=test_case["expectation_type"],
                    test=test_case["test"],
                    pk_column=test_case["pk_column"],
                )
        else:
>           evaluate_json_test_v3_api(
                validator=test_case["validator_with_data"],
                expectation_type=test_case["expectation_type"],
                test=test_case["test"],
                pk_column=test_case["pk_column"],
            )

tests/test_definitions/test_expectations_v3_api.py:418:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
great_expectations/self_check/util.py:2901: in evaluate_json_test_v3_api
    check_json_test_result(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

test = {'exact_match_out': False, 'in': {'column': 'b'}, 'input': {'column': 'b'}, 'out': {'success': False, 'unexpected_index_list': [7], 'unexpected_list': [1]}, ...}
result = {'exception_info': {'exception_message': None, 'exception_traceback': None, 'raised_exception': False}, 'expectation_c... 10, 'missing_count': 7, 'missing_percent': 70.0, 'partial_unexpected_counts': [{'count': 1, 'value': 1.0}], ...}, ...}
data_asset = <great_expectations.execution_engine.pandas_batch_data.PandasBatchData object at 0x7f9502bf7b50>
pk_column = True

    def check_json_test_result(  # noqa: C901 - 52
        test, result, data_asset=None, pk_column=False
    ) -> None:

        # check for id_pk results in cases where pk_column is true and unexpected_index_list already exists
        # this will work for testing since result_format is COMPLETE
        if pk_column:
            if not result["success"]:
                if "unexpected_index_list" in result["result"]:
                    assert "unexpected_index_query" in result["result"]

        if "unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )
            elif "unexpected_list" in test["output"]:
                (
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )

        if "partial_unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "partial_unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )
            elif "partial_unexpected_list" in test["output"]:
                (
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )

        # Determine if np.allclose(..) might be needed for float comparison
        try_allclose = False
        if "observed_value" in test["output"]:
            if RX_FLOAT.match(repr(test["output"]["observed_value"])):
                try_allclose = True

        # Check results
        if test["exact_match_out"] is True:
            if "result" in result and "observed_value" in result["result"]:
                if isinstance(result["result"]["observed_value"], (np.floating, float)):
                    assert np.allclose(
                        result["result"]["observed_value"],
                        expectationValidationResultSchema.load(test["output"])["result"][
                            "observed_value"
                        ],
                        rtol=RTOL,
                        atol=ATOL,
                    ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {expectationValidationResultSchema.load(test['output'])['result']['observed_value']}"
                else:
                    assert result == expectationValidationResultSchema.load(
                        test["output"]
                    ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
            else:
                assert result == expectationValidationResultSchema.load(
                    test["output"]
                ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
        else:
            # Convert result to json since our tests are reading from json so cannot easily contain richer types (e.g. NaN)
            # NOTE - 20191031 - JPC - we may eventually want to change these tests as we update our view on how
            # representations, serializations, and objects should interact and how much of that is shown to the user.
            result = result.to_json_dict()
            for key, value in test["output"].items():
                # Apply our great expectations-specific test logic

                if key == "success":
                    if isinstance(value, (np.floating, float)):
                        try:
                            assert np.allclose(
                                result["success"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['success']} not np.allclose to {value}"
                        except TypeError:
                            assert (
                                result["success"] == value
                            ), f"{result['success']} != {value}"
                    else:
                        assert result["success"] == value, f"{result['success']} != {value}"

                elif key == "observed_value":
                    if "tolerance" in test:
                        if isinstance(value, dict):
                            assert set(result["result"]["observed_value"].keys()) == set(
                                value.keys()
                            ), f"{set(result['result']['observed_value'].keys())} != {set(value.keys())}"
                            for k, v in value.items():
                                assert np.allclose(
                                    result["result"]["observed_value"][k],
                                    v,
                                    rtol=test["tolerance"],
                                )
                        else:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=test["tolerance"],
                            )
                    else:
                        if isinstance(value, dict) and "values" in value:
                            try:
                                assert np.allclose(
                                    result["result"]["observed_value"]["values"],
                                    value["values"],
                                    rtol=RTOL,
                                    atol=ATOL,
                                ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']['values']} not np.allclose to {value['values']}"
                            except TypeError as e:
                                print(e)
                                assert (
                                    result["result"]["observed_value"] == value
                                ), f"{result['result']['observed_value']} != {value}"
                        elif try_allclose:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {value}"
                        else:
                            assert (
                                result["result"]["observed_value"] == value
                            ), f"{result['result']['observed_value']} != {value}"

                # NOTE: This is a key used ONLY for testing cases where an expectation is legitimately allowed to return
                # any of multiple possible observed_values. expect_column_values_to_be_of_type is one such expectation.
                elif key == "observed_value_list":
                    assert result["result"]["observed_value"] in value

                elif key == "unexpected_index_list":
                    if isinstance(data_asset, (SqlAlchemyDataset, SparkDFDataset)):
                        pass
                    elif isinstance(data_asset, (SqlAlchemyBatchData, SparkDFBatchData)):
                        pass
                    else:
>                       assert (
                            result["result"]["unexpected_index_list"] == value
                        ), f"{result['result']['unexpected_index_list']} != {value}"
E                       AssertionError: [{'b': 1.0, 'pk_index': 7}] != [7]

great_expectations/self_check/util.py:3075: AssertionError
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 4074.12it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 431.69it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 609.87it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 336.28it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 434.07it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 338.64it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 542.06it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 504.60it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 502.59it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 500.20it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_value_lengths_to_be_between:negative_test_matching_min_max_too_small] _

test_case = {'expectation_type': 'expect_column_value_lengths_to_be_between', 'pk_column': True, 'skip': False, 'test': {'exact_ma...: 8, 'min_value': 4}, 'include_in_gallery': True, 'input': {'column': 's2', 'max_value': 8, 'min_value': 4}, ...}, ...}

    @pytest.mark.order(index=0)
    @pytest.mark.integration
    @pytest.mark.slow  # 12.68s
    def test_case_runner_v3_api(test_case):
        if test_case["skip"]:
            pytest.skip()

        # Note: this should never be done in practice, but we are wiping expectations to reuse batches during testing.
        # test_case["batch"]._initialize_expectations()
        if "parse_strings_as_datetimes" in test_case["test"]["in"]:
            with pytest.deprecated_call():
                evaluate_json_test_v3_api(
                    validator=test_case["validator_with_data"],
                    expectation_type=test_case["expectation_type"],
                    test=test_case["test"],
                    pk_column=test_case["pk_column"],
                )
        else:
>           evaluate_json_test_v3_api(
                validator=test_case["validator_with_data"],
                expectation_type=test_case["expectation_type"],
                test=test_case["test"],
                pk_column=test_case["pk_column"],
            )

tests/test_definitions/test_expectations_v3_api.py:418:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
great_expectations/self_check/util.py:2901: in evaluate_json_test_v3_api
    check_json_test_result(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

test = {'exact_match_out': False, 'in': {'column': 's2', 'max_value': 8, 'min_value': 4}, 'include_in_gallery': True, 'input': {'column': 's2', 'max_value': 8, 'min_value': 4}, ...}
result = {'exception_info': {'exception_message': None, 'exception_traceback': None, 'raised_exception': False}, 'expectation_c...issing_count': 0, 'missing_percent': 0.0, 'partial_unexpected_counts': [{'count': 1, 'value': 'collected'}], ...}, ...}
data_asset = <great_expectations.execution_engine.pandas_batch_data.PandasBatchData object at 0x7f9502ac0a00>
pk_column = True

    def check_json_test_result(  # noqa: C901 - 52
        test, result, data_asset=None, pk_column=False
    ) -> None:

        # check for id_pk results in cases where pk_column is true and unexpected_index_list already exists
        # this will work for testing since result_format is COMPLETE
        if pk_column:
            if not result["success"]:
                if "unexpected_index_list" in result["result"]:
                    assert "unexpected_index_query" in result["result"]

        if "unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )
            elif "unexpected_list" in test["output"]:
                (
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )

        if "partial_unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "partial_unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )
            elif "partial_unexpected_list" in test["output"]:
                (
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )

        # Determine if np.allclose(..) might be needed for float comparison
        try_allclose = False
        if "observed_value" in test["output"]:
            if RX_FLOAT.match(repr(test["output"]["observed_value"])):
                try_allclose = True

        # Check results
        if test["exact_match_out"] is True:
            if "result" in result and "observed_value" in result["result"]:
                if isinstance(result["result"]["observed_value"], (np.floating, float)):
                    assert np.allclose(
                        result["result"]["observed_value"],
                        expectationValidationResultSchema.load(test["output"])["result"][
                            "observed_value"
                        ],
                        rtol=RTOL,
                        atol=ATOL,
                    ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {expectationValidationResultSchema.load(test['output'])['result']['observed_value']}"
                else:
                    assert result == expectationValidationResultSchema.load(
                        test["output"]
                    ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
            else:
                assert result == expectationValidationResultSchema.load(
                    test["output"]
                ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
        else:
            # Convert result to json since our tests are reading from json so cannot easily contain richer types (e.g. NaN)
            # NOTE - 20191031 - JPC - we may eventually want to change these tests as we update our view on how
            # representations, serializations, and objects should interact and how much of that is shown to the user.
            result = result.to_json_dict()
            for key, value in test["output"].items():
                # Apply our great expectations-specific test logic

                if key == "success":
                    if isinstance(value, (np.floating, float)):
                        try:
                            assert np.allclose(
                                result["success"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['success']} not np.allclose to {value}"
                        except TypeError:
                            assert (
                                result["success"] == value
                            ), f"{result['success']} != {value}"
                    else:
                        assert result["success"] == value, f"{result['success']} != {value}"

                elif key == "observed_value":
                    if "tolerance" in test:
                        if isinstance(value, dict):
                            assert set(result["result"]["observed_value"].keys()) == set(
                                value.keys()
                            ), f"{set(result['result']['observed_value'].keys())} != {set(value.keys())}"
                            for k, v in value.items():
                                assert np.allclose(
                                    result["result"]["observed_value"][k],
                                    v,
                                    rtol=test["tolerance"],
                                )
                        else:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=test["tolerance"],
                            )
                    else:
                        if isinstance(value, dict) and "values" in value:
                            try:
                                assert np.allclose(
                                    result["result"]["observed_value"]["values"],
                                    value["values"],
                                    rtol=RTOL,
                                    atol=ATOL,
                                ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']['values']} not np.allclose to {value['values']}"
                            except TypeError as e:
                                print(e)
                                assert (
                                    result["result"]["observed_value"] == value
                                ), f"{result['result']['observed_value']} != {value}"
                        elif try_allclose:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {value}"
                        else:
                            assert (
                                result["result"]["observed_value"] == value
                            ), f"{result['result']['observed_value']} != {value}"

                # NOTE: This is a key used ONLY for testing cases where an expectation is legitimately allowed to return
                # any of multiple possible observed_values. expect_column_values_to_be_of_type is one such expectation.
                elif key == "observed_value_list":
                    assert result["result"]["observed_value"] in value

                elif key == "unexpected_index_list":
                    if isinstance(data_asset, (SqlAlchemyDataset, SparkDFDataset)):
                        pass
                    elif isinstance(data_asset, (SqlAlchemyBatchData, SparkDFBatchData)):
                        pass
                    else:
>                       assert (
                            result["result"]["unexpected_index_list"] == value
                        ), f"{result['result']['unexpected_index_list']} != {value}"
E                       AssertionError: [{'s2': 'collected', 'pk_index': 2}] != [2]

great_expectations/self_check/util.py:3075: AssertionError
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/11 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/11 [00:00<?, ?it/s]Calculating Metrics:  18%|█▊        | 2/11 [00:00<00:00, 4158.95it/s]Calculating Metrics:  18%|█▊        | 2/11 [00:00<00:00, 333.00it/s]Calculating Metrics:  27%|██▋       | 3/11 [00:00<00:00, 479.24it/s]Calculating Metrics:  27%|██▋       | 3/11 [00:00<00:00, 256.98it/s]Calculating Metrics:  45%|████▌     | 5/11 [00:00<00:00, 376.48it/s]Calculating Metrics:  45%|████▌     | 5/11 [00:00<00:00, 276.78it/s]Calculating Metrics:  64%|██████▎   | 7/11 [00:00<00:00, 353.45it/s]Calculating Metrics:  64%|██████▎   | 7/11 [00:00<00:00, 296.99it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 405.13it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 378.64it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 377.46it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 376.18it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_value_lengths_to_be_between:negative_test_matching_max_min_too_large] _

test_case = {'expectation_type': 'expect_column_value_lengths_to_be_between', 'pk_column': True, 'skip': False, 'test': {'exact_ma...value': 5}, 'out': {'success': False, 'unexpected_index_list': [0, 1], 'unexpected_list': ['calm', 'cool']}, ...}, ...}

    @pytest.mark.order(index=0)
    @pytest.mark.integration
    @pytest.mark.slow  # 12.68s
    def test_case_runner_v3_api(test_case):
        if test_case["skip"]:
            pytest.skip()

        # Note: this should never be done in practice, but we are wiping expectations to reuse batches during testing.
        # test_case["batch"]._initialize_expectations()
        if "parse_strings_as_datetimes" in test_case["test"]["in"]:
            with pytest.deprecated_call():
                evaluate_json_test_v3_api(
                    validator=test_case["validator_with_data"],
                    expectation_type=test_case["expectation_type"],
                    test=test_case["test"],
                    pk_column=test_case["pk_column"],
                )
        else:
>           evaluate_json_test_v3_api(
                validator=test_case["validator_with_data"],
                expectation_type=test_case["expectation_type"],
                test=test_case["test"],
                pk_column=test_case["pk_column"],
            )

tests/test_definitions/test_expectations_v3_api.py:418:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
great_expectations/self_check/util.py:2901: in evaluate_json_test_v3_api
    check_json_test_result(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

test = {'exact_match_out': False, 'in': {'column': 's2', 'max_value': 9, 'min_value': 5}, 'input': {'column': 's2', 'max_valu... 'min_value': 5}, 'out': {'success': False, 'unexpected_index_list': [0, 1], 'unexpected_list': ['calm', 'cool']}, ...}
result = {'exception_info': {'exception_message': None, 'exception_traceback': None, 'raised_exception': False}, 'expectation_c..._percent': 0.0, 'partial_unexpected_counts': [{'count': 1, 'value': 'calm'}, {'count': 1, 'value': 'cool'}], ...}, ...}
data_asset = <great_expectations.execution_engine.pandas_batch_data.PandasBatchData object at 0x7f9502ac0a00>
pk_column = True

    def check_json_test_result(  # noqa: C901 - 52
        test, result, data_asset=None, pk_column=False
    ) -> None:

        # check for id_pk results in cases where pk_column is true and unexpected_index_list already exists
        # this will work for testing since result_format is COMPLETE
        if pk_column:
            if not result["success"]:
                if "unexpected_index_list" in result["result"]:
                    assert "unexpected_index_query" in result["result"]

        if "unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )
            elif "unexpected_list" in test["output"]:
                (
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )

        if "partial_unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "partial_unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )
            elif "partial_unexpected_list" in test["output"]:
                (
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )

        # Determine if np.allclose(..) might be needed for float comparison
        try_allclose = False
        if "observed_value" in test["output"]:
            if RX_FLOAT.match(repr(test["output"]["observed_value"])):
                try_allclose = True

        # Check results
        if test["exact_match_out"] is True:
            if "result" in result and "observed_value" in result["result"]:
                if isinstance(result["result"]["observed_value"], (np.floating, float)):
                    assert np.allclose(
                        result["result"]["observed_value"],
                        expectationValidationResultSchema.load(test["output"])["result"][
                            "observed_value"
                        ],
                        rtol=RTOL,
                        atol=ATOL,
                    ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {expectationValidationResultSchema.load(test['output'])['result']['observed_value']}"
                else:
                    assert result == expectationValidationResultSchema.load(
                        test["output"]
                    ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
            else:
                assert result == expectationValidationResultSchema.load(
                    test["output"]
                ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
        else:
            # Convert result to json since our tests are reading from json so cannot easily contain richer types (e.g. NaN)
            # NOTE - 20191031 - JPC - we may eventually want to change these tests as we update our view on how
            # representations, serializations, and objects should interact and how much of that is shown to the user.
            result = result.to_json_dict()
            for key, value in test["output"].items():
                # Apply our great expectations-specific test logic

                if key == "success":
                    if isinstance(value, (np.floating, float)):
                        try:
                            assert np.allclose(
                                result["success"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['success']} not np.allclose to {value}"
                        except TypeError:
                            assert (
                                result["success"] == value
                            ), f"{result['success']} != {value}"
                    else:
                        assert result["success"] == value, f"{result['success']} != {value}"

                elif key == "observed_value":
                    if "tolerance" in test:
                        if isinstance(value, dict):
                            assert set(result["result"]["observed_value"].keys()) == set(
                                value.keys()
                            ), f"{set(result['result']['observed_value'].keys())} != {set(value.keys())}"
                            for k, v in value.items():
                                assert np.allclose(
                                    result["result"]["observed_value"][k],
                                    v,
                                    rtol=test["tolerance"],
                                )
                        else:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=test["tolerance"],
                            )
                    else:
                        if isinstance(value, dict) and "values" in value:
                            try:
                                assert np.allclose(
                                    result["result"]["observed_value"]["values"],
                                    value["values"],
                                    rtol=RTOL,
                                    atol=ATOL,
                                ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']['values']} not np.allclose to {value['values']}"
                            except TypeError as e:
                                print(e)
                                assert (
                                    result["result"]["observed_value"] == value
                                ), f"{result['result']['observed_value']} != {value}"
                        elif try_allclose:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {value}"
                        else:
                            assert (
                                result["result"]["observed_value"] == value
                            ), f"{result['result']['observed_value']} != {value}"

                # NOTE: This is a key used ONLY for testing cases where an expectation is legitimately allowed to return
                # any of multiple possible observed_values. expect_column_values_to_be_of_type is one such expectation.
                elif key == "observed_value_list":
                    assert result["result"]["observed_value"] in value

                elif key == "unexpected_index_list":
                    if isinstance(data_asset, (SqlAlchemyDataset, SparkDFDataset)):
                        pass
                    elif isinstance(data_asset, (SqlAlchemyBatchData, SparkDFBatchData)):
                        pass
                    else:
>                       assert (
                            result["result"]["unexpected_index_list"] == value
                        ), f"{result['result']['unexpected_index_list']} != {value}"
E                       AssertionError: [{'s2': 'cool', 'pk_index': 0}, {'s2': 'calm', 'pk_index': 1}] != [0, 1]

great_expectations/self_check/util.py:3075: AssertionError
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/11 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/11 [00:00<?, ?it/s]Calculating Metrics:  18%|█▊        | 2/11 [00:00<00:00, 4183.84it/s]Calculating Metrics:  18%|█▊        | 2/11 [00:00<00:00, 330.31it/s]Calculating Metrics:  27%|██▋       | 3/11 [00:00<00:00, 472.74it/s]Calculating Metrics:  27%|██▋       | 3/11 [00:00<00:00, 255.30it/s]Calculating Metrics:  45%|████▌     | 5/11 [00:00<00:00, 372.14it/s]Calculating Metrics:  45%|████▌     | 5/11 [00:00<00:00, 272.47it/s]Calculating Metrics:  64%|██████▎   | 7/11 [00:00<00:00, 347.48it/s]Calculating Metrics:  64%|██████▎   | 7/11 [00:00<00:00, 290.46it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 396.50it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 371.71it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 370.72it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 369.51it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_value_lengths_to_be_between:negative_test_with_max_lt_min] _

test_case = {'expectation_type': 'expect_column_value_lengths_to_be_between', 'pk_column': True, 'skip': False, 'test': {'exact_ma... 'unexpected_index_list': [0, 1, 2, 3, 4], 'unexpected_list': ['sassy', 'sexy', 'silly', 'slimy', 'smart']}, ...}, ...}

    @pytest.mark.order(index=0)
    @pytest.mark.integration
    @pytest.mark.slow  # 12.68s
    def test_case_runner_v3_api(test_case):
        if test_case["skip"]:
            pytest.skip()

        # Note: this should never be done in practice, but we are wiping expectations to reuse batches during testing.
        # test_case["batch"]._initialize_expectations()
        if "parse_strings_as_datetimes" in test_case["test"]["in"]:
            with pytest.deprecated_call():
                evaluate_json_test_v3_api(
                    validator=test_case["validator_with_data"],
                    expectation_type=test_case["expectation_type"],
                    test=test_case["test"],
                    pk_column=test_case["pk_column"],
                )
        else:
>           evaluate_json_test_v3_api(
                validator=test_case["validator_with_data"],
                expectation_type=test_case["expectation_type"],
                test=test_case["test"],
                pk_column=test_case["pk_column"],
            )

tests/test_definitions/test_expectations_v3_api.py:418:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
great_expectations/self_check/util.py:2901: in evaluate_json_test_v3_api
    check_json_test_result(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

test = {'exact_match_out': False, 'in': {'column': 's1', 'max_value': 0, 'min_value': 1}, 'input': {'column': 's1', 'max_valu...False, 'unexpected_index_list': [0, 1, 2, 3, 4], 'unexpected_list': ['sassy', 'sexy', 'silly', 'slimy', 'smart']}, ...}
result = {'exception_info': {'exception_message': None, 'exception_traceback': None, 'raised_exception': False}, 'expectation_c...': 'sexy'}, {'count': 1, 'value': 'silly'}, {'count': 1, 'value': 'slimy'}, {'count': 1, 'value': 'smart'}], ...}, ...}
data_asset = <great_expectations.execution_engine.pandas_batch_data.PandasBatchData object at 0x7f9502ac0a00>
pk_column = True

    def check_json_test_result(  # noqa: C901 - 52
        test, result, data_asset=None, pk_column=False
    ) -> None:

        # check for id_pk results in cases where pk_column is true and unexpected_index_list already exists
        # this will work for testing since result_format is COMPLETE
        if pk_column:
            if not result["success"]:
                if "unexpected_index_list" in result["result"]:
                    assert "unexpected_index_query" in result["result"]

        if "unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )
            elif "unexpected_list" in test["output"]:
                (
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )

        if "partial_unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "partial_unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )
            elif "partial_unexpected_list" in test["output"]:
                (
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )

        # Determine if np.allclose(..) might be needed for float comparison
        try_allclose = False
        if "observed_value" in test["output"]:
            if RX_FLOAT.match(repr(test["output"]["observed_value"])):
                try_allclose = True

        # Check results
        if test["exact_match_out"] is True:
            if "result" in result and "observed_value" in result["result"]:
                if isinstance(result["result"]["observed_value"], (np.floating, float)):
                    assert np.allclose(
                        result["result"]["observed_value"],
                        expectationValidationResultSchema.load(test["output"])["result"][
                            "observed_value"
                        ],
                        rtol=RTOL,
                        atol=ATOL,
                    ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {expectationValidationResultSchema.load(test['output'])['result']['observed_value']}"
                else:
                    assert result == expectationValidationResultSchema.load(
                        test["output"]
                    ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
            else:
                assert result == expectationValidationResultSchema.load(
                    test["output"]
                ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
        else:
            # Convert result to json since our tests are reading from json so cannot easily contain richer types (e.g. NaN)
            # NOTE - 20191031 - JPC - we may eventually want to change these tests as we update our view on how
            # representations, serializations, and objects should interact and how much of that is shown to the user.
            result = result.to_json_dict()
            for key, value in test["output"].items():
                # Apply our great expectations-specific test logic

                if key == "success":
                    if isinstance(value, (np.floating, float)):
                        try:
                            assert np.allclose(
                                result["success"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['success']} not np.allclose to {value}"
                        except TypeError:
                            assert (
                                result["success"] == value
                            ), f"{result['success']} != {value}"
                    else:
                        assert result["success"] == value, f"{result['success']} != {value}"

                elif key == "observed_value":
                    if "tolerance" in test:
                        if isinstance(value, dict):
                            assert set(result["result"]["observed_value"].keys()) == set(
                                value.keys()
                            ), f"{set(result['result']['observed_value'].keys())} != {set(value.keys())}"
                            for k, v in value.items():
                                assert np.allclose(
                                    result["result"]["observed_value"][k],
                                    v,
                                    rtol=test["tolerance"],
                                )
                        else:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=test["tolerance"],
                            )
                    else:
                        if isinstance(value, dict) and "values" in value:
                            try:
                                assert np.allclose(
                                    result["result"]["observed_value"]["values"],
                                    value["values"],
                                    rtol=RTOL,
                                    atol=ATOL,
                                ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']['values']} not np.allclose to {value['values']}"
                            except TypeError as e:
                                print(e)
                                assert (
                                    result["result"]["observed_value"] == value
                                ), f"{result['result']['observed_value']} != {value}"
                        elif try_allclose:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {value}"
                        else:
                            assert (
                                result["result"]["observed_value"] == value
                            ), f"{result['result']['observed_value']} != {value}"

                # NOTE: This is a key used ONLY for testing cases where an expectation is legitimately allowed to return
                # any of multiple possible observed_values. expect_column_values_to_be_of_type is one such expectation.
                elif key == "observed_value_list":
                    assert result["result"]["observed_value"] in value

                elif key == "unexpected_index_list":
                    if isinstance(data_asset, (SqlAlchemyDataset, SparkDFDataset)):
                        pass
                    elif isinstance(data_asset, (SqlAlchemyBatchData, SparkDFBatchData)):
                        pass
                    else:
>                       assert (
                            result["result"]["unexpected_index_list"] == value
                        ), f"{result['result']['unexpected_index_list']} != {value}"
E                       AssertionError: [{'s1': 'smart', 'pk_index': 0}, {'s1': 'silly', 'pk_index': 1}, {'s1': 'sassy', 'pk_index': 2}, {'s1': 'slimy', 'pk_index': 3}, {'s1': 'sexy', 'pk_index': 4}] != [0, 1, 2, 3, 4]

great_expectations/self_check/util.py:3075: AssertionError
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/11 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/11 [00:00<?, ?it/s]Calculating Metrics:  18%|█▊        | 2/11 [00:00<00:00, 4056.39it/s]Calculating Metrics:  18%|█▊        | 2/11 [00:00<00:00, 331.17it/s]Calculating Metrics:  27%|██▋       | 3/11 [00:00<00:00, 476.05it/s]Calculating Metrics:  27%|██▋       | 3/11 [00:00<00:00, 256.37it/s]Calculating Metrics:  45%|████▌     | 5/11 [00:00<00:00, 376.76it/s]Calculating Metrics:  45%|████▌     | 5/11 [00:00<00:00, 276.40it/s]Calculating Metrics:  64%|██████▎   | 7/11 [00:00<00:00, 352.96it/s]Calculating Metrics:  64%|██████▎   | 7/11 [00:00<00:00, 296.03it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 403.48it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 377.88it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 376.85it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 375.63it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_value_lengths_to_be_between:test_conditional_expectation_fails] _

test_case = {'expectation_type': 'expect_column_value_lengths_to_be_between', 'pk_column': True, 'skip': False, 'test': {'exact_ma...ceptions': True, 'column': 's1', 'condition_parser': 'pandas', 'max_value': 4, ...}, 'only_for': ['pandas'], ...}, ...}

    @pytest.mark.order(index=0)
    @pytest.mark.integration
    @pytest.mark.slow  # 12.68s
    def test_case_runner_v3_api(test_case):
        if test_case["skip"]:
            pytest.skip()

        # Note: this should never be done in practice, but we are wiping expectations to reuse batches during testing.
        # test_case["batch"]._initialize_expectations()
        if "parse_strings_as_datetimes" in test_case["test"]["in"]:
            with pytest.deprecated_call():
                evaluate_json_test_v3_api(
                    validator=test_case["validator_with_data"],
                    expectation_type=test_case["expectation_type"],
                    test=test_case["test"],
                    pk_column=test_case["pk_column"],
                )
        else:
>           evaluate_json_test_v3_api(
                validator=test_case["validator_with_data"],
                expectation_type=test_case["expectation_type"],
                test=test_case["test"],
                pk_column=test_case["pk_column"],
            )

tests/test_definitions/test_expectations_v3_api.py:418:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
great_expectations/self_check/util.py:2901: in evaluate_json_test_v3_api
    check_json_test_result(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

test = {'exact_match_out': False, 'in': {'catch_exceptions': True, 'column': 's1', 'condition_parser': 'pandas', 'max_value':...tch_exceptions': True, 'column': 's1', 'condition_parser': 'pandas', 'max_value': 4, ...}, 'only_for': ['pandas'], ...}
result = {'exception_info': {'exception_message': None, 'exception_traceback': None, 'raised_exception': False}, 'expectation_c..., 'missing_count': 0, 'missing_percent': 0.0, 'partial_unexpected_counts': [{'count': 1, 'value': 'slimy'}], ...}, ...}
data_asset = <great_expectations.execution_engine.pandas_batch_data.PandasBatchData object at 0x7f9502ac0a00>
pk_column = True

    def check_json_test_result(  # noqa: C901 - 52
        test, result, data_asset=None, pk_column=False
    ) -> None:

        # check for id_pk results in cases where pk_column is true and unexpected_index_list already exists
        # this will work for testing since result_format is COMPLETE
        if pk_column:
            if not result["success"]:
                if "unexpected_index_list" in result["result"]:
                    assert "unexpected_index_query" in result["result"]

        if "unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )
            elif "unexpected_list" in test["output"]:
                (
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )

        if "partial_unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "partial_unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )
            elif "partial_unexpected_list" in test["output"]:
                (
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )

        # Determine if np.allclose(..) might be needed for float comparison
        try_allclose = False
        if "observed_value" in test["output"]:
            if RX_FLOAT.match(repr(test["output"]["observed_value"])):
                try_allclose = True

        # Check results
        if test["exact_match_out"] is True:
            if "result" in result and "observed_value" in result["result"]:
                if isinstance(result["result"]["observed_value"], (np.floating, float)):
                    assert np.allclose(
                        result["result"]["observed_value"],
                        expectationValidationResultSchema.load(test["output"])["result"][
                            "observed_value"
                        ],
                        rtol=RTOL,
                        atol=ATOL,
                    ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {expectationValidationResultSchema.load(test['output'])['result']['observed_value']}"
                else:
                    assert result == expectationValidationResultSchema.load(
                        test["output"]
                    ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
            else:
                assert result == expectationValidationResultSchema.load(
                    test["output"]
                ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
        else:
            # Convert result to json since our tests are reading from json so cannot easily contain richer types (e.g. NaN)
            # NOTE - 20191031 - JPC - we may eventually want to change these tests as we update our view on how
            # representations, serializations, and objects should interact and how much of that is shown to the user.
            result = result.to_json_dict()
            for key, value in test["output"].items():
                # Apply our great expectations-specific test logic

                if key == "success":
                    if isinstance(value, (np.floating, float)):
                        try:
                            assert np.allclose(
                                result["success"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['success']} not np.allclose to {value}"
                        except TypeError:
                            assert (
                                result["success"] == value
                            ), f"{result['success']} != {value}"
                    else:
                        assert result["success"] == value, f"{result['success']} != {value}"

                elif key == "observed_value":
                    if "tolerance" in test:
                        if isinstance(value, dict):
                            assert set(result["result"]["observed_value"].keys()) == set(
                                value.keys()
                            ), f"{set(result['result']['observed_value'].keys())} != {set(value.keys())}"
                            for k, v in value.items():
                                assert np.allclose(
                                    result["result"]["observed_value"][k],
                                    v,
                                    rtol=test["tolerance"],
                                )
                        else:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=test["tolerance"],
                            )
                    else:
                        if isinstance(value, dict) and "values" in value:
                            try:
                                assert np.allclose(
                                    result["result"]["observed_value"]["values"],
                                    value["values"],
                                    rtol=RTOL,
                                    atol=ATOL,
                                ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']['values']} not np.allclose to {value['values']}"
                            except TypeError as e:
                                print(e)
                                assert (
                                    result["result"]["observed_value"] == value
                                ), f"{result['result']['observed_value']} != {value}"
                        elif try_allclose:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {value}"
                        else:
                            assert (
                                result["result"]["observed_value"] == value
                            ), f"{result['result']['observed_value']} != {value}"

                # NOTE: This is a key used ONLY for testing cases where an expectation is legitimately allowed to return
                # any of multiple possible observed_values. expect_column_values_to_be_of_type is one such expectation.
                elif key == "observed_value_list":
                    assert result["result"]["observed_value"] in value

                elif key == "unexpected_index_list":
                    if isinstance(data_asset, (SqlAlchemyDataset, SparkDFDataset)):
                        pass
                    elif isinstance(data_asset, (SqlAlchemyBatchData, SparkDFBatchData)):
                        pass
                    else:
>                       assert (
                            result["result"]["unexpected_index_list"] == value
                        ), f"{result['result']['unexpected_index_list']} != {value}"
E                       AssertionError: [{'s1': 'slimy', 'pk_index': 3}] != [3]

great_expectations/self_check/util.py:3075: AssertionError
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/11 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/11 [00:00<?, ?it/s]Calculating Metrics:  18%|█▊        | 2/11 [00:00<00:00, 833.94it/s]Calculating Metrics:  18%|█▊        | 2/11 [00:00<00:00, 309.50it/s]Calculating Metrics:  27%|██▋       | 3/11 [00:00<00:00, 450.37it/s]Calculating Metrics:  27%|██▋       | 3/11 [00:00<00:00, 280.21it/s]Calculating Metrics:  45%|████▌     | 5/11 [00:00<00:00, 367.21it/s]Calculating Metrics:  45%|████▌     | 5/11 [00:00<00:00, 290.06it/s]Calculating Metrics:  64%|██████▎   | 7/11 [00:00<00:00, 360.88it/s]Calculating Metrics:  64%|██████▎   | 7/11 [00:00<00:00, 314.04it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 396.49it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 377.10it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 376.07it/s]Calculating Metrics: 100%|██████████| 11/11 [00:00<00:00, 374.80it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_unique:basic_negative_case_all_non_unique_character_values] _

test_case = {'expectation_type': 'expect_column_values_to_be_unique', 'pk_column': True, 'skip': False, 'test': {'exact_match_out': False, 'in': {'column': 'a'}, 'include_in_gallery': True, 'input': {'column': 'a'}, ...}, ...}

    @pytest.mark.order(index=0)
    @pytest.mark.integration
    @pytest.mark.slow  # 12.68s
    def test_case_runner_v3_api(test_case):
        if test_case["skip"]:
            pytest.skip()

        # Note: this should never be done in practice, but we are wiping expectations to reuse batches during testing.
        # test_case["batch"]._initialize_expectations()
        if "parse_strings_as_datetimes" in test_case["test"]["in"]:
            with pytest.deprecated_call():
                evaluate_json_test_v3_api(
                    validator=test_case["validator_with_data"],
                    expectation_type=test_case["expectation_type"],
                    test=test_case["test"],
                    pk_column=test_case["pk_column"],
                )
        else:
>           evaluate_json_test_v3_api(
                validator=test_case["validator_with_data"],
                expectation_type=test_case["expectation_type"],
                test=test_case["test"],
                pk_column=test_case["pk_column"],
            )

tests/test_definitions/test_expectations_v3_api.py:418:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
great_expectations/self_check/util.py:2901: in evaluate_json_test_v3_api
    check_json_test_result(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

test = {'exact_match_out': False, 'in': {'column': 'a'}, 'include_in_gallery': True, 'input': {'column': 'a'}, ...}
result = {'exception_info': {'exception_message': None, 'exception_traceback': None, 'raised_exception': False}, 'expectation_c...: 4, 'missing_count': 1, 'missing_percent': 25.0, 'partial_unexpected_counts': [{'count': 3, 'value': '2'}], ...}, ...}
data_asset = <great_expectations.execution_engine.pandas_batch_data.PandasBatchData object at 0x7f9502bc62e0>
pk_column = True

    def check_json_test_result(  # noqa: C901 - 52
        test, result, data_asset=None, pk_column=False
    ) -> None:

        # check for id_pk results in cases where pk_column is true and unexpected_index_list already exists
        # this will work for testing since result_format is COMPLETE
        if pk_column:
            if not result["success"]:
                if "unexpected_index_list" in result["result"]:
                    assert "unexpected_index_query" in result["result"]

        if "unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )
            elif "unexpected_list" in test["output"]:
                (
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )

        if "partial_unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "partial_unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )
            elif "partial_unexpected_list" in test["output"]:
                (
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )

        # Determine if np.allclose(..) might be needed for float comparison
        try_allclose = False
        if "observed_value" in test["output"]:
            if RX_FLOAT.match(repr(test["output"]["observed_value"])):
                try_allclose = True

        # Check results
        if test["exact_match_out"] is True:
            if "result" in result and "observed_value" in result["result"]:
                if isinstance(result["result"]["observed_value"], (np.floating, float)):
                    assert np.allclose(
                        result["result"]["observed_value"],
                        expectationValidationResultSchema.load(test["output"])["result"][
                            "observed_value"
                        ],
                        rtol=RTOL,
                        atol=ATOL,
                    ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {expectationValidationResultSchema.load(test['output'])['result']['observed_value']}"
                else:
                    assert result == expectationValidationResultSchema.load(
                        test["output"]
                    ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
            else:
                assert result == expectationValidationResultSchema.load(
                    test["output"]
                ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
        else:
            # Convert result to json since our tests are reading from json so cannot easily contain richer types (e.g. NaN)
            # NOTE - 20191031 - JPC - we may eventually want to change these tests as we update our view on how
            # representations, serializations, and objects should interact and how much of that is shown to the user.
            result = result.to_json_dict()
            for key, value in test["output"].items():
                # Apply our great expectations-specific test logic

                if key == "success":
                    if isinstance(value, (np.floating, float)):
                        try:
                            assert np.allclose(
                                result["success"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['success']} not np.allclose to {value}"
                        except TypeError:
                            assert (
                                result["success"] == value
                            ), f"{result['success']} != {value}"
                    else:
                        assert result["success"] == value, f"{result['success']} != {value}"

                elif key == "observed_value":
                    if "tolerance" in test:
                        if isinstance(value, dict):
                            assert set(result["result"]["observed_value"].keys()) == set(
                                value.keys()
                            ), f"{set(result['result']['observed_value'].keys())} != {set(value.keys())}"
                            for k, v in value.items():
                                assert np.allclose(
                                    result["result"]["observed_value"][k],
                                    v,
                                    rtol=test["tolerance"],
                                )
                        else:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=test["tolerance"],
                            )
                    else:
                        if isinstance(value, dict) and "values" in value:
                            try:
                                assert np.allclose(
                                    result["result"]["observed_value"]["values"],
                                    value["values"],
                                    rtol=RTOL,
                                    atol=ATOL,
                                ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']['values']} not np.allclose to {value['values']}"
                            except TypeError as e:
                                print(e)
                                assert (
                                    result["result"]["observed_value"] == value
                                ), f"{result['result']['observed_value']} != {value}"
                        elif try_allclose:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {value}"
                        else:
                            assert (
                                result["result"]["observed_value"] == value
                            ), f"{result['result']['observed_value']} != {value}"

                # NOTE: This is a key used ONLY for testing cases where an expectation is legitimately allowed to return
                # any of multiple possible observed_values. expect_column_values_to_be_of_type is one such expectation.
                elif key == "observed_value_list":
                    assert result["result"]["observed_value"] in value

                elif key == "unexpected_index_list":
                    if isinstance(data_asset, (SqlAlchemyDataset, SparkDFDataset)):
                        pass
                    elif isinstance(data_asset, (SqlAlchemyBatchData, SparkDFBatchData)):
                        pass
                    else:
>                       assert (
                            result["result"]["unexpected_index_list"] == value
                        ), f"{result['result']['unexpected_index_list']} != {value}"
E                       AssertionError: [{'a': '2', 'pk_index': 0}, {'a': '2', 'pk_index': 1}, {'a': '2', 'pk_index': 2}] != [0, 1, 2]

great_expectations/self_check/util.py:3075: AssertionError
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 4194.30it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 499.23it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 705.52it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 395.35it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 553.70it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 425.24it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 654.87it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 609.34it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 606.39it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 602.96it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_unique:multi_type_column_contains_2_and_quoted_2_suppressed_for_sqalchemy] _

test_case = {'expectation_type': 'expect_column_values_to_be_unique', 'pk_column': True, 'skip': False, 'test': {'exact_match_out'...: 'c', 'mostly': 0.3}, 'out': {'success': True, 'unexpected_index_list': [0, 1], 'unexpected_list': [1, 1]}, ...}, ...}

    @pytest.mark.order(index=0)
    @pytest.mark.integration
    @pytest.mark.slow  # 12.68s
    def test_case_runner_v3_api(test_case):
        if test_case["skip"]:
            pytest.skip()

        # Note: this should never be done in practice, but we are wiping expectations to reuse batches during testing.
        # test_case["batch"]._initialize_expectations()
        if "parse_strings_as_datetimes" in test_case["test"]["in"]:
            with pytest.deprecated_call():
                evaluate_json_test_v3_api(
                    validator=test_case["validator_with_data"],
                    expectation_type=test_case["expectation_type"],
                    test=test_case["test"],
                    pk_column=test_case["pk_column"],
                )
        else:
>           evaluate_json_test_v3_api(
                validator=test_case["validator_with_data"],
                expectation_type=test_case["expectation_type"],
                test=test_case["test"],
                pk_column=test_case["pk_column"],
            )

tests/test_definitions/test_expectations_v3_api.py:418:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
great_expectations/self_check/util.py:2901: in evaluate_json_test_v3_api
    check_json_test_result(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

test = {'exact_match_out': False, 'in': {'column': 'c', 'mostly': 0.3}, 'input': {'column': 'c', 'mostly': 0.3}, 'out': {'success': True, 'unexpected_index_list': [0, 1], 'unexpected_list': [1, 1]}, ...}
result = {'exception_info': {'exception_message': None, 'exception_traceback': None, 'raised_exception': False}, 'expectation_c...: 4, 'missing_count': 1, 'missing_percent': 25.0, 'partial_unexpected_counts': [{'count': 2, 'value': 1.0}], ...}, ...}
data_asset = <great_expectations.execution_engine.pandas_batch_data.PandasBatchData object at 0x7f9502bc62e0>
pk_column = True

    def check_json_test_result(  # noqa: C901 - 52
        test, result, data_asset=None, pk_column=False
    ) -> None:

        # check for id_pk results in cases where pk_column is true and unexpected_index_list already exists
        # this will work for testing since result_format is COMPLETE
        if pk_column:
            if not result["success"]:
                if "unexpected_index_list" in result["result"]:
                    assert "unexpected_index_query" in result["result"]

        if "unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )
            elif "unexpected_list" in test["output"]:
                (
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )

        if "partial_unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "partial_unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )
            elif "partial_unexpected_list" in test["output"]:
                (
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )

        # Determine if np.allclose(..) might be needed for float comparison
        try_allclose = False
        if "observed_value" in test["output"]:
            if RX_FLOAT.match(repr(test["output"]["observed_value"])):
                try_allclose = True

        # Check results
        if test["exact_match_out"] is True:
            if "result" in result and "observed_value" in result["result"]:
                if isinstance(result["result"]["observed_value"], (np.floating, float)):
                    assert np.allclose(
                        result["result"]["observed_value"],
                        expectationValidationResultSchema.load(test["output"])["result"][
                            "observed_value"
                        ],
                        rtol=RTOL,
                        atol=ATOL,
                    ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {expectationValidationResultSchema.load(test['output'])['result']['observed_value']}"
                else:
                    assert result == expectationValidationResultSchema.load(
                        test["output"]
                    ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
            else:
                assert result == expectationValidationResultSchema.load(
                    test["output"]
                ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
        else:
            # Convert result to json since our tests are reading from json so cannot easily contain richer types (e.g. NaN)
            # NOTE - 20191031 - JPC - we may eventually want to change these tests as we update our view on how
            # representations, serializations, and objects should interact and how much of that is shown to the user.
            result = result.to_json_dict()
            for key, value in test["output"].items():
                # Apply our great expectations-specific test logic

                if key == "success":
                    if isinstance(value, (np.floating, float)):
                        try:
                            assert np.allclose(
                                result["success"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['success']} not np.allclose to {value}"
                        except TypeError:
                            assert (
                                result["success"] == value
                            ), f"{result['success']} != {value}"
                    else:
                        assert result["success"] == value, f"{result['success']} != {value}"

                elif key == "observed_value":
                    if "tolerance" in test:
                        if isinstance(value, dict):
                            assert set(result["result"]["observed_value"].keys()) == set(
                                value.keys()
                            ), f"{set(result['result']['observed_value'].keys())} != {set(value.keys())}"
                            for k, v in value.items():
                                assert np.allclose(
                                    result["result"]["observed_value"][k],
                                    v,
                                    rtol=test["tolerance"],
                                )
                        else:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=test["tolerance"],
                            )
                    else:
                        if isinstance(value, dict) and "values" in value:
                            try:
                                assert np.allclose(
                                    result["result"]["observed_value"]["values"],
                                    value["values"],
                                    rtol=RTOL,
                                    atol=ATOL,
                                ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']['values']} not np.allclose to {value['values']}"
                            except TypeError as e:
                                print(e)
                                assert (
                                    result["result"]["observed_value"] == value
                                ), f"{result['result']['observed_value']} != {value}"
                        elif try_allclose:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {value}"
                        else:
                            assert (
                                result["result"]["observed_value"] == value
                            ), f"{result['result']['observed_value']} != {value}"

                # NOTE: This is a key used ONLY for testing cases where an expectation is legitimately allowed to return
                # any of multiple possible observed_values. expect_column_values_to_be_of_type is one such expectation.
                elif key == "observed_value_list":
                    assert result["result"]["observed_value"] in value

                elif key == "unexpected_index_list":
                    if isinstance(data_asset, (SqlAlchemyDataset, SparkDFDataset)):
                        pass
                    elif isinstance(data_asset, (SqlAlchemyBatchData, SparkDFBatchData)):
                        pass
                    else:
>                       assert (
                            result["result"]["unexpected_index_list"] == value
                        ), f"{result['result']['unexpected_index_list']} != {value}"
E                       AssertionError: [{'c': 1.0, 'pk_index': 0}, {'c': 1.0, 'pk_index': 1}] != [0, 1]

great_expectations/self_check/util.py:3075: AssertionError
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 4202.71it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 500.90it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 707.38it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 395.47it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 554.68it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 426.81it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 659.46it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 612.23it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 609.09it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 605.54it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_unique:positive_case_using_mostly] _

test_case = {'expectation_type': 'expect_column_values_to_be_unique', 'pk_column': True, 'skip': False, 'test': {'exact_match_out'...: 'c', 'mostly': 0.3}, 'out': {'success': True, 'unexpected_index_list': [0, 1], 'unexpected_list': [1, 1]}, ...}, ...}

    @pytest.mark.order(index=0)
    @pytest.mark.integration
    @pytest.mark.slow  # 12.68s
    def test_case_runner_v3_api(test_case):
        if test_case["skip"]:
            pytest.skip()

        # Note: this should never be done in practice, but we are wiping expectations to reuse batches during testing.
        # test_case["batch"]._initialize_expectations()
        if "parse_strings_as_datetimes" in test_case["test"]["in"]:
            with pytest.deprecated_call():
                evaluate_json_test_v3_api(
                    validator=test_case["validator_with_data"],
                    expectation_type=test_case["expectation_type"],
                    test=test_case["test"],
                    pk_column=test_case["pk_column"],
                )
        else:
>           evaluate_json_test_v3_api(
                validator=test_case["validator_with_data"],
                expectation_type=test_case["expectation_type"],
                test=test_case["test"],
                pk_column=test_case["pk_column"],
            )

tests/test_definitions/test_expectations_v3_api.py:418:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
great_expectations/self_check/util.py:2901: in evaluate_json_test_v3_api
    check_json_test_result(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

test = {'exact_match_out': False, 'in': {'column': 'c', 'mostly': 0.3}, 'input': {'column': 'c', 'mostly': 0.3}, 'out': {'success': True, 'unexpected_index_list': [0, 1], 'unexpected_list': [1, 1]}, ...}
result = {'exception_info': {'exception_message': None, 'exception_traceback': None, 'raised_exception': False}, 'expectation_c...: 4, 'missing_count': 1, 'missing_percent': 25.0, 'partial_unexpected_counts': [{'count': 2, 'value': 1.0}], ...}, ...}
data_asset = <great_expectations.execution_engine.pandas_batch_data.PandasBatchData object at 0x7f9502bc62e0>
pk_column = True

    def check_json_test_result(  # noqa: C901 - 52
        test, result, data_asset=None, pk_column=False
    ) -> None:

        # check for id_pk results in cases where pk_column is true and unexpected_index_list already exists
        # this will work for testing since result_format is COMPLETE
        if pk_column:
            if not result["success"]:
                if "unexpected_index_list" in result["result"]:
                    assert "unexpected_index_query" in result["result"]

        if "unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )
            elif "unexpected_list" in test["output"]:
                (
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )

        if "partial_unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "partial_unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )
            elif "partial_unexpected_list" in test["output"]:
                (
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )

        # Determine if np.allclose(..) might be needed for float comparison
        try_allclose = False
        if "observed_value" in test["output"]:
            if RX_FLOAT.match(repr(test["output"]["observed_value"])):
                try_allclose = True

        # Check results
        if test["exact_match_out"] is True:
            if "result" in result and "observed_value" in result["result"]:
                if isinstance(result["result"]["observed_value"], (np.floating, float)):
                    assert np.allclose(
                        result["result"]["observed_value"],
                        expectationValidationResultSchema.load(test["output"])["result"][
                            "observed_value"
                        ],
                        rtol=RTOL,
                        atol=ATOL,
                    ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {expectationValidationResultSchema.load(test['output'])['result']['observed_value']}"
                else:
                    assert result == expectationValidationResultSchema.load(
                        test["output"]
                    ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
            else:
                assert result == expectationValidationResultSchema.load(
                    test["output"]
                ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
        else:
            # Convert result to json since our tests are reading from json so cannot easily contain richer types (e.g. NaN)
            # NOTE - 20191031 - JPC - we may eventually want to change these tests as we update our view on how
            # representations, serializations, and objects should interact and how much of that is shown to the user.
            result = result.to_json_dict()
            for key, value in test["output"].items():
                # Apply our great expectations-specific test logic

                if key == "success":
                    if isinstance(value, (np.floating, float)):
                        try:
                            assert np.allclose(
                                result["success"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['success']} not np.allclose to {value}"
                        except TypeError:
                            assert (
                                result["success"] == value
                            ), f"{result['success']} != {value}"
                    else:
                        assert result["success"] == value, f"{result['success']} != {value}"

                elif key == "observed_value":
                    if "tolerance" in test:
                        if isinstance(value, dict):
                            assert set(result["result"]["observed_value"].keys()) == set(
                                value.keys()
                            ), f"{set(result['result']['observed_value'].keys())} != {set(value.keys())}"
                            for k, v in value.items():
                                assert np.allclose(
                                    result["result"]["observed_value"][k],
                                    v,
                                    rtol=test["tolerance"],
                                )
                        else:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=test["tolerance"],
                            )
                    else:
                        if isinstance(value, dict) and "values" in value:
                            try:
                                assert np.allclose(
                                    result["result"]["observed_value"]["values"],
                                    value["values"],
                                    rtol=RTOL,
                                    atol=ATOL,
                                ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']['values']} not np.allclose to {value['values']}"
                            except TypeError as e:
                                print(e)
                                assert (
                                    result["result"]["observed_value"] == value
                                ), f"{result['result']['observed_value']} != {value}"
                        elif try_allclose:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {value}"
                        else:
                            assert (
                                result["result"]["observed_value"] == value
                            ), f"{result['result']['observed_value']} != {value}"

                # NOTE: This is a key used ONLY for testing cases where an expectation is legitimately allowed to return
                # any of multiple possible observed_values. expect_column_values_to_be_of_type is one such expectation.
                elif key == "observed_value_list":
                    assert result["result"]["observed_value"] in value

                elif key == "unexpected_index_list":
                    if isinstance(data_asset, (SqlAlchemyDataset, SparkDFDataset)):
                        pass
                    elif isinstance(data_asset, (SqlAlchemyBatchData, SparkDFBatchData)):
                        pass
                    else:
>                       assert (
                            result["result"]["unexpected_index_list"] == value
                        ), f"{result['result']['unexpected_index_list']} != {value}"
E                       AssertionError: [{'c': 1.0, 'pk_index': 0}, {'c': 1.0, 'pk_index': 1}] != [0, 1]

great_expectations/self_check/util.py:3075: AssertionError
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 4256.02it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 498.76it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 704.57it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 395.73it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 557.74it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 427.50it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 656.04it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 608.50it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 605.29it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 601.69it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_unique:negative_case_using_mostly] _

test_case = {'expectation_type': 'expect_column_values_to_be_unique', 'pk_column': True, 'skip': False, 'test': {'exact_match_out'... 'c', 'mostly': 0.4}, 'out': {'success': False, 'unexpected_index_list': [0, 1], 'unexpected_list': [1, 1]}, ...}, ...}

    @pytest.mark.order(index=0)
    @pytest.mark.integration
    @pytest.mark.slow  # 12.68s
    def test_case_runner_v3_api(test_case):
        if test_case["skip"]:
            pytest.skip()

        # Note: this should never be done in practice, but we are wiping expectations to reuse batches during testing.
        # test_case["batch"]._initialize_expectations()
        if "parse_strings_as_datetimes" in test_case["test"]["in"]:
            with pytest.deprecated_call():
                evaluate_json_test_v3_api(
                    validator=test_case["validator_with_data"],
                    expectation_type=test_case["expectation_type"],
                    test=test_case["test"],
                    pk_column=test_case["pk_column"],
                )
        else:
>           evaluate_json_test_v3_api(
                validator=test_case["validator_with_data"],
                expectation_type=test_case["expectation_type"],
                test=test_case["test"],
                pk_column=test_case["pk_column"],
            )

tests/test_definitions/test_expectations_v3_api.py:418:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
great_expectations/self_check/util.py:2901: in evaluate_json_test_v3_api
    check_json_test_result(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

test = {'exact_match_out': False, 'in': {'column': 'c', 'mostly': 0.4}, 'input': {'column': 'c', 'mostly': 0.4}, 'out': {'success': False, 'unexpected_index_list': [0, 1], 'unexpected_list': [1, 1]}, ...}
result = {'exception_info': {'exception_message': None, 'exception_traceback': None, 'raised_exception': False}, 'expectation_c...: 4, 'missing_count': 1, 'missing_percent': 25.0, 'partial_unexpected_counts': [{'count': 2, 'value': 1.0}], ...}, ...}
data_asset = <great_expectations.execution_engine.pandas_batch_data.PandasBatchData object at 0x7f9502bc62e0>
pk_column = True

    def check_json_test_result(  # noqa: C901 - 52
        test, result, data_asset=None, pk_column=False
    ) -> None:

        # check for id_pk results in cases where pk_column is true and unexpected_index_list already exists
        # this will work for testing since result_format is COMPLETE
        if pk_column:
            if not result["success"]:
                if "unexpected_index_list" in result["result"]:
                    assert "unexpected_index_query" in result["result"]

        if "unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )
            elif "unexpected_list" in test["output"]:
                (
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )

        if "partial_unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "partial_unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )
            elif "partial_unexpected_list" in test["output"]:
                (
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )

        # Determine if np.allclose(..) might be needed for float comparison
        try_allclose = False
        if "observed_value" in test["output"]:
            if RX_FLOAT.match(repr(test["output"]["observed_value"])):
                try_allclose = True

        # Check results
        if test["exact_match_out"] is True:
            if "result" in result and "observed_value" in result["result"]:
                if isinstance(result["result"]["observed_value"], (np.floating, float)):
                    assert np.allclose(
                        result["result"]["observed_value"],
                        expectationValidationResultSchema.load(test["output"])["result"][
                            "observed_value"
                        ],
                        rtol=RTOL,
                        atol=ATOL,
                    ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {expectationValidationResultSchema.load(test['output'])['result']['observed_value']}"
                else:
                    assert result == expectationValidationResultSchema.load(
                        test["output"]
                    ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
            else:
                assert result == expectationValidationResultSchema.load(
                    test["output"]
                ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
        else:
            # Convert result to json since our tests are reading from json so cannot easily contain richer types (e.g. NaN)
            # NOTE - 20191031 - JPC - we may eventually want to change these tests as we update our view on how
            # representations, serializations, and objects should interact and how much of that is shown to the user.
            result = result.to_json_dict()
            for key, value in test["output"].items():
                # Apply our great expectations-specific test logic

                if key == "success":
                    if isinstance(value, (np.floating, float)):
                        try:
                            assert np.allclose(
                                result["success"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['success']} not np.allclose to {value}"
                        except TypeError:
                            assert (
                                result["success"] == value
                            ), f"{result['success']} != {value}"
                    else:
                        assert result["success"] == value, f"{result['success']} != {value}"

                elif key == "observed_value":
                    if "tolerance" in test:
                        if isinstance(value, dict):
                            assert set(result["result"]["observed_value"].keys()) == set(
                                value.keys()
                            ), f"{set(result['result']['observed_value'].keys())} != {set(value.keys())}"
                            for k, v in value.items():
                                assert np.allclose(
                                    result["result"]["observed_value"][k],
                                    v,
                                    rtol=test["tolerance"],
                                )
                        else:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=test["tolerance"],
                            )
                    else:
                        if isinstance(value, dict) and "values" in value:
                            try:
                                assert np.allclose(
                                    result["result"]["observed_value"]["values"],
                                    value["values"],
                                    rtol=RTOL,
                                    atol=ATOL,
                                ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']['values']} not np.allclose to {value['values']}"
                            except TypeError as e:
                                print(e)
                                assert (
                                    result["result"]["observed_value"] == value
                                ), f"{result['result']['observed_value']} != {value}"
                        elif try_allclose:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {value}"
                        else:
                            assert (
                                result["result"]["observed_value"] == value
                            ), f"{result['result']['observed_value']} != {value}"

                # NOTE: This is a key used ONLY for testing cases where an expectation is legitimately allowed to return
                # any of multiple possible observed_values. expect_column_values_to_be_of_type is one such expectation.
                elif key == "observed_value_list":
                    assert result["result"]["observed_value"] in value

                elif key == "unexpected_index_list":
                    if isinstance(data_asset, (SqlAlchemyDataset, SparkDFDataset)):
                        pass
                    elif isinstance(data_asset, (SqlAlchemyBatchData, SparkDFBatchData)):
                        pass
                    else:
>                       assert (
                            result["result"]["unexpected_index_list"] == value
                        ), f"{result['result']['unexpected_index_list']} != {value}"
E                       AssertionError: [{'c': 1.0, 'pk_index': 0}, {'c': 1.0, 'pk_index': 1}] != [0, 1]

great_expectations/self_check/util.py:3075: AssertionError
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 2577.94it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 376.10it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 531.73it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 299.35it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 404.25it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 315.42it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 488.04it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 454.90it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 452.47it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 449.99it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_unique:negative_case_non_unique_numeric_values] _

test_case = {'expectation_type': 'expect_column_values_to_be_unique', 'pk_column': True, 'skip': False, 'test': {'exact_match_out'...ut': {'column': 'c'}, 'out': {'success': False, 'unexpected_index_list': [0, 1], 'unexpected_list': [1, 1]}, ...}, ...}

    @pytest.mark.order(index=0)
    @pytest.mark.integration
    @pytest.mark.slow  # 12.68s
    def test_case_runner_v3_api(test_case):
        if test_case["skip"]:
            pytest.skip()

        # Note: this should never be done in practice, but we are wiping expectations to reuse batches during testing.
        # test_case["batch"]._initialize_expectations()
        if "parse_strings_as_datetimes" in test_case["test"]["in"]:
            with pytest.deprecated_call():
                evaluate_json_test_v3_api(
                    validator=test_case["validator_with_data"],
                    expectation_type=test_case["expectation_type"],
                    test=test_case["test"],
                    pk_column=test_case["pk_column"],
                )
        else:
>           evaluate_json_test_v3_api(
                validator=test_case["validator_with_data"],
                expectation_type=test_case["expectation_type"],
                test=test_case["test"],
                pk_column=test_case["pk_column"],
            )

tests/test_definitions/test_expectations_v3_api.py:418:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
great_expectations/self_check/util.py:2901: in evaluate_json_test_v3_api
    check_json_test_result(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

test = {'exact_match_out': False, 'in': {'column': 'c'}, 'input': {'column': 'c'}, 'out': {'success': False, 'unexpected_index_list': [0, 1], 'unexpected_list': [1, 1]}, ...}
result = {'exception_info': {'exception_message': None, 'exception_traceback': None, 'raised_exception': False}, 'expectation_c...: 4, 'missing_count': 1, 'missing_percent': 25.0, 'partial_unexpected_counts': [{'count': 2, 'value': 1.0}], ...}, ...}
data_asset = <great_expectations.execution_engine.pandas_batch_data.PandasBatchData object at 0x7f9502bc62e0>
pk_column = True

    def check_json_test_result(  # noqa: C901 - 52
        test, result, data_asset=None, pk_column=False
    ) -> None:

        # check for id_pk results in cases where pk_column is true and unexpected_index_list already exists
        # this will work for testing since result_format is COMPLETE
        if pk_column:
            if not result["success"]:
                if "unexpected_index_list" in result["result"]:
                    assert "unexpected_index_query" in result["result"]

        if "unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )
            elif "unexpected_list" in test["output"]:
                (
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )

        if "partial_unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "partial_unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )
            elif "partial_unexpected_list" in test["output"]:
                (
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )

        # Determine if np.allclose(..) might be needed for float comparison
        try_allclose = False
        if "observed_value" in test["output"]:
            if RX_FLOAT.match(repr(test["output"]["observed_value"])):
                try_allclose = True

        # Check results
        if test["exact_match_out"] is True:
            if "result" in result and "observed_value" in result["result"]:
                if isinstance(result["result"]["observed_value"], (np.floating, float)):
                    assert np.allclose(
                        result["result"]["observed_value"],
                        expectationValidationResultSchema.load(test["output"])["result"][
                            "observed_value"
                        ],
                        rtol=RTOL,
                        atol=ATOL,
                    ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {expectationValidationResultSchema.load(test['output'])['result']['observed_value']}"
                else:
                    assert result == expectationValidationResultSchema.load(
                        test["output"]
                    ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
            else:
                assert result == expectationValidationResultSchema.load(
                    test["output"]
                ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
        else:
            # Convert result to json since our tests are reading from json so cannot easily contain richer types (e.g. NaN)
            # NOTE - 20191031 - JPC - we may eventually want to change these tests as we update our view on how
            # representations, serializations, and objects should interact and how much of that is shown to the user.
            result = result.to_json_dict()
            for key, value in test["output"].items():
                # Apply our great expectations-specific test logic

                if key == "success":
                    if isinstance(value, (np.floating, float)):
                        try:
                            assert np.allclose(
                                result["success"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['success']} not np.allclose to {value}"
                        except TypeError:
                            assert (
                                result["success"] == value
                            ), f"{result['success']} != {value}"
                    else:
                        assert result["success"] == value, f"{result['success']} != {value}"

                elif key == "observed_value":
                    if "tolerance" in test:
                        if isinstance(value, dict):
                            assert set(result["result"]["observed_value"].keys()) == set(
                                value.keys()
                            ), f"{set(result['result']['observed_value'].keys())} != {set(value.keys())}"
                            for k, v in value.items():
                                assert np.allclose(
                                    result["result"]["observed_value"][k],
                                    v,
                                    rtol=test["tolerance"],
                                )
                        else:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=test["tolerance"],
                            )
                    else:
                        if isinstance(value, dict) and "values" in value:
                            try:
                                assert np.allclose(
                                    result["result"]["observed_value"]["values"],
                                    value["values"],
                                    rtol=RTOL,
                                    atol=ATOL,
                                ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']['values']} not np.allclose to {value['values']}"
                            except TypeError as e:
                                print(e)
                                assert (
                                    result["result"]["observed_value"] == value
                                ), f"{result['result']['observed_value']} != {value}"
                        elif try_allclose:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {value}"
                        else:
                            assert (
                                result["result"]["observed_value"] == value
                            ), f"{result['result']['observed_value']} != {value}"

                # NOTE: This is a key used ONLY for testing cases where an expectation is legitimately allowed to return
                # any of multiple possible observed_values. expect_column_values_to_be_of_type is one such expectation.
                elif key == "observed_value_list":
                    assert result["result"]["observed_value"] in value

                elif key == "unexpected_index_list":
                    if isinstance(data_asset, (SqlAlchemyDataset, SparkDFDataset)):
                        pass
                    elif isinstance(data_asset, (SqlAlchemyBatchData, SparkDFBatchData)):
                        pass
                    else:
>                       assert (
                            result["result"]["unexpected_index_list"] == value
                        ), f"{result['result']['unexpected_index_list']} != {value}"
E                       AssertionError: [{'c': 1.0, 'pk_index': 0}, {'c': 1.0, 'pk_index': 1}] != [0, 1]

great_expectations/self_check/util.py:3075: AssertionError
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 4148.67it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 486.16it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 686.35it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 385.69it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 533.21it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 412.19it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 638.98it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 595.03it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 592.24it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 588.86it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_unique:negative_multiple_duplicate_values] _

test_case = {'expectation_type': 'expect_column_values_to_be_unique', 'pk_column': True, 'skip': False, 'test': {'exact_match_out'..., 'out': {'success': False, 'unexpected_index_list': [0, 1, 2, 3], 'unexpected_list': ['1', '1', '2', '2']}, ...}, ...}

    @pytest.mark.order(index=0)
    @pytest.mark.integration
    @pytest.mark.slow  # 12.68s
    def test_case_runner_v3_api(test_case):
        if test_case["skip"]:
            pytest.skip()

        # Note: this should never be done in practice, but we are wiping expectations to reuse batches during testing.
        # test_case["batch"]._initialize_expectations()
        if "parse_strings_as_datetimes" in test_case["test"]["in"]:
            with pytest.deprecated_call():
                evaluate_json_test_v3_api(
                    validator=test_case["validator_with_data"],
                    expectation_type=test_case["expectation_type"],
                    test=test_case["test"],
                    pk_column=test_case["pk_column"],
                )
        else:
>           evaluate_json_test_v3_api(
                validator=test_case["validator_with_data"],
                expectation_type=test_case["expectation_type"],
                test=test_case["test"],
                pk_column=test_case["pk_column"],
            )

tests/test_definitions/test_expectations_v3_api.py:418:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
great_expectations/self_check/util.py:2901: in evaluate_json_test_v3_api
    check_json_test_result(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

test = {'exact_match_out': False, 'in': {'column': 'mult_dup'}, 'input': {'column': 'mult_dup'}, 'out': {'success': False, 'unexpected_index_list': [0, 1, 2, 3], 'unexpected_list': ['1', '1', '2', '2']}, ...}
result = {'exception_info': {'exception_message': None, 'exception_traceback': None, 'raised_exception': False}, 'expectation_c...issing_percent': 0.0, 'partial_unexpected_counts': [{'count': 2, 'value': '1'}, {'count': 2, 'value': '2'}], ...}, ...}
data_asset = <great_expectations.execution_engine.pandas_batch_data.PandasBatchData object at 0x7f9502bc62e0>
pk_column = True

    def check_json_test_result(  # noqa: C901 - 52
        test, result, data_asset=None, pk_column=False
    ) -> None:

        # check for id_pk results in cases where pk_column is true and unexpected_index_list already exists
        # this will work for testing since result_format is COMPLETE
        if pk_column:
            if not result["success"]:
                if "unexpected_index_list" in result["result"]:
                    assert "unexpected_index_query" in result["result"]

        if "unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )
            elif "unexpected_list" in test["output"]:
                (
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )

        if "partial_unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "partial_unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )
            elif "partial_unexpected_list" in test["output"]:
                (
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )

        # Determine if np.allclose(..) might be needed for float comparison
        try_allclose = False
        if "observed_value" in test["output"]:
            if RX_FLOAT.match(repr(test["output"]["observed_value"])):
                try_allclose = True

        # Check results
        if test["exact_match_out"] is True:
            if "result" in result and "observed_value" in result["result"]:
                if isinstance(result["result"]["observed_value"], (np.floating, float)):
                    assert np.allclose(
                        result["result"]["observed_value"],
                        expectationValidationResultSchema.load(test["output"])["result"][
                            "observed_value"
                        ],
                        rtol=RTOL,
                        atol=ATOL,
                    ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {expectationValidationResultSchema.load(test['output'])['result']['observed_value']}"
                else:
                    assert result == expectationValidationResultSchema.load(
                        test["output"]
                    ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
            else:
                assert result == expectationValidationResultSchema.load(
                    test["output"]
                ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
        else:
            # Convert result to json since our tests are reading from json so cannot easily contain richer types (e.g. NaN)
            # NOTE - 20191031 - JPC - we may eventually want to change these tests as we update our view on how
            # representations, serializations, and objects should interact and how much of that is shown to the user.
            result = result.to_json_dict()
            for key, value in test["output"].items():
                # Apply our great expectations-specific test logic

                if key == "success":
                    if isinstance(value, (np.floating, float)):
                        try:
                            assert np.allclose(
                                result["success"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['success']} not np.allclose to {value}"
                        except TypeError:
                            assert (
                                result["success"] == value
                            ), f"{result['success']} != {value}"
                    else:
                        assert result["success"] == value, f"{result['success']} != {value}"

                elif key == "observed_value":
                    if "tolerance" in test:
                        if isinstance(value, dict):
                            assert set(result["result"]["observed_value"].keys()) == set(
                                value.keys()
                            ), f"{set(result['result']['observed_value'].keys())} != {set(value.keys())}"
                            for k, v in value.items():
                                assert np.allclose(
                                    result["result"]["observed_value"][k],
                                    v,
                                    rtol=test["tolerance"],
                                )
                        else:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=test["tolerance"],
                            )
                    else:
                        if isinstance(value, dict) and "values" in value:
                            try:
                                assert np.allclose(
                                    result["result"]["observed_value"]["values"],
                                    value["values"],
                                    rtol=RTOL,
                                    atol=ATOL,
                                ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']['values']} not np.allclose to {value['values']}"
                            except TypeError as e:
                                print(e)
                                assert (
                                    result["result"]["observed_value"] == value
                                ), f"{result['result']['observed_value']} != {value}"
                        elif try_allclose:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {value}"
                        else:
                            assert (
                                result["result"]["observed_value"] == value
                            ), f"{result['result']['observed_value']} != {value}"

                # NOTE: This is a key used ONLY for testing cases where an expectation is legitimately allowed to return
                # any of multiple possible observed_values. expect_column_values_to_be_of_type is one such expectation.
                elif key == "observed_value_list":
                    assert result["result"]["observed_value"] in value

                elif key == "unexpected_index_list":
                    if isinstance(data_asset, (SqlAlchemyDataset, SparkDFDataset)):
                        pass
                    elif isinstance(data_asset, (SqlAlchemyBatchData, SparkDFBatchData)):
                        pass
                    else:
>                       assert (
                            result["result"]["unexpected_index_list"] == value
                        ), f"{result['result']['unexpected_index_list']} != {value}"
E                       AssertionError: [{'mult_dup': '1', 'pk_index': 0}, {'mult_dup': '1', 'pk_index': 1}, {'mult_dup': '2', 'pk_index': 2}, {'mult_dup': '2', 'pk_index': 3}] != [0, 1, 2, 3]

great_expectations/self_check/util.py:3075: AssertionError
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 3975.64it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 471.24it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 660.21it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 378.74it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 510.09it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 394.54it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 607.57it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 566.58it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 563.99it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 560.86it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_match_json_schema:basic_negative_test] _

test_case = {'expectation_type': 'expect_column_values_to_match_json_schema', 'pk_column': True, 'skip': False, 'test': {'exact_ma...rue, 'input': {'column': 'x', 'json_schema': {'properties': {'a': {'type': 'integer'}}, 'required': ['b']}}, ...}, ...}

    @pytest.mark.order(index=0)
    @pytest.mark.integration
    @pytest.mark.slow  # 12.68s
    def test_case_runner_v3_api(test_case):
        if test_case["skip"]:
            pytest.skip()

        # Note: this should never be done in practice, but we are wiping expectations to reuse batches during testing.
        # test_case["batch"]._initialize_expectations()
        if "parse_strings_as_datetimes" in test_case["test"]["in"]:
            with pytest.deprecated_call():
                evaluate_json_test_v3_api(
                    validator=test_case["validator_with_data"],
                    expectation_type=test_case["expectation_type"],
                    test=test_case["test"],
                    pk_column=test_case["pk_column"],
                )
        else:
>           evaluate_json_test_v3_api(
                validator=test_case["validator_with_data"],
                expectation_type=test_case["expectation_type"],
                test=test_case["test"],
                pk_column=test_case["pk_column"],
            )

tests/test_definitions/test_expectations_v3_api.py:418:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
great_expectations/self_check/util.py:2901: in evaluate_json_test_v3_api
    check_json_test_result(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

test = {'exact_match_out': False, 'in': {'column': 'x', 'json_schema': {'properties': {'a': {'type': 'integer'}}, 'required':...ry': True, 'input': {'column': 'x', 'json_schema': {'properties': {'a': {'type': 'integer'}}, 'required': ['b']}}, ...}
result = {'exception_info': {'exception_message': None, 'exception_traceback': None, 'raised_exception': False}, 'expectation_c...2}'}, {'count': 1, 'value': '{"a":3}'}, {'count': 1, 'value': '{"a":4}'}, {'count': 1, 'value': '{"a":5}'}], ...}, ...}
data_asset = <great_expectations.execution_engine.pandas_batch_data.PandasBatchData object at 0x7f9502a3d4c0>
pk_column = True

    def check_json_test_result(  # noqa: C901 - 52
        test, result, data_asset=None, pk_column=False
    ) -> None:

        # check for id_pk results in cases where pk_column is true and unexpected_index_list already exists
        # this will work for testing since result_format is COMPLETE
        if pk_column:
            if not result["success"]:
                if "unexpected_index_list" in result["result"]:
                    assert "unexpected_index_query" in result["result"]

        if "unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )
            elif "unexpected_list" in test["output"]:
                (
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )

        if "partial_unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "partial_unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )
            elif "partial_unexpected_list" in test["output"]:
                (
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )

        # Determine if np.allclose(..) might be needed for float comparison
        try_allclose = False
        if "observed_value" in test["output"]:
            if RX_FLOAT.match(repr(test["output"]["observed_value"])):
                try_allclose = True

        # Check results
        if test["exact_match_out"] is True:
            if "result" in result and "observed_value" in result["result"]:
                if isinstance(result["result"]["observed_value"], (np.floating, float)):
                    assert np.allclose(
                        result["result"]["observed_value"],
                        expectationValidationResultSchema.load(test["output"])["result"][
                            "observed_value"
                        ],
                        rtol=RTOL,
                        atol=ATOL,
                    ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {expectationValidationResultSchema.load(test['output'])['result']['observed_value']}"
                else:
                    assert result == expectationValidationResultSchema.load(
                        test["output"]
                    ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
            else:
                assert result == expectationValidationResultSchema.load(
                    test["output"]
                ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
        else:
            # Convert result to json since our tests are reading from json so cannot easily contain richer types (e.g. NaN)
            # NOTE - 20191031 - JPC - we may eventually want to change these tests as we update our view on how
            # representations, serializations, and objects should interact and how much of that is shown to the user.
            result = result.to_json_dict()
            for key, value in test["output"].items():
                # Apply our great expectations-specific test logic

                if key == "success":
                    if isinstance(value, (np.floating, float)):
                        try:
                            assert np.allclose(
                                result["success"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['success']} not np.allclose to {value}"
                        except TypeError:
                            assert (
                                result["success"] == value
                            ), f"{result['success']} != {value}"
                    else:
                        assert result["success"] == value, f"{result['success']} != {value}"

                elif key == "observed_value":
                    if "tolerance" in test:
                        if isinstance(value, dict):
                            assert set(result["result"]["observed_value"].keys()) == set(
                                value.keys()
                            ), f"{set(result['result']['observed_value'].keys())} != {set(value.keys())}"
                            for k, v in value.items():
                                assert np.allclose(
                                    result["result"]["observed_value"][k],
                                    v,
                                    rtol=test["tolerance"],
                                )
                        else:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=test["tolerance"],
                            )
                    else:
                        if isinstance(value, dict) and "values" in value:
                            try:
                                assert np.allclose(
                                    result["result"]["observed_value"]["values"],
                                    value["values"],
                                    rtol=RTOL,
                                    atol=ATOL,
                                ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']['values']} not np.allclose to {value['values']}"
                            except TypeError as e:
                                print(e)
                                assert (
                                    result["result"]["observed_value"] == value
                                ), f"{result['result']['observed_value']} != {value}"
                        elif try_allclose:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {value}"
                        else:
                            assert (
                                result["result"]["observed_value"] == value
                            ), f"{result['result']['observed_value']} != {value}"

                # NOTE: This is a key used ONLY for testing cases where an expectation is legitimately allowed to return
                # any of multiple possible observed_values. expect_column_values_to_be_of_type is one such expectation.
                elif key == "observed_value_list":
                    assert result["result"]["observed_value"] in value

                elif key == "unexpected_index_list":
                    if isinstance(data_asset, (SqlAlchemyDataset, SparkDFDataset)):
                        pass
                    elif isinstance(data_asset, (SqlAlchemyBatchData, SparkDFBatchData)):
                        pass
                    else:
>                       assert (
                            result["result"]["unexpected_index_list"] == value
                        ), f"{result['result']['unexpected_index_list']} != {value}"
E                       AssertionError: [{'x': '{"a":1}', 'pk_index': 0}, {'x': '{"a":2}', 'pk_index': 1}, {'x': '{"a":3}', 'pk_index': 2}, {'x': '{"a":4}', 'pk_index': 3}, {'x': '{"a":5}', 'pk_index': 4}] != [0, 1, 2, 3, 4]

great_expectations/self_check/util.py:3075: AssertionError
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 3164.32it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 388.20it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 552.80it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 318.51it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 276.92it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 234.10it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 397.87it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 376.72it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 375.50it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 374.10it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_null:negative_no_missing_values] _

test_case = {'expectation_type': 'expect_column_values_to_be_null', 'pk_column': True, 'skip': False, 'test': {'exact_match_out': False, 'in': {'column': 'no_null'}, 'include_in_gallery': True, 'input': {'column': 'no_null'}, ...}, ...}

    @pytest.mark.order(index=0)
    @pytest.mark.integration
    @pytest.mark.slow  # 12.68s
    def test_case_runner_v3_api(test_case):
        if test_case["skip"]:
            pytest.skip()

        # Note: this should never be done in practice, but we are wiping expectations to reuse batches during testing.
        # test_case["batch"]._initialize_expectations()
        if "parse_strings_as_datetimes" in test_case["test"]["in"]:
            with pytest.deprecated_call():
                evaluate_json_test_v3_api(
                    validator=test_case["validator_with_data"],
                    expectation_type=test_case["expectation_type"],
                    test=test_case["test"],
                    pk_column=test_case["pk_column"],
                )
        else:
>           evaluate_json_test_v3_api(
                validator=test_case["validator_with_data"],
                expectation_type=test_case["expectation_type"],
                test=test_case["test"],
                pk_column=test_case["pk_column"],
            )

tests/test_definitions/test_expectations_v3_api.py:418:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
great_expectations/self_check/util.py:2901: in evaluate_json_test_v3_api
    check_json_test_result(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

test = {'exact_match_out': False, 'in': {'column': 'no_null'}, 'include_in_gallery': True, 'input': {'column': 'no_null'}, ...}
result = {'exception_info': {'exception_message': None, 'exception_traceback': None, 'raised_exception': False}, 'expectation_c...ll': 'c', 'pk_index': 2}, {'no_null': 'd', 'pk_index': 3}], 'partial_unexpected_list': ['a', 'b', 'c', 'd'], ...}, ...}
data_asset = <great_expectations.execution_engine.pandas_batch_data.PandasBatchData object at 0x7f9502c5d430>
pk_column = True

    def check_json_test_result(  # noqa: C901 - 52
        test, result, data_asset=None, pk_column=False
    ) -> None:

        # check for id_pk results in cases where pk_column is true and unexpected_index_list already exists
        # this will work for testing since result_format is COMPLETE
        if pk_column:
            if not result["success"]:
                if "unexpected_index_list" in result["result"]:
                    assert "unexpected_index_query" in result["result"]

        if "unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )
            elif "unexpected_list" in test["output"]:
                (
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )

        if "partial_unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "partial_unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )
            elif "partial_unexpected_list" in test["output"]:
                (
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )

        # Determine if np.allclose(..) might be needed for float comparison
        try_allclose = False
        if "observed_value" in test["output"]:
            if RX_FLOAT.match(repr(test["output"]["observed_value"])):
                try_allclose = True

        # Check results
        if test["exact_match_out"] is True:
            if "result" in result and "observed_value" in result["result"]:
                if isinstance(result["result"]["observed_value"], (np.floating, float)):
                    assert np.allclose(
                        result["result"]["observed_value"],
                        expectationValidationResultSchema.load(test["output"])["result"][
                            "observed_value"
                        ],
                        rtol=RTOL,
                        atol=ATOL,
                    ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {expectationValidationResultSchema.load(test['output'])['result']['observed_value']}"
                else:
                    assert result == expectationValidationResultSchema.load(
                        test["output"]
                    ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
            else:
                assert result == expectationValidationResultSchema.load(
                    test["output"]
                ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
        else:
            # Convert result to json since our tests are reading from json so cannot easily contain richer types (e.g. NaN)
            # NOTE - 20191031 - JPC - we may eventually want to change these tests as we update our view on how
            # representations, serializations, and objects should interact and how much of that is shown to the user.
            result = result.to_json_dict()
            for key, value in test["output"].items():
                # Apply our great expectations-specific test logic

                if key == "success":
                    if isinstance(value, (np.floating, float)):
                        try:
                            assert np.allclose(
                                result["success"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['success']} not np.allclose to {value}"
                        except TypeError:
                            assert (
                                result["success"] == value
                            ), f"{result['success']} != {value}"
                    else:
                        assert result["success"] == value, f"{result['success']} != {value}"

                elif key == "observed_value":
                    if "tolerance" in test:
                        if isinstance(value, dict):
                            assert set(result["result"]["observed_value"].keys()) == set(
                                value.keys()
                            ), f"{set(result['result']['observed_value'].keys())} != {set(value.keys())}"
                            for k, v in value.items():
                                assert np.allclose(
                                    result["result"]["observed_value"][k],
                                    v,
                                    rtol=test["tolerance"],
                                )
                        else:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=test["tolerance"],
                            )
                    else:
                        if isinstance(value, dict) and "values" in value:
                            try:
                                assert np.allclose(
                                    result["result"]["observed_value"]["values"],
                                    value["values"],
                                    rtol=RTOL,
                                    atol=ATOL,
                                ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']['values']} not np.allclose to {value['values']}"
                            except TypeError as e:
                                print(e)
                                assert (
                                    result["result"]["observed_value"] == value
                                ), f"{result['result']['observed_value']} != {value}"
                        elif try_allclose:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {value}"
                        else:
                            assert (
                                result["result"]["observed_value"] == value
                            ), f"{result['result']['observed_value']} != {value}"

                # NOTE: This is a key used ONLY for testing cases where an expectation is legitimately allowed to return
                # any of multiple possible observed_values. expect_column_values_to_be_of_type is one such expectation.
                elif key == "observed_value_list":
                    assert result["result"]["observed_value"] in value

                elif key == "unexpected_index_list":
                    if isinstance(data_asset, (SqlAlchemyDataset, SparkDFDataset)):
                        pass
                    elif isinstance(data_asset, (SqlAlchemyBatchData, SparkDFBatchData)):
                        pass
                    else:
>                       assert (
                            result["result"]["unexpected_index_list"] == value
                        ), f"{result['result']['unexpected_index_list']} != {value}"
E                       AssertionError: [{'no_null': 'a', 'pk_index': 0}, {'no_null': 'b', 'pk_index': 1}, {'no_null': 'c', 'pk_index': 2}, {'no_null': 'd', 'pk_index': 3}] != [0, 1, 2, 3]

great_expectations/self_check/util.py:3075: AssertionError
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/8 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/8 [00:00<?, ?it/s]Calculating Metrics:  25%|██▌       | 2/8 [00:00<00:00, 4114.08it/s]Calculating Metrics:  25%|██▌       | 2/8 [00:00<00:00, 621.29it/s]Calculating Metrics:  38%|███▊      | 3/8 [00:00<00:00, 859.61it/s]Calculating Metrics:  38%|███▊      | 3/8 [00:00<00:00, 494.96it/s]Calculating Metrics:  50%|█████     | 4/8 [00:00<00:00, 592.83it/s]Calculating Metrics:  50%|█████     | 4/8 [00:00<00:00, 447.48it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 704.39it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 652.20it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 648.08it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 643.14it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_null:positive_mostly] _

test_case = {'expectation_type': 'expect_column_values_to_be_null', 'pk_column': True, 'skip': False, 'test': {'exact_match_out': ...hree_null', 'mostly': 0.75}, 'out': {'success': True, 'unexpected_index_list': [3], 'unexpected_list': [5]}, ...}, ...}

    @pytest.mark.order(index=0)
    @pytest.mark.integration
    @pytest.mark.slow  # 12.68s
    def test_case_runner_v3_api(test_case):
        if test_case["skip"]:
            pytest.skip()

        # Note: this should never be done in practice, but we are wiping expectations to reuse batches during testing.
        # test_case["batch"]._initialize_expectations()
        if "parse_strings_as_datetimes" in test_case["test"]["in"]:
            with pytest.deprecated_call():
                evaluate_json_test_v3_api(
                    validator=test_case["validator_with_data"],
                    expectation_type=test_case["expectation_type"],
                    test=test_case["test"],
                    pk_column=test_case["pk_column"],
                )
        else:
>           evaluate_json_test_v3_api(
                validator=test_case["validator_with_data"],
                expectation_type=test_case["expectation_type"],
                test=test_case["test"],
                pk_column=test_case["pk_column"],
            )

tests/test_definitions/test_expectations_v3_api.py:418:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
great_expectations/self_check/util.py:2901: in evaluate_json_test_v3_api
    check_json_test_result(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

test = {'exact_match_out': False, 'in': {'column': 'three_null', 'mostly': 0.75}, 'input': {'column': 'three_null', 'mostly': 0.75}, 'out': {'success': True, 'unexpected_index_list': [3], 'unexpected_list': [5]}, ...}
result = {'exception_info': {'exception_message': None, 'exception_traceback': None, 'raised_exception': False}, 'expectation_c...}], 'partial_unexpected_index_list': [{'pk_index': 3, 'three_null': 5.0}], 'partial_unexpected_list': [5.0], ...}, ...}
data_asset = <great_expectations.execution_engine.pandas_batch_data.PandasBatchData object at 0x7f9502c5d430>
pk_column = True

    def check_json_test_result(  # noqa: C901 - 52
        test, result, data_asset=None, pk_column=False
    ) -> None:

        # check for id_pk results in cases where pk_column is true and unexpected_index_list already exists
        # this will work for testing since result_format is COMPLETE
        if pk_column:
            if not result["success"]:
                if "unexpected_index_list" in result["result"]:
                    assert "unexpected_index_query" in result["result"]

        if "unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )
            elif "unexpected_list" in test["output"]:
                (
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )

        if "partial_unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "partial_unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )
            elif "partial_unexpected_list" in test["output"]:
                (
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )

        # Determine if np.allclose(..) might be needed for float comparison
        try_allclose = False
        if "observed_value" in test["output"]:
            if RX_FLOAT.match(repr(test["output"]["observed_value"])):
                try_allclose = True

        # Check results
        if test["exact_match_out"] is True:
            if "result" in result and "observed_value" in result["result"]:
                if isinstance(result["result"]["observed_value"], (np.floating, float)):
                    assert np.allclose(
                        result["result"]["observed_value"],
                        expectationValidationResultSchema.load(test["output"])["result"][
                            "observed_value"
                        ],
                        rtol=RTOL,
                        atol=ATOL,
                    ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {expectationValidationResultSchema.load(test['output'])['result']['observed_value']}"
                else:
                    assert result == expectationValidationResultSchema.load(
                        test["output"]
                    ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
            else:
                assert result == expectationValidationResultSchema.load(
                    test["output"]
                ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
        else:
            # Convert result to json since our tests are reading from json so cannot easily contain richer types (e.g. NaN)
            # NOTE - 20191031 - JPC - we may eventually want to change these tests as we update our view on how
            # representations, serializations, and objects should interact and how much of that is shown to the user.
            result = result.to_json_dict()
            for key, value in test["output"].items():
                # Apply our great expectations-specific test logic

                if key == "success":
                    if isinstance(value, (np.floating, float)):
                        try:
                            assert np.allclose(
                                result["success"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['success']} not np.allclose to {value}"
                        except TypeError:
                            assert (
                                result["success"] == value
                            ), f"{result['success']} != {value}"
                    else:
                        assert result["success"] == value, f"{result['success']} != {value}"

                elif key == "observed_value":
                    if "tolerance" in test:
                        if isinstance(value, dict):
                            assert set(result["result"]["observed_value"].keys()) == set(
                                value.keys()
                            ), f"{set(result['result']['observed_value'].keys())} != {set(value.keys())}"
                            for k, v in value.items():
                                assert np.allclose(
                                    result["result"]["observed_value"][k],
                                    v,
                                    rtol=test["tolerance"],
                                )
                        else:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=test["tolerance"],
                            )
                    else:
                        if isinstance(value, dict) and "values" in value:
                            try:
                                assert np.allclose(
                                    result["result"]["observed_value"]["values"],
                                    value["values"],
                                    rtol=RTOL,
                                    atol=ATOL,
                                ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']['values']} not np.allclose to {value['values']}"
                            except TypeError as e:
                                print(e)
                                assert (
                                    result["result"]["observed_value"] == value
                                ), f"{result['result']['observed_value']} != {value}"
                        elif try_allclose:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {value}"
                        else:
                            assert (
                                result["result"]["observed_value"] == value
                            ), f"{result['result']['observed_value']} != {value}"

                # NOTE: This is a key used ONLY for testing cases where an expectation is legitimately allowed to return
                # any of multiple possible observed_values. expect_column_values_to_be_of_type is one such expectation.
                elif key == "observed_value_list":
                    assert result["result"]["observed_value"] in value

                elif key == "unexpected_index_list":
                    if isinstance(data_asset, (SqlAlchemyDataset, SparkDFDataset)):
                        pass
                    elif isinstance(data_asset, (SqlAlchemyBatchData, SparkDFBatchData)):
                        pass
                    else:
>                       assert (
                            result["result"]["unexpected_index_list"] == value
                        ), f"{result['result']['unexpected_index_list']} != {value}"
E                       AssertionError: [{'three_null': 5.0, 'pk_index': 3}] != [3]

great_expectations/self_check/util.py:3075: AssertionError
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/8 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/8 [00:00<?, ?it/s]Calculating Metrics:  25%|██▌       | 2/8 [00:00<00:00, 4236.67it/s]Calculating Metrics:  25%|██▌       | 2/8 [00:00<00:00, 630.72it/s]Calculating Metrics:  38%|███▊      | 3/8 [00:00<00:00, 877.16it/s]Calculating Metrics:  38%|███▊      | 3/8 [00:00<00:00, 501.17it/s]Calculating Metrics:  50%|█████     | 4/8 [00:00<00:00, 602.95it/s]Calculating Metrics:  50%|█████     | 4/8 [00:00<00:00, 453.46it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 715.31it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 661.04it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 656.77it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 651.63it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_null:negative_mostly] _

test_case = {'expectation_type': 'expect_column_values_to_be_null', 'pk_column': True, 'skip': False, 'test': {'exact_match_out': ...hree_null', 'mostly': 0.8}, 'out': {'success': False, 'unexpected_index_list': [3], 'unexpected_list': [5]}, ...}, ...}

    @pytest.mark.order(index=0)
    @pytest.mark.integration
    @pytest.mark.slow  # 12.68s
    def test_case_runner_v3_api(test_case):
        if test_case["skip"]:
            pytest.skip()

        # Note: this should never be done in practice, but we are wiping expectations to reuse batches during testing.
        # test_case["batch"]._initialize_expectations()
        if "parse_strings_as_datetimes" in test_case["test"]["in"]:
            with pytest.deprecated_call():
                evaluate_json_test_v3_api(
                    validator=test_case["validator_with_data"],
                    expectation_type=test_case["expectation_type"],
                    test=test_case["test"],
                    pk_column=test_case["pk_column"],
                )
        else:
>           evaluate_json_test_v3_api(
                validator=test_case["validator_with_data"],
                expectation_type=test_case["expectation_type"],
                test=test_case["test"],
                pk_column=test_case["pk_column"],
            )

tests/test_definitions/test_expectations_v3_api.py:418:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
great_expectations/self_check/util.py:2901: in evaluate_json_test_v3_api
    check_json_test_result(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

test = {'exact_match_out': False, 'in': {'column': 'three_null', 'mostly': 0.8}, 'input': {'column': 'three_null', 'mostly': 0.8}, 'out': {'success': False, 'unexpected_index_list': [3], 'unexpected_list': [5]}, ...}
result = {'exception_info': {'exception_message': None, 'exception_traceback': None, 'raised_exception': False}, 'expectation_c...}], 'partial_unexpected_index_list': [{'pk_index': 3, 'three_null': 5.0}], 'partial_unexpected_list': [5.0], ...}, ...}
data_asset = <great_expectations.execution_engine.pandas_batch_data.PandasBatchData object at 0x7f9502c5d430>
pk_column = True

    def check_json_test_result(  # noqa: C901 - 52
        test, result, data_asset=None, pk_column=False
    ) -> None:

        # check for id_pk results in cases where pk_column is true and unexpected_index_list already exists
        # this will work for testing since result_format is COMPLETE
        if pk_column:
            if not result["success"]:
                if "unexpected_index_list" in result["result"]:
                    assert "unexpected_index_query" in result["result"]

        if "unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )
            elif "unexpected_list" in test["output"]:
                (
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )

        if "partial_unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "partial_unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )
            elif "partial_unexpected_list" in test["output"]:
                (
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )

        # Determine if np.allclose(..) might be needed for float comparison
        try_allclose = False
        if "observed_value" in test["output"]:
            if RX_FLOAT.match(repr(test["output"]["observed_value"])):
                try_allclose = True

        # Check results
        if test["exact_match_out"] is True:
            if "result" in result and "observed_value" in result["result"]:
                if isinstance(result["result"]["observed_value"], (np.floating, float)):
                    assert np.allclose(
                        result["result"]["observed_value"],
                        expectationValidationResultSchema.load(test["output"])["result"][
                            "observed_value"
                        ],
                        rtol=RTOL,
                        atol=ATOL,
                    ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {expectationValidationResultSchema.load(test['output'])['result']['observed_value']}"
                else:
                    assert result == expectationValidationResultSchema.load(
                        test["output"]
                    ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
            else:
                assert result == expectationValidationResultSchema.load(
                    test["output"]
                ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
        else:
            # Convert result to json since our tests are reading from json so cannot easily contain richer types (e.g. NaN)
            # NOTE - 20191031 - JPC - we may eventually want to change these tests as we update our view on how
            # representations, serializations, and objects should interact and how much of that is shown to the user.
            result = result.to_json_dict()
            for key, value in test["output"].items():
                # Apply our great expectations-specific test logic

                if key == "success":
                    if isinstance(value, (np.floating, float)):
                        try:
                            assert np.allclose(
                                result["success"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['success']} not np.allclose to {value}"
                        except TypeError:
                            assert (
                                result["success"] == value
                            ), f"{result['success']} != {value}"
                    else:
                        assert result["success"] == value, f"{result['success']} != {value}"

                elif key == "observed_value":
                    if "tolerance" in test:
                        if isinstance(value, dict):
                            assert set(result["result"]["observed_value"].keys()) == set(
                                value.keys()
                            ), f"{set(result['result']['observed_value'].keys())} != {set(value.keys())}"
                            for k, v in value.items():
                                assert np.allclose(
                                    result["result"]["observed_value"][k],
                                    v,
                                    rtol=test["tolerance"],
                                )
                        else:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=test["tolerance"],
                            )
                    else:
                        if isinstance(value, dict) and "values" in value:
                            try:
                                assert np.allclose(
                                    result["result"]["observed_value"]["values"],
                                    value["values"],
                                    rtol=RTOL,
                                    atol=ATOL,
                                ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']['values']} not np.allclose to {value['values']}"
                            except TypeError as e:
                                print(e)
                                assert (
                                    result["result"]["observed_value"] == value
                                ), f"{result['result']['observed_value']} != {value}"
                        elif try_allclose:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {value}"
                        else:
                            assert (
                                result["result"]["observed_value"] == value
                            ), f"{result['result']['observed_value']} != {value}"

                # NOTE: This is a key used ONLY for testing cases where an expectation is legitimately allowed to return
                # any of multiple possible observed_values. expect_column_values_to_be_of_type is one such expectation.
                elif key == "observed_value_list":
                    assert result["result"]["observed_value"] in value

                elif key == "unexpected_index_list":
                    if isinstance(data_asset, (SqlAlchemyDataset, SparkDFDataset)):
                        pass
                    elif isinstance(data_asset, (SqlAlchemyBatchData, SparkDFBatchData)):
                        pass
                    else:
>                       assert (
                            result["result"]["unexpected_index_list"] == value
                        ), f"{result['result']['unexpected_index_list']} != {value}"
E                       AssertionError: [{'three_null': 5.0, 'pk_index': 3}] != [3]

great_expectations/self_check/util.py:3075: AssertionError
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/8 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/8 [00:00<?, ?it/s]Calculating Metrics:  25%|██▌       | 2/8 [00:00<00:00, 3344.74it/s]Calculating Metrics:  25%|██▌       | 2/8 [00:00<00:00, 503.28it/s]Calculating Metrics:  38%|███▊      | 3/8 [00:00<00:00, 700.14it/s]Calculating Metrics:  38%|███▊      | 3/8 [00:00<00:00, 400.92it/s]Calculating Metrics:  50%|█████     | 4/8 [00:00<00:00, 486.73it/s]Calculating Metrics:  50%|█████     | 4/8 [00:00<00:00, 365.27it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 579.75it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 531.03it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 526.70it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 522.40it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_null:negative_case_with_75percent_non_null_values_no_mostly] _

test_case = {'expectation_type': 'expect_column_values_to_be_null', 'pk_column': True, 'skip': False, 'test': {'exact_match_out': ...': 'one_null'}, 'out': {'success': False, 'unexpected_index_list': [0, 1, 3], 'unexpected_list': [4, 5, 7]}, ...}, ...}

    @pytest.mark.order(index=0)
    @pytest.mark.integration
    @pytest.mark.slow  # 12.68s
    def test_case_runner_v3_api(test_case):
        if test_case["skip"]:
            pytest.skip()

        # Note: this should never be done in practice, but we are wiping expectations to reuse batches during testing.
        # test_case["batch"]._initialize_expectations()
        if "parse_strings_as_datetimes" in test_case["test"]["in"]:
            with pytest.deprecated_call():
                evaluate_json_test_v3_api(
                    validator=test_case["validator_with_data"],
                    expectation_type=test_case["expectation_type"],
                    test=test_case["test"],
                    pk_column=test_case["pk_column"],
                )
        else:
>           evaluate_json_test_v3_api(
                validator=test_case["validator_with_data"],
                expectation_type=test_case["expectation_type"],
                test=test_case["test"],
                pk_column=test_case["pk_column"],
            )

tests/test_definitions/test_expectations_v3_api.py:418:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
great_expectations/self_check/util.py:2901: in evaluate_json_test_v3_api
    check_json_test_result(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

test = {'exact_match_out': False, 'in': {'column': 'one_null'}, 'input': {'column': 'one_null'}, 'out': {'success': False, 'unexpected_index_list': [0, 1, 3], 'unexpected_list': [4, 5, 7]}, ...}
result = {'exception_info': {'exception_message': None, 'exception_traceback': None, 'raised_exception': False}, 'expectation_c...e_null': 5.0, 'pk_index': 1}, {'one_null': 7.0, 'pk_index': 3}], 'partial_unexpected_list': [4.0, 5.0, 7.0], ...}, ...}
data_asset = <great_expectations.execution_engine.pandas_batch_data.PandasBatchData object at 0x7f9502c5d430>
pk_column = True

    def check_json_test_result(  # noqa: C901 - 52
        test, result, data_asset=None, pk_column=False
    ) -> None:

        # check for id_pk results in cases where pk_column is true and unexpected_index_list already exists
        # this will work for testing since result_format is COMPLETE
        if pk_column:
            if not result["success"]:
                if "unexpected_index_list" in result["result"]:
                    assert "unexpected_index_query" in result["result"]

        if "unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )
            elif "unexpected_list" in test["output"]:
                (
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )

        if "partial_unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "partial_unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )
            elif "partial_unexpected_list" in test["output"]:
                (
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )

        # Determine if np.allclose(..) might be needed for float comparison
        try_allclose = False
        if "observed_value" in test["output"]:
            if RX_FLOAT.match(repr(test["output"]["observed_value"])):
                try_allclose = True

        # Check results
        if test["exact_match_out"] is True:
            if "result" in result and "observed_value" in result["result"]:
                if isinstance(result["result"]["observed_value"], (np.floating, float)):
                    assert np.allclose(
                        result["result"]["observed_value"],
                        expectationValidationResultSchema.load(test["output"])["result"][
                            "observed_value"
                        ],
                        rtol=RTOL,
                        atol=ATOL,
                    ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {expectationValidationResultSchema.load(test['output'])['result']['observed_value']}"
                else:
                    assert result == expectationValidationResultSchema.load(
                        test["output"]
                    ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
            else:
                assert result == expectationValidationResultSchema.load(
                    test["output"]
                ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
        else:
            # Convert result to json since our tests are reading from json so cannot easily contain richer types (e.g. NaN)
            # NOTE - 20191031 - JPC - we may eventually want to change these tests as we update our view on how
            # representations, serializations, and objects should interact and how much of that is shown to the user.
            result = result.to_json_dict()
            for key, value in test["output"].items():
                # Apply our great expectations-specific test logic

                if key == "success":
                    if isinstance(value, (np.floating, float)):
                        try:
                            assert np.allclose(
                                result["success"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['success']} not np.allclose to {value}"
                        except TypeError:
                            assert (
                                result["success"] == value
                            ), f"{result['success']} != {value}"
                    else:
                        assert result["success"] == value, f"{result['success']} != {value}"

                elif key == "observed_value":
                    if "tolerance" in test:
                        if isinstance(value, dict):
                            assert set(result["result"]["observed_value"].keys()) == set(
                                value.keys()
                            ), f"{set(result['result']['observed_value'].keys())} != {set(value.keys())}"
                            for k, v in value.items():
                                assert np.allclose(
                                    result["result"]["observed_value"][k],
                                    v,
                                    rtol=test["tolerance"],
                                )
                        else:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=test["tolerance"],
                            )
                    else:
                        if isinstance(value, dict) and "values" in value:
                            try:
                                assert np.allclose(
                                    result["result"]["observed_value"]["values"],
                                    value["values"],
                                    rtol=RTOL,
                                    atol=ATOL,
                                ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']['values']} not np.allclose to {value['values']}"
                            except TypeError as e:
                                print(e)
                                assert (
                                    result["result"]["observed_value"] == value
                                ), f"{result['result']['observed_value']} != {value}"
                        elif try_allclose:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {value}"
                        else:
                            assert (
                                result["result"]["observed_value"] == value
                            ), f"{result['result']['observed_value']} != {value}"

                # NOTE: This is a key used ONLY for testing cases where an expectation is legitimately allowed to return
                # any of multiple possible observed_values. expect_column_values_to_be_of_type is one such expectation.
                elif key == "observed_value_list":
                    assert result["result"]["observed_value"] in value

                elif key == "unexpected_index_list":
                    if isinstance(data_asset, (SqlAlchemyDataset, SparkDFDataset)):
                        pass
                    elif isinstance(data_asset, (SqlAlchemyBatchData, SparkDFBatchData)):
                        pass
                    else:
>                       assert (
                            result["result"]["unexpected_index_list"] == value
                        ), f"{result['result']['unexpected_index_list']} != {value}"
E                       AssertionError: [{'one_null': 4.0, 'pk_index': 0}, {'one_null': 5.0, 'pk_index': 1}, {'one_null': 7.0, 'pk_index': 3}] != [0, 1, 3]

great_expectations/self_check/util.py:3075: AssertionError
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/8 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/8 [00:00<?, ?it/s]Calculating Metrics:  25%|██▌       | 2/8 [00:00<00:00, 4238.81it/s]Calculating Metrics:  25%|██▌       | 2/8 [00:00<00:00, 628.60it/s]Calculating Metrics:  38%|███▊      | 3/8 [00:00<00:00, 874.97it/s]Calculating Metrics:  38%|███▊      | 3/8 [00:00<00:00, 504.30it/s]Calculating Metrics:  50%|█████     | 4/8 [00:00<00:00, 610.97it/s]Calculating Metrics:  50%|█████     | 4/8 [00:00<00:00, 458.51it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 725.71it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 669.68it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 665.39it/s]Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 660.23it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_in_type_list:negative_test_string_and_int_values_summary_output] _

test_case = {'expectation_type': 'expect_column_values_to_be_in_type_list', 'pk_column': True, 'skip': False, 'test': {'exact_matc...lumn': 's', 'type_list': ['bool']}, 'input': {'column': 's', 'type_list': ['bool']}, 'only_for': ['pandas'], ...}, ...}

    @pytest.mark.order(index=0)
    @pytest.mark.integration
    @pytest.mark.slow  # 12.68s
    def test_case_runner_v3_api(test_case):
        if test_case["skip"]:
            pytest.skip()

        # Note: this should never be done in practice, but we are wiping expectations to reuse batches during testing.
        # test_case["batch"]._initialize_expectations()
        if "parse_strings_as_datetimes" in test_case["test"]["in"]:
            with pytest.deprecated_call():
                evaluate_json_test_v3_api(
                    validator=test_case["validator_with_data"],
                    expectation_type=test_case["expectation_type"],
                    test=test_case["test"],
                    pk_column=test_case["pk_column"],
                )
        else:
>           evaluate_json_test_v3_api(
                validator=test_case["validator_with_data"],
                expectation_type=test_case["expectation_type"],
                test=test_case["test"],
                pk_column=test_case["pk_column"],
            )

tests/test_definitions/test_expectations_v3_api.py:418:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
great_expectations/self_check/util.py:2901: in evaluate_json_test_v3_api
    check_json_test_result(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

test = {'exact_match_out': False, 'in': {'column': 's', 'type_list': ['bool']}, 'input': {'column': 's', 'type_list': ['bool']}, 'only_for': ['pandas'], ...}
result = {'exception_info': {'exception_message': None, 'exception_traceback': None, 'raised_exception': False}, 'expectation_c...ected_counts': [{'count': 1, 'value': '1'}, {'count': 1, 'value': 'hello'}, {'count': 1, 'value': 'jello'}], ...}, ...}
data_asset = <great_expectations.execution_engine.pandas_batch_data.PandasBatchData object at 0x7f9502bd12e0>
pk_column = True

    def check_json_test_result(  # noqa: C901 - 52
        test, result, data_asset=None, pk_column=False
    ) -> None:

        # check for id_pk results in cases where pk_column is true and unexpected_index_list already exists
        # this will work for testing since result_format is COMPLETE
        if pk_column:
            if not result["success"]:
                if "unexpected_index_list" in result["result"]:
                    assert "unexpected_index_query" in result["result"]

        if "unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )
            elif "unexpected_list" in test["output"]:
                (
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )

        if "partial_unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "partial_unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )
            elif "partial_unexpected_list" in test["output"]:
                (
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )

        # Determine if np.allclose(..) might be needed for float comparison
        try_allclose = False
        if "observed_value" in test["output"]:
            if RX_FLOAT.match(repr(test["output"]["observed_value"])):
                try_allclose = True

        # Check results
        if test["exact_match_out"] is True:
            if "result" in result and "observed_value" in result["result"]:
                if isinstance(result["result"]["observed_value"], (np.floating, float)):
                    assert np.allclose(
                        result["result"]["observed_value"],
                        expectationValidationResultSchema.load(test["output"])["result"][
                            "observed_value"
                        ],
                        rtol=RTOL,
                        atol=ATOL,
                    ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {expectationValidationResultSchema.load(test['output'])['result']['observed_value']}"
                else:
                    assert result == expectationValidationResultSchema.load(
                        test["output"]
                    ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
            else:
                assert result == expectationValidationResultSchema.load(
                    test["output"]
                ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
        else:
            # Convert result to json since our tests are reading from json so cannot easily contain richer types (e.g. NaN)
            # NOTE - 20191031 - JPC - we may eventually want to change these tests as we update our view on how
            # representations, serializations, and objects should interact and how much of that is shown to the user.
            result = result.to_json_dict()
            for key, value in test["output"].items():
                # Apply our great expectations-specific test logic

                if key == "success":
                    if isinstance(value, (np.floating, float)):
                        try:
                            assert np.allclose(
                                result["success"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['success']} not np.allclose to {value}"
                        except TypeError:
                            assert (
                                result["success"] == value
                            ), f"{result['success']} != {value}"
                    else:
                        assert result["success"] == value, f"{result['success']} != {value}"

                elif key == "observed_value":
                    if "tolerance" in test:
                        if isinstance(value, dict):
                            assert set(result["result"]["observed_value"].keys()) == set(
                                value.keys()
                            ), f"{set(result['result']['observed_value'].keys())} != {set(value.keys())}"
                            for k, v in value.items():
                                assert np.allclose(
                                    result["result"]["observed_value"][k],
                                    v,
                                    rtol=test["tolerance"],
                                )
                        else:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=test["tolerance"],
                            )
                    else:
                        if isinstance(value, dict) and "values" in value:
                            try:
                                assert np.allclose(
                                    result["result"]["observed_value"]["values"],
                                    value["values"],
                                    rtol=RTOL,
                                    atol=ATOL,
                                ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']['values']} not np.allclose to {value['values']}"
                            except TypeError as e:
                                print(e)
                                assert (
                                    result["result"]["observed_value"] == value
                                ), f"{result['result']['observed_value']} != {value}"
                        elif try_allclose:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {value}"
                        else:
                            assert (
                                result["result"]["observed_value"] == value
                            ), f"{result['result']['observed_value']} != {value}"

                # NOTE: This is a key used ONLY for testing cases where an expectation is legitimately allowed to return
                # any of multiple possible observed_values. expect_column_values_to_be_of_type is one such expectation.
                elif key == "observed_value_list":
                    assert result["result"]["observed_value"] in value

                elif key == "unexpected_index_list":
                    if isinstance(data_asset, (SqlAlchemyDataset, SparkDFDataset)):
                        pass
                    elif isinstance(data_asset, (SqlAlchemyBatchData, SparkDFBatchData)):
                        pass
                    else:
>                       assert (
                            result["result"]["unexpected_index_list"] == value
                        ), f"{result['result']['unexpected_index_list']} != {value}"
E                       AssertionError: [{'s': 'hello', 'pk_index': 0}, {'s': 'jello', 'pk_index': 1}, {'s': '1', 'pk_index': 2}] != [0, 1, 2]

great_expectations/self_check/util.py:3075: AssertionError
----------------------------- Captured stderr call -----------------------------
Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 4230.26it/s]Calculating Metrics:  20%|██        | 2/10 [00:00<00:00, 448.73it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 635.18it/s]Calculating Metrics:  30%|███       | 3/10 [00:00<00:00, 352.82it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 501.96it/s]Calculating Metrics:  50%|█████     | 5/10 [00:00<00:00, 383.44it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 591.34it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 548.49it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 546.09it/s]Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 543.28it/s]
_ test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_in_type_list:negative_test_string_and_int_values_complete_output] _

test_case = {'expectation_type': 'expect_column_values_to_be_in_type_list', 'pk_column': True, 'skip': False, 'test': {'exact_matc...lumn': 's', 'type_list': ['bool']}, 'input': {'column': 's', 'type_list': ['bool']}, 'only_for': ['pandas'], ...}, ...}

    @pytest.mark.order(index=0)
    @pytest.mark.integration
    @pytest.mark.slow  # 12.68s
    def test_case_runner_v3_api(test_case):
        if test_case["skip"]:
            pytest.skip()

        # Note: this should never be done in practice, but we are wiping expectations to reuse batches during testing.
        # test_case["batch"]._initialize_expectations()
        if "parse_strings_as_datetimes" in test_case["test"]["in"]:
            with pytest.deprecated_call():
                evaluate_json_test_v3_api(
                    validator=test_case["validator_with_data"],
                    expectation_type=test_case["expectation_type"],
                    test=test_case["test"],
                    pk_column=test_case["pk_column"],
                )
        else:
>           evaluate_json_test_v3_api(
                validator=test_case["validator_with_data"],
                expectation_type=test_case["expectation_type"],
                test=test_case["test"],
                pk_column=test_case["pk_column"],
            )

tests/test_definitions/test_expectations_v3_api.py:418:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
great_expectations/self_check/util.py:2901: in evaluate_json_test_v3_api
    check_json_test_result(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

test = {'exact_match_out': False, 'in': {'column': 's', 'type_list': ['bool']}, 'input': {'column': 's', 'type_list': ['bool']}, 'only_for': ['pandas'], ...}
result = {'exception_info': {'exception_message': None, 'exception_traceback': None, 'raised_exception': False}, 'expectation_c...ected_counts': [{'count': 1, 'value': '1'}, {'count': 1, 'value': 'hello'}, {'count': 1, 'value': 'jello'}], ...}, ...}
data_asset = <great_expectations.execution_engine.pandas_batch_data.PandasBatchData object at 0x7f9502bd12e0>
pk_column = True

    def check_json_test_result(  # noqa: C901 - 52
        test, result, data_asset=None, pk_column=False
    ) -> None:

        # check for id_pk results in cases where pk_column is true and unexpected_index_list already exists
        # this will work for testing since result_format is COMPLETE
        if pk_column:
            if not result["success"]:
                if "unexpected_index_list" in result["result"]:
                    assert "unexpected_index_query" in result["result"]

        if "unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )
            elif "unexpected_list" in test["output"]:
                (
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["unexpected_list"],
                    result["result"]["unexpected_list"],
                )

        if "partial_unexpected_list" in result["result"]:
            if ("result" in test["output"]) and (
                "partial_unexpected_list" in test["output"]["result"]
            ):
                (
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["result"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )
            elif "partial_unexpected_list" in test["output"]:
                (
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                ) = sort_unexpected_values(
                    test["output"]["partial_unexpected_list"],
                    result["result"]["partial_unexpected_list"],
                )

        # Determine if np.allclose(..) might be needed for float comparison
        try_allclose = False
        if "observed_value" in test["output"]:
            if RX_FLOAT.match(repr(test["output"]["observed_value"])):
                try_allclose = True

        # Check results
        if test["exact_match_out"] is True:
            if "result" in result and "observed_value" in result["result"]:
                if isinstance(result["result"]["observed_value"], (np.floating, float)):
                    assert np.allclose(
                        result["result"]["observed_value"],
                        expectationValidationResultSchema.load(test["output"])["result"][
                            "observed_value"
                        ],
                        rtol=RTOL,
                        atol=ATOL,
                    ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {expectationValidationResultSchema.load(test['output'])['result']['observed_value']}"
                else:
                    assert result == expectationValidationResultSchema.load(
                        test["output"]
                    ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
            else:
                assert result == expectationValidationResultSchema.load(
                    test["output"]
                ), f"{result} != {expectationValidationResultSchema.load(test['output'])}"
        else:
            # Convert result to json since our tests are reading from json so cannot easily contain richer types (e.g. NaN)
            # NOTE - 20191031 - JPC - we may eventually want to change these tests as we update our view on how
            # representations, serializations, and objects should interact and how much of that is shown to the user.
            result = result.to_json_dict()
            for key, value in test["output"].items():
                # Apply our great expectations-specific test logic

                if key == "success":
                    if isinstance(value, (np.floating, float)):
                        try:
                            assert np.allclose(
                                result["success"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['success']} not np.allclose to {value}"
                        except TypeError:
                            assert (
                                result["success"] == value
                            ), f"{result['success']} != {value}"
                    else:
                        assert result["success"] == value, f"{result['success']} != {value}"

                elif key == "observed_value":
                    if "tolerance" in test:
                        if isinstance(value, dict):
                            assert set(result["result"]["observed_value"].keys()) == set(
                                value.keys()
                            ), f"{set(result['result']['observed_value'].keys())} != {set(value.keys())}"
                            for k, v in value.items():
                                assert np.allclose(
                                    result["result"]["observed_value"][k],
                                    v,
                                    rtol=test["tolerance"],
                                )
                        else:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=test["tolerance"],
                            )
                    else:
                        if isinstance(value, dict) and "values" in value:
                            try:
                                assert np.allclose(
                                    result["result"]["observed_value"]["values"],
                                    value["values"],
                                    rtol=RTOL,
                                    atol=ATOL,
                                ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']['values']} not np.allclose to {value['values']}"
                            except TypeError as e:
                                print(e)
                                assert (
                                    result["result"]["observed_value"] == value
                                ), f"{result['result']['observed_value']} != {value}"
                        elif try_allclose:
                            assert np.allclose(
                                result["result"]["observed_value"],
                                value,
                                rtol=RTOL,
                                atol=ATOL,
                            ), f"(RTOL={RTOL}, ATOL={ATOL}) {result['result']['observed_value']} not np.allclose to {value}"
                        else:
                            assert (
                                result["result"]["observed_value"] == value
                            ), f"{result['result']['observed_value']} != {value}"

                # NOTE: This is a key used ONLY for testing cases where an expectation is legitimately allowed to return
                # any of multiple possible observed_values. expect_column_values_to_be_of_type is one such expectation.
                elif key == "observed_value_list":
                    assert result["result"]["observed_value"] in value

                elif key == "unexpected_index_list":
                    if isinstance(data_asset, (SqlAlchemyDataset, SparkDFDataset)):
                        pass
                    elif isinstance(data_asset, (SqlAlchemyBatchData, SparkDFBatchData)):
                        pass
                    else:
>                       assert (
                            result["result"]["unexpected_index_list"] == value
                        ), f"{result['result']['unexpected_index_list']} != {value}"
E                       AssertionError: [{'s': 'hello', 'pk_index': 0}, {'s': 'jello', 'pk_index': 1}, {'s': '1', 'pk_index': 2}] != [0, 1, 2]
FAILED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_between:negative_test_to_verify_that_the_denominator_for_mostly_works_with_missing_values0]
FAILED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_between:positive_test_to_verify_that_the_denominator_for_mostly_works_with_missing_values0]
FAILED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_between:test_allow_cross_type_comparisons0]
FAILED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_between:test_allow_cross_type_comparisons_again0]
FAILED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_between:test_strict_min_failure0]
FAILED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_between:test_conditional_expectation_fails0]
FAILED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_match_regex:negative_test_insufficient_mostly_and_one_non_matching_value]
FAILED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_match_regex:positive_test_column_name_has_space]
FAILED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_between:basic_negative_test1]
FAILED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_between:another_negative_test1]
FAILED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_between:basic_negative_test_with_timestamps1]
FAILED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_not_be_in_set:negative_test_case_include_one_existing_column_value]
FAILED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_not_be_in_set:basic_negative_strings_set_all_character_values]
FAILED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_not_be_in_set:negative_test_float_set_two_out_of_three_column_values_included_mostly]
FAILED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_not_be_in_set:negative_test_float_set_two_out_of_three_column_values_included_mostly_summary_output]
FAILED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_not_be_in_set:positive_test_float_set_two_out_of_three_column_values_included_mostly]
FAILED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_in_set:negative_test_case_exclude_existing_column_value]
FAILED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_in_set:negative_empty_values_set]
FAILED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_in_set:negative_test_strings_set_extra_value_in_column]
FAILED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_in_set:negative_test_numbers_set_no_matching_values]
FAILED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_in_set:negative_test_float_set]
FAILED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_in_set:test_conditional_expectation_fails]
FAILED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_in_set:basic_negative_test_case_datetime_set]
FAILED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_decreasing:positive_test_with_mostly]
FAILED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_decreasing:basic_negative_test]
FAILED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_decreasing:basic_negative_test_with_strictly]
FAILED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_not_match_regex:negative_test_insufficient_mostly_and_one_non_matching_value]
FAILED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_not_match_regex:positive_test_exact_mostly_w_one_non_matching_value]
FAILED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_not_match_regex:positive_test_sufficient_mostly_w_one_non_matching_value]
FAILED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_not_match_regex:negative_test_one_missing_value_and_insufficent_mostly]
FAILED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_not_match_regex:negative_test_empty_regex]
FAILED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_not_match_regex:negative_test_match_characters_not_at_the_beginning_of_string_exact_mostly]
FAILED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_not_match_regex_list:basic_negative_test]
FAILED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_not_match_regex_list:negative_test_with_more_string-ish_strings]
FAILED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_increasing:positive_test_with_strictly]
FAILED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_increasing:basic_negative_test]
FAILED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_increasing:negative_test_with_interspersed_nulls]
FAILED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_value_lengths_to_be_between:negative_test_matching_min_max_too_small]
FAILED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_value_lengths_to_be_between:negative_test_matching_max_min_too_large]
FAILED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_value_lengths_to_be_between:negative_test_with_max_lt_min]
FAILED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_value_lengths_to_be_between:test_conditional_expectation_fails]
FAILED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_unique:basic_negative_case_all_non_unique_character_values]
FAILED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_unique:multi_type_column_contains_2_and_quoted_2_suppressed_for_sqalchemy]
FAILED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_unique:positive_case_using_mostly]
FAILED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_unique:negative_case_using_mostly]
FAILED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_unique:negative_case_non_unique_numeric_values]
FAILED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_unique:negative_multiple_duplicate_values]
FAILED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_match_json_schema:basic_negative_test]
FAILED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_null:negative_no_missing_values]
FAILED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_null:positive_mostly]
FAILED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_null:negative_mostly]
FAILED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_null:negative_case_with_75percent_non_null_values_no_mostly]
FAILED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_in_type_list:negative_test_string_and_int_values_summary_output]
FAILED tests/test_definitions/test_expectations_v3_api.py::test_case_runner_v3_api[pandas/column_map_expectations/expect_column_values_to_be_in_type_list:negative_test_string_and_int_values_complete_output]
========== 54 failed, 865 passed, 190 skipped, 45 warnings in 50.53s ===========
